[2018-12-22 10:05:42.831839 UTC] Starting env pool
[2018-12-22 10:05:42.871369 UTC] Starting iteration 0
[2018-12-22 10:05:42.871792 UTC] Start collecting samples
[2018-12-22 10:05:46.441977 UTC] Computing input variables for policy optimization
[2018-12-22 10:05:46.657391 UTC] Performing policy update
[2018-12-22 10:05:46.658095 UTC] Computing gradient in Euclidean space
[2018-12-22 10:05:46.768194 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:05:47.966160 UTC] Performing line search
[2018-12-22 10:05:48.094223 UTC] Updating baseline
[2018-12-22 10:05:49.256443 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| ExpectedImprovement  | 0.03549    |
| ActualImprovement    | 0.034164   |
| ImprovementRatio     | 0.96262    |
| MeanKL               | 0.0065574  |
| Entropy              | 8.5136     |
| Perplexity           | 4982.2     |
| AveragePolicyStd     | 1          |
| AveragePolicyStd[0]  | 1          |
| AveragePolicyStd[1]  | 1          |
| AveragePolicyStd[2]  | 1          |
| AveragePolicyStd[3]  | 1          |
| AveragePolicyStd[4]  | 1          |
| AveragePolicyStd[5]  | 1          |
| AverageReturn        | -11.455    |
| MinReturn            | -56.6      |
| MaxReturn            | -0.38713   |
| StdReturn            | 6.565      |
| AverageEpisodeLength | 17.86      |
| MinEpisodeLength     | 13         |
| MaxEpisodeLength     | 44         |
| StdEpisodeLength     | 5.1711     |
| TotalNEpisodes       | 267        |
| TotalNSamples        | 4873       |
| ExplainedVariance    | -0.0082386 |
-------------------------------------
[2018-12-22 10:05:49.364053 UTC] Saving snapshot
[2018-12-22 10:05:49.372197 UTC] Starting iteration 1
[2018-12-22 10:05:49.372387 UTC] Start collecting samples
[2018-12-22 10:05:52.916685 UTC] Computing input variables for policy optimization
[2018-12-22 10:05:53.149968 UTC] Performing policy update
[2018-12-22 10:05:53.151807 UTC] Computing gradient in Euclidean space
[2018-12-22 10:05:53.243338 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:05:54.324744 UTC] Performing line search
[2018-12-22 10:05:54.453309 UTC] Updating baseline
[2018-12-22 10:05:55.709996 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| ExpectedImprovement  | 0.038277  |
| ActualImprovement    | 0.036946  |
| ImprovementRatio     | 0.96521   |
| MeanKL               | 0.0065442 |
| Entropy              | 8.4783    |
| Perplexity           | 4809.4    |
| AveragePolicyStd     | 0.99414   |
| AveragePolicyStd[0]  | 0.9998    |
| AveragePolicyStd[1]  | 0.99418   |
| AveragePolicyStd[2]  | 0.99111   |
| AveragePolicyStd[3]  | 0.99427   |
| AveragePolicyStd[4]  | 0.9953    |
| AveragePolicyStd[5]  | 0.99017   |
| AverageReturn        | -11.141   |
| MinReturn            | -30.299   |
| MaxReturn            | -0.3015   |
| StdReturn            | 5.4087    |
| AverageEpisodeLength | 17.6      |
| MinEpisodeLength     | 12        |
| MaxEpisodeLength     | 42        |
| StdEpisodeLength     | 4.4317    |
| TotalNEpisodes       | 560       |
| TotalNSamples        | 9915      |
| ExplainedVariance    | 0.22146   |
------------------------------------
[2018-12-22 10:05:55.805246 UTC] Saving snapshot
[2018-12-22 10:05:55.805484 UTC] Starting iteration 2
[2018-12-22 10:05:55.805616 UTC] Start collecting samples
[2018-12-22 10:05:59.392689 UTC] Computing input variables for policy optimization
[2018-12-22 10:05:59.623580 UTC] Performing policy update
[2018-12-22 10:05:59.624307 UTC] Computing gradient in Euclidean space
[2018-12-22 10:05:59.716049 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:00.789128 UTC] Performing line search
[2018-12-22 10:06:00.918467 UTC] Updating baseline
[2018-12-22 10:06:02.307720 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| ExpectedImprovement  | 0.036913  |
| ActualImprovement    | 0.035645  |
| ImprovementRatio     | 0.96565   |
| MeanKL               | 0.0066065 |
| Entropy              | 8.4176    |
| Perplexity           | 4526.1    |
| AveragePolicyStd     | 0.98414   |
| AveragePolicyStd[0]  | 0.99185   |
| AveragePolicyStd[1]  | 0.98549   |
| AveragePolicyStd[2]  | 0.98242   |
| AveragePolicyStd[3]  | 0.98348   |
| AveragePolicyStd[4]  | 0.98413   |
| AveragePolicyStd[5]  | 0.97744   |
| AverageReturn        | -10.144   |
| MinReturn            | -32.466   |
| MaxReturn            | -1.0314   |
| StdReturn            | 5.3426    |
| AverageEpisodeLength | 16.77     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 28        |
| StdEpisodeLength     | 2.5371    |
| TotalNEpisodes       | 848       |
| TotalNSamples        | 14882     |
| ExplainedVariance    | 0.31061   |
------------------------------------
[2018-12-22 10:06:02.403586 UTC] Saving snapshot
[2018-12-22 10:06:02.403834 UTC] Starting iteration 3
[2018-12-22 10:06:02.403969 UTC] Start collecting samples
[2018-12-22 10:06:06.046883 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:06.279804 UTC] Performing policy update
[2018-12-22 10:06:06.280647 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:06.372007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:07.424274 UTC] Performing line search
[2018-12-22 10:06:07.552746 UTC] Updating baseline
[2018-12-22 10:06:08.830730 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| ExpectedImprovement  | 0.038234  |
| ActualImprovement    | 0.036804  |
| ImprovementRatio     | 0.96261   |
| MeanKL               | 0.0065799 |
| Entropy              | 8.3429    |
| Perplexity           | 4200.1    |
| AveragePolicyStd     | 0.97198   |
| AveragePolicyStd[0]  | 0.98898   |
| AveragePolicyStd[1]  | 0.97085   |
| AveragePolicyStd[2]  | 0.96895   |
| AveragePolicyStd[3]  | 0.97242   |
| AveragePolicyStd[4]  | 0.97085   |
| AveragePolicyStd[5]  | 0.95981   |
| AverageReturn        | -8.9107   |
| MinReturn            | -21.786   |
| MaxReturn            | -0.054931 |
| StdReturn            | 4.1534    |
| AverageEpisodeLength | 17.77     |
| MinEpisodeLength     | 12        |
| MaxEpisodeLength     | 45        |
| StdEpisodeLength     | 4.3586    |
| TotalNEpisodes       | 1135      |
| TotalNSamples        | 19895     |
| ExplainedVariance    | 0.33069   |
------------------------------------
[2018-12-22 10:06:08.932730 UTC] Saving snapshot
[2018-12-22 10:06:08.932977 UTC] Starting iteration 4
[2018-12-22 10:06:08.933097 UTC] Start collecting samples
[2018-12-22 10:06:12.636514 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:12.862802 UTC] Performing policy update
[2018-12-22 10:06:12.863421 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:12.954023 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:14.023547 UTC] Performing line search
[2018-12-22 10:06:14.150903 UTC] Updating baseline
[2018-12-22 10:06:15.403930 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| ExpectedImprovement  | 0.034451  |
| ActualImprovement    | 0.034219  |
| ImprovementRatio     | 0.99325   |
| MeanKL               | 0.0065915 |
| Entropy              | 8.2542    |
| Perplexity           | 3843.8    |
| AveragePolicyStd     | 0.95776   |
| AveragePolicyStd[0]  | 0.9815    |
| AveragePolicyStd[1]  | 0.95753   |
| AveragePolicyStd[2]  | 0.95188   |
| AveragePolicyStd[3]  | 0.95823   |
| AveragePolicyStd[4]  | 0.95443   |
| AveragePolicyStd[5]  | 0.94299   |
| AverageReturn        | -8.0181   |
| MinReturn            | -25.67    |
| MaxReturn            | 0.70961   |
| StdReturn            | 4.3798    |
| AverageEpisodeLength | 17.44     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 27        |
| StdEpisodeLength     | 2.273     |
| TotalNEpisodes       | 1420      |
| TotalNSamples        | 24917     |
| ExplainedVariance    | 0.25574   |
------------------------------------
[2018-12-22 10:06:15.510180 UTC] Saving snapshot
[2018-12-22 10:06:15.510419 UTC] Starting iteration 5
[2018-12-22 10:06:15.510537 UTC] Start collecting samples
[2018-12-22 10:06:19.205099 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:19.434297 UTC] Performing policy update
[2018-12-22 10:06:19.434912 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:19.526881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:20.604459 UTC] Performing line search
[2018-12-22 10:06:20.732937 UTC] Updating baseline
[2018-12-22 10:06:21.971210 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| ExpectedImprovement  | 0.034655  |
| ActualImprovement    | 0.032257  |
| ImprovementRatio     | 0.93078   |
| MeanKL               | 0.0065537 |
| Entropy              | 8.1674    |
| Perplexity           | 3524.1    |
| AveragePolicyStd     | 0.94405   |
| AveragePolicyStd[0]  | 0.97342   |
| AveragePolicyStd[1]  | 0.94771   |
| AveragePolicyStd[2]  | 0.93929   |
| AveragePolicyStd[3]  | 0.9423    |
| AveragePolicyStd[4]  | 0.94043   |
| AveragePolicyStd[5]  | 0.92116   |
| AverageReturn        | -7.4078   |
| MinReturn            | -51.293   |
| MaxReturn            | 1.567     |
| StdReturn            | 5.9177    |
| AverageEpisodeLength | 17.9      |
| MinEpisodeLength     | 12        |
| MaxEpisodeLength     | 61        |
| StdEpisodeLength     | 4.8528    |
| TotalNEpisodes       | 1700      |
| TotalNSamples        | 29907     |
| ExplainedVariance    | 0.15281   |
------------------------------------
[2018-12-22 10:06:22.080644 UTC] Saving snapshot
[2018-12-22 10:06:22.080887 UTC] Starting iteration 6
[2018-12-22 10:06:22.081012 UTC] Start collecting samples
[2018-12-22 10:06:25.836648 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:26.067930 UTC] Performing policy update
[2018-12-22 10:06:26.068642 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:26.159276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:27.270659 UTC] Performing line search
[2018-12-22 10:06:27.403499 UTC] Updating baseline
[2018-12-22 10:06:28.829931 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| ExpectedImprovement  | 0.032423  |
| ActualImprovement    | 0.03197   |
| ImprovementRatio     | 0.98602   |
| MeanKL               | 0.0065416 |
| Entropy              | 8.0775    |
| Perplexity           | 3221.1    |
| AveragePolicyStd     | 0.93002   |
| AveragePolicyStd[0]  | 0.95563   |
| AveragePolicyStd[1]  | 0.93772   |
| AveragePolicyStd[2]  | 0.92996   |
| AveragePolicyStd[3]  | 0.92474   |
| AveragePolicyStd[4]  | 0.92883   |
| AveragePolicyStd[5]  | 0.90321   |
| AverageReturn        | -6.2052   |
| MinReturn            | -23.587   |
| MaxReturn            | 4.3947    |
| StdReturn            | 4.3522    |
| AverageEpisodeLength | 17.84     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 35        |
| StdEpisodeLength     | 3.0618    |
| TotalNEpisodes       | 1985      |
| TotalNSamples        | 34909     |
| ExplainedVariance    | 0.16133   |
------------------------------------
[2018-12-22 10:06:28.951383 UTC] Saving snapshot
[2018-12-22 10:06:28.951687 UTC] Starting iteration 7
[2018-12-22 10:06:28.951836 UTC] Start collecting samples
[2018-12-22 10:06:32.949613 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:33.174834 UTC] Performing policy update
[2018-12-22 10:06:33.175494 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:33.266368 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:34.330688 UTC] Performing line search
[2018-12-22 10:06:34.456671 UTC] Updating baseline
[2018-12-22 10:06:35.714429 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| ExpectedImprovement  | 0.032244  |
| ActualImprovement    | 0.031829  |
| ImprovementRatio     | 0.98711   |
| MeanKL               | 0.0066396 |
| Entropy              | 7.9802    |
| Perplexity           | 2922.5    |
| AveragePolicyStd     | 0.91514   |
| AveragePolicyStd[0]  | 0.94695   |
| AveragePolicyStd[1]  | 0.92517   |
| AveragePolicyStd[2]  | 0.9183    |
| AveragePolicyStd[3]  | 0.90744   |
| AveragePolicyStd[4]  | 0.91178   |
| AveragePolicyStd[5]  | 0.88123   |
| AverageReturn        | -4.7818   |
| MinReturn            | -12.41    |
| MaxReturn            | 19.959    |
| StdReturn            | 4.1055    |
| AverageEpisodeLength | 17.47     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 37        |
| StdEpisodeLength     | 2.6961    |
| TotalNEpisodes       | 2268      |
| TotalNSamples        | 39946     |
| ExplainedVariance    | 0.21329   |
------------------------------------
[2018-12-22 10:06:35.836719 UTC] Saving snapshot
[2018-12-22 10:06:35.836964 UTC] Starting iteration 8
[2018-12-22 10:06:35.837082 UTC] Start collecting samples
[2018-12-22 10:06:39.633308 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:39.859866 UTC] Performing policy update
[2018-12-22 10:06:39.860629 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:39.954335 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:41.017948 UTC] Performing line search
[2018-12-22 10:06:41.145537 UTC] Updating baseline
[2018-12-22 10:06:42.500188 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| ExpectedImprovement  | 0.029007  |
| ActualImprovement    | 0.028867  |
| ImprovementRatio     | 0.99518   |
| MeanKL               | 0.0065483 |
| Entropy              | 7.8761    |
| Perplexity           | 2633.6    |
| AveragePolicyStd     | 0.89945   |
| AveragePolicyStd[0]  | 0.93199   |
| AveragePolicyStd[1]  | 0.90826   |
| AveragePolicyStd[2]  | 0.90695   |
| AveragePolicyStd[3]  | 0.88936   |
| AveragePolicyStd[4]  | 0.89853   |
| AveragePolicyStd[5]  | 0.86161   |
| AverageReturn        | -3.6435   |
| MinReturn            | -10.625   |
| MaxReturn            | 1.7388    |
| StdReturn            | 2.5972    |
| AverageEpisodeLength | 17.95     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 29        |
| StdEpisodeLength     | 2.389     |
| TotalNEpisodes       | 2545      |
| TotalNSamples        | 44912     |
| ExplainedVariance    | 0.2277    |
------------------------------------
[2018-12-22 10:06:42.620202 UTC] Saving snapshot
[2018-12-22 10:06:42.620445 UTC] Starting iteration 9
[2018-12-22 10:06:42.620572 UTC] Start collecting samples
[2018-12-22 10:06:46.534102 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:46.759925 UTC] Performing policy update
[2018-12-22 10:06:46.760734 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:46.850494 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:47.911814 UTC] Performing line search
[2018-12-22 10:06:48.042811 UTC] Updating baseline
[2018-12-22 10:06:49.425833 UTC] Computing logging information
------------------------------------
| Iteration            | 9         |
| ExpectedImprovement  | 0.031866  |
| ActualImprovement    | 0.031166  |
| ImprovementRatio     | 0.97805   |
| MeanKL               | 0.0065965 |
| Entropy              | 7.761     |
| Perplexity           | 2347.3    |
| AveragePolicyStd     | 0.8824    |
| AveragePolicyStd[0]  | 0.91149   |
| AveragePolicyStd[1]  | 0.89836   |
| AveragePolicyStd[2]  | 0.88992   |
| AveragePolicyStd[3]  | 0.865     |
| AveragePolicyStd[4]  | 0.88669   |
| AveragePolicyStd[5]  | 0.84297   |
| AverageReturn        | -3.9659   |
| MinReturn            | -12.754   |
| MaxReturn            | 3.2172    |
| StdReturn            | 3.3119    |
| AverageEpisodeLength | 18        |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 33        |
| StdEpisodeLength     | 2.6758    |
| TotalNEpisodes       | 2825      |
| TotalNSamples        | 49942     |
| ExplainedVariance    | 0.18062   |
------------------------------------
[2018-12-22 10:06:49.557321 UTC] Saving snapshot
[2018-12-22 10:06:49.557563 UTC] Starting iteration 10
[2018-12-22 10:06:49.557699 UTC] Start collecting samples
[2018-12-22 10:06:53.444490 UTC] Computing input variables for policy optimization
[2018-12-22 10:06:53.668851 UTC] Performing policy update
[2018-12-22 10:06:53.669447 UTC] Computing gradient in Euclidean space
[2018-12-22 10:06:53.763269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:06:54.791647 UTC] Performing line search
[2018-12-22 10:06:54.920320 UTC] Updating baseline
[2018-12-22 10:06:56.308508 UTC] Computing logging information
-----------------------------------
| Iteration            | 10       |
| ExpectedImprovement  | 0.034098 |
| ActualImprovement    | 0.033248 |
| ImprovementRatio     | 0.97506  |
| MeanKL               | 0.00674  |
| Entropy              | 7.6451   |
| Perplexity           | 2090.4   |
| AveragePolicyStd     | 0.86553  |
| AveragePolicyStd[0]  | 0.89896  |
| AveragePolicyStd[1]  | 0.87767  |
| AveragePolicyStd[2]  | 0.86619  |
| AveragePolicyStd[3]  | 0.8533   |
| AveragePolicyStd[4]  | 0.87105  |
| AveragePolicyStd[5]  | 0.826    |
| AverageReturn        | -3.2704  |
| MinReturn            | -9.893   |
| MaxReturn            | 7.9569   |
| StdReturn            | 3.1707   |
| AverageEpisodeLength | 18.47    |
| MinEpisodeLength     | 14       |
| MaxEpisodeLength     | 32       |
| StdEpisodeLength     | 3.1319   |
| TotalNEpisodes       | 3104     |
| TotalNSamples        | 55006    |
| ExplainedVariance    | 0.16386  |
-----------------------------------
[2018-12-22 10:06:56.440244 UTC] Saving snapshot
[2018-12-22 10:06:56.448200 UTC] Starting iteration 11
[2018-12-22 10:06:56.448405 UTC] Start collecting samples
[2018-12-22 10:07:00.358283 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:00.583095 UTC] Performing policy update
[2018-12-22 10:07:00.583854 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:00.671251 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:01.631318 UTC] Performing line search
[2018-12-22 10:07:01.748949 UTC] Updating baseline
[2018-12-22 10:07:03.009330 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| ExpectedImprovement  | 0.032317  |
| ActualImprovement    | 0.031591  |
| ImprovementRatio     | 0.97755   |
| MeanKL               | 0.0068525 |
| Entropy              | 7.5135    |
| Perplexity           | 1832.7    |
| AveragePolicyStd     | 0.84676   |
| AveragePolicyStd[0]  | 0.88266   |
| AveragePolicyStd[1]  | 0.85782   |
| AveragePolicyStd[2]  | 0.84817   |
| AveragePolicyStd[3]  | 0.8316    |
| AveragePolicyStd[4]  | 0.84947   |
| AveragePolicyStd[5]  | 0.81082   |
| AverageReturn        | -2.0433   |
| MinReturn            | -14.144   |
| MaxReturn            | 9.1719    |
| StdReturn            | 3.3772    |
| AverageEpisodeLength | 18.08     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 31        |
| StdEpisodeLength     | 2.8024    |
| TotalNEpisodes       | 3380      |
| TotalNSamples        | 60000     |
| ExplainedVariance    | 0.17918   |
------------------------------------
[2018-12-22 10:07:03.143044 UTC] Saving snapshot
[2018-12-22 10:07:03.143282 UTC] Starting iteration 12
[2018-12-22 10:07:03.143404 UTC] Start collecting samples
[2018-12-22 10:07:07.067539 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:07.288803 UTC] Performing policy update
[2018-12-22 10:07:07.290598 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:07.381199 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:08.460290 UTC] Performing line search
[2018-12-22 10:07:08.587772 UTC] Updating baseline
[2018-12-22 10:07:09.946733 UTC] Computing logging information
------------------------------------
| Iteration            | 12        |
| ExpectedImprovement  | 0.029068  |
| ActualImprovement    | 0.028834  |
| ImprovementRatio     | 0.99193   |
| MeanKL               | 0.0065842 |
| Entropy              | 7.3823    |
| Perplexity           | 1607.3    |
| AveragePolicyStd     | 0.82841   |
| AveragePolicyStd[0]  | 0.85806   |
| AveragePolicyStd[1]  | 0.84194   |
| AveragePolicyStd[2]  | 0.82828   |
| AveragePolicyStd[3]  | 0.81999   |
| AveragePolicyStd[4]  | 0.83108   |
| AveragePolicyStd[5]  | 0.79114   |
| AverageReturn        | -1.364    |
| MinReturn            | -14.704   |
| MaxReturn            | 4.5736    |
| StdReturn            | 2.9968    |
| AverageEpisodeLength | 18        |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 29        |
| StdEpisodeLength     | 2.4576    |
| TotalNEpisodes       | 3653      |
| TotalNSamples        | 64943     |
| ExplainedVariance    | 0.15811   |
------------------------------------
[2018-12-22 10:07:10.088825 UTC] Saving snapshot
[2018-12-22 10:07:10.089070 UTC] Starting iteration 13
[2018-12-22 10:07:10.089189 UTC] Start collecting samples
[2018-12-22 10:07:14.166953 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:14.396789 UTC] Performing policy update
[2018-12-22 10:07:14.397554 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:14.489205 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:15.558964 UTC] Performing line search
[2018-12-22 10:07:15.688342 UTC] Updating baseline
[2018-12-22 10:07:16.951308 UTC] Computing logging information
------------------------------------
| Iteration            | 13        |
| ExpectedImprovement  | 0.027576  |
| ActualImprovement    | 0.027187  |
| ImprovementRatio     | 0.98589   |
| MeanKL               | 0.0068232 |
| Entropy              | 7.2638    |
| Perplexity           | 1427.6    |
| AveragePolicyStd     | 0.81229   |
| AveragePolicyStd[0]  | 0.84009   |
| AveragePolicyStd[1]  | 0.83529   |
| AveragePolicyStd[2]  | 0.81233   |
| AveragePolicyStd[3]  | 0.80758   |
| AveragePolicyStd[4]  | 0.80975   |
| AveragePolicyStd[5]  | 0.76871   |
| AverageReturn        | -0.87587  |
| MinReturn            | -13.931   |
| MaxReturn            | 7.7302    |
| StdReturn            | 3.1468    |
| AverageEpisodeLength | 18.24     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 33        |
| StdEpisodeLength     | 3.1941    |
| TotalNEpisodes       | 3930      |
| TotalNSamples        | 69963     |
| ExplainedVariance    | 0.16097   |
------------------------------------
[2018-12-22 10:07:17.098086 UTC] Saving snapshot
[2018-12-22 10:07:17.098351 UTC] Starting iteration 14
[2018-12-22 10:07:17.098484 UTC] Start collecting samples
[2018-12-22 10:07:21.135838 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:21.355545 UTC] Performing policy update
[2018-12-22 10:07:21.356250 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:21.447895 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:22.523782 UTC] Performing line search
[2018-12-22 10:07:22.650807 UTC] Updating baseline
[2018-12-22 10:07:24.012764 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| ExpectedImprovement  | 0.028721  |
| ActualImprovement    | 0.027966  |
| ImprovementRatio     | 0.97373   |
| MeanKL               | 0.0065288 |
| Entropy              | 7.1405    |
| Perplexity           | 1262      |
| AveragePolicyStd     | 0.79578   |
| AveragePolicyStd[0]  | 0.8234    |
| AveragePolicyStd[1]  | 0.82185   |
| AveragePolicyStd[2]  | 0.79349   |
| AveragePolicyStd[3]  | 0.78547   |
| AveragePolicyStd[4]  | 0.79492   |
| AveragePolicyStd[5]  | 0.75552   |
| AverageReturn        | 0.083309  |
| MinReturn            | -6.4143   |
| MaxReturn            | 13.696    |
| StdReturn            | 3.0522    |
| AverageEpisodeLength | 18.19     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 33        |
| StdEpisodeLength     | 2.9451    |
| TotalNEpisodes       | 4203      |
| TotalNSamples        | 74963     |
| ExplainedVariance    | 0.17228   |
------------------------------------
[2018-12-22 10:07:24.163524 UTC] Saving snapshot
[2018-12-22 10:07:24.163807 UTC] Starting iteration 15
[2018-12-22 10:07:24.163929 UTC] Start collecting samples
[2018-12-22 10:07:28.272924 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:28.495520 UTC] Performing policy update
[2018-12-22 10:07:28.496310 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:28.588549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:29.658472 UTC] Performing line search
[2018-12-22 10:07:29.787608 UTC] Updating baseline
[2018-12-22 10:07:31.136245 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| ExpectedImprovement  | 0.028206  |
| ActualImprovement    | 0.027784  |
| ImprovementRatio     | 0.98504   |
| MeanKL               | 0.0065686 |
| Entropy              | 7.0134    |
| Perplexity           | 1111.4    |
| AveragePolicyStd     | 0.77913   |
| AveragePolicyStd[0]  | 0.81093   |
| AveragePolicyStd[1]  | 0.80292   |
| AveragePolicyStd[2]  | 0.77188   |
| AveragePolicyStd[3]  | 0.76953   |
| AveragePolicyStd[4]  | 0.78069   |
| AveragePolicyStd[5]  | 0.7388    |
| AverageReturn        | 0.30859   |
| MinReturn            | -13.43    |
| MaxReturn            | 17.501    |
| StdReturn            | 3.918     |
| AverageEpisodeLength | 18.25     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 35        |
| StdEpisodeLength     | 3.2783    |
| TotalNEpisodes       | 4480      |
| TotalNSamples        | 80006     |
| ExplainedVariance    | 0.13632   |
------------------------------------
[2018-12-22 10:07:31.293542 UTC] Saving snapshot
[2018-12-22 10:07:31.293814 UTC] Starting iteration 16
[2018-12-22 10:07:31.293939 UTC] Start collecting samples
[2018-12-22 10:07:35.411198 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:35.635266 UTC] Performing policy update
[2018-12-22 10:07:35.635878 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:35.726565 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:36.801774 UTC] Performing line search
[2018-12-22 10:07:36.929388 UTC] Updating baseline
[2018-12-22 10:07:38.234732 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| ExpectedImprovement  | 0.028299  |
| ActualImprovement    | 0.027726  |
| ImprovementRatio     | 0.97974   |
| MeanKL               | 0.0067254 |
| Entropy              | 6.8828    |
| Perplexity           | 975.38    |
| AveragePolicyStd     | 0.76243   |
| AveragePolicyStd[0]  | 0.79732   |
| AveragePolicyStd[1]  | 0.7948    |
| AveragePolicyStd[2]  | 0.75041   |
| AveragePolicyStd[3]  | 0.74862   |
| AveragePolicyStd[4]  | 0.75636   |
| AveragePolicyStd[5]  | 0.72707   |
| AverageReturn        | 1.1504    |
| MinReturn            | -7.2018   |
| MaxReturn            | 6.8814    |
| StdReturn            | 2.7232    |
| AverageEpisodeLength | 18.56     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 34        |
| StdEpisodeLength     | 3.0832    |
| TotalNEpisodes       | 4754      |
| TotalNSamples        | 85006     |
| ExplainedVariance    | 0.1608    |
------------------------------------
[2018-12-22 10:07:38.395385 UTC] Saving snapshot
[2018-12-22 10:07:38.395639 UTC] Starting iteration 17
[2018-12-22 10:07:38.395766 UTC] Start collecting samples
[2018-12-22 10:07:42.527572 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:42.753854 UTC] Performing policy update
[2018-12-22 10:07:42.754504 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:42.843863 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:43.911991 UTC] Performing line search
[2018-12-22 10:07:44.042732 UTC] Updating baseline
[2018-12-22 10:07:45.320073 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| ExpectedImprovement  | 0.029412  |
| ActualImprovement    | 0.028545  |
| ImprovementRatio     | 0.97055   |
| MeanKL               | 0.0066409 |
| Entropy              | 6.7464    |
| Perplexity           | 850.99    |
| AveragePolicyStd     | 0.74533   |
| AveragePolicyStd[0]  | 0.78021   |
| AveragePolicyStd[1]  | 0.7824    |
| AveragePolicyStd[2]  | 0.73169   |
| AveragePolicyStd[3]  | 0.73073   |
| AveragePolicyStd[4]  | 0.73382   |
| AveragePolicyStd[5]  | 0.71317   |
| AverageReturn        | 1.4167    |
| MinReturn            | -6.6776   |
| MaxReturn            | 12.16     |
| StdReturn            | 3.148     |
| AverageEpisodeLength | 18.35     |
| MinEpisodeLength     | 15        |
| MaxEpisodeLength     | 30        |
| StdEpisodeLength     | 2.5822    |
| TotalNEpisodes       | 5031      |
| TotalNSamples        | 90019     |
| ExplainedVariance    | 0.26133   |
------------------------------------
[2018-12-22 10:07:45.484496 UTC] Saving snapshot
[2018-12-22 10:07:45.484778 UTC] Starting iteration 18
[2018-12-22 10:07:45.484896 UTC] Start collecting samples
[2018-12-22 10:07:49.650796 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:49.873027 UTC] Performing policy update
[2018-12-22 10:07:49.873615 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:49.967241 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:51.019110 UTC] Performing line search
[2018-12-22 10:07:51.146895 UTC] Updating baseline
[2018-12-22 10:07:52.410631 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| ExpectedImprovement  | 0.026336  |
| ActualImprovement    | 0.02605   |
| ImprovementRatio     | 0.98915   |
| MeanKL               | 0.0065316 |
| Entropy              | 6.6241    |
| Perplexity           | 753.03    |
| AveragePolicyStd     | 0.73041   |
| AveragePolicyStd[0]  | 0.77188   |
| AveragePolicyStd[1]  | 0.76654   |
| AveragePolicyStd[2]  | 0.71592   |
| AveragePolicyStd[3]  | 0.71509   |
| AveragePolicyStd[4]  | 0.72087   |
| AveragePolicyStd[5]  | 0.69218   |
| AverageReturn        | 1.9143    |
| MinReturn            | -6.0784   |
| MaxReturn            | 14.146    |
| StdReturn            | 3.1497    |
| AverageEpisodeLength | 18.59     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 33        |
| StdEpisodeLength     | 2.9499    |
| TotalNEpisodes       | 5301      |
| TotalNSamples        | 95005     |
| ExplainedVariance    | 0.25153   |
------------------------------------
[2018-12-22 10:07:52.576489 UTC] Saving snapshot
[2018-12-22 10:07:52.576752 UTC] Starting iteration 19
[2018-12-22 10:07:52.576882 UTC] Start collecting samples
[2018-12-22 10:07:56.705964 UTC] Computing input variables for policy optimization
[2018-12-22 10:07:56.925864 UTC] Performing policy update
[2018-12-22 10:07:56.926470 UTC] Computing gradient in Euclidean space
[2018-12-22 10:07:57.016204 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:07:58.094060 UTC] Performing line search
[2018-12-22 10:07:58.220930 UTC] Updating baseline
[2018-12-22 10:07:59.475347 UTC] Computing logging information
-------------------------------------
| Iteration            | 19         |
| ExpectedImprovement  | 0.026151   |
| ActualImprovement    | 0.026262   |
| ImprovementRatio     | 1.0042     |
| MeanKL               | 0.0067649  |
| Entropy              | 6.4894     |
| Perplexity           | 658.15     |
| AveragePolicyStd     | 0.71422    |
| AveragePolicyStd[0]  | 0.75576    |
| AveragePolicyStd[1]  | 0.75114    |
| AveragePolicyStd[2]  | 0.69118    |
| AveragePolicyStd[3]  | 0.70007    |
| AveragePolicyStd[4]  | 0.70488    |
| AveragePolicyStd[5]  | 0.68226    |
| AverageReturn        | 3.1957     |
| MinReturn            | -4.9284    |
| MaxReturn            | 15.298     |
| StdReturn            | 3.7078     |
| AverageEpisodeLength | 18.76      |
| MinEpisodeLength     | 13         |
| MaxEpisodeLength     | 33         |
| StdEpisodeLength     | 3.191      |
| TotalNEpisodes       | 5570       |
| TotalNSamples        | 1.0001e+05 |
| ExplainedVariance    | 0.30208    |
-------------------------------------
[2018-12-22 10:07:59.647368 UTC] Saving snapshot
[2018-12-22 10:07:59.647653 UTC] Starting iteration 20
[2018-12-22 10:07:59.647775 UTC] Start collecting samples
[2018-12-22 10:08:03.881584 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:04.107470 UTC] Performing policy update
[2018-12-22 10:08:04.108054 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:04.198582 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:05.267265 UTC] Performing line search
[2018-12-22 10:08:05.395394 UTC] Updating baseline
[2018-12-22 10:08:06.666454 UTC] Computing logging information
-------------------------------------
| Iteration            | 20         |
| ExpectedImprovement  | 0.028344   |
| ActualImprovement    | 0.028173   |
| ImprovementRatio     | 0.99397    |
| MeanKL               | 0.0065964  |
| Entropy              | 6.3654     |
| Perplexity           | 581.36     |
| AveragePolicyStd     | 0.69956    |
| AveragePolicyStd[0]  | 0.73937    |
| AveragePolicyStd[1]  | 0.73398    |
| AveragePolicyStd[2]  | 0.6787     |
| AveragePolicyStd[3]  | 0.68099    |
| AveragePolicyStd[4]  | 0.69392    |
| AveragePolicyStd[5]  | 0.67043    |
| AverageReturn        | 2.919      |
| MinReturn            | -7.5688    |
| MaxReturn            | 12.433     |
| StdReturn            | 3.1968     |
| AverageEpisodeLength | 18.86      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 29         |
| StdEpisodeLength     | 2.8983     |
| TotalNEpisodes       | 5841       |
| TotalNSamples        | 1.0505e+05 |
| ExplainedVariance    | 0.35097    |
-------------------------------------
[2018-12-22 10:08:06.841155 UTC] Saving snapshot
[2018-12-22 10:08:06.849321 UTC] Starting iteration 21
[2018-12-22 10:08:06.849532 UTC] Start collecting samples
[2018-12-22 10:08:11.125160 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:11.347417 UTC] Performing policy update
[2018-12-22 10:08:11.348000 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:11.439668 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:12.524392 UTC] Performing line search
[2018-12-22 10:08:12.653157 UTC] Updating baseline
[2018-12-22 10:08:13.915098 UTC] Computing logging information
-------------------------------------
| Iteration            | 21         |
| ExpectedImprovement  | 0.024532   |
| ActualImprovement    | 0.024494   |
| ImprovementRatio     | 0.99846    |
| MeanKL               | 0.0066849  |
| Entropy              | 6.2411     |
| Perplexity           | 513.41     |
| AveragePolicyStd     | 0.68525    |
| AveragePolicyStd[0]  | 0.7247     |
| AveragePolicyStd[1]  | 0.71796    |
| AveragePolicyStd[2]  | 0.66764    |
| AveragePolicyStd[3]  | 0.65779    |
| AveragePolicyStd[4]  | 0.68667    |
| AveragePolicyStd[5]  | 0.65677    |
| AverageReturn        | 3.5793     |
| MinReturn            | -2.1446    |
| MaxReturn            | 10.772     |
| StdReturn            | 2.5564     |
| AverageEpisodeLength | 18.22      |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 26         |
| StdEpisodeLength     | 1.9004     |
| TotalNEpisodes       | 6112       |
| TotalNSamples        | 1.1007e+05 |
| ExplainedVariance    | 0.39364    |
-------------------------------------
[2018-12-22 10:08:14.101983 UTC] Saving snapshot
[2018-12-22 10:08:14.102226 UTC] Starting iteration 22
[2018-12-22 10:08:14.102343 UTC] Start collecting samples
[2018-12-22 10:08:18.396330 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:18.614086 UTC] Performing policy update
[2018-12-22 10:08:18.614705 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:18.705435 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:19.782064 UTC] Performing line search
[2018-12-22 10:08:19.910757 UTC] Updating baseline
[2018-12-22 10:08:21.177178 UTC] Computing logging information
-------------------------------------
| Iteration            | 22         |
| ExpectedImprovement  | 0.026066   |
| ActualImprovement    | 0.026158   |
| ImprovementRatio     | 1.0035     |
| MeanKL               | 0.0066556  |
| Entropy              | 6.1182     |
| Perplexity           | 454.06     |
| AveragePolicyStd     | 0.67147    |
| AveragePolicyStd[0]  | 0.71572    |
| AveragePolicyStd[1]  | 0.70566    |
| AveragePolicyStd[2]  | 0.6564     |
| AveragePolicyStd[3]  | 0.6411     |
| AveragePolicyStd[4]  | 0.66806    |
| AveragePolicyStd[5]  | 0.64185    |
| AverageReturn        | 4.8272     |
| MinReturn            | -2.7617    |
| MaxReturn            | 12.275     |
| StdReturn            | 3.084      |
| AverageEpisodeLength | 18.68      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 34         |
| StdEpisodeLength     | 2.6072     |
| TotalNEpisodes       | 6378       |
| TotalNSamples        | 1.1506e+05 |
| ExplainedVariance    | 0.40391    |
-------------------------------------
[2018-12-22 10:08:21.363069 UTC] Saving snapshot
[2018-12-22 10:08:21.363307 UTC] Starting iteration 23
[2018-12-22 10:08:21.363440 UTC] Start collecting samples
[2018-12-22 10:08:25.705056 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:25.930839 UTC] Performing policy update
[2018-12-22 10:08:25.931671 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:26.024789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:27.101363 UTC] Performing line search
[2018-12-22 10:08:27.229917 UTC] Updating baseline
[2018-12-22 10:08:28.683230 UTC] Computing logging information
-------------------------------------
| Iteration            | 23         |
| ExpectedImprovement  | 0.026287   |
| ActualImprovement    | 0.025723   |
| ImprovementRatio     | 0.97856    |
| MeanKL               | 0.0065928  |
| Entropy              | 6.0097     |
| Perplexity           | 407.34     |
| AveragePolicyStd     | 0.65945    |
| AveragePolicyStd[0]  | 0.70447    |
| AveragePolicyStd[1]  | 0.68886    |
| AveragePolicyStd[2]  | 0.64096    |
| AveragePolicyStd[3]  | 0.62711    |
| AveragePolicyStd[4]  | 0.6653     |
| AveragePolicyStd[5]  | 0.63       |
| AverageReturn        | 4.4442     |
| MinReturn            | -4.4563    |
| MaxReturn            | 11.593     |
| StdReturn            | 3.017      |
| AverageEpisodeLength | 18.47      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 31         |
| StdEpisodeLength     | 2.5474     |
| TotalNEpisodes       | 6645       |
| TotalNSamples        | 1.2006e+05 |
| ExplainedVariance    | 0.41539    |
-------------------------------------
[2018-12-22 10:08:28.873662 UTC] Saving snapshot
[2018-12-22 10:08:28.873935 UTC] Starting iteration 24
[2018-12-22 10:08:28.874069 UTC] Start collecting samples
[2018-12-22 10:08:33.196183 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:33.415611 UTC] Performing policy update
[2018-12-22 10:08:33.416254 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:33.505870 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:34.578367 UTC] Performing line search
[2018-12-22 10:08:34.706691 UTC] Updating baseline
[2018-12-22 10:08:36.066386 UTC] Computing logging information
-------------------------------------
| Iteration            | 24         |
| ExpectedImprovement  | 0.026945   |
| ActualImprovement    | 0.026555   |
| ImprovementRatio     | 0.9855     |
| MeanKL               | 0.0065262  |
| Entropy              | 5.8863     |
| Perplexity           | 360.06     |
| AveragePolicyStd     | 0.64602    |
| AveragePolicyStd[0]  | 0.68899    |
| AveragePolicyStd[1]  | 0.67659    |
| AveragePolicyStd[2]  | 0.62667    |
| AveragePolicyStd[3]  | 0.618      |
| AveragePolicyStd[4]  | 0.65028    |
| AveragePolicyStd[5]  | 0.61559    |
| AverageReturn        | 5.0552     |
| MinReturn            | -3.2697    |
| MaxReturn            | 14.656     |
| StdReturn            | 2.9705     |
| AverageEpisodeLength | 18.6       |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 37         |
| StdEpisodeLength     | 3.3526     |
| TotalNEpisodes       | 6910       |
| TotalNSamples        | 1.2499e+05 |
| ExplainedVariance    | 0.46622    |
-------------------------------------
[2018-12-22 10:08:36.261514 UTC] Saving snapshot
[2018-12-22 10:08:36.261830 UTC] Starting iteration 25
[2018-12-22 10:08:36.261964 UTC] Start collecting samples
[2018-12-22 10:08:40.597984 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:40.819940 UTC] Performing policy update
[2018-12-22 10:08:40.820698 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:40.909475 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:41.978544 UTC] Performing line search
[2018-12-22 10:08:42.105216 UTC] Updating baseline
[2018-12-22 10:08:43.446139 UTC] Computing logging information
-------------------------------------
| Iteration            | 25         |
| ExpectedImprovement  | 0.023806   |
| ActualImprovement    | 0.023501   |
| ImprovementRatio     | 0.98722    |
| MeanKL               | 0.0064832  |
| Entropy              | 5.76       |
| Perplexity           | 317.36     |
| AveragePolicyStd     | 0.63256    |
| AveragePolicyStd[0]  | 0.67466    |
| AveragePolicyStd[1]  | 0.65928    |
| AveragePolicyStd[2]  | 0.61178    |
| AveragePolicyStd[3]  | 0.60826    |
| AveragePolicyStd[4]  | 0.64096    |
| AveragePolicyStd[5]  | 0.60042    |
| AverageReturn        | 6.1584     |
| MinReturn            | -0.10007   |
| MaxReturn            | 20.319     |
| StdReturn            | 2.9864     |
| AverageEpisodeLength | 18.48      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 32         |
| StdEpisodeLength     | 2.8053     |
| TotalNEpisodes       | 7178       |
| TotalNSamples        | 1.3004e+05 |
| ExplainedVariance    | 0.44028    |
-------------------------------------
[2018-12-22 10:08:43.643773 UTC] Saving snapshot
[2018-12-22 10:08:43.644021 UTC] Starting iteration 26
[2018-12-22 10:08:43.644144 UTC] Start collecting samples
[2018-12-22 10:08:48.013657 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:48.231072 UTC] Performing policy update
[2018-12-22 10:08:48.231837 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:48.322701 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:49.391949 UTC] Performing line search
[2018-12-22 10:08:49.520538 UTC] Updating baseline
[2018-12-22 10:08:50.794342 UTC] Computing logging information
-------------------------------------
| Iteration            | 26         |
| ExpectedImprovement  | 0.02627    |
| ActualImprovement    | 0.025981   |
| ImprovementRatio     | 0.98898    |
| MeanKL               | 0.0066965  |
| Entropy              | 5.634      |
| Perplexity           | 279.77     |
| AveragePolicyStd     | 0.6195     |
| AveragePolicyStd[0]  | 0.66099    |
| AveragePolicyStd[1]  | 0.6474     |
| AveragePolicyStd[2]  | 0.60364    |
| AveragePolicyStd[3]  | 0.59351    |
| AveragePolicyStd[4]  | 0.63061    |
| AveragePolicyStd[5]  | 0.58082    |
| AverageReturn        | 6.0931     |
| MinReturn            | -0.58161   |
| MaxReturn            | 17.702     |
| StdReturn            | 3.043      |
| AverageEpisodeLength | 19.38      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 37         |
| StdEpisodeLength     | 3.5994     |
| TotalNEpisodes       | 7445       |
| TotalNSamples        | 1.3507e+05 |
| ExplainedVariance    | 0.51483    |
-------------------------------------
[2018-12-22 10:08:50.995213 UTC] Saving snapshot
[2018-12-22 10:08:50.995456 UTC] Starting iteration 27
[2018-12-22 10:08:50.995574 UTC] Start collecting samples
[2018-12-22 10:08:55.419935 UTC] Computing input variables for policy optimization
[2018-12-22 10:08:55.638318 UTC] Performing policy update
[2018-12-22 10:08:55.639105 UTC] Computing gradient in Euclidean space
[2018-12-22 10:08:55.729952 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:08:56.791886 UTC] Performing line search
[2018-12-22 10:08:56.918186 UTC] Updating baseline
[2018-12-22 10:08:58.176805 UTC] Computing logging information
-------------------------------------
| Iteration            | 27         |
| ExpectedImprovement  | 0.026161   |
| ActualImprovement    | 0.025864   |
| ImprovementRatio     | 0.98863    |
| MeanKL               | 0.0067953  |
| Entropy              | 5.5291     |
| Perplexity           | 251.92     |
| AveragePolicyStd     | 0.60874    |
| AveragePolicyStd[0]  | 0.6572     |
| AveragePolicyStd[1]  | 0.6285     |
| AveragePolicyStd[2]  | 0.59392    |
| AveragePolicyStd[3]  | 0.58214    |
| AveragePolicyStd[4]  | 0.61437    |
| AveragePolicyStd[5]  | 0.57631    |
| AverageReturn        | 7.3919     |
| MinReturn            | -0.24351   |
| MaxReturn            | 17.731     |
| StdReturn            | 2.8179     |
| AverageEpisodeLength | 18.86      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 30         |
| StdEpisodeLength     | 2.6383     |
| TotalNEpisodes       | 7710       |
| TotalNSamples        | 1.4005e+05 |
| ExplainedVariance    | 0.60322    |
-------------------------------------
[2018-12-22 10:08:58.377544 UTC] Saving snapshot
[2018-12-22 10:08:58.377824 UTC] Starting iteration 28
[2018-12-22 10:08:58.377963 UTC] Start collecting samples
[2018-12-22 10:09:02.850701 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:03.069203 UTC] Performing policy update
[2018-12-22 10:09:03.070032 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:03.161484 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:04.243703 UTC] Performing line search
[2018-12-22 10:09:04.372032 UTC] Updating baseline
[2018-12-22 10:09:05.629212 UTC] Computing logging information
-------------------------------------
| Iteration            | 28         |
| ExpectedImprovement  | 0.027328   |
| ActualImprovement    | 0.027367   |
| ImprovementRatio     | 1.0014     |
| MeanKL               | 0.0067083  |
| Entropy              | 5.4113     |
| Perplexity           | 223.91     |
| AveragePolicyStd     | 0.59687    |
| AveragePolicyStd[0]  | 0.64244    |
| AveragePolicyStd[1]  | 0.61422    |
| AveragePolicyStd[2]  | 0.58336    |
| AveragePolicyStd[3]  | 0.57655    |
| AveragePolicyStd[4]  | 0.60372    |
| AveragePolicyStd[5]  | 0.5609     |
| AverageReturn        | 7.1214     |
| MinReturn            | 0.68684    |
| MaxReturn            | 15.008     |
| StdReturn            | 2.8146     |
| AverageEpisodeLength | 18.38      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 28         |
| StdEpisodeLength     | 1.9431     |
| TotalNEpisodes       | 7980       |
| TotalNSamples        | 1.4509e+05 |
| ExplainedVariance    | 0.63483    |
-------------------------------------
[2018-12-22 10:09:05.843541 UTC] Saving snapshot
[2018-12-22 10:09:05.843790 UTC] Starting iteration 29
[2018-12-22 10:09:05.843906 UTC] Start collecting samples
[2018-12-22 10:09:10.316521 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:10.538887 UTC] Performing policy update
[2018-12-22 10:09:10.539640 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:10.630095 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:11.685989 UTC] Performing line search
[2018-12-22 10:09:11.813268 UTC] Updating baseline
[2018-12-22 10:09:13.157420 UTC] Computing logging information
-------------------------------------
| Iteration            | 29         |
| ExpectedImprovement  | 0.029813   |
| ActualImprovement    | 0.029341   |
| ImprovementRatio     | 0.98415    |
| MeanKL               | 0.0065409  |
| Entropy              | 5.3121     |
| Perplexity           | 202.78     |
| AveragePolicyStd     | 0.58704    |
| AveragePolicyStd[0]  | 0.62816    |
| AveragePolicyStd[1]  | 0.60514    |
| AveragePolicyStd[2]  | 0.57616    |
| AveragePolicyStd[3]  | 0.56254    |
| AveragePolicyStd[4]  | 0.59526    |
| AveragePolicyStd[5]  | 0.55497    |
| AverageReturn        | 7.6315     |
| MinReturn            | 1.4065     |
| MaxReturn            | 16.626     |
| StdReturn            | 2.9868     |
| AverageEpisodeLength | 18.94      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 44         |
| StdEpisodeLength     | 4.1105     |
| TotalNEpisodes       | 8248       |
| TotalNSamples        | 1.5011e+05 |
| ExplainedVariance    | 0.67732    |
-------------------------------------
[2018-12-22 10:09:13.371355 UTC] Saving snapshot
[2018-12-22 10:09:13.371611 UTC] Starting iteration 30
[2018-12-22 10:09:13.371735 UTC] Start collecting samples
[2018-12-22 10:09:17.872215 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:18.092283 UTC] Performing policy update
[2018-12-22 10:09:18.093051 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:18.183518 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:19.250918 UTC] Performing line search
[2018-12-22 10:09:19.377830 UTC] Updating baseline
[2018-12-22 10:09:20.666783 UTC] Computing logging information
------------------------------------
| Iteration            | 30        |
| ExpectedImprovement  | 0.029708  |
| ActualImprovement    | 0.029182  |
| ImprovementRatio     | 0.98229   |
| MeanKL               | 0.00652   |
| Entropy              | 5.2068    |
| Perplexity           | 182.52    |
| AveragePolicyStd     | 0.57688   |
| AveragePolicyStd[0]  | 0.62034   |
| AveragePolicyStd[1]  | 0.59428   |
| AveragePolicyStd[2]  | 0.56732   |
| AveragePolicyStd[3]  | 0.55572   |
| AveragePolicyStd[4]  | 0.58235   |
| AveragePolicyStd[5]  | 0.54124   |
| AverageReturn        | 8.2309    |
| MinReturn            | 1.9418    |
| MaxReturn            | 16.579    |
| StdReturn            | 2.9675    |
| AverageEpisodeLength | 18.53     |
| MinEpisodeLength     | 15        |
| MaxEpisodeLength     | 40        |
| StdEpisodeLength     | 2.9341    |
| TotalNEpisodes       | 8515      |
| TotalNSamples        | 1.551e+05 |
| ExplainedVariance    | 0.68161   |
------------------------------------
[2018-12-22 10:09:20.884758 UTC] Saving snapshot
[2018-12-22 10:09:20.892800 UTC] Starting iteration 31
[2018-12-22 10:09:20.892992 UTC] Start collecting samples
[2018-12-22 10:09:25.357506 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:25.572918 UTC] Performing policy update
[2018-12-22 10:09:25.573540 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:25.665248 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:26.728169 UTC] Performing line search
[2018-12-22 10:09:26.855884 UTC] Updating baseline
[2018-12-22 10:09:28.105883 UTC] Computing logging information
-------------------------------------
| Iteration            | 31         |
| ExpectedImprovement  | 0.026901   |
| ActualImprovement    | 0.026499   |
| ImprovementRatio     | 0.98504    |
| MeanKL               | 0.0066273  |
| Entropy              | 5.083      |
| Perplexity           | 161.26     |
| AveragePolicyStd     | 0.56515    |
| AveragePolicyStd[0]  | 0.61275    |
| AveragePolicyStd[1]  | 0.58242    |
| AveragePolicyStd[2]  | 0.55168    |
| AveragePolicyStd[3]  | 0.54256    |
| AveragePolicyStd[4]  | 0.56844    |
| AveragePolicyStd[5]  | 0.53307    |
| AverageReturn        | 9.1335     |
| MinReturn            | 0.63417    |
| MaxReturn            | 17.47      |
| StdReturn            | 2.9927     |
| AverageEpisodeLength | 19.22      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 31         |
| StdEpisodeLength     | 3.1419     |
| TotalNEpisodes       | 8777       |
| TotalNSamples        | 1.6013e+05 |
| ExplainedVariance    | 0.65131    |
-------------------------------------
[2018-12-22 10:09:28.321378 UTC] Saving snapshot
[2018-12-22 10:09:28.321838 UTC] Starting iteration 32
[2018-12-22 10:09:28.321995 UTC] Start collecting samples
[2018-12-22 10:09:32.795265 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:33.005002 UTC] Performing policy update
[2018-12-22 10:09:33.005689 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:33.096277 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:34.162823 UTC] Performing line search
[2018-12-22 10:09:34.291258 UTC] Updating baseline
[2018-12-22 10:09:35.532180 UTC] Computing logging information
-------------------------------------
| Iteration            | 32         |
| ExpectedImprovement  | 0.030173   |
| ActualImprovement    | 0.029868   |
| ImprovementRatio     | 0.98988    |
| MeanKL               | 0.0065774  |
| Entropy              | 4.9676     |
| Perplexity           | 143.69     |
| AveragePolicyStd     | 0.55447    |
| AveragePolicyStd[0]  | 0.60624    |
| AveragePolicyStd[1]  | 0.57004    |
| AveragePolicyStd[2]  | 0.53714    |
| AveragePolicyStd[3]  | 0.52928    |
| AveragePolicyStd[4]  | 0.5595     |
| AveragePolicyStd[5]  | 0.52466    |
| AverageReturn        | 9.3186     |
| MinReturn            | 1.8353     |
| MaxReturn            | 21.394     |
| StdReturn            | 3.5304     |
| AverageEpisodeLength | 19.32      |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 38         |
| StdEpisodeLength     | 3.5068     |
| TotalNEpisodes       | 9031       |
| TotalNSamples        | 1.6506e+05 |
| ExplainedVariance    | 0.64253    |
-------------------------------------
[2018-12-22 10:09:35.760993 UTC] Saving snapshot
[2018-12-22 10:09:35.761230 UTC] Starting iteration 33
[2018-12-22 10:09:35.761346 UTC] Start collecting samples
[2018-12-22 10:09:40.287986 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:40.502928 UTC] Performing policy update
[2018-12-22 10:09:40.503528 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:40.594261 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:41.658096 UTC] Performing line search
[2018-12-22 10:09:41.790290 UTC] Updating baseline
[2018-12-22 10:09:43.017295 UTC] Computing logging information
------------------------------------
| Iteration            | 33        |
| ExpectedImprovement  | 0.026018  |
| ActualImprovement    | 0.025706  |
| ImprovementRatio     | 0.98804   |
| MeanKL               | 0.0065358 |
| Entropy              | 4.8674    |
| Perplexity           | 129.98    |
| AveragePolicyStd     | 0.54537   |
| AveragePolicyStd[0]  | 0.5999    |
| AveragePolicyStd[1]  | 0.55743   |
| AveragePolicyStd[2]  | 0.52175   |
| AveragePolicyStd[3]  | 0.5247    |
| AveragePolicyStd[4]  | 0.55416   |
| AveragePolicyStd[5]  | 0.51427   |
| AverageReturn        | 10.601    |
| MinReturn            | 1.7216    |
| MaxReturn            | 19.011    |
| StdReturn            | 3.4156    |
| AverageEpisodeLength | 20.21     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 36        |
| StdEpisodeLength     | 3.6723    |
| TotalNEpisodes       | 9286      |
| TotalNSamples        | 1.701e+05 |
| ExplainedVariance    | 0.70091   |
------------------------------------
[2018-12-22 10:09:43.246019 UTC] Saving snapshot
[2018-12-22 10:09:43.246259 UTC] Starting iteration 34
[2018-12-22 10:09:43.246376 UTC] Start collecting samples
[2018-12-22 10:09:47.804036 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:48.018384 UTC] Performing policy update
[2018-12-22 10:09:48.019024 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:48.111092 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:49.170288 UTC] Performing line search
[2018-12-22 10:09:49.297189 UTC] Updating baseline
[2018-12-22 10:09:50.540699 UTC] Computing logging information
-------------------------------------
| Iteration            | 34         |
| ExpectedImprovement  | 0.027028   |
| ActualImprovement    | 0.026667   |
| ImprovementRatio     | 0.98667    |
| MeanKL               | 0.0067732  |
| Entropy              | 4.7767     |
| Perplexity           | 118.71     |
| AveragePolicyStd     | 0.5372     |
| AveragePolicyStd[0]  | 0.59269    |
| AveragePolicyStd[1]  | 0.54684    |
| AveragePolicyStd[2]  | 0.5119     |
| AveragePolicyStd[3]  | 0.51584    |
| AveragePolicyStd[4]  | 0.54666    |
| AveragePolicyStd[5]  | 0.50929    |
| AverageReturn        | 11.056     |
| MinReturn            | 5.138      |
| MaxReturn            | 19.675     |
| StdReturn            | 2.9839     |
| AverageEpisodeLength | 19.92      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 31         |
| StdEpisodeLength     | 3.1518     |
| TotalNEpisodes       | 9541       |
| TotalNSamples        | 1.7508e+05 |
| ExplainedVariance    | 0.73839    |
-------------------------------------
[2018-12-22 10:09:50.772481 UTC] Saving snapshot
[2018-12-22 10:09:50.772738 UTC] Starting iteration 35
[2018-12-22 10:09:50.772855 UTC] Start collecting samples
[2018-12-22 10:09:55.451469 UTC] Computing input variables for policy optimization
[2018-12-22 10:09:55.680046 UTC] Performing policy update
[2018-12-22 10:09:55.680801 UTC] Computing gradient in Euclidean space
[2018-12-22 10:09:55.776134 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:09:56.894718 UTC] Performing line search
[2018-12-22 10:09:57.027344 UTC] Updating baseline
[2018-12-22 10:09:58.326729 UTC] Computing logging information
-------------------------------------
| Iteration            | 35         |
| ExpectedImprovement  | 0.028595   |
| ActualImprovement    | 0.028217   |
| ImprovementRatio     | 0.9868     |
| MeanKL               | 0.0066933  |
| Entropy              | 4.6662     |
| Perplexity           | 106.29     |
| AveragePolicyStd     | 0.52758    |
| AveragePolicyStd[0]  | 0.59008    |
| AveragePolicyStd[1]  | 0.53738    |
| AveragePolicyStd[2]  | 0.49812    |
| AveragePolicyStd[3]  | 0.50794    |
| AveragePolicyStd[4]  | 0.53446    |
| AveragePolicyStd[5]  | 0.49753    |
| AverageReturn        | 10.897     |
| MinReturn            | 4.9897     |
| MaxReturn            | 18.678     |
| StdReturn            | 2.9461     |
| AverageEpisodeLength | 19.94      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 3.5123     |
| TotalNEpisodes       | 9791       |
| TotalNSamples        | 1.8015e+05 |
| ExplainedVariance    | 0.78516    |
-------------------------------------
[2018-12-22 10:09:58.579870 UTC] Saving snapshot
[2018-12-22 10:09:58.580168 UTC] Starting iteration 36
[2018-12-22 10:09:58.580288 UTC] Start collecting samples
[2018-12-22 10:10:03.483544 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:03.706963 UTC] Performing policy update
[2018-12-22 10:10:03.707651 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:03.802118 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:04.871365 UTC] Performing line search
[2018-12-22 10:10:04.999470 UTC] Updating baseline
[2018-12-22 10:10:06.261151 UTC] Computing logging information
-------------------------------------
| Iteration            | 36         |
| ExpectedImprovement  | 0.027799   |
| ActualImprovement    | 0.026889   |
| ImprovementRatio     | 0.96726    |
| MeanKL               | 0.0066758  |
| Entropy              | 4.5656     |
| Perplexity           | 96.122     |
| AveragePolicyStd     | 0.51878    |
| AveragePolicyStd[0]  | 0.57691    |
| AveragePolicyStd[1]  | 0.52715    |
| AveragePolicyStd[2]  | 0.48907    |
| AveragePolicyStd[3]  | 0.50532    |
| AveragePolicyStd[4]  | 0.52847    |
| AveragePolicyStd[5]  | 0.48574    |
| AverageReturn        | 11.485     |
| MinReturn            | 5.9563     |
| MaxReturn            | 17.179     |
| StdReturn            | 2.5773     |
| AverageEpisodeLength | 19.69      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 25         |
| StdEpisodeLength     | 2.3694     |
| TotalNEpisodes       | 10039      |
| TotalNSamples        | 1.8514e+05 |
| ExplainedVariance    | 0.77856    |
-------------------------------------
[2018-12-22 10:10:06.506050 UTC] Saving snapshot
[2018-12-22 10:10:06.506298 UTC] Starting iteration 37
[2018-12-22 10:10:06.506441 UTC] Start collecting samples
[2018-12-22 10:10:10.997869 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:11.202885 UTC] Performing policy update
[2018-12-22 10:10:11.203582 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:11.294491 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:12.361601 UTC] Performing line search
[2018-12-22 10:10:12.487828 UTC] Updating baseline
[2018-12-22 10:10:13.743043 UTC] Computing logging information
-------------------------------------
| Iteration            | 37         |
| ExpectedImprovement  | 0.026897   |
| ActualImprovement    | 0.026454   |
| ImprovementRatio     | 0.98353    |
| MeanKL               | 0.0065465  |
| Entropy              | 4.4555     |
| Perplexity           | 86.096     |
| AveragePolicyStd     | 0.5095     |
| AveragePolicyStd[0]  | 0.57431    |
| AveragePolicyStd[1]  | 0.51966    |
| AveragePolicyStd[2]  | 0.47851    |
| AveragePolicyStd[3]  | 0.49087    |
| AveragePolicyStd[4]  | 0.51524    |
| AveragePolicyStd[5]  | 0.47845    |
| AverageReturn        | 12.548     |
| MinReturn            | 6.7123     |
| MaxReturn            | 20.419     |
| StdReturn            | 3.3692     |
| AverageEpisodeLength | 20.79      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 30         |
| StdEpisodeLength     | 3.2536     |
| TotalNEpisodes       | 10279      |
| TotalNSamples        | 1.9015e+05 |
| ExplainedVariance    | 0.77563    |
-------------------------------------
[2018-12-22 10:10:13.995951 UTC] Saving snapshot
[2018-12-22 10:10:13.996185 UTC] Starting iteration 38
[2018-12-22 10:10:13.996299 UTC] Start collecting samples
[2018-12-22 10:10:18.550987 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:18.753926 UTC] Performing policy update
[2018-12-22 10:10:18.754477 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:18.844018 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:19.798232 UTC] Performing line search
[2018-12-22 10:10:19.914110 UTC] Updating baseline
[2018-12-22 10:10:21.175732 UTC] Computing logging information
-------------------------------------
| Iteration            | 38         |
| ExpectedImprovement  | 0.02557    |
| ActualImprovement    | 0.024959   |
| ImprovementRatio     | 0.97611    |
| MeanKL               | 0.0065763  |
| Entropy              | 4.3413     |
| Perplexity           | 76.811     |
| AveragePolicyStd     | 0.49999    |
| AveragePolicyStd[0]  | 0.56621    |
| AveragePolicyStd[1]  | 0.5113     |
| AveragePolicyStd[2]  | 0.47153    |
| AveragePolicyStd[3]  | 0.47846    |
| AveragePolicyStd[4]  | 0.50533    |
| AveragePolicyStd[5]  | 0.4671     |
| AverageReturn        | 13.47      |
| MinReturn            | 5.6866     |
| MaxReturn            | 21.435     |
| StdReturn            | 2.9682     |
| AverageEpisodeLength | 21.56      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 33         |
| StdEpisodeLength     | 3.1411     |
| TotalNEpisodes       | 10513      |
| TotalNSamples        | 1.9512e+05 |
| ExplainedVariance    | 0.79326    |
-------------------------------------
[2018-12-22 10:10:21.420380 UTC] Saving snapshot
[2018-12-22 10:10:21.420676 UTC] Starting iteration 39
[2018-12-22 10:10:21.420880 UTC] Start collecting samples
[2018-12-22 10:10:26.054806 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:26.259412 UTC] Performing policy update
[2018-12-22 10:10:26.260160 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:26.343982 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:27.296537 UTC] Performing line search
[2018-12-22 10:10:27.411007 UTC] Updating baseline
[2018-12-22 10:10:28.658297 UTC] Computing logging information
-------------------------------------
| Iteration            | 39         |
| ExpectedImprovement  | 0.023169   |
| ActualImprovement    | 0.023056   |
| ImprovementRatio     | 0.99512    |
| MeanKL               | 0.0065383  |
| Entropy              | 4.2507     |
| Perplexity           | 70.153     |
| AveragePolicyStd     | 0.49259    |
| AveragePolicyStd[0]  | 0.5606     |
| AveragePolicyStd[1]  | 0.51043    |
| AveragePolicyStd[2]  | 0.46364    |
| AveragePolicyStd[3]  | 0.47462    |
| AveragePolicyStd[4]  | 0.48812    |
| AveragePolicyStd[5]  | 0.45813    |
| AverageReturn        | 14.248     |
| MinReturn            | 6.0245     |
| MaxReturn            | 23.686     |
| StdReturn            | 3.6987     |
| AverageEpisodeLength | 21.78      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 3.1673     |
| TotalNEpisodes       | 10751      |
| TotalNSamples        | 2.0019e+05 |
| ExplainedVariance    | 0.80967    |
-------------------------------------
[2018-12-22 10:10:28.912090 UTC] Saving snapshot
[2018-12-22 10:10:28.912411 UTC] Starting iteration 40
[2018-12-22 10:10:28.912539 UTC] Start collecting samples
[2018-12-22 10:10:33.455770 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:33.650953 UTC] Performing policy update
[2018-12-22 10:10:33.651559 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:33.742928 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:34.814341 UTC] Performing line search
[2018-12-22 10:10:34.941295 UTC] Updating baseline
[2018-12-22 10:10:36.195709 UTC] Computing logging information
-------------------------------------
| Iteration            | 40         |
| ExpectedImprovement  | 0.02502    |
| ActualImprovement    | 0.024957   |
| ImprovementRatio     | 0.99747    |
| MeanKL               | 0.0067136  |
| Entropy              | 4.1422     |
| Perplexity           | 62.942     |
| AveragePolicyStd     | 0.48383    |
| AveragePolicyStd[0]  | 0.55375    |
| AveragePolicyStd[1]  | 0.50086    |
| AveragePolicyStd[2]  | 0.45628    |
| AveragePolicyStd[3]  | 0.46512    |
| AveragePolicyStd[4]  | 0.47745    |
| AveragePolicyStd[5]  | 0.44953    |
| AverageReturn        | 14.91      |
| MinReturn            | 7.9667     |
| MaxReturn            | 25.985     |
| StdReturn            | 3.0287     |
| AverageEpisodeLength | 22.21      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 3.542      |
| TotalNEpisodes       | 10976      |
| TotalNSamples        | 2.0517e+05 |
| ExplainedVariance    | 0.82628    |
-------------------------------------
[2018-12-22 10:10:36.451014 UTC] Saving snapshot
[2018-12-22 10:10:36.458968 UTC] Starting iteration 41
[2018-12-22 10:10:36.459143 UTC] Start collecting samples
[2018-12-22 10:10:41.014360 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:41.213260 UTC] Performing policy update
[2018-12-22 10:10:41.213951 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:41.303191 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:42.370194 UTC] Performing line search
[2018-12-22 10:10:42.497840 UTC] Updating baseline
[2018-12-22 10:10:43.738944 UTC] Computing logging information
-------------------------------------
| Iteration            | 41         |
| ExpectedImprovement  | 0.026627   |
| ActualImprovement    | 0.025914   |
| ImprovementRatio     | 0.97322    |
| MeanKL               | 0.006581   |
| Entropy              | 4.0429     |
| Perplexity           | 56.992     |
| AveragePolicyStd     | 0.47584    |
| AveragePolicyStd[0]  | 0.54304    |
| AveragePolicyStd[1]  | 0.4939     |
| AveragePolicyStd[2]  | 0.44844    |
| AveragePolicyStd[3]  | 0.46262    |
| AveragePolicyStd[4]  | 0.46321    |
| AveragePolicyStd[5]  | 0.44382    |
| AverageReturn        | 14.961     |
| MinReturn            | 7.7989     |
| MaxReturn            | 24.443     |
| StdReturn            | 3.0005     |
| AverageEpisodeLength | 22.18      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 42         |
| StdEpisodeLength     | 3.3478     |
| TotalNEpisodes       | 11204      |
| TotalNSamples        | 2.1017e+05 |
| ExplainedVariance    | 0.85989    |
-------------------------------------
[2018-12-22 10:10:44.004138 UTC] Saving snapshot
[2018-12-22 10:10:44.004384 UTC] Starting iteration 42
[2018-12-22 10:10:44.004502 UTC] Start collecting samples
[2018-12-22 10:10:48.576403 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:48.771938 UTC] Performing policy update
[2018-12-22 10:10:48.772702 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:48.862099 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:49.811199 UTC] Performing line search
[2018-12-22 10:10:49.930616 UTC] Updating baseline
[2018-12-22 10:10:51.188532 UTC] Computing logging information
-------------------------------------
| Iteration            | 42         |
| ExpectedImprovement  | 0.0252     |
| ActualImprovement    | 0.024557   |
| ImprovementRatio     | 0.97448    |
| MeanKL               | 0.0067102  |
| Entropy              | 3.9308     |
| Perplexity           | 50.947     |
| AveragePolicyStd     | 0.46718    |
| AveragePolicyStd[0]  | 0.53869    |
| AveragePolicyStd[1]  | 0.4857     |
| AveragePolicyStd[2]  | 0.44109    |
| AveragePolicyStd[3]  | 0.45353    |
| AveragePolicyStd[4]  | 0.44747    |
| AveragePolicyStd[5]  | 0.4366     |
| AverageReturn        | 16.049     |
| MinReturn            | 9.8626     |
| MaxReturn            | 22.218     |
| StdReturn            | 2.7357     |
| AverageEpisodeLength | 22.13      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 28         |
| StdEpisodeLength     | 2.5948     |
| TotalNEpisodes       | 11429      |
| TotalNSamples        | 2.1520e+05 |
| ExplainedVariance    | 0.87491    |
-------------------------------------
[2018-12-22 10:10:51.447765 UTC] Saving snapshot
[2018-12-22 10:10:51.448007 UTC] Starting iteration 43
[2018-12-22 10:10:51.448122 UTC] Start collecting samples
[2018-12-22 10:10:55.998224 UTC] Computing input variables for policy optimization
[2018-12-22 10:10:56.190621 UTC] Performing policy update
[2018-12-22 10:10:56.192551 UTC] Computing gradient in Euclidean space
[2018-12-22 10:10:56.282245 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:10:57.349009 UTC] Performing line search
[2018-12-22 10:10:57.477602 UTC] Updating baseline
[2018-12-22 10:10:58.731261 UTC] Computing logging information
-------------------------------------
| Iteration            | 43         |
| ExpectedImprovement  | 0.027456   |
| ActualImprovement    | 0.02657    |
| ImprovementRatio     | 0.96773    |
| MeanKL               | 0.006756   |
| Entropy              | 3.8278     |
| Perplexity           | 45.962     |
| AveragePolicyStd     | 0.45921    |
| AveragePolicyStd[0]  | 0.52971    |
| AveragePolicyStd[1]  | 0.47505    |
| AveragePolicyStd[2]  | 0.43537    |
| AveragePolicyStd[3]  | 0.44766    |
| AveragePolicyStd[4]  | 0.43776    |
| AveragePolicyStd[5]  | 0.42969    |
| AverageReturn        | 16.612     |
| MinReturn            | 10.647     |
| MaxReturn            | 22.696     |
| StdReturn            | 2.4704     |
| AverageEpisodeLength | 23.24      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 32         |
| StdEpisodeLength     | 2.7134     |
| TotalNEpisodes       | 11646      |
| TotalNSamples        | 2.2021e+05 |
| ExplainedVariance    | 0.88428    |
-------------------------------------
[2018-12-22 10:10:59.001414 UTC] Saving snapshot
[2018-12-22 10:10:59.001707 UTC] Starting iteration 44
[2018-12-22 10:10:59.001855 UTC] Start collecting samples
[2018-12-22 10:11:03.552085 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:03.741018 UTC] Performing policy update
[2018-12-22 10:11:03.741643 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:03.832147 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:04.905678 UTC] Performing line search
[2018-12-22 10:11:05.034697 UTC] Updating baseline
[2018-12-22 10:11:06.290317 UTC] Computing logging information
-------------------------------------
| Iteration            | 44         |
| ExpectedImprovement  | 0.027056   |
| ActualImprovement    | 0.02661    |
| ImprovementRatio     | 0.98353    |
| MeanKL               | 0.0065774  |
| Entropy              | 3.7317     |
| Perplexity           | 41.752     |
| AveragePolicyStd     | 0.45208    |
| AveragePolicyStd[0]  | 0.52546    |
| AveragePolicyStd[1]  | 0.4691     |
| AveragePolicyStd[2]  | 0.42992    |
| AveragePolicyStd[3]  | 0.44245    |
| AveragePolicyStd[4]  | 0.42701    |
| AveragePolicyStd[5]  | 0.41856    |
| AverageReturn        | 17.37      |
| MinReturn            | 8.3832     |
| MaxReturn            | 24.376     |
| StdReturn            | 2.7425     |
| AverageEpisodeLength | 23.3       |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 29         |
| StdEpisodeLength     | 2.7185     |
| TotalNEpisodes       | 11857      |
| TotalNSamples        | 2.2513e+05 |
| ExplainedVariance    | 0.89665    |
-------------------------------------
[2018-12-22 10:11:06.560607 UTC] Saving snapshot
[2018-12-22 10:11:06.560890 UTC] Starting iteration 45
[2018-12-22 10:11:06.561012 UTC] Start collecting samples
[2018-12-22 10:11:10.983718 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:11.172330 UTC] Performing policy update
[2018-12-22 10:11:11.172977 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:11.262778 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:12.219778 UTC] Performing line search
[2018-12-22 10:11:12.335338 UTC] Updating baseline
[2018-12-22 10:11:13.618308 UTC] Computing logging information
-------------------------------------
| Iteration            | 45         |
| ExpectedImprovement  | 0.025554   |
| ActualImprovement    | 0.02513    |
| ImprovementRatio     | 0.98342    |
| MeanKL               | 0.0066473  |
| Entropy              | 3.6376     |
| Perplexity           | 38.002     |
| AveragePolicyStd     | 0.44515    |
| AveragePolicyStd[0]  | 0.51972    |
| AveragePolicyStd[1]  | 0.46284    |
| AveragePolicyStd[2]  | 0.42302    |
| AveragePolicyStd[3]  | 0.43739    |
| AveragePolicyStd[4]  | 0.41446    |
| AveragePolicyStd[5]  | 0.41349    |
| AverageReturn        | 17.987     |
| MinReturn            | 10.908     |
| MaxReturn            | 22.978     |
| StdReturn            | 2.6169     |
| AverageEpisodeLength | 24.49      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 33         |
| StdEpisodeLength     | 3.2109     |
| TotalNEpisodes       | 12066      |
| TotalNSamples        | 2.3016e+05 |
| ExplainedVariance    | 0.91704    |
-------------------------------------
[2018-12-22 10:11:13.891770 UTC] Saving snapshot
[2018-12-22 10:11:13.892019 UTC] Starting iteration 46
[2018-12-22 10:11:13.892139 UTC] Start collecting samples
[2018-12-22 10:11:18.404896 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:18.591291 UTC] Performing policy update
[2018-12-22 10:11:18.593154 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:18.684870 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:19.761362 UTC] Performing line search
[2018-12-22 10:11:19.889631 UTC] Updating baseline
[2018-12-22 10:11:21.134948 UTC] Computing logging information
-------------------------------------
| Iteration            | 46         |
| ExpectedImprovement  | 0.028349   |
| ActualImprovement    | 0.027823   |
| ImprovementRatio     | 0.98144    |
| MeanKL               | 0.0067581  |
| Entropy              | 3.5346     |
| Perplexity           | 34.28      |
| AveragePolicyStd     | 0.43774    |
| AveragePolicyStd[0]  | 0.51574    |
| AveragePolicyStd[1]  | 0.45526    |
| AveragePolicyStd[2]  | 0.41253    |
| AveragePolicyStd[3]  | 0.43002    |
| AveragePolicyStd[4]  | 0.40524    |
| AveragePolicyStd[5]  | 0.40762    |
| AverageReturn        | 18.402     |
| MinReturn            | 11.645     |
| MaxReturn            | 26.754     |
| StdReturn            | 2.7923     |
| AverageEpisodeLength | 24.81      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 37         |
| StdEpisodeLength     | 3.6102     |
| TotalNEpisodes       | 12270      |
| TotalNSamples        | 2.3517e+05 |
| ExplainedVariance    | 0.90979    |
-------------------------------------
[2018-12-22 10:11:21.411055 UTC] Saving snapshot
[2018-12-22 10:11:21.411288 UTC] Starting iteration 47
[2018-12-22 10:11:21.411417 UTC] Start collecting samples
[2018-12-22 10:11:25.884838 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:26.066036 UTC] Performing policy update
[2018-12-22 10:11:26.066663 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:26.156206 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:27.213251 UTC] Performing line search
[2018-12-22 10:11:27.340883 UTC] Updating baseline
[2018-12-22 10:11:28.585511 UTC] Computing logging information
-------------------------------------
| Iteration            | 47         |
| ExpectedImprovement  | 0.027994   |
| ActualImprovement    | 0.028298   |
| ImprovementRatio     | 1.0109     |
| MeanKL               | 0.0065938  |
| Entropy              | 3.4351     |
| Perplexity           | 31.035     |
| AveragePolicyStd     | 0.43066    |
| AveragePolicyStd[0]  | 0.51011    |
| AveragePolicyStd[1]  | 0.44811    |
| AveragePolicyStd[2]  | 0.40855    |
| AveragePolicyStd[3]  | 0.42254    |
| AveragePolicyStd[4]  | 0.39645    |
| AveragePolicyStd[5]  | 0.39817    |
| AverageReturn        | 20.243     |
| MinReturn            | 11.687     |
| MaxReturn            | 36.12      |
| StdReturn            | 3.4506     |
| AverageEpisodeLength | 26.19      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 41         |
| StdEpisodeLength     | 3.7032     |
| TotalNEpisodes       | 12461      |
| TotalNSamples        | 2.4018e+05 |
| ExplainedVariance    | 0.89318    |
-------------------------------------
[2018-12-22 10:11:28.861290 UTC] Saving snapshot
[2018-12-22 10:11:28.861638 UTC] Starting iteration 48
[2018-12-22 10:11:28.862004 UTC] Start collecting samples
[2018-12-22 10:11:33.417249 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:33.609569 UTC] Performing policy update
[2018-12-22 10:11:33.610256 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:33.704102 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:34.830390 UTC] Performing line search
[2018-12-22 10:11:34.964567 UTC] Updating baseline
[2018-12-22 10:11:36.275669 UTC] Computing logging information
-------------------------------------
| Iteration            | 48         |
| ExpectedImprovement  | 0.029563   |
| ActualImprovement    | 0.029069   |
| ImprovementRatio     | 0.98328    |
| MeanKL               | 0.0066508  |
| Entropy              | 3.3721     |
| Perplexity           | 29.14      |
| AveragePolicyStd     | 0.42625    |
| AveragePolicyStd[0]  | 0.50659    |
| AveragePolicyStd[1]  | 0.44417    |
| AveragePolicyStd[2]  | 0.40419    |
| AveragePolicyStd[3]  | 0.41889    |
| AveragePolicyStd[4]  | 0.39004    |
| AveragePolicyStd[5]  | 0.39361    |
| AverageReturn        | 20.216     |
| MinReturn            | 11.874     |
| MaxReturn            | 31.404     |
| StdReturn            | 3.4658     |
| AverageEpisodeLength | 26.58      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 44         |
| StdEpisodeLength     | 4.6951     |
| TotalNEpisodes       | 12649      |
| TotalNSamples        | 2.4526e+05 |
| ExplainedVariance    | 0.90694    |
-------------------------------------
[2018-12-22 10:11:36.587029 UTC] Saving snapshot
[2018-12-22 10:11:36.587267 UTC] Starting iteration 49
[2018-12-22 10:11:36.587386 UTC] Start collecting samples
[2018-12-22 10:11:41.080014 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:41.247487 UTC] Performing policy update
[2018-12-22 10:11:41.248120 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:41.338697 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:42.406618 UTC] Performing line search
[2018-12-22 10:11:42.536290 UTC] Updating baseline
[2018-12-22 10:11:43.780969 UTC] Computing logging information
-------------------------------------
| Iteration            | 49         |
| ExpectedImprovement  | 0.030836   |
| ActualImprovement    | 0.030396   |
| ImprovementRatio     | 0.98574    |
| MeanKL               | 0.0066403  |
| Entropy              | 3.3023     |
| Perplexity           | 27.176     |
| AveragePolicyStd     | 0.42135    |
| AveragePolicyStd[0]  | 0.50046    |
| AveragePolicyStd[1]  | 0.43972    |
| AveragePolicyStd[2]  | 0.3995     |
| AveragePolicyStd[3]  | 0.41586    |
| AveragePolicyStd[4]  | 0.38189    |
| AveragePolicyStd[5]  | 0.39067    |
| AverageReturn        | 21.6       |
| MinReturn            | 14.652     |
| MaxReturn            | 37.533     |
| StdReturn            | 3.6661     |
| AverageEpisodeLength | 28.72      |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 41         |
| StdEpisodeLength     | 4.3476     |
| TotalNEpisodes       | 12820      |
| TotalNSamples        | 2.5019e+05 |
| ExplainedVariance    | 0.91666    |
-------------------------------------
[2018-12-22 10:11:44.069502 UTC] Saving snapshot
[2018-12-22 10:11:44.069794 UTC] Starting iteration 50
[2018-12-22 10:11:44.069933 UTC] Start collecting samples
[2018-12-22 10:11:48.457822 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:48.620284 UTC] Performing policy update
[2018-12-22 10:11:48.621019 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:48.710059 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:49.769933 UTC] Performing line search
[2018-12-22 10:11:49.896270 UTC] Updating baseline
[2018-12-22 10:11:51.141602 UTC] Computing logging information
------------------------------------
| Iteration            | 50        |
| ExpectedImprovement  | 0.029318  |
| ActualImprovement    | 0.029122  |
| ImprovementRatio     | 0.99332   |
| MeanKL               | 0.0065151 |
| Entropy              | 3.2377    |
| Perplexity           | 25.475    |
| AveragePolicyStd     | 0.41691   |
| AveragePolicyStd[0]  | 0.49686   |
| AveragePolicyStd[1]  | 0.43563   |
| AveragePolicyStd[2]  | 0.39531   |
| AveragePolicyStd[3]  | 0.41128   |
| AveragePolicyStd[4]  | 0.37712   |
| AveragePolicyStd[5]  | 0.38528   |
| AverageReturn        | 22.966    |
| MinReturn            | 13.482    |
| MaxReturn            | 58.835    |
| StdReturn            | 5.3231    |
| AverageEpisodeLength | 29.95     |
| MinEpisodeLength     | 19        |
| MaxEpisodeLength     | 71        |
| StdEpisodeLength     | 6.8006    |
| TotalNEpisodes       | 12987     |
| TotalNSamples        | 2.551e+05 |
| ExplainedVariance    | 0.7939    |
------------------------------------
[2018-12-22 10:11:51.427491 UTC] Saving snapshot
[2018-12-22 10:11:51.435891 UTC] Starting iteration 51
[2018-12-22 10:11:51.436099 UTC] Start collecting samples
[2018-12-22 10:11:55.676304 UTC] Computing input variables for policy optimization
[2018-12-22 10:11:55.835960 UTC] Performing policy update
[2018-12-22 10:11:55.836751 UTC] Computing gradient in Euclidean space
[2018-12-22 10:11:55.929021 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:11:56.984666 UTC] Performing line search
[2018-12-22 10:11:57.111770 UTC] Updating baseline
[2018-12-22 10:11:58.350802 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| ExpectedImprovement  | 0.027487   |
| ActualImprovement    | 0.027058   |
| ImprovementRatio     | 0.98438    |
| MeanKL               | 0.0066643  |
| Entropy              | 3.2126     |
| Perplexity           | 24.843     |
| AveragePolicyStd     | 0.41535    |
| AveragePolicyStd[0]  | 0.49964    |
| AveragePolicyStd[1]  | 0.4341     |
| AveragePolicyStd[2]  | 0.39299    |
| AveragePolicyStd[3]  | 0.40828    |
| AveragePolicyStd[4]  | 0.37573    |
| AveragePolicyStd[5]  | 0.38134    |
| AverageReturn        | 25.398     |
| MinReturn            | 13.507     |
| MaxReturn            | 48.321     |
| StdReturn            | 6.7084     |
| AverageEpisodeLength | 33.59      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 67         |
| StdEpisodeLength     | 8.8012     |
| TotalNEpisodes       | 13138      |
| TotalNSamples        | 2.6015e+05 |
| ExplainedVariance    | 0.82265    |
-------------------------------------
[2018-12-22 10:11:58.642144 UTC] Saving snapshot
[2018-12-22 10:11:58.642392 UTC] Starting iteration 52
[2018-12-22 10:11:58.642513 UTC] Start collecting samples
[2018-12-22 10:12:02.834127 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:02.982756 UTC] Performing policy update
[2018-12-22 10:12:02.983324 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:03.072946 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:04.144964 UTC] Performing line search
[2018-12-22 10:12:04.273578 UTC] Updating baseline
[2018-12-22 10:12:05.525771 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| ExpectedImprovement  | 0.026448   |
| ActualImprovement    | 0.026611   |
| ImprovementRatio     | 1.0062     |
| MeanKL               | 0.0066497  |
| Entropy              | 3.1608     |
| Perplexity           | 23.589     |
| AveragePolicyStd     | 0.4119     |
| AveragePolicyStd[0]  | 0.49857    |
| AveragePolicyStd[1]  | 0.42958    |
| AveragePolicyStd[2]  | 0.38926    |
| AveragePolicyStd[3]  | 0.40568    |
| AveragePolicyStd[4]  | 0.37171    |
| AveragePolicyStd[5]  | 0.37661    |
| AverageReturn        | 28.971     |
| MinReturn            | 16.371     |
| MaxReturn            | 51.759     |
| StdReturn            | 8.764      |
| AverageEpisodeLength | 36.77      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 66         |
| StdEpisodeLength     | 10.47      |
| TotalNEpisodes       | 13276      |
| TotalNSamples        | 2.6517e+05 |
| ExplainedVariance    | 0.80301    |
-------------------------------------
[2018-12-22 10:12:05.820267 UTC] Saving snapshot
[2018-12-22 10:12:05.820515 UTC] Starting iteration 53
[2018-12-22 10:12:05.820650 UTC] Start collecting samples
[2018-12-22 10:12:09.855857 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:09.997853 UTC] Performing policy update
[2018-12-22 10:12:09.998634 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:10.089725 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:11.162469 UTC] Performing line search
[2018-12-22 10:12:11.291252 UTC] Updating baseline
[2018-12-22 10:12:12.536529 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| ExpectedImprovement  | 0.027511   |
| ActualImprovement    | 0.027284   |
| ImprovementRatio     | 0.99171    |
| MeanKL               | 0.0067356  |
| Entropy              | 3.1333     |
| Perplexity           | 22.949     |
| AveragePolicyStd     | 0.41006    |
| AveragePolicyStd[0]  | 0.49709    |
| AveragePolicyStd[1]  | 0.42815    |
| AveragePolicyStd[2]  | 0.38662    |
| AveragePolicyStd[3]  | 0.40334    |
| AveragePolicyStd[4]  | 0.36772    |
| AveragePolicyStd[5]  | 0.37744    |
| AverageReturn        | 30.576     |
| MinReturn            | 16.111     |
| MaxReturn            | 57.369     |
| StdReturn            | 9.0529     |
| AverageEpisodeLength | 40.57      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 76         |
| StdEpisodeLength     | 10.941     |
| TotalNEpisodes       | 13397      |
| TotalNSamples        | 2.7007e+05 |
| ExplainedVariance    | 0.83347    |
-------------------------------------
[2018-12-22 10:12:12.833996 UTC] Saving snapshot
[2018-12-22 10:12:12.834240 UTC] Starting iteration 54
[2018-12-22 10:12:12.834373 UTC] Start collecting samples
[2018-12-22 10:12:16.788646 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:16.920248 UTC] Performing policy update
[2018-12-22 10:12:16.920916 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:17.011390 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:18.070837 UTC] Performing line search
[2018-12-22 10:12:18.198021 UTC] Updating baseline
[2018-12-22 10:12:19.435201 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| ExpectedImprovement  | 0.025875   |
| ActualImprovement    | 0.025665   |
| ImprovementRatio     | 0.99188    |
| MeanKL               | 0.006687   |
| Entropy              | 3.101      |
| Perplexity           | 22.221     |
| AveragePolicyStd     | 0.40786    |
| AveragePolicyStd[0]  | 0.49356    |
| AveragePolicyStd[1]  | 0.42791    |
| AveragePolicyStd[2]  | 0.38695    |
| AveragePolicyStd[3]  | 0.39942    |
| AveragePolicyStd[4]  | 0.36506    |
| AveragePolicyStd[5]  | 0.37427    |
| AverageReturn        | 34.024     |
| MinReturn            | 19.038     |
| MaxReturn            | 64.633     |
| StdReturn            | 9.9328     |
| AverageEpisodeLength | 44.95      |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 89         |
| StdEpisodeLength     | 13.055     |
| TotalNEpisodes       | 13505      |
| TotalNSamples        | 2.7492e+05 |
| ExplainedVariance    | 0.815      |
-------------------------------------
[2018-12-22 10:12:19.730342 UTC] Saving snapshot
[2018-12-22 10:12:19.730666 UTC] Starting iteration 55
[2018-12-22 10:12:19.730788 UTC] Start collecting samples
[2018-12-22 10:12:23.626711 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:23.757610 UTC] Performing policy update
[2018-12-22 10:12:23.758188 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:23.847537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:24.912123 UTC] Performing line search
[2018-12-22 10:12:25.038334 UTC] Updating baseline
[2018-12-22 10:12:26.356919 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| ExpectedImprovement  | 0.025423   |
| ActualImprovement    | 0.025114   |
| ImprovementRatio     | 0.98786    |
| MeanKL               | 0.006659   |
| Entropy              | 3.0638     |
| Perplexity           | 21.409     |
| AveragePolicyStd     | 0.4054     |
| AveragePolicyStd[0]  | 0.49102    |
| AveragePolicyStd[1]  | 0.42735    |
| AveragePolicyStd[2]  | 0.38613    |
| AveragePolicyStd[3]  | 0.3955     |
| AveragePolicyStd[4]  | 0.36338    |
| AveragePolicyStd[5]  | 0.36902    |
| AverageReturn        | 38.46      |
| MinReturn            | 17.938     |
| MaxReturn            | 67.212     |
| StdReturn            | 11.465     |
| AverageEpisodeLength | 50.64      |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 92         |
| StdEpisodeLength     | 14.894     |
| TotalNEpisodes       | 13603      |
| TotalNSamples        | 2.7991e+05 |
| ExplainedVariance    | 0.79196    |
-------------------------------------
[2018-12-22 10:12:26.655884 UTC] Saving snapshot
[2018-12-22 10:12:26.656141 UTC] Starting iteration 56
[2018-12-22 10:12:26.656259 UTC] Start collecting samples
[2018-12-22 10:12:30.530324 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:30.658188 UTC] Performing policy update
[2018-12-22 10:12:30.659167 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:30.750149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:31.811965 UTC] Performing line search
[2018-12-22 10:12:31.941850 UTC] Updating baseline
[2018-12-22 10:12:33.166288 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| ExpectedImprovement  | 0.024544   |
| ActualImprovement    | 0.024164   |
| ImprovementRatio     | 0.98453    |
| MeanKL               | 0.0067933  |
| Entropy              | 3.0205     |
| Perplexity           | 20.501     |
| AveragePolicyStd     | 0.40254    |
| AveragePolicyStd[0]  | 0.48798    |
| AveragePolicyStd[1]  | 0.42491    |
| AveragePolicyStd[2]  | 0.38515    |
| AveragePolicyStd[3]  | 0.39365    |
| AveragePolicyStd[4]  | 0.36023    |
| AveragePolicyStd[5]  | 0.36335    |
| AverageReturn        | 42.404     |
| MinReturn            | 17.319     |
| MaxReturn            | 70.146     |
| StdReturn            | 12.699     |
| AverageEpisodeLength | 52.78      |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 94         |
| StdEpisodeLength     | 16.424     |
| TotalNEpisodes       | 13699      |
| TotalNSamples        | 2.8498e+05 |
| ExplainedVariance    | 0.81278    |
-------------------------------------
[2018-12-22 10:12:33.462708 UTC] Saving snapshot
[2018-12-22 10:12:33.462970 UTC] Starting iteration 57
[2018-12-22 10:12:33.463093 UTC] Start collecting samples
[2018-12-22 10:12:37.300372 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:37.423236 UTC] Performing policy update
[2018-12-22 10:12:37.424338 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:37.512639 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:38.572761 UTC] Performing line search
[2018-12-22 10:12:38.699382 UTC] Updating baseline
[2018-12-22 10:12:39.935323 UTC] Computing logging information
-------------------------------------
| Iteration            | 57         |
| ExpectedImprovement  | 0.026874   |
| ActualImprovement    | 0.026069   |
| ImprovementRatio     | 0.97006    |
| MeanKL               | 0.0068241  |
| Entropy              | 2.9938     |
| Perplexity           | 19.961     |
| AveragePolicyStd     | 0.40082    |
| AveragePolicyStd[0]  | 0.48633    |
| AveragePolicyStd[1]  | 0.42314    |
| AveragePolicyStd[2]  | 0.38599    |
| AveragePolicyStd[3]  | 0.39242    |
| AveragePolicyStd[4]  | 0.35717    |
| AveragePolicyStd[5]  | 0.35986    |
| AverageReturn        | 43.049     |
| MinReturn            | 19.744     |
| MaxReturn            | 68.556     |
| StdReturn            | 11.521     |
| AverageEpisodeLength | 53.37      |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 103        |
| StdEpisodeLength     | 15.949     |
| TotalNEpisodes       | 13791      |
| TotalNSamples        | 2.8992e+05 |
| ExplainedVariance    | 0.85672    |
-------------------------------------
[2018-12-22 10:12:40.236241 UTC] Saving snapshot
[2018-12-22 10:12:40.236480 UTC] Starting iteration 58
[2018-12-22 10:12:40.236617 UTC] Start collecting samples
[2018-12-22 10:12:43.922016 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:44.037983 UTC] Performing policy update
[2018-12-22 10:12:44.038565 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:44.127841 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:45.180471 UTC] Performing line search
[2018-12-22 10:12:45.305770 UTC] Updating baseline
[2018-12-22 10:12:46.616641 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| ExpectedImprovement  | 0.022519   |
| ActualImprovement    | 0.021842   |
| ImprovementRatio     | 0.96991    |
| MeanKL               | 0.0066891  |
| Entropy              | 2.938      |
| Perplexity           | 18.878     |
| AveragePolicyStd     | 0.39719    |
| AveragePolicyStd[0]  | 0.48238    |
| AveragePolicyStd[1]  | 0.41815    |
| AveragePolicyStd[2]  | 0.38641    |
| AveragePolicyStd[3]  | 0.39061    |
| AveragePolicyStd[4]  | 0.35232    |
| AveragePolicyStd[5]  | 0.35325    |
| AverageReturn        | 48.313     |
| MinReturn            | 19.744     |
| MaxReturn            | 82.025     |
| StdReturn            | 12.002     |
| AverageEpisodeLength | 64.25      |
| MinEpisodeLength     | 30         |
| MaxEpisodeLength     | 134        |
| StdEpisodeLength     | 20.855     |
| TotalNEpisodes       | 13865      |
| TotalNSamples        | 2.9495e+05 |
| ExplainedVariance    | 0.87135    |
-------------------------------------
[2018-12-22 10:12:46.917417 UTC] Saving snapshot
[2018-12-22 10:12:46.917728 UTC] Starting iteration 59
[2018-12-22 10:12:46.917889 UTC] Start collecting samples
[2018-12-22 10:12:50.580801 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:50.696502 UTC] Performing policy update
[2018-12-22 10:12:50.697206 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:50.787693 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:51.852056 UTC] Performing line search
[2018-12-22 10:12:51.978339 UTC] Updating baseline
[2018-12-22 10:12:53.210441 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| ExpectedImprovement  | 0.021172   |
| ActualImprovement    | 0.020522   |
| ImprovementRatio     | 0.96933    |
| MeanKL               | 0.0069286  |
| Entropy              | 2.8851     |
| Perplexity           | 17.905     |
| AveragePolicyStd     | 0.39378    |
| AveragePolicyStd[0]  | 0.4785     |
| AveragePolicyStd[1]  | 0.41774    |
| AveragePolicyStd[2]  | 0.38337    |
| AveragePolicyStd[3]  | 0.38522    |
| AveragePolicyStd[4]  | 0.347      |
| AveragePolicyStd[5]  | 0.35084    |
| AverageReturn        | 52.132     |
| MinReturn            | 26.949     |
| MaxReturn            | 94.92      |
| StdReturn            | 12.653     |
| AverageEpisodeLength | 68.86      |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 194        |
| StdEpisodeLength     | 24.539     |
| TotalNEpisodes       | 13938      |
| TotalNSamples        | 2.9996e+05 |
| ExplainedVariance    | 0.83839    |
-------------------------------------
[2018-12-22 10:12:53.512832 UTC] Saving snapshot
[2018-12-22 10:12:53.513097 UTC] Starting iteration 60
[2018-12-22 10:12:53.513218 UTC] Start collecting samples
[2018-12-22 10:12:57.057017 UTC] Computing input variables for policy optimization
[2018-12-22 10:12:57.160898 UTC] Performing policy update
[2018-12-22 10:12:57.161450 UTC] Computing gradient in Euclidean space
[2018-12-22 10:12:57.251254 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:12:58.306665 UTC] Performing line search
[2018-12-22 10:12:58.432303 UTC] Updating baseline
[2018-12-22 10:12:59.646803 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| ExpectedImprovement  | 0.017684   |
| ActualImprovement    | 0.017583   |
| ImprovementRatio     | 0.99429    |
| MeanKL               | 0.0066036  |
| Entropy              | 2.8423     |
| Perplexity           | 17.155     |
| AveragePolicyStd     | 0.39111    |
| AveragePolicyStd[0]  | 0.47574    |
| AveragePolicyStd[1]  | 0.41735    |
| AveragePolicyStd[2]  | 0.38255    |
| AveragePolicyStd[3]  | 0.38271    |
| AveragePolicyStd[4]  | 0.34321    |
| AveragePolicyStd[5]  | 0.34512    |
| AverageReturn        | 56.357     |
| MinReturn            | 26.949     |
| MaxReturn            | 94.92      |
| StdReturn            | 13.798     |
| AverageEpisodeLength | 76.42      |
| MinEpisodeLength     | 36         |
| MaxEpisodeLength     | 194        |
| StdEpisodeLength     | 28.524     |
| TotalNEpisodes       | 13994      |
| TotalNSamples        | 3.0448e+05 |
| ExplainedVariance    | 0.82267    |
-------------------------------------
[2018-12-22 10:12:59.952996 UTC] Saving snapshot
[2018-12-22 10:12:59.961065 UTC] Starting iteration 61
[2018-12-22 10:12:59.961254 UTC] Start collecting samples
[2018-12-22 10:13:03.569122 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:03.676335 UTC] Performing policy update
[2018-12-22 10:13:03.676987 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:03.769480 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:04.824950 UTC] Performing line search
[2018-12-22 10:13:04.950799 UTC] Updating baseline
[2018-12-22 10:13:06.173854 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| ExpectedImprovement  | 0.018504   |
| ActualImprovement    | 0.018099   |
| ImprovementRatio     | 0.9781     |
| MeanKL               | 0.0066142  |
| Entropy              | 2.796      |
| Perplexity           | 16.379     |
| AveragePolicyStd     | 0.38836    |
| AveragePolicyStd[0]  | 0.4751     |
| AveragePolicyStd[1]  | 0.41608    |
| AveragePolicyStd[2]  | 0.38426    |
| AveragePolicyStd[3]  | 0.37756    |
| AveragePolicyStd[4]  | 0.33727    |
| AveragePolicyStd[5]  | 0.33987    |
| AverageReturn        | 60.702     |
| MinReturn            | 32.532     |
| MaxReturn            | 131.27     |
| StdReturn            | 16.186     |
| AverageEpisodeLength | 84.18      |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 217        |
| StdEpisodeLength     | 33.328     |
| TotalNEpisodes       | 14055      |
| TotalNSamples        | 3.0958e+05 |
| ExplainedVariance    | 0.84408    |
-------------------------------------
[2018-12-22 10:13:06.477134 UTC] Saving snapshot
[2018-12-22 10:13:06.477392 UTC] Starting iteration 62
[2018-12-22 10:13:06.477513 UTC] Start collecting samples
[2018-12-22 10:13:09.973371 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:10.073662 UTC] Performing policy update
[2018-12-22 10:13:10.074407 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:10.163121 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:11.216668 UTC] Performing line search
[2018-12-22 10:13:11.341432 UTC] Updating baseline
[2018-12-22 10:13:12.567659 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| ExpectedImprovement  | 0.01748    |
| ActualImprovement    | 0.017261   |
| ImprovementRatio     | 0.98748    |
| MeanKL               | 0.00648    |
| Entropy              | 2.7453     |
| Perplexity           | 15.569     |
| AveragePolicyStd     | 0.38527    |
| AveragePolicyStd[0]  | 0.47326    |
| AveragePolicyStd[1]  | 0.41593    |
| AveragePolicyStd[2]  | 0.37966    |
| AveragePolicyStd[3]  | 0.37407    |
| AveragePolicyStd[4]  | 0.33304    |
| AveragePolicyStd[5]  | 0.33565    |
| AverageReturn        | 64.332     |
| MinReturn            | 32.532     |
| MaxReturn            | 166.16     |
| StdReturn            | 20.619     |
| AverageEpisodeLength | 88.83      |
| MinEpisodeLength     | 45         |
| MaxEpisodeLength     | 318        |
| StdEpisodeLength     | 44.532     |
| TotalNEpisodes       | 14105      |
| TotalNSamples        | 3.1446e+05 |
| ExplainedVariance    | 0.79176    |
-------------------------------------
[2018-12-22 10:13:12.873112 UTC] Saving snapshot
[2018-12-22 10:13:12.873364 UTC] Starting iteration 63
[2018-12-22 10:13:12.873485 UTC] Start collecting samples
[2018-12-22 10:13:16.363500 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:16.461761 UTC] Performing policy update
[2018-12-22 10:13:16.462379 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:16.551461 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:17.598332 UTC] Performing line search
[2018-12-22 10:13:17.724095 UTC] Updating baseline
[2018-12-22 10:13:18.952241 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| ExpectedImprovement  | 0.018188   |
| ActualImprovement    | 0.018082   |
| ImprovementRatio     | 0.99419    |
| MeanKL               | 0.0066716  |
| Entropy              | 2.7371     |
| Perplexity           | 15.441     |
| AveragePolicyStd     | 0.38462    |
| AveragePolicyStd[0]  | 0.4678     |
| AveragePolicyStd[1]  | 0.41777    |
| AveragePolicyStd[2]  | 0.37942    |
| AveragePolicyStd[3]  | 0.3748     |
| AveragePolicyStd[4]  | 0.33       |
| AveragePolicyStd[5]  | 0.33793    |
| AverageReturn        | 72.036     |
| MinReturn            | 36.185     |
| MaxReturn            | 192.17     |
| StdReturn            | 24.318     |
| AverageEpisodeLength | 101.68     |
| MinEpisodeLength     | 42         |
| MaxEpisodeLength     | 375        |
| StdEpisodeLength     | 55.641     |
| TotalNEpisodes       | 14152      |
| TotalNSamples        | 3.1951e+05 |
| ExplainedVariance    | 0.8189     |
-------------------------------------
[2018-12-22 10:13:19.259975 UTC] Saving snapshot
[2018-12-22 10:13:19.260274 UTC] Starting iteration 64
[2018-12-22 10:13:19.260405 UTC] Start collecting samples
[2018-12-22 10:13:22.742073 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:22.840936 UTC] Performing policy update
[2018-12-22 10:13:22.841561 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:22.930749 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:23.987307 UTC] Performing line search
[2018-12-22 10:13:24.111238 UTC] Updating baseline
[2018-12-22 10:13:25.400069 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| ExpectedImprovement  | 0.018868   |
| ActualImprovement    | 0.017641   |
| ImprovementRatio     | 0.93499    |
| MeanKL               | 0.0067529  |
| Entropy              | 2.7086     |
| Perplexity           | 15.009     |
| AveragePolicyStd     | 0.38295    |
| AveragePolicyStd[0]  | 0.46931    |
| AveragePolicyStd[1]  | 0.4166     |
| AveragePolicyStd[2]  | 0.37351    |
| AveragePolicyStd[3]  | 0.37439    |
| AveragePolicyStd[4]  | 0.32974    |
| AveragePolicyStd[5]  | 0.33415    |
| AverageReturn        | 73.956     |
| MinReturn            | 13.407     |
| MaxReturn            | 192.17     |
| StdReturn            | 24.384     |
| AverageEpisodeLength | 105.1      |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 375        |
| StdEpisodeLength     | 53.909     |
| TotalNEpisodes       | 14199      |
| TotalNSamples        | 3.2441e+05 |
| ExplainedVariance    | 0.82281    |
-------------------------------------
[2018-12-22 10:13:25.705063 UTC] Saving snapshot
[2018-12-22 10:13:25.705304 UTC] Starting iteration 65
[2018-12-22 10:13:25.705426 UTC] Start collecting samples
[2018-12-22 10:13:29.191123 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:29.291234 UTC] Performing policy update
[2018-12-22 10:13:29.291816 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:29.381698 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:30.432885 UTC] Performing line search
[2018-12-22 10:13:30.557999 UTC] Updating baseline
[2018-12-22 10:13:31.774557 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| ExpectedImprovement  | 0.019137   |
| ActualImprovement    | 0.018971   |
| ImprovementRatio     | 0.99134    |
| MeanKL               | 0.0066923  |
| Entropy              | 2.6834     |
| Perplexity           | 14.634     |
| AveragePolicyStd     | 0.38145    |
| AveragePolicyStd[0]  | 0.46708    |
| AveragePolicyStd[1]  | 0.41752    |
| AveragePolicyStd[2]  | 0.37435    |
| AveragePolicyStd[3]  | 0.37162    |
| AveragePolicyStd[4]  | 0.32669    |
| AveragePolicyStd[5]  | 0.33141    |
| AverageReturn        | 73.728     |
| MinReturn            | 13.407     |
| MaxReturn            | 163.24     |
| StdReturn            | 23.772     |
| AverageEpisodeLength | 103.51     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 272        |
| StdEpisodeLength     | 48.027     |
| TotalNEpisodes       | 14247      |
| TotalNSamples        | 3.2942e+05 |
| ExplainedVariance    | 0.85733    |
-------------------------------------
[2018-12-22 10:13:32.086312 UTC] Saving snapshot
[2018-12-22 10:13:32.086545 UTC] Starting iteration 66
[2018-12-22 10:13:32.086687 UTC] Start collecting samples
[2018-12-22 10:13:35.502813 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:35.597890 UTC] Performing policy update
[2018-12-22 10:13:35.598490 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:35.688640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:36.739635 UTC] Performing line search
[2018-12-22 10:13:36.865512 UTC] Updating baseline
[2018-12-22 10:13:38.131997 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| ExpectedImprovement  | 0.019672   |
| ActualImprovement    | 0.018986   |
| ImprovementRatio     | 0.96515    |
| MeanKL               | 0.0066356  |
| Entropy              | 2.645      |
| Perplexity           | 14.084     |
| AveragePolicyStd     | 0.37923    |
| AveragePolicyStd[0]  | 0.46853    |
| AveragePolicyStd[1]  | 0.41654    |
| AveragePolicyStd[2]  | 0.36659    |
| AveragePolicyStd[3]  | 0.37089    |
| AveragePolicyStd[4]  | 0.32351    |
| AveragePolicyStd[5]  | 0.32929    |
| AverageReturn        | 78.118     |
| MinReturn            | 26.254     |
| MaxReturn            | 163.24     |
| StdReturn            | 24.27      |
| AverageEpisodeLength | 110.29     |
| MinEpisodeLength     | 48         |
| MaxEpisodeLength     | 272        |
| StdEpisodeLength     | 49.906     |
| TotalNEpisodes       | 14289      |
| TotalNSamples        | 3.3447e+05 |
| ExplainedVariance    | 0.82281    |
-------------------------------------
[2018-12-22 10:13:38.435772 UTC] Saving snapshot
[2018-12-22 10:13:38.436593 UTC] Starting iteration 67
[2018-12-22 10:13:38.436714 UTC] Start collecting samples
[2018-12-22 10:13:41.818980 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:41.912175 UTC] Performing policy update
[2018-12-22 10:13:41.912744 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:42.002303 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:43.046393 UTC] Performing line search
[2018-12-22 10:13:43.173116 UTC] Updating baseline
[2018-12-22 10:13:44.398336 UTC] Computing logging information
-------------------------------------
| Iteration            | 67         |
| ExpectedImprovement  | 0.020952   |
| ActualImprovement    | 0.02002    |
| ImprovementRatio     | 0.9555     |
| MeanKL               | 0.0066097  |
| Entropy              | 2.6226     |
| Perplexity           | 13.771     |
| AveragePolicyStd     | 0.37788    |
| AveragePolicyStd[0]  | 0.46651    |
| AveragePolicyStd[1]  | 0.41696    |
| AveragePolicyStd[2]  | 0.36445    |
| AveragePolicyStd[3]  | 0.37081    |
| AveragePolicyStd[4]  | 0.32139    |
| AveragePolicyStd[5]  | 0.32718    |
| AverageReturn        | 80.037     |
| MinReturn            | 28.832     |
| MaxReturn            | 154.81     |
| StdReturn            | 23.953     |
| AverageEpisodeLength | 112.47     |
| MinEpisodeLength     | 48         |
| MaxEpisodeLength     | 263        |
| StdEpisodeLength     | 47.097     |
| TotalNEpisodes       | 14328      |
| TotalNSamples        | 3.3883e+05 |
| ExplainedVariance    | 0.84124    |
-------------------------------------
[2018-12-22 10:13:44.709111 UTC] Saving snapshot
[2018-12-22 10:13:44.709363 UTC] Starting iteration 68
[2018-12-22 10:13:44.709482 UTC] Start collecting samples
[2018-12-22 10:13:48.133364 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:48.228919 UTC] Performing policy update
[2018-12-22 10:13:48.229496 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:48.317229 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:49.252229 UTC] Performing line search
[2018-12-22 10:13:49.365243 UTC] Updating baseline
[2018-12-22 10:13:50.759100 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| ExpectedImprovement  | 0.018491   |
| ActualImprovement    | 0.018402   |
| ImprovementRatio     | 0.99517    |
| MeanKL               | 0.0066025  |
| Entropy              | 2.5687     |
| Perplexity           | 13.049     |
| AveragePolicyStd     | 0.37466    |
| AveragePolicyStd[0]  | 0.4626     |
| AveragePolicyStd[1]  | 0.41518    |
| AveragePolicyStd[2]  | 0.36275    |
| AveragePolicyStd[3]  | 0.36907    |
| AveragePolicyStd[4]  | 0.31539    |
| AveragePolicyStd[5]  | 0.32297    |
| AverageReturn        | 85.744     |
| MinReturn            | 28.832     |
| MaxReturn            | 182.92     |
| StdReturn            | 28.687     |
| AverageEpisodeLength | 122        |
| MinEpisodeLength     | 50         |
| MaxEpisodeLength     | 312        |
| StdEpisodeLength     | 54.686     |
| TotalNEpisodes       | 14371      |
| TotalNSamples        | 3.4440e+05 |
| ExplainedVariance    | 0.85964    |
-------------------------------------
[2018-12-22 10:13:51.064834 UTC] Saving snapshot
[2018-12-22 10:13:51.065078 UTC] Starting iteration 69
[2018-12-22 10:13:51.065193 UTC] Start collecting samples
[2018-12-22 10:13:54.460372 UTC] Computing input variables for policy optimization
[2018-12-22 10:13:54.556183 UTC] Performing policy update
[2018-12-22 10:13:54.557975 UTC] Computing gradient in Euclidean space
[2018-12-22 10:13:54.646327 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:13:55.690999 UTC] Performing line search
[2018-12-22 10:13:55.817926 UTC] Updating baseline
[2018-12-22 10:13:57.028623 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| ExpectedImprovement  | 0.018555   |
| ActualImprovement    | 0.018032   |
| ImprovementRatio     | 0.97178    |
| MeanKL               | 0.0067218  |
| Entropy              | 2.5443     |
| Perplexity           | 12.735     |
| AveragePolicyStd     | 0.37303    |
| AveragePolicyStd[0]  | 0.45978    |
| AveragePolicyStd[1]  | 0.4117     |
| AveragePolicyStd[2]  | 0.36423    |
| AveragePolicyStd[3]  | 0.36513    |
| AveragePolicyStd[4]  | 0.31383    |
| AveragePolicyStd[5]  | 0.32353    |
| AverageReturn        | 88.774     |
| MinReturn            | 50.559     |
| MaxReturn            | 204.79     |
| StdReturn            | 29.585     |
| AverageEpisodeLength | 124.85     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 363        |
| StdEpisodeLength     | 57.774     |
| TotalNEpisodes       | 14410      |
| TotalNSamples        | 3.4914e+05 |
| ExplainedVariance    | 0.91523    |
-------------------------------------
[2018-12-22 10:13:57.335990 UTC] Saving snapshot
[2018-12-22 10:13:57.336224 UTC] Starting iteration 70
[2018-12-22 10:13:57.336339 UTC] Start collecting samples
[2018-12-22 10:14:00.707913 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:00.799091 UTC] Performing policy update
[2018-12-22 10:14:00.799698 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:00.888637 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:01.949202 UTC] Performing line search
[2018-12-22 10:14:02.074609 UTC] Updating baseline
[2018-12-22 10:14:03.277950 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| ExpectedImprovement  | 0.020452   |
| ActualImprovement    | 0.019729   |
| ImprovementRatio     | 0.96468    |
| MeanKL               | 0.0066748  |
| Entropy              | 2.5263     |
| Perplexity           | 12.508     |
| AveragePolicyStd     | 0.37191    |
| AveragePolicyStd[0]  | 0.45603    |
| AveragePolicyStd[1]  | 0.41149    |
| AveragePolicyStd[2]  | 0.36737    |
| AveragePolicyStd[3]  | 0.36345    |
| AveragePolicyStd[4]  | 0.3124     |
| AveragePolicyStd[5]  | 0.32073    |
| AverageReturn        | 92.755     |
| MinReturn            | 50.559     |
| MaxReturn            | 280.77     |
| StdReturn            | 34.873     |
| AverageEpisodeLength | 128.76     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 478        |
| StdEpisodeLength     | 67.851     |
| TotalNEpisodes       | 14445      |
| TotalNSamples        | 3.5408e+05 |
| ExplainedVariance    | 0.93739    |
-------------------------------------
[2018-12-22 10:14:03.591277 UTC] Saving snapshot
[2018-12-22 10:14:03.598931 UTC] Starting iteration 71
[2018-12-22 10:14:03.599120 UTC] Start collecting samples
[2018-12-22 10:14:06.858931 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:06.942194 UTC] Performing policy update
[2018-12-22 10:14:06.942923 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:07.031624 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:08.087475 UTC] Performing line search
[2018-12-22 10:14:08.212988 UTC] Updating baseline
[2018-12-22 10:14:09.579132 UTC] Computing logging information
-------------------------------------
| Iteration            | 71         |
| ExpectedImprovement  | 0.018022   |
| ActualImprovement    | 0.017682   |
| ImprovementRatio     | 0.9811     |
| MeanKL               | 0.0067518  |
| Entropy              | 2.4966     |
| Perplexity           | 12.141     |
| AveragePolicyStd     | 0.37013    |
| AveragePolicyStd[0]  | 0.45232    |
| AveragePolicyStd[1]  | 0.41249    |
| AveragePolicyStd[2]  | 0.36468    |
| AveragePolicyStd[3]  | 0.36341    |
| AveragePolicyStd[4]  | 0.31072    |
| AveragePolicyStd[5]  | 0.31717    |
| AverageReturn        | 97.172     |
| MinReturn            | 50.559     |
| MaxReturn            | 280.77     |
| StdReturn            | 38.618     |
| AverageEpisodeLength | 138.29     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 478        |
| StdEpisodeLength     | 75.925     |
| TotalNEpisodes       | 14468      |
| TotalNSamples        | 3.5783e+05 |
| ExplainedVariance    | 0.84699    |
-------------------------------------
[2018-12-22 10:14:09.896344 UTC] Saving snapshot
[2018-12-22 10:14:09.896610 UTC] Starting iteration 72
[2018-12-22 10:14:09.896740 UTC] Start collecting samples
[2018-12-22 10:14:13.213957 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:13.301012 UTC] Performing policy update
[2018-12-22 10:14:13.301891 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:13.391311 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:14.436826 UTC] Performing line search
[2018-12-22 10:14:14.560914 UTC] Updating baseline
[2018-12-22 10:14:15.759421 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| ExpectedImprovement  | 0.016195   |
| ActualImprovement    | 0.016149   |
| ImprovementRatio     | 0.99712    |
| MeanKL               | 0.0069616  |
| Entropy              | 2.4543     |
| Perplexity           | 11.638     |
| AveragePolicyStd     | 0.36738    |
| AveragePolicyStd[0]  | 0.44696    |
| AveragePolicyStd[1]  | 0.40912    |
| AveragePolicyStd[2]  | 0.35954    |
| AveragePolicyStd[3]  | 0.36214    |
| AveragePolicyStd[4]  | 0.30907    |
| AveragePolicyStd[5]  | 0.31744    |
| AverageReturn        | 108.7      |
| MinReturn            | 59.936     |
| MaxReturn            | 280.77     |
| StdReturn            | 46.124     |
| AverageEpisodeLength | 160.18     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 478        |
| StdEpisodeLength     | 91.854     |
| TotalNEpisodes       | 14497      |
| TotalNSamples        | 3.6374e+05 |
| ExplainedVariance    | 0.86614    |
-------------------------------------
[2018-12-22 10:14:16.074156 UTC] Saving snapshot
[2018-12-22 10:14:16.074430 UTC] Starting iteration 73
[2018-12-22 10:14:16.074551 UTC] Start collecting samples
[2018-12-22 10:14:19.302761 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:19.386260 UTC] Performing policy update
[2018-12-22 10:14:19.387151 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:19.477381 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:20.529068 UTC] Performing line search
[2018-12-22 10:14:20.654446 UTC] Updating baseline
[2018-12-22 10:14:21.848417 UTC] Computing logging information
------------------------------------
| Iteration            | 73        |
| ExpectedImprovement  | 0.01674   |
| ActualImprovement    | 0.016007  |
| ImprovementRatio     | 0.95626   |
| MeanKL               | 0.0067431 |
| Entropy              | 2.427     |
| Perplexity           | 11.325    |
| AveragePolicyStd     | 0.36578   |
| AveragePolicyStd[0]  | 0.44157   |
| AveragePolicyStd[1]  | 0.41062   |
| AveragePolicyStd[2]  | 0.3615    |
| AveragePolicyStd[3]  | 0.36155   |
| AveragePolicyStd[4]  | 0.30536   |
| AveragePolicyStd[5]  | 0.3141    |
| AverageReturn        | 118.26    |
| MinReturn            | 59.936    |
| MaxReturn            | 395.9     |
| StdReturn            | 54.804    |
| AverageEpisodeLength | 177.98    |
| MinEpisodeLength     | 66        |
| MaxEpisodeLength     | 710       |
| StdEpisodeLength     | 107.38    |
| TotalNEpisodes       | 14518     |
| TotalNSamples        | 3.681e+05 |
| ExplainedVariance    | 0.85044   |
------------------------------------
[2018-12-22 10:14:22.163448 UTC] Saving snapshot
[2018-12-22 10:14:22.163700 UTC] Starting iteration 74
[2018-12-22 10:14:22.163820 UTC] Start collecting samples
[2018-12-22 10:14:25.383418 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:25.465950 UTC] Performing policy update
[2018-12-22 10:14:25.466564 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:25.554965 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:26.600383 UTC] Performing line search
[2018-12-22 10:14:26.724060 UTC] Updating baseline
[2018-12-22 10:14:28.006973 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| ExpectedImprovement  | 0.018566   |
| ActualImprovement    | 0.018223   |
| ImprovementRatio     | 0.9815     |
| MeanKL               | 0.0067296  |
| Entropy              | 2.3889     |
| Perplexity           | 10.902     |
| AveragePolicyStd     | 0.36334    |
| AveragePolicyStd[0]  | 0.43213    |
| AveragePolicyStd[1]  | 0.40878    |
| AveragePolicyStd[2]  | 0.36324    |
| AveragePolicyStd[3]  | 0.36188    |
| AveragePolicyStd[4]  | 0.30266    |
| AveragePolicyStd[5]  | 0.31135    |
| AverageReturn        | 129.49     |
| MinReturn            | 59.936     |
| MaxReturn            | 395.9      |
| StdReturn            | 62.086     |
| AverageEpisodeLength | 197.11     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 710        |
| StdEpisodeLength     | 118.82     |
| TotalNEpisodes       | 14537      |
| TotalNSamples        | 3.7242e+05 |
| ExplainedVariance    | 0.90347    |
-------------------------------------
[2018-12-22 10:14:28.320333 UTC] Saving snapshot
[2018-12-22 10:14:28.320600 UTC] Starting iteration 75
[2018-12-22 10:14:28.320722 UTC] Start collecting samples
[2018-12-22 10:14:31.618260 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:31.703659 UTC] Performing policy update
[2018-12-22 10:14:31.704212 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:31.797140 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:32.859544 UTC] Performing line search
[2018-12-22 10:14:32.983681 UTC] Updating baseline
[2018-12-22 10:14:34.179498 UTC] Computing logging information
-------------------------------------
| Iteration            | 75         |
| ExpectedImprovement  | 0.021419   |
| ActualImprovement    | 0.021092   |
| ImprovementRatio     | 0.98475    |
| MeanKL               | 0.0066658  |
| Entropy              | 2.3736     |
| Perplexity           | 10.736     |
| AveragePolicyStd     | 0.36252    |
| AveragePolicyStd[0]  | 0.43042    |
| AveragePolicyStd[1]  | 0.41038    |
| AveragePolicyStd[2]  | 0.36135    |
| AveragePolicyStd[3]  | 0.36259    |
| AveragePolicyStd[4]  | 0.30007    |
| AveragePolicyStd[5]  | 0.3103     |
| AverageReturn        | 138.89     |
| MinReturn            | 59.936     |
| MaxReturn            | 395.9      |
| StdReturn            | 68.229     |
| AverageEpisodeLength | 210.74     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 710        |
| StdEpisodeLength     | 126.34     |
| TotalNEpisodes       | 14563      |
| TotalNSamples        | 3.7826e+05 |
| ExplainedVariance    | 0.93182    |
-------------------------------------
[2018-12-22 10:14:34.492102 UTC] Saving snapshot
[2018-12-22 10:14:34.492359 UTC] Starting iteration 76
[2018-12-22 10:14:34.492481 UTC] Start collecting samples
[2018-12-22 10:14:37.681713 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:37.763309 UTC] Performing policy update
[2018-12-22 10:14:37.763898 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:37.852988 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:38.895896 UTC] Performing line search
[2018-12-22 10:14:39.019392 UTC] Updating baseline
[2018-12-22 10:14:40.209779 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| ExpectedImprovement  | 0.015607   |
| ActualImprovement    | 0.015171   |
| ImprovementRatio     | 0.97205    |
| MeanKL               | 0.0067074  |
| Entropy              | 2.3559     |
| Perplexity           | 10.547     |
| AveragePolicyStd     | 0.36142    |
| AveragePolicyStd[0]  | 0.42509    |
| AveragePolicyStd[1]  | 0.40978    |
| AveragePolicyStd[2]  | 0.36311    |
| AveragePolicyStd[3]  | 0.36366    |
| AveragePolicyStd[4]  | 0.29736    |
| AveragePolicyStd[5]  | 0.30951    |
| AverageReturn        | 146.03     |
| MinReturn            | 64.234     |
| MaxReturn            | 584.89     |
| StdReturn            | 80.229     |
| AverageEpisodeLength | 219.57     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.6      |
| TotalNEpisodes       | 14582      |
| TotalNSamples        | 3.8294e+05 |
| ExplainedVariance    | 0.76946    |
-------------------------------------
[2018-12-22 10:14:40.520351 UTC] Saving snapshot
[2018-12-22 10:14:40.520701 UTC] Starting iteration 77
[2018-12-22 10:14:40.520895 UTC] Start collecting samples
[2018-12-22 10:14:43.690180 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:43.769786 UTC] Performing policy update
[2018-12-22 10:14:43.770424 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:43.859843 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:44.907079 UTC] Performing line search
[2018-12-22 10:14:45.030598 UTC] Updating baseline
[2018-12-22 10:14:46.402767 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| ExpectedImprovement  | 0.018223   |
| ActualImprovement    | 0.018357   |
| ImprovementRatio     | 1.0073     |
| MeanKL               | 0.0066424  |
| Entropy              | 2.3266     |
| Perplexity           | 10.243     |
| AveragePolicyStd     | 0.35953    |
| AveragePolicyStd[0]  | 0.41805    |
| AveragePolicyStd[1]  | 0.4043     |
| AveragePolicyStd[2]  | 0.36577    |
| AveragePolicyStd[3]  | 0.36613    |
| AveragePolicyStd[4]  | 0.29438    |
| AveragePolicyStd[5]  | 0.30854    |
| AverageReturn        | 152.48     |
| MinReturn            | 64.234     |
| MaxReturn            | 584.89     |
| StdReturn            | 84.373     |
| AverageEpisodeLength | 229.86     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.49     |
| TotalNEpisodes       | 14598      |
| TotalNSamples        | 3.8685e+05 |
| ExplainedVariance    | 0.87167    |
-------------------------------------
[2018-12-22 10:14:46.713254 UTC] Saving snapshot
[2018-12-22 10:14:46.713516 UTC] Starting iteration 78
[2018-12-22 10:14:46.713652 UTC] Start collecting samples
[2018-12-22 10:14:49.907048 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:49.989844 UTC] Performing policy update
[2018-12-22 10:14:49.990418 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:50.078596 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:51.124854 UTC] Performing line search
[2018-12-22 10:14:51.250471 UTC] Updating baseline
[2018-12-22 10:14:52.612188 UTC] Computing logging information
-------------------------------------
| Iteration            | 78         |
| ExpectedImprovement  | 0.021442   |
| ActualImprovement    | 0.021166   |
| ImprovementRatio     | 0.98713    |
| MeanKL               | 0.0067251  |
| Entropy              | 2.3138     |
| Perplexity           | 10.112     |
| AveragePolicyStd     | 0.35885    |
| AveragePolicyStd[0]  | 0.41477    |
| AveragePolicyStd[1]  | 0.40644    |
| AveragePolicyStd[2]  | 0.36558    |
| AveragePolicyStd[3]  | 0.36617    |
| AveragePolicyStd[4]  | 0.29016    |
| AveragePolicyStd[5]  | 0.30996    |
| AverageReturn        | 165.88     |
| MinReturn            | 64.234     |
| MaxReturn            | 591.6      |
| StdReturn            | 98.857     |
| AverageEpisodeLength | 251.25     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.51     |
| TotalNEpisodes       | 14615      |
| TotalNSamples        | 3.9246e+05 |
| ExplainedVariance    | 0.90417    |
-------------------------------------
[2018-12-22 10:14:52.925647 UTC] Saving snapshot
[2018-12-22 10:14:52.925906 UTC] Starting iteration 79
[2018-12-22 10:14:52.926026 UTC] Start collecting samples
[2018-12-22 10:14:56.047100 UTC] Computing input variables for policy optimization
[2018-12-22 10:14:56.123603 UTC] Performing policy update
[2018-12-22 10:14:56.124252 UTC] Computing gradient in Euclidean space
[2018-12-22 10:14:56.212837 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:14:57.254210 UTC] Performing line search
[2018-12-22 10:14:57.376561 UTC] Updating baseline
[2018-12-22 10:14:58.575855 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| ExpectedImprovement  | 0.017308   |
| ActualImprovement    | 0.017299   |
| ImprovementRatio     | 0.99947    |
| MeanKL               | 0.0066301  |
| Entropy              | 2.2884     |
| Perplexity           | 9.8595     |
| AveragePolicyStd     | 0.35737    |
| AveragePolicyStd[0]  | 0.41164    |
| AveragePolicyStd[1]  | 0.40495    |
| AveragePolicyStd[2]  | 0.36414    |
| AveragePolicyStd[3]  | 0.3671     |
| AveragePolicyStd[4]  | 0.28769    |
| AveragePolicyStd[5]  | 0.30869    |
| AverageReturn        | 172.73     |
| MinReturn            | 64.234     |
| MaxReturn            | 591.6      |
| StdReturn            | 104.47     |
| AverageEpisodeLength | 262.21     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.37     |
| TotalNEpisodes       | 14624      |
| TotalNSamples        | 3.9525e+05 |
| ExplainedVariance    | 0.96004    |
-------------------------------------
[2018-12-22 10:14:58.886942 UTC] Saving snapshot
[2018-12-22 10:14:58.887208 UTC] Starting iteration 80
[2018-12-22 10:14:58.887327 UTC] Start collecting samples
[2018-12-22 10:15:01.969769 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:02.040444 UTC] Performing policy update
[2018-12-22 10:15:02.041164 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:02.129884 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:03.168159 UTC] Performing line search
[2018-12-22 10:15:03.292839 UTC] Updating baseline
[2018-12-22 10:15:04.525219 UTC] Computing logging information
-------------------------------------
| Iteration            | 80         |
| ExpectedImprovement  | 0.019922   |
| ActualImprovement    | 0.019994   |
| ImprovementRatio     | 1.0036     |
| MeanKL               | 0.0064743  |
| Entropy              | 2.2667     |
| Perplexity           | 9.6472     |
| AveragePolicyStd     | 0.35593    |
| AveragePolicyStd[0]  | 0.40822    |
| AveragePolicyStd[1]  | 0.4043     |
| AveragePolicyStd[2]  | 0.36412    |
| AveragePolicyStd[3]  | 0.36131    |
| AveragePolicyStd[4]  | 0.288      |
| AveragePolicyStd[5]  | 0.30964    |
| AverageReturn        | 177.1      |
| MinReturn            | 64.234     |
| MaxReturn            | 591.6      |
| StdReturn            | 107.86     |
| AverageEpisodeLength | 269.5      |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.05     |
| TotalNEpisodes       | 14629      |
| TotalNSamples        | 3.9757e+05 |
| ExplainedVariance    | 0.72483    |
-------------------------------------
[2018-12-22 10:15:04.856212 UTC] Saving snapshot
[2018-12-22 10:15:04.865003 UTC] Starting iteration 81
[2018-12-22 10:15:04.865212 UTC] Start collecting samples
[2018-12-22 10:15:08.266148 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:08.351780 UTC] Performing policy update
[2018-12-22 10:15:08.352515 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:08.445256 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:09.540075 UTC] Performing line search
[2018-12-22 10:15:09.671839 UTC] Updating baseline
[2018-12-22 10:15:10.926521 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| ExpectedImprovement  | 0.012572   |
| ActualImprovement    | 0.012445   |
| ImprovementRatio     | 0.98987    |
| MeanKL               | 0.0070588  |
| Entropy              | 2.2526     |
| Perplexity           | 9.5129     |
| AveragePolicyStd     | 0.35491    |
| AveragePolicyStd[0]  | 0.40404    |
| AveragePolicyStd[1]  | 0.40191    |
| AveragePolicyStd[2]  | 0.3648     |
| AveragePolicyStd[3]  | 0.35971    |
| AveragePolicyStd[4]  | 0.28954    |
| AveragePolicyStd[5]  | 0.30947    |
| AverageReturn        | 214.82     |
| MinReturn            | 64.234     |
| MaxReturn            | 596.57     |
| StdReturn            | 144.2      |
| AverageEpisodeLength | 333.97     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.85     |
| TotalNEpisodes       | 14645      |
| TotalNSamples        | 4.0778e+05 |
| ExplainedVariance    | 0.63884    |
-------------------------------------
[2018-12-22 10:15:11.260804 UTC] Saving snapshot
[2018-12-22 10:15:11.261060 UTC] Starting iteration 82
[2018-12-22 10:15:11.261178 UTC] Start collecting samples
[2018-12-22 10:15:14.618876 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:14.698321 UTC] Performing policy update
[2018-12-22 10:15:14.699026 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:14.792162 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:15.851890 UTC] Performing line search
[2018-12-22 10:15:15.978863 UTC] Updating baseline
[2018-12-22 10:15:17.177081 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| ExpectedImprovement  | 0.015506   |
| ActualImprovement    | 0.014727   |
| ImprovementRatio     | 0.94976    |
| MeanKL               | 0.0067355  |
| Entropy              | 2.2275     |
| Perplexity           | 9.2762     |
| AveragePolicyStd     | 0.35346    |
| AveragePolicyStd[0]  | 0.39674    |
| AveragePolicyStd[1]  | 0.40143    |
| AveragePolicyStd[2]  | 0.36754    |
| AveragePolicyStd[3]  | 0.36103    |
| AveragePolicyStd[4]  | 0.28608    |
| AveragePolicyStd[5]  | 0.30795    |
| AverageReturn        | 224.2      |
| MinReturn            | 64.234     |
| MaxReturn            | 596.57     |
| StdReturn            | 146.36     |
| AverageEpisodeLength | 349.59     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 258.07     |
| TotalNEpisodes       | 14653      |
| TotalNSamples        | 4.1107e+05 |
| ExplainedVariance    | 0.87062    |
-------------------------------------
[2018-12-22 10:15:17.493670 UTC] Saving snapshot
[2018-12-22 10:15:17.493937 UTC] Starting iteration 83
[2018-12-22 10:15:17.494066 UTC] Start collecting samples
[2018-12-22 10:15:20.633097 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:20.710870 UTC] Performing policy update
[2018-12-22 10:15:20.711495 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:20.800929 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:21.843137 UTC] Performing line search
[2018-12-22 10:15:21.966775 UTC] Updating baseline
[2018-12-22 10:15:23.167607 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| ExpectedImprovement  | 0.018459   |
| ActualImprovement    | 0.01859    |
| ImprovementRatio     | 1.0071     |
| MeanKL               | 0.0066814  |
| Entropy              | 2.2183     |
| Perplexity           | 9.1917     |
| AveragePolicyStd     | 0.35288    |
| AveragePolicyStd[0]  | 0.39286    |
| AveragePolicyStd[1]  | 0.40025    |
| AveragePolicyStd[2]  | 0.37147    |
| AveragePolicyStd[3]  | 0.35904    |
| AveragePolicyStd[4]  | 0.28545    |
| AveragePolicyStd[5]  | 0.30818    |
| AverageReturn        | 237.05     |
| MinReturn            | 64.234     |
| MaxReturn            | 608.16     |
| StdReturn            | 150.35     |
| AverageEpisodeLength | 370.58     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 263.46     |
| TotalNEpisodes       | 14663      |
| TotalNSamples        | 4.1531e+05 |
| ExplainedVariance    | 0.93024    |
-------------------------------------
[2018-12-22 10:15:23.479855 UTC] Saving snapshot
[2018-12-22 10:15:23.480132 UTC] Starting iteration 84
[2018-12-22 10:15:23.480249 UTC] Start collecting samples
[2018-12-22 10:15:26.562735 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:26.635670 UTC] Performing policy update
[2018-12-22 10:15:26.636527 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:26.724235 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:27.763178 UTC] Performing line search
[2018-12-22 10:15:27.885989 UTC] Updating baseline
[2018-12-22 10:15:29.094164 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| ExpectedImprovement  | 0.019769   |
| ActualImprovement    | 0.019672   |
| ImprovementRatio     | 0.99506    |
| MeanKL               | 0.0069461  |
| Entropy              | 2.2302     |
| Perplexity           | 9.302      |
| AveragePolicyStd     | 0.35361    |
| AveragePolicyStd[0]  | 0.39287    |
| AveragePolicyStd[1]  | 0.39928    |
| AveragePolicyStd[2]  | 0.37461    |
| AveragePolicyStd[3]  | 0.36106    |
| AveragePolicyStd[4]  | 0.2843     |
| AveragePolicyStd[5]  | 0.30952    |
| AverageReturn        | 251.16     |
| MinReturn            | 64.234     |
| MaxReturn            | 608.16     |
| StdReturn            | 154.49     |
| AverageEpisodeLength | 395.55     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 270.69     |
| TotalNEpisodes       | 14668      |
| TotalNSamples        | 4.1855e+05 |
| ExplainedVariance    | 0.54533    |
-------------------------------------
[2018-12-22 10:15:29.408419 UTC] Saving snapshot
[2018-12-22 10:15:29.408681 UTC] Starting iteration 85
[2018-12-22 10:15:29.408803 UTC] Start collecting samples
[2018-12-22 10:15:32.501449 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:32.576157 UTC] Performing policy update
[2018-12-22 10:15:32.576783 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:32.666024 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:33.702264 UTC] Performing line search
[2018-12-22 10:15:33.828678 UTC] Updating baseline
[2018-12-22 10:15:35.018622 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| ExpectedImprovement  | 0.012757   |
| ActualImprovement    | 0.01208    |
| ImprovementRatio     | 0.94694    |
| MeanKL               | 0.007201   |
| Entropy              | 2.2402     |
| Perplexity           | 9.3948     |
| AveragePolicyStd     | 0.35424    |
| AveragePolicyStd[0]  | 0.39382    |
| AveragePolicyStd[1]  | 0.40103    |
| AveragePolicyStd[2]  | 0.37488    |
| AveragePolicyStd[3]  | 0.36147    |
| AveragePolicyStd[4]  | 0.28401    |
| AveragePolicyStd[5]  | 0.31023    |
| AverageReturn        | 268.06     |
| MinReturn            | 64.234     |
| MaxReturn            | 617.84     |
| StdReturn            | 164.8      |
| AverageEpisodeLength | 424.27     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 288.08     |
| TotalNEpisodes       | 14675      |
| TotalNSamples        | 4.2404e+05 |
| ExplainedVariance    | 0.43893    |
-------------------------------------
[2018-12-22 10:15:35.327007 UTC] Saving snapshot
[2018-12-22 10:15:35.327259 UTC] Starting iteration 86
[2018-12-22 10:15:35.327381 UTC] Start collecting samples
[2018-12-22 10:15:38.395768 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:38.468066 UTC] Performing policy update
[2018-12-22 10:15:38.468932 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:38.557366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:39.594407 UTC] Performing line search
[2018-12-22 10:15:39.717799 UTC] Updating baseline
[2018-12-22 10:15:41.085034 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| ExpectedImprovement  | 0.012648   |
| ActualImprovement    | 0.01187    |
| ImprovementRatio     | 0.93848    |
| MeanKL               | 0.0066792  |
| Entropy              | 2.2459     |
| Perplexity           | 9.4493     |
| AveragePolicyStd     | 0.35459    |
| AveragePolicyStd[0]  | 0.39426    |
| AveragePolicyStd[1]  | 0.40012    |
| AveragePolicyStd[2]  | 0.37653    |
| AveragePolicyStd[3]  | 0.36171    |
| AveragePolicyStd[4]  | 0.28292    |
| AveragePolicyStd[5]  | 0.31202    |
| AverageReturn        | 290.03     |
| MinReturn            | 76.938     |
| MaxReturn            | 617.84     |
| StdReturn            | 171.42     |
| AverageEpisodeLength | 461.33     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 299.64     |
| TotalNEpisodes       | 14680      |
| TotalNSamples        | 4.2856e+05 |
| ExplainedVariance    | 0.31405    |
-------------------------------------
[2018-12-22 10:15:41.399953 UTC] Saving snapshot
[2018-12-22 10:15:41.400204 UTC] Starting iteration 87
[2018-12-22 10:15:41.400320 UTC] Start collecting samples
[2018-12-22 10:15:44.488280 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:44.562273 UTC] Performing policy update
[2018-12-22 10:15:44.562925 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:44.650824 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:45.683861 UTC] Performing line search
[2018-12-22 10:15:45.809691 UTC] Updating baseline
[2018-12-22 10:15:47.165959 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| ExpectedImprovement  | 0.017781   |
| ActualImprovement    | 0.015874   |
| ImprovementRatio     | 0.89273    |
| MeanKL               | 0.0064751  |
| Entropy              | 2.2287     |
| Perplexity           | 9.2876     |
| AveragePolicyStd     | 0.3535     |
| AveragePolicyStd[0]  | 0.39344    |
| AveragePolicyStd[1]  | 0.39815    |
| AveragePolicyStd[2]  | 0.37681    |
| AveragePolicyStd[3]  | 0.35852    |
| AveragePolicyStd[4]  | 0.28484    |
| AveragePolicyStd[5]  | 0.30925    |
| AverageReturn        | 311.29     |
| MinReturn            | 23.396     |
| MaxReturn            | 617.84     |
| StdReturn            | 178.23     |
| AverageEpisodeLength | 498.56     |
| MinEpisodeLength     | 40         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 310.78     |
| TotalNEpisodes       | 14687      |
| TotalNSamples        | 4.3362e+05 |
| ExplainedVariance    | 0.18431    |
-------------------------------------
[2018-12-22 10:15:47.478977 UTC] Saving snapshot
[2018-12-22 10:15:47.479247 UTC] Starting iteration 88
[2018-12-22 10:15:47.479370 UTC] Start collecting samples
[2018-12-22 10:15:50.593470 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:50.669134 UTC] Performing policy update
[2018-12-22 10:15:50.669760 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:50.759026 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:51.795890 UTC] Performing line search
[2018-12-22 10:15:51.924200 UTC] Updating baseline
[2018-12-22 10:15:53.193784 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| ExpectedImprovement  | 0.01465    |
| ActualImprovement    | 0.014171   |
| ImprovementRatio     | 0.96729    |
| MeanKL               | 0.006667   |
| Entropy              | 2.1949     |
| Perplexity           | 8.9787     |
| AveragePolicyStd     | 0.35154    |
| AveragePolicyStd[0]  | 0.39224    |
| AveragePolicyStd[1]  | 0.39577    |
| AveragePolicyStd[2]  | 0.37615    |
| AveragePolicyStd[3]  | 0.35432    |
| AveragePolicyStd[4]  | 0.28315    |
| AveragePolicyStd[5]  | 0.30763    |
| AverageReturn        | 331.04     |
| MinReturn            | 23.396     |
| MaxReturn            | 617.84     |
| StdReturn            | 179.95     |
| AverageEpisodeLength | 531.71     |
| MinEpisodeLength     | 40         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 314.83     |
| TotalNEpisodes       | 14696      |
| TotalNSamples        | 4.3942e+05 |
| ExplainedVariance    | 0.59946    |
-------------------------------------
[2018-12-22 10:15:53.503395 UTC] Saving snapshot
[2018-12-22 10:15:53.503652 UTC] Starting iteration 89
[2018-12-22 10:15:53.503772 UTC] Start collecting samples
[2018-12-22 10:15:56.610726 UTC] Computing input variables for policy optimization
[2018-12-22 10:15:56.685098 UTC] Performing policy update
[2018-12-22 10:15:56.685705 UTC] Computing gradient in Euclidean space
[2018-12-22 10:15:56.773519 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:15:57.815259 UTC] Performing line search
[2018-12-22 10:15:57.942599 UTC] Updating baseline
[2018-12-22 10:15:59.236706 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| ExpectedImprovement  | 0.02299    |
| ActualImprovement    | 0.020104   |
| ImprovementRatio     | 0.87444    |
| MeanKL               | 0.0064592  |
| Entropy              | 2.2        |
| Perplexity           | 9.0247     |
| AveragePolicyStd     | 0.35188    |
| AveragePolicyStd[0]  | 0.39096    |
| AveragePolicyStd[1]  | 0.3992     |
| AveragePolicyStd[2]  | 0.37495    |
| AveragePolicyStd[3]  | 0.35464    |
| AveragePolicyStd[4]  | 0.28213    |
| AveragePolicyStd[5]  | 0.30936    |
| AverageReturn        | 344.07     |
| MinReturn            | 8.8189     |
| MaxReturn            | 617.84     |
| StdReturn            | 183.34     |
| AverageEpisodeLength | 553.48     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 321.5      |
| TotalNEpisodes       | 14704      |
| TotalNSamples        | 4.4373e+05 |
| ExplainedVariance    | 0.43444    |
-------------------------------------
[2018-12-22 10:15:59.550902 UTC] Saving snapshot
[2018-12-22 10:15:59.551145 UTC] Starting iteration 90
[2018-12-22 10:15:59.551263 UTC] Start collecting samples
[2018-12-22 10:16:02.636431 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:02.709236 UTC] Performing policy update
[2018-12-22 10:16:02.709839 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:02.796964 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:03.836979 UTC] Performing line search
[2018-12-22 10:16:03.965022 UTC] Updating baseline
[2018-12-22 10:16:05.417431 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| ExpectedImprovement  | 0.01478    |
| ActualImprovement    | 0.014132   |
| ImprovementRatio     | 0.95612    |
| MeanKL               | 0.0070378  |
| Entropy              | 2.1752     |
| Perplexity           | 8.8037     |
| AveragePolicyStd     | 0.35039    |
| AveragePolicyStd[0]  | 0.3889     |
| AveragePolicyStd[1]  | 0.39839    |
| AveragePolicyStd[2]  | 0.37255    |
| AveragePolicyStd[3]  | 0.35299    |
| AveragePolicyStd[4]  | 0.28236    |
| AveragePolicyStd[5]  | 0.30715    |
| AverageReturn        | 357.44     |
| MinReturn            | 8.8189     |
| MaxReturn            | 621.98     |
| StdReturn            | 185.56     |
| AverageEpisodeLength | 575.98     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 324.83     |
| TotalNEpisodes       | 14709      |
| TotalNSamples        | 4.4725e+05 |
| ExplainedVariance    | 0.31249    |
-------------------------------------
[2018-12-22 10:16:05.725290 UTC] Saving snapshot
[2018-12-22 10:16:05.733292 UTC] Starting iteration 91
[2018-12-22 10:16:05.733503 UTC] Start collecting samples
[2018-12-22 10:16:08.872632 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:08.949601 UTC] Performing policy update
[2018-12-22 10:16:08.950197 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:09.038783 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:10.080912 UTC] Performing line search
[2018-12-22 10:16:10.204753 UTC] Updating baseline
[2018-12-22 10:16:11.384121 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| ExpectedImprovement  | 0.014186   |
| ActualImprovement    | 0.013745   |
| ImprovementRatio     | 0.96895    |
| MeanKL               | 0.006728   |
| Entropy              | 2.1667     |
| Perplexity           | 8.7294     |
| AveragePolicyStd     | 0.34986    |
| AveragePolicyStd[0]  | 0.3912     |
| AveragePolicyStd[1]  | 0.39929    |
| AveragePolicyStd[2]  | 0.3726     |
| AveragePolicyStd[3]  | 0.34477    |
| AveragePolicyStd[4]  | 0.28596    |
| AveragePolicyStd[5]  | 0.30535    |
| AverageReturn        | 373.81     |
| MinReturn            | 8.8189     |
| MaxReturn            | 621.98     |
| StdReturn            | 187.88     |
| AverageEpisodeLength | 603.38     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 329.03     |
| TotalNEpisodes       | 14720      |
| TotalNSamples        | 4.5448e+05 |
| ExplainedVariance    | 0.37567    |
-------------------------------------
[2018-12-22 10:16:11.700368 UTC] Saving snapshot
[2018-12-22 10:16:11.700648 UTC] Starting iteration 92
[2018-12-22 10:16:11.700771 UTC] Start collecting samples
[2018-12-22 10:16:14.846395 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:14.924544 UTC] Performing policy update
[2018-12-22 10:16:14.925135 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:15.015074 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:16.065134 UTC] Performing line search
[2018-12-22 10:16:16.188475 UTC] Updating baseline
[2018-12-22 10:16:17.392989 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| ExpectedImprovement  | 0.014556   |
| ActualImprovement    | 0.014002   |
| ImprovementRatio     | 0.96192    |
| MeanKL               | 0.0068447  |
| Entropy              | 2.1633     |
| Perplexity           | 8.6994     |
| AveragePolicyStd     | 0.34971    |
| AveragePolicyStd[0]  | 0.39041    |
| AveragePolicyStd[1]  | 0.39916    |
| AveragePolicyStd[2]  | 0.37152    |
| AveragePolicyStd[3]  | 0.34719    |
| AveragePolicyStd[4]  | 0.28292    |
| AveragePolicyStd[5]  | 0.30703    |
| AverageReturn        | 388.24     |
| MinReturn            | 8.8189     |
| MaxReturn            | 626        |
| StdReturn            | 191.13     |
| AverageEpisodeLength | 625.52     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 333.96     |
| TotalNEpisodes       | 14730      |
| TotalNSamples        | 4.6050e+05 |
| ExplainedVariance    | 0.70231    |
-------------------------------------
[2018-12-22 10:16:17.708292 UTC] Saving snapshot
[2018-12-22 10:16:17.708551 UTC] Starting iteration 93
[2018-12-22 10:16:17.708691 UTC] Start collecting samples
[2018-12-22 10:16:20.852147 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:20.929625 UTC] Performing policy update
[2018-12-22 10:16:20.930270 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:21.019480 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:22.062781 UTC] Performing line search
[2018-12-22 10:16:22.186979 UTC] Updating baseline
[2018-12-22 10:16:23.486501 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| ExpectedImprovement  | 0.01563    |
| ActualImprovement    | 0.015663   |
| ImprovementRatio     | 1.0022     |
| MeanKL               | 0.0069505  |
| Entropy              | 2.1689     |
| Perplexity           | 8.7491     |
| AveragePolicyStd     | 0.34991    |
| AveragePolicyStd[0]  | 0.38656    |
| AveragePolicyStd[1]  | 0.39879    |
| AveragePolicyStd[2]  | 0.37042    |
| AveragePolicyStd[3]  | 0.35147    |
| AveragePolicyStd[4]  | 0.28275    |
| AveragePolicyStd[5]  | 0.30946    |
| AverageReturn        | 385.78     |
| MinReturn            | 8.8189     |
| MaxReturn            | 626        |
| StdReturn            | 192.17     |
| AverageEpisodeLength | 617.9      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 335.4      |
| TotalNEpisodes       | 14739      |
| TotalNSamples        | 4.6565e+05 |
| ExplainedVariance    | 0.69719    |
-------------------------------------
[2018-12-22 10:16:23.806796 UTC] Saving snapshot
[2018-12-22 10:16:23.807075 UTC] Starting iteration 94
[2018-12-22 10:16:23.807192 UTC] Start collecting samples
[2018-12-22 10:16:26.933808 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:27.012222 UTC] Performing policy update
[2018-12-22 10:16:27.012868 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:27.103513 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:28.162614 UTC] Performing line search
[2018-12-22 10:16:28.286108 UTC] Updating baseline
[2018-12-22 10:16:29.591751 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| ExpectedImprovement  | 0.013678   |
| ActualImprovement    | 0.013258   |
| ImprovementRatio     | 0.96932    |
| MeanKL               | 0.0071134  |
| Entropy              | 2.1726     |
| Perplexity           | 8.7811     |
| AveragePolicyStd     | 0.35009    |
| AveragePolicyStd[0]  | 0.38236    |
| AveragePolicyStd[1]  | 0.40062    |
| AveragePolicyStd[2]  | 0.37206    |
| AveragePolicyStd[3]  | 0.3529     |
| AveragePolicyStd[4]  | 0.28331    |
| AveragePolicyStd[5]  | 0.3093     |
| AverageReturn        | 389.09     |
| MinReturn            | 8.8189     |
| MaxReturn            | 626        |
| StdReturn            | 192.15     |
| AverageEpisodeLength | 620.72     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 334.96     |
| TotalNEpisodes       | 14749      |
| TotalNSamples        | 4.7107e+05 |
| ExplainedVariance    | 0.72405    |
-------------------------------------
[2018-12-22 10:16:29.915559 UTC] Saving snapshot
[2018-12-22 10:16:29.915897 UTC] Starting iteration 95
[2018-12-22 10:16:29.916064 UTC] Start collecting samples
[2018-12-22 10:16:33.025222 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:33.097675 UTC] Performing policy update
[2018-12-22 10:16:33.098468 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:33.186415 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:34.237260 UTC] Performing line search
[2018-12-22 10:16:34.361396 UTC] Updating baseline
[2018-12-22 10:16:36.317785 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| ExpectedImprovement  | 0.017044   |
| ActualImprovement    | 0.016844   |
| ImprovementRatio     | 0.98824    |
| MeanKL               | 0.0069497  |
| Entropy              | 2.181      |
| Perplexity           | 8.8551     |
| AveragePolicyStd     | 0.35062    |
| AveragePolicyStd[0]  | 0.38177    |
| AveragePolicyStd[1]  | 0.40009    |
| AveragePolicyStd[2]  | 0.37591    |
| AveragePolicyStd[3]  | 0.35407    |
| AveragePolicyStd[4]  | 0.28318    |
| AveragePolicyStd[5]  | 0.30873    |
| AverageReturn        | 390.27     |
| MinReturn            | 8.8189     |
| MaxReturn            | 626        |
| StdReturn            | 193.65     |
| AverageEpisodeLength | 621.12     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 337.36     |
| TotalNEpisodes       | 14756      |
| TotalNSamples        | 4.7431e+05 |
| ExplainedVariance    | 0.84082    |
-------------------------------------
[2018-12-22 10:16:36.631349 UTC] Saving snapshot
[2018-12-22 10:16:36.631620 UTC] Starting iteration 96
[2018-12-22 10:16:36.631743 UTC] Start collecting samples
[2018-12-22 10:16:39.859526 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:39.942167 UTC] Performing policy update
[2018-12-22 10:16:39.942812 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:40.037442 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:41.139117 UTC] Performing line search
[2018-12-22 10:16:41.269556 UTC] Updating baseline
[2018-12-22 10:16:42.526097 UTC] Computing logging information
------------------------------------
| Iteration            | 96        |
| ExpectedImprovement  | 0.012636  |
| ActualImprovement    | 0.012805  |
| ImprovementRatio     | 1.0134    |
| MeanKL               | 0.0068878 |
| Entropy              | 2.175     |
| Perplexity           | 8.8025    |
| AveragePolicyStd     | 0.35025   |
| AveragePolicyStd[0]  | 0.3795    |
| AveragePolicyStd[1]  | 0.39796   |
| AveragePolicyStd[2]  | 0.37745   |
| AveragePolicyStd[3]  | 0.35529   |
| AveragePolicyStd[4]  | 0.28248   |
| AveragePolicyStd[5]  | 0.30881   |
| AverageReturn        | 397.84    |
| MinReturn            | 8.8189    |
| MaxReturn            | 637.02    |
| StdReturn            | 192.75    |
| AverageEpisodeLength | 631.42    |
| MinEpisodeLength     | 20        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 334.36    |
| TotalNEpisodes       | 14766     |
| TotalNSamples        | 4.809e+05 |
| ExplainedVariance    | 0.62157   |
------------------------------------
[2018-12-22 10:16:42.861399 UTC] Saving snapshot
[2018-12-22 10:16:42.861724 UTC] Starting iteration 97
[2018-12-22 10:16:42.861892 UTC] Start collecting samples
[2018-12-22 10:16:46.017486 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:46.092535 UTC] Performing policy update
[2018-12-22 10:16:46.093213 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:46.181865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:47.224859 UTC] Performing line search
[2018-12-22 10:16:47.348741 UTC] Updating baseline
[2018-12-22 10:16:48.546923 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| ExpectedImprovement  | 0.013876   |
| ActualImprovement    | 0.013932   |
| ImprovementRatio     | 1.004      |
| MeanKL               | 0.0070675  |
| Entropy              | 2.1824     |
| Perplexity           | 8.8676     |
| AveragePolicyStd     | 0.35065    |
| AveragePolicyStd[0]  | 0.37478    |
| AveragePolicyStd[1]  | 0.39715    |
| AveragePolicyStd[2]  | 0.37961    |
| AveragePolicyStd[3]  | 0.36102    |
| AveragePolicyStd[4]  | 0.28301    |
| AveragePolicyStd[5]  | 0.30831    |
| AverageReturn        | 390.72     |
| MinReturn            | 8.8189     |
| MaxReturn            | 637.02     |
| StdReturn            | 194.02     |
| AverageEpisodeLength | 617.36     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 335.92     |
| TotalNEpisodes       | 14773      |
| TotalNSamples        | 4.8413e+05 |
| ExplainedVariance    | 0.6379     |
-------------------------------------
[2018-12-22 10:16:48.862298 UTC] Saving snapshot
[2018-12-22 10:16:48.862553 UTC] Starting iteration 98
[2018-12-22 10:16:48.862691 UTC] Start collecting samples
[2018-12-22 10:16:51.940514 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:52.013361 UTC] Performing policy update
[2018-12-22 10:16:52.013962 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:52.101077 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:53.095177 UTC] Performing line search
[2018-12-22 10:16:53.218676 UTC] Updating baseline
[2018-12-22 10:16:54.515829 UTC] Computing logging information
-------------------------------------
| Iteration            | 98         |
| ExpectedImprovement  | 0.014994   |
| ActualImprovement    | 0.014017   |
| ImprovementRatio     | 0.93487    |
| MeanKL               | 0.006964   |
| Entropy              | 2.1772     |
| Perplexity           | 8.8212     |
| AveragePolicyStd     | 0.35011    |
| AveragePolicyStd[0]  | 0.369      |
| AveragePolicyStd[1]  | 0.39894    |
| AveragePolicyStd[2]  | 0.37672    |
| AveragePolicyStd[3]  | 0.36034    |
| AveragePolicyStd[4]  | 0.28761    |
| AveragePolicyStd[5]  | 0.30806    |
| AverageReturn        | 388.69     |
| MinReturn            | 8.8189     |
| MaxReturn            | 641.82     |
| StdReturn            | 195.58     |
| AverageEpisodeLength | 610.86     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 335.85     |
| TotalNEpisodes       | 14779      |
| TotalNSamples        | 4.8864e+05 |
| ExplainedVariance    | 0.21707    |
-------------------------------------
[2018-12-22 10:16:54.827052 UTC] Saving snapshot
[2018-12-22 10:16:54.827312 UTC] Starting iteration 99
[2018-12-22 10:16:54.827447 UTC] Start collecting samples
[2018-12-22 10:16:57.906264 UTC] Computing input variables for policy optimization
[2018-12-22 10:16:57.984769 UTC] Performing policy update
[2018-12-22 10:16:57.985354 UTC] Computing gradient in Euclidean space
[2018-12-22 10:16:58.069765 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:16:58.996756 UTC] Performing line search
[2018-12-22 10:16:59.107644 UTC] Updating baseline
[2018-12-22 10:17:00.412549 UTC] Computing logging information
------------------------------------
| Iteration            | 99        |
| ExpectedImprovement  | 0.014671  |
| ActualImprovement    | 0.013413  |
| ImprovementRatio     | 0.91423   |
| MeanKL               | 0.0071121 |
| Entropy              | 2.1515    |
| Perplexity           | 8.5978    |
| AveragePolicyStd     | 0.34873   |
| AveragePolicyStd[0]  | 0.36641   |
| AveragePolicyStd[1]  | 0.40002   |
| AveragePolicyStd[2]  | 0.37699   |
| AveragePolicyStd[3]  | 0.35806   |
| AveragePolicyStd[4]  | 0.28682   |
| AveragePolicyStd[5]  | 0.3041    |
| AverageReturn        | 389.58    |
| MinReturn            | 8.8189    |
| MaxReturn            | 641.82    |
| StdReturn            | 195.07    |
| AverageEpisodeLength | 610.99    |
| MinEpisodeLength     | 20        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 334.86    |
| TotalNEpisodes       | 14786     |
| TotalNSamples        | 4.941e+05 |
| ExplainedVariance    | 0.28452   |
------------------------------------
[2018-12-22 10:17:00.728523 UTC] Saving snapshot
[2018-12-22 10:17:00.728790 UTC] Starting iteration 100
[2018-12-22 10:17:00.728914 UTC] Start collecting samples
[2018-12-22 10:17:03.796051 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:03.869005 UTC] Performing policy update
[2018-12-22 10:17:03.870839 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:03.964072 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:05.018132 UTC] Performing line search
[2018-12-22 10:17:05.142726 UTC] Updating baseline
[2018-12-22 10:17:06.434808 UTC] Computing logging information
------------------------------------
| Iteration            | 100       |
| ExpectedImprovement  | 0.013687  |
| ActualImprovement    | 0.013193  |
| ImprovementRatio     | 0.96391   |
| MeanKL               | 0.0071571 |
| Entropy              | 2.1491    |
| Perplexity           | 8.5768    |
| AveragePolicyStd     | 0.34868   |
| AveragePolicyStd[0]  | 0.36886   |
| AveragePolicyStd[1]  | 0.40112   |
| AveragePolicyStd[2]  | 0.37558   |
| AveragePolicyStd[3]  | 0.35595   |
| AveragePolicyStd[4]  | 0.28269   |
| AveragePolicyStd[5]  | 0.30787   |
| AverageReturn        | 396.57    |
| MinReturn            | 8.8189    |
| MaxReturn            | 651.16    |
| StdReturn            | 199.2     |
| AverageEpisodeLength | 620.9     |
| MinEpisodeLength     | 20        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 340.43    |
| TotalNEpisodes       | 14790     |
| TotalNSamples        | 4.981e+05 |
| ExplainedVariance    | 0.23552   |
------------------------------------
[2018-12-22 10:17:06.751733 UTC] Saving snapshot
[2018-12-22 10:17:06.760021 UTC] Starting iteration 101
[2018-12-22 10:17:06.760219 UTC] Start collecting samples
[2018-12-22 10:17:09.845320 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:09.919610 UTC] Performing policy update
[2018-12-22 10:17:09.920237 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:10.009417 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:11.048650 UTC] Performing line search
[2018-12-22 10:17:11.171850 UTC] Updating baseline
[2018-12-22 10:17:12.472275 UTC] Computing logging information
-------------------------------------
| Iteration            | 101        |
| ExpectedImprovement  | 0.019591   |
| ActualImprovement    | 0.017101   |
| ImprovementRatio     | 0.87293    |
| MeanKL               | 0.0066457  |
| Entropy              | 2.1412     |
| Perplexity           | 8.5096     |
| AveragePolicyStd     | 0.34825    |
| AveragePolicyStd[0]  | 0.36681    |
| AveragePolicyStd[1]  | 0.40194    |
| AveragePolicyStd[2]  | 0.37643    |
| AveragePolicyStd[3]  | 0.35516    |
| AveragePolicyStd[4]  | 0.28353    |
| AveragePolicyStd[5]  | 0.30562    |
| AverageReturn        | 409.65     |
| MinReturn            | 8.8189     |
| MaxReturn            | 651.16     |
| StdReturn            | 199.19     |
| AverageEpisodeLength | 642.37     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 339.27     |
| TotalNEpisodes       | 14796      |
| TotalNSamples        | 5.0365e+05 |
| ExplainedVariance    | 0.074359   |
-------------------------------------
[2018-12-22 10:17:12.788365 UTC] Saving snapshot
[2018-12-22 10:17:12.788625 UTC] Starting iteration 102
[2018-12-22 10:17:12.788746 UTC] Start collecting samples
[2018-12-22 10:17:15.862498 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:15.935500 UTC] Performing policy update
[2018-12-22 10:17:15.936241 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:16.025607 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:17.070094 UTC] Performing line search
[2018-12-22 10:17:17.193782 UTC] Updating baseline
[2018-12-22 10:17:18.401922 UTC] Computing logging information
-------------------------------------
| Iteration            | 102        |
| ExpectedImprovement  | 0.01385    |
| ActualImprovement    | 0.013313   |
| ImprovementRatio     | 0.96119    |
| MeanKL               | 0.0074868  |
| Entropy              | 2.1067     |
| Perplexity           | 8.221      |
| AveragePolicyStd     | 0.34661    |
| AveragePolicyStd[0]  | 0.36512    |
| AveragePolicyStd[1]  | 0.40467    |
| AveragePolicyStd[2]  | 0.3767     |
| AveragePolicyStd[3]  | 0.35293    |
| AveragePolicyStd[4]  | 0.27719    |
| AveragePolicyStd[5]  | 0.30304    |
| AverageReturn        | 413.77     |
| MinReturn            | 8.8189     |
| MaxReturn            | 651.16     |
| StdReturn            | 199.33     |
| AverageEpisodeLength | 649.16     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 339.41     |
| TotalNEpisodes       | 14800      |
| TotalNSamples        | 5.0764e+05 |
| ExplainedVariance    | 0.14657    |
-------------------------------------
[2018-12-22 10:17:18.719044 UTC] Saving snapshot
[2018-12-22 10:17:18.719348 UTC] Starting iteration 103
[2018-12-22 10:17:18.719474 UTC] Start collecting samples
[2018-12-22 10:17:21.832502 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:21.908800 UTC] Performing policy update
[2018-12-22 10:17:21.909424 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:22.000183 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:23.038425 UTC] Performing line search
[2018-12-22 10:17:23.161283 UTC] Updating baseline
[2018-12-22 10:17:24.392654 UTC] Computing logging information
------------------------------------
| Iteration            | 103       |
| ExpectedImprovement  | 0.01839   |
| ActualImprovement    | 0.016402  |
| ImprovementRatio     | 0.89192   |
| MeanKL               | 0.0066102 |
| Entropy              | 2.1053    |
| Perplexity           | 8.21      |
| AveragePolicyStd     | 0.34661   |
| AveragePolicyStd[0]  | 0.36591   |
| AveragePolicyStd[1]  | 0.40655   |
| AveragePolicyStd[2]  | 0.37555   |
| AveragePolicyStd[3]  | 0.35272   |
| AveragePolicyStd[4]  | 0.27677   |
| AveragePolicyStd[5]  | 0.30214   |
| AverageReturn        | 427.45    |
| MinReturn            | 100.74    |
| MaxReturn            | 651.16    |
| StdReturn            | 196.39    |
| AverageEpisodeLength | 670.46    |
| MinEpisodeLength     | 128       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 334.09    |
| TotalNEpisodes       | 14809     |
| TotalNSamples        | 5.143e+05 |
| ExplainedVariance    | 0.47057   |
------------------------------------
[2018-12-22 10:17:24.701889 UTC] Saving snapshot
[2018-12-22 10:17:24.702184 UTC] Starting iteration 104
[2018-12-22 10:17:24.702302 UTC] Start collecting samples
[2018-12-22 10:17:27.789860 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:27.866295 UTC] Performing policy update
[2018-12-22 10:17:27.867129 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:27.959700 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:29.010749 UTC] Performing line search
[2018-12-22 10:17:29.137395 UTC] Updating baseline
[2018-12-22 10:17:30.442006 UTC] Computing logging information
-------------------------------------
| Iteration            | 104        |
| ExpectedImprovement  | 0.017112   |
| ActualImprovement    | 0.015329   |
| ImprovementRatio     | 0.89579    |
| MeanKL               | 0.00693    |
| Entropy              | 2.0816     |
| Perplexity           | 8.0171     |
| AveragePolicyStd     | 0.34522    |
| AveragePolicyStd[0]  | 0.36515    |
| AveragePolicyStd[1]  | 0.40153    |
| AveragePolicyStd[2]  | 0.3759     |
| AveragePolicyStd[3]  | 0.35251    |
| AveragePolicyStd[4]  | 0.27456    |
| AveragePolicyStd[5]  | 0.30166    |
| AverageReturn        | 428.96     |
| MinReturn            | 100.74     |
| MaxReturn            | 651.16     |
| StdReturn            | 196.49     |
| AverageEpisodeLength | 670.8      |
| MinEpisodeLength     | 128        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 333.66     |
| TotalNEpisodes       | 14816      |
| TotalNSamples        | 5.1911e+05 |
| ExplainedVariance    | 0.483      |
-------------------------------------
[2018-12-22 10:17:30.759595 UTC] Saving snapshot
[2018-12-22 10:17:30.759845 UTC] Starting iteration 105
[2018-12-22 10:17:30.759968 UTC] Start collecting samples
[2018-12-22 10:17:33.844312 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:33.918641 UTC] Performing policy update
[2018-12-22 10:17:33.919199 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:34.009932 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:35.046316 UTC] Performing line search
[2018-12-22 10:17:35.170878 UTC] Updating baseline
[2018-12-22 10:17:36.364016 UTC] Computing logging information
-------------------------------------
| Iteration            | 105        |
| ExpectedImprovement  | 0.01627    |
| ActualImprovement    | 0.015959   |
| ImprovementRatio     | 0.98088    |
| MeanKL               | 0.0069818  |
| Entropy              | 2.0889     |
| Perplexity           | 8.0761     |
| AveragePolicyStd     | 0.34568    |
| AveragePolicyStd[0]  | 0.36546    |
| AveragePolicyStd[1]  | 0.40539    |
| AveragePolicyStd[2]  | 0.37491    |
| AveragePolicyStd[3]  | 0.35117    |
| AveragePolicyStd[4]  | 0.27527    |
| AveragePolicyStd[5]  | 0.3019     |
| AverageReturn        | 429.83     |
| MinReturn            | 100.74     |
| MaxReturn            | 678.62     |
| StdReturn            | 194.9      |
| AverageEpisodeLength | 669.16     |
| MinEpisodeLength     | 128        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 329.15     |
| TotalNEpisodes       | 14822      |
| TotalNSamples        | 5.2268e+05 |
| ExplainedVariance    | 0.82275    |
-------------------------------------
[2018-12-22 10:17:36.679055 UTC] Saving snapshot
[2018-12-22 10:17:36.679297 UTC] Starting iteration 106
[2018-12-22 10:17:36.679553 UTC] Start collecting samples
[2018-12-22 10:17:39.792781 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:39.869796 UTC] Performing policy update
[2018-12-22 10:17:39.870519 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:39.963977 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:41.004629 UTC] Performing line search
[2018-12-22 10:17:41.128479 UTC] Updating baseline
[2018-12-22 10:17:42.314693 UTC] Computing logging information
-------------------------------------
| Iteration            | 106        |
| ExpectedImprovement  | 0.015873   |
| ActualImprovement    | 0.015185   |
| ImprovementRatio     | 0.95665    |
| MeanKL               | 0.006985   |
| Entropy              | 2.1047     |
| Perplexity           | 8.2043     |
| AveragePolicyStd     | 0.34669    |
| AveragePolicyStd[0]  | 0.36463    |
| AveragePolicyStd[1]  | 0.41014    |
| AveragePolicyStd[2]  | 0.37252    |
| AveragePolicyStd[3]  | 0.35634    |
| AveragePolicyStd[4]  | 0.2764     |
| AveragePolicyStd[5]  | 0.30011    |
| AverageReturn        | 439.7      |
| MinReturn            | 88.519     |
| MaxReturn            | 678.62     |
| StdReturn            | 194.84     |
| AverageEpisodeLength | 685.46     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 328.97     |
| TotalNEpisodes       | 14831      |
| TotalNSamples        | 5.2927e+05 |
| ExplainedVariance    | 0.40595    |
-------------------------------------
[2018-12-22 10:17:42.627355 UTC] Saving snapshot
[2018-12-22 10:17:42.627640 UTC] Starting iteration 107
[2018-12-22 10:17:42.627765 UTC] Start collecting samples
[2018-12-22 10:17:45.731127 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:45.806429 UTC] Performing policy update
[2018-12-22 10:17:45.807087 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:45.895924 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:46.937797 UTC] Performing line search
[2018-12-22 10:17:47.062614 UTC] Updating baseline
[2018-12-22 10:17:48.270224 UTC] Computing logging information
-------------------------------------
| Iteration            | 107        |
| ExpectedImprovement  | 0.010857   |
| ActualImprovement    | 0.010442   |
| ImprovementRatio     | 0.96176    |
| MeanKL               | 0.0070439  |
| Entropy              | 2.1022     |
| Perplexity           | 8.1838     |
| AveragePolicyStd     | 0.34658    |
| AveragePolicyStd[0]  | 0.3655     |
| AveragePolicyStd[1]  | 0.40875    |
| AveragePolicyStd[2]  | 0.37376    |
| AveragePolicyStd[3]  | 0.35639    |
| AveragePolicyStd[4]  | 0.27585    |
| AveragePolicyStd[5]  | 0.29923    |
| AverageReturn        | 444.55     |
| MinReturn            | 88.519     |
| MaxReturn            | 678.62     |
| StdReturn            | 193.5      |
| AverageEpisodeLength | 692.43     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 325.9      |
| TotalNEpisodes       | 14836      |
| TotalNSamples        | 5.3238e+05 |
| ExplainedVariance    | 0.62122    |
-------------------------------------
[2018-12-22 10:17:48.586358 UTC] Saving snapshot
[2018-12-22 10:17:48.586614 UTC] Starting iteration 108
[2018-12-22 10:17:48.586736 UTC] Start collecting samples
[2018-12-22 10:17:51.672021 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:51.746848 UTC] Performing policy update
[2018-12-22 10:17:51.747473 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:51.840305 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:52.878019 UTC] Performing line search
[2018-12-22 10:17:53.001778 UTC] Updating baseline
[2018-12-22 10:17:54.347145 UTC] Computing logging information
-------------------------------------
| Iteration            | 108        |
| ExpectedImprovement  | 0.013611   |
| ActualImprovement    | 0.013045   |
| ImprovementRatio     | 0.95835    |
| MeanKL               | 0.0069393  |
| Entropy              | 2.1048     |
| Perplexity           | 8.2056     |
| AveragePolicyStd     | 0.34664    |
| AveragePolicyStd[0]  | 0.35867    |
| AveragePolicyStd[1]  | 0.40821    |
| AveragePolicyStd[2]  | 0.37436    |
| AveragePolicyStd[3]  | 0.36107    |
| AveragePolicyStd[4]  | 0.27608    |
| AveragePolicyStd[5]  | 0.30144    |
| AverageReturn        | 457.52     |
| MinReturn            | 88.519     |
| MaxReturn            | 678.62     |
| StdReturn            | 191.59     |
| AverageEpisodeLength | 714.11     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 322.53     |
| TotalNEpisodes       | 14844      |
| TotalNSamples        | 5.3929e+05 |
| ExplainedVariance    | 0.26788    |
-------------------------------------
[2018-12-22 10:17:54.665103 UTC] Saving snapshot
[2018-12-22 10:17:54.665351 UTC] Starting iteration 109
[2018-12-22 10:17:54.665470 UTC] Start collecting samples
[2018-12-22 10:17:57.735074 UTC] Computing input variables for policy optimization
[2018-12-22 10:17:57.810159 UTC] Performing policy update
[2018-12-22 10:17:57.810745 UTC] Computing gradient in Euclidean space
[2018-12-22 10:17:57.900054 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:17:58.930983 UTC] Performing line search
[2018-12-22 10:17:59.055041 UTC] Updating baseline
[2018-12-22 10:18:00.257942 UTC] Computing logging information
-------------------------------------
| Iteration            | 109        |
| ExpectedImprovement  | 0.015773   |
| ActualImprovement    | 0.015117   |
| ImprovementRatio     | 0.95838    |
| MeanKL               | 0.0070869  |
| Entropy              | 2.118      |
| Perplexity           | 8.3143     |
| AveragePolicyStd     | 0.34752    |
| AveragePolicyStd[0]  | 0.36154    |
| AveragePolicyStd[1]  | 0.41223    |
| AveragePolicyStd[2]  | 0.37094    |
| AveragePolicyStd[3]  | 0.36294    |
| AveragePolicyStd[4]  | 0.27449    |
| AveragePolicyStd[5]  | 0.30301    |
| AverageReturn        | 464.82     |
| MinReturn            | 88.519     |
| MaxReturn            | 678.62     |
| StdReturn            | 191.01     |
| AverageEpisodeLength | 725.44     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 320.05     |
| TotalNEpisodes       | 14849      |
| TotalNSamples        | 5.4362e+05 |
| ExplainedVariance    | 0.14023    |
-------------------------------------
[2018-12-22 10:18:00.568432 UTC] Saving snapshot
[2018-12-22 10:18:00.568691 UTC] Starting iteration 110
[2018-12-22 10:18:00.568813 UTC] Start collecting samples
[2018-12-22 10:18:03.639762 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:03.712634 UTC] Performing policy update
[2018-12-22 10:18:03.713326 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:03.803365 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:04.803354 UTC] Performing line search
[2018-12-22 10:18:04.926713 UTC] Updating baseline
[2018-12-22 10:18:06.110923 UTC] Computing logging information
-------------------------------------
| Iteration            | 110        |
| ExpectedImprovement  | 0.014523   |
| ActualImprovement    | 0.013181   |
| ImprovementRatio     | 0.90758    |
| MeanKL               | 0.0070057  |
| Entropy              | 2.1301     |
| Perplexity           | 8.4158     |
| AveragePolicyStd     | 0.34837    |
| AveragePolicyStd[0]  | 0.36161    |
| AveragePolicyStd[1]  | 0.41749    |
| AveragePolicyStd[2]  | 0.36926    |
| AveragePolicyStd[3]  | 0.36473    |
| AveragePolicyStd[4]  | 0.27491    |
| AveragePolicyStd[5]  | 0.30219    |
| AverageReturn        | 475.79     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 187.55     |
| AverageEpisodeLength | 743.27     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 313.84     |
| TotalNEpisodes       | 14853      |
| TotalNSamples        | 5.4683e+05 |
| ExplainedVariance    | 0.44771    |
-------------------------------------
[2018-12-22 10:18:06.423163 UTC] Saving snapshot
[2018-12-22 10:18:06.431112 UTC] Starting iteration 111
[2018-12-22 10:18:06.431294 UTC] Start collecting samples
[2018-12-22 10:18:09.553360 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:09.631806 UTC] Performing policy update
[2018-12-22 10:18:09.632422 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:09.717476 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:10.688827 UTC] Performing line search
[2018-12-22 10:18:10.802596 UTC] Updating baseline
[2018-12-22 10:18:12.015952 UTC] Computing logging information
-------------------------------------
| Iteration            | 111        |
| ExpectedImprovement  | 0.014061   |
| ActualImprovement    | 0.013222   |
| ImprovementRatio     | 0.94036    |
| MeanKL               | 0.0069555  |
| Entropy              | 2.1328     |
| Perplexity           | 8.4382     |
| AveragePolicyStd     | 0.34848    |
| AveragePolicyStd[0]  | 0.36158    |
| AveragePolicyStd[1]  | 0.41374    |
| AveragePolicyStd[2]  | 0.37185    |
| AveragePolicyStd[3]  | 0.36576    |
| AveragePolicyStd[4]  | 0.27265    |
| AveragePolicyStd[5]  | 0.30531    |
| AverageReturn        | 484.48     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 191.85     |
| AverageEpisodeLength | 755.87     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 319.88     |
| TotalNEpisodes       | 14864      |
| TotalNSamples        | 5.5489e+05 |
| ExplainedVariance    | 0.34039    |
-------------------------------------
[2018-12-22 10:18:12.334695 UTC] Saving snapshot
[2018-12-22 10:18:12.334955 UTC] Starting iteration 112
[2018-12-22 10:18:12.335077 UTC] Start collecting samples
[2018-12-22 10:18:15.420961 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:15.495713 UTC] Performing policy update
[2018-12-22 10:18:15.496412 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:15.585765 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:16.635260 UTC] Performing line search
[2018-12-22 10:18:16.759662 UTC] Updating baseline
[2018-12-22 10:18:17.987084 UTC] Computing logging information
-------------------------------------
| Iteration            | 112        |
| ExpectedImprovement  | 0.014934   |
| ActualImprovement    | 0.014288   |
| ImprovementRatio     | 0.95669    |
| MeanKL               | 0.006888   |
| Entropy              | 2.137      |
| Perplexity           | 8.4742     |
| AveragePolicyStd     | 0.34874    |
| AveragePolicyStd[0]  | 0.36162    |
| AveragePolicyStd[1]  | 0.41465    |
| AveragePolicyStd[2]  | 0.37447    |
| AveragePolicyStd[3]  | 0.36299    |
| AveragePolicyStd[4]  | 0.27268    |
| AveragePolicyStd[5]  | 0.30603    |
| AverageReturn        | 488.07     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 190.98     |
| AverageEpisodeLength | 761.35     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 318.54     |
| TotalNEpisodes       | 14869      |
| TotalNSamples        | 5.5843e+05 |
| ExplainedVariance    | 0.39426    |
-------------------------------------
[2018-12-22 10:18:18.304898 UTC] Saving snapshot
[2018-12-22 10:18:18.305132 UTC] Starting iteration 113
[2018-12-22 10:18:18.305247 UTC] Start collecting samples
[2018-12-22 10:18:21.439486 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:21.515937 UTC] Performing policy update
[2018-12-22 10:18:21.516527 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:21.605973 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:22.655826 UTC] Performing line search
[2018-12-22 10:18:22.779322 UTC] Updating baseline
[2018-12-22 10:18:23.991231 UTC] Computing logging information
-------------------------------------
| Iteration            | 113        |
| ExpectedImprovement  | 0.013369   |
| ActualImprovement    | 0.012884   |
| ImprovementRatio     | 0.96379    |
| MeanKL               | 0.0072489  |
| Entropy              | 2.1379     |
| Perplexity           | 8.4817     |
| AveragePolicyStd     | 0.34868    |
| AveragePolicyStd[0]  | 0.36123    |
| AveragePolicyStd[1]  | 0.41279    |
| AveragePolicyStd[2]  | 0.37432    |
| AveragePolicyStd[3]  | 0.36233    |
| AveragePolicyStd[4]  | 0.27269    |
| AveragePolicyStd[5]  | 0.3087     |
| AverageReturn        | 487.83     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 194.57     |
| AverageEpisodeLength | 758.26     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 324.42     |
| TotalNEpisodes       | 14879      |
| TotalNSamples        | 5.6447e+05 |
| ExplainedVariance    | 0.50747    |
-------------------------------------
[2018-12-22 10:18:24.303111 UTC] Saving snapshot
[2018-12-22 10:18:24.303356 UTC] Starting iteration 114
[2018-12-22 10:18:24.303475 UTC] Start collecting samples
[2018-12-22 10:18:27.378125 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:27.452899 UTC] Performing policy update
[2018-12-22 10:18:27.453567 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:27.542980 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:28.584082 UTC] Performing line search
[2018-12-22 10:18:28.707522 UTC] Updating baseline
[2018-12-22 10:18:30.244762 UTC] Computing logging information
-------------------------------------
| Iteration            | 114        |
| ExpectedImprovement  | 0.014508   |
| ActualImprovement    | 0.013969   |
| ImprovementRatio     | 0.96288    |
| MeanKL               | 0.0075856  |
| Entropy              | 2.1441     |
| Perplexity           | 8.5344     |
| AveragePolicyStd     | 0.34903    |
| AveragePolicyStd[0]  | 0.36164    |
| AveragePolicyStd[1]  | 0.4128     |
| AveragePolicyStd[2]  | 0.37308    |
| AveragePolicyStd[3]  | 0.36404    |
| AveragePolicyStd[4]  | 0.27203    |
| AveragePolicyStd[5]  | 0.31057    |
| AverageReturn        | 491.13     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 192.75     |
| AverageEpisodeLength | 762.89     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 321.06     |
| TotalNEpisodes       | 14885      |
| TotalNSamples        | 5.7023e+05 |
| ExplainedVariance    | 0.13248    |
-------------------------------------
[2018-12-22 10:18:30.566615 UTC] Saving snapshot
[2018-12-22 10:18:30.566853 UTC] Starting iteration 115
[2018-12-22 10:18:30.566986 UTC] Start collecting samples
[2018-12-22 10:18:33.602676 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:33.674227 UTC] Performing policy update
[2018-12-22 10:18:33.674831 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:33.763408 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:34.810991 UTC] Performing line search
[2018-12-22 10:18:34.935188 UTC] Updating baseline
[2018-12-22 10:18:36.480861 UTC] Computing logging information
-------------------------------------
| Iteration            | 115        |
| ExpectedImprovement  | 0.013338   |
| ActualImprovement    | 0.013508   |
| ImprovementRatio     | 1.0127     |
| MeanKL               | 0.0071117  |
| Entropy              | 2.126      |
| Perplexity           | 8.3812     |
| AveragePolicyStd     | 0.34786    |
| AveragePolicyStd[0]  | 0.36023    |
| AveragePolicyStd[1]  | 0.40923    |
| AveragePolicyStd[2]  | 0.37122    |
| AveragePolicyStd[3]  | 0.36388    |
| AveragePolicyStd[4]  | 0.27179    |
| AveragePolicyStd[5]  | 0.31083    |
| AverageReturn        | 496.07     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 189.7      |
| AverageEpisodeLength | 771.3      |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 316.11     |
| TotalNEpisodes       | 14887      |
| TotalNSamples        | 5.7223e+05 |
| ExplainedVariance    | 0.30007    |
-------------------------------------
[2018-12-22 10:18:36.796733 UTC] Saving snapshot
[2018-12-22 10:18:36.796973 UTC] Starting iteration 116
[2018-12-22 10:18:36.797093 UTC] Start collecting samples
[2018-12-22 10:18:39.858357 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:39.933537 UTC] Performing policy update
[2018-12-22 10:18:39.934137 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:40.023665 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:41.066215 UTC] Performing line search
[2018-12-22 10:18:41.189672 UTC] Updating baseline
[2018-12-22 10:18:42.483150 UTC] Computing logging information
-------------------------------------
| Iteration            | 116        |
| ExpectedImprovement  | 0.012046   |
| ActualImprovement    | 0.011374   |
| ImprovementRatio     | 0.94416    |
| MeanKL               | 0.0075516  |
| Entropy              | 2.1378     |
| Perplexity           | 8.4805     |
| AveragePolicyStd     | 0.34853    |
| AveragePolicyStd[0]  | 0.36       |
| AveragePolicyStd[1]  | 0.40934    |
| AveragePolicyStd[2]  | 0.37286    |
| AveragePolicyStd[3]  | 0.36483    |
| AveragePolicyStd[4]  | 0.27211    |
| AveragePolicyStd[5]  | 0.31205    |
| AverageReturn        | 495.77     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 190.27     |
| AverageEpisodeLength | 767.49     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.45     |
| TotalNEpisodes       | 14893      |
| TotalNSamples        | 5.7769e+05 |
| ExplainedVariance    | 0.32765    |
-------------------------------------
[2018-12-22 10:18:42.799546 UTC] Saving snapshot
[2018-12-22 10:18:42.799806 UTC] Starting iteration 117
[2018-12-22 10:18:42.799932 UTC] Start collecting samples
[2018-12-22 10:18:45.902678 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:45.982586 UTC] Performing policy update
[2018-12-22 10:18:45.983250 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:46.072789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:47.119479 UTC] Performing line search
[2018-12-22 10:18:47.242677 UTC] Updating baseline
[2018-12-22 10:18:48.613892 UTC] Computing logging information
-------------------------------------
| Iteration            | 117        |
| ExpectedImprovement  | 0.019582   |
| ActualImprovement    | 0.017441   |
| ImprovementRatio     | 0.89064    |
| MeanKL               | 0.0067318  |
| Entropy              | 2.121      |
| Perplexity           | 8.3397     |
| AveragePolicyStd     | 0.34748    |
| AveragePolicyStd[0]  | 0.35967    |
| AveragePolicyStd[1]  | 0.40622    |
| AveragePolicyStd[2]  | 0.37158    |
| AveragePolicyStd[3]  | 0.36363    |
| AveragePolicyStd[4]  | 0.27149    |
| AveragePolicyStd[5]  | 0.31232    |
| AverageReturn        | 492.98     |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 192.83     |
| AverageEpisodeLength | 758.13     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 316.92     |
| TotalNEpisodes       | 14902      |
| TotalNSamples        | 5.8546e+05 |
| ExplainedVariance    | 0.66634    |
-------------------------------------
[2018-12-22 10:18:48.931367 UTC] Saving snapshot
[2018-12-22 10:18:48.931627 UTC] Starting iteration 118
[2018-12-22 10:18:48.931746 UTC] Start collecting samples
[2018-12-22 10:18:51.992951 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:52.064732 UTC] Performing policy update
[2018-12-22 10:18:52.065303 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:52.153710 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:53.189711 UTC] Performing line search
[2018-12-22 10:18:53.314270 UTC] Updating baseline
[2018-12-22 10:18:54.510448 UTC] Computing logging information
-------------------------------------
| Iteration            | 118        |
| ExpectedImprovement  | 0.012786   |
| ActualImprovement    | 0.012749   |
| ImprovementRatio     | 0.99706    |
| MeanKL               | 0.0069726  |
| Entropy              | 2.108      |
| Perplexity           | 8.2322     |
| AveragePolicyStd     | 0.34659    |
| AveragePolicyStd[0]  | 0.35909    |
| AveragePolicyStd[1]  | 0.40331    |
| AveragePolicyStd[2]  | 0.37071    |
| AveragePolicyStd[3]  | 0.36114    |
| AveragePolicyStd[4]  | 0.27208    |
| AveragePolicyStd[5]  | 0.31321    |
| AverageReturn        | 495.3      |
| MinReturn            | 88.519     |
| MaxReturn            | 681.01     |
| StdReturn            | 191.68     |
| AverageEpisodeLength | 761.46     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.66     |
| TotalNEpisodes       | 14906      |
| TotalNSamples        | 5.8744e+05 |
| ExplainedVariance    | 0.45934    |
-------------------------------------
[2018-12-22 10:18:54.826901 UTC] Saving snapshot
[2018-12-22 10:18:54.827135 UTC] Starting iteration 119
[2018-12-22 10:18:54.827260 UTC] Start collecting samples
[2018-12-22 10:18:57.876720 UTC] Computing input variables for policy optimization
[2018-12-22 10:18:57.951614 UTC] Performing policy update
[2018-12-22 10:18:57.952173 UTC] Computing gradient in Euclidean space
[2018-12-22 10:18:58.039334 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:18:59.078198 UTC] Performing line search
[2018-12-22 10:18:59.201788 UTC] Updating baseline
[2018-12-22 10:19:00.731603 UTC] Computing logging information
-------------------------------------
| Iteration            | 119        |
| ExpectedImprovement  | 0.01543    |
| ActualImprovement    | 0.014372   |
| ImprovementRatio     | 0.93142    |
| MeanKL               | 0.0070438  |
| Entropy              | 2.0773     |
| Perplexity           | 7.9827     |
| AveragePolicyStd     | 0.34471    |
| AveragePolicyStd[0]  | 0.35701    |
| AveragePolicyStd[1]  | 0.40263    |
| AveragePolicyStd[2]  | 0.3702     |
| AveragePolicyStd[3]  | 0.352      |
| AveragePolicyStd[4]  | 0.27255    |
| AveragePolicyStd[5]  | 0.31384    |
| AverageReturn        | 496.41     |
| MinReturn            | 88.519     |
| MaxReturn            | 683.93     |
| StdReturn            | 192.59     |
| AverageEpisodeLength | 761.46     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.66     |
| TotalNEpisodes       | 14910      |
| TotalNSamples        | 5.9144e+05 |
| ExplainedVariance    | 0.08858    |
-------------------------------------
[2018-12-22 10:19:01.047416 UTC] Saving snapshot
[2018-12-22 10:19:01.047672 UTC] Starting iteration 120
[2018-12-22 10:19:01.047793 UTC] Start collecting samples
[2018-12-22 10:19:04.174537 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:04.252146 UTC] Performing policy update
[2018-12-22 10:19:04.252856 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:04.340947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:05.373684 UTC] Performing line search
[2018-12-22 10:19:05.497331 UTC] Updating baseline
[2018-12-22 10:19:06.863607 UTC] Computing logging information
-------------------------------------
| Iteration            | 120        |
| ExpectedImprovement  | 0.015355   |
| ActualImprovement    | 0.014469   |
| ImprovementRatio     | 0.94227    |
| MeanKL               | 0.0070657  |
| Entropy              | 2.0589     |
| Perplexity           | 7.837      |
| AveragePolicyStd     | 0.34359    |
| AveragePolicyStd[0]  | 0.35653    |
| AveragePolicyStd[1]  | 0.40051    |
| AveragePolicyStd[2]  | 0.36725    |
| AveragePolicyStd[3]  | 0.35196    |
| AveragePolicyStd[4]  | 0.27241    |
| AveragePolicyStd[5]  | 0.31285    |
| AverageReturn        | 511.17     |
| MinReturn            | 88.519     |
| MaxReturn            | 683.93     |
| StdReturn            | 187.66     |
| AverageEpisodeLength | 785.35     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 306.55     |
| TotalNEpisodes       | 14919      |
| TotalNSamples        | 5.9952e+05 |
| ExplainedVariance    | 0.29556    |
-------------------------------------
[2018-12-22 10:19:07.179997 UTC] Saving snapshot
[2018-12-22 10:19:07.187939 UTC] Starting iteration 121
[2018-12-22 10:19:07.188133 UTC] Start collecting samples
[2018-12-22 10:19:10.238525 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:10.310806 UTC] Performing policy update
[2018-12-22 10:19:10.311726 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:10.400390 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:11.448832 UTC] Performing line search
[2018-12-22 10:19:11.572482 UTC] Updating baseline
[2018-12-22 10:19:12.967464 UTC] Computing logging information
-------------------------------------
| Iteration            | 121        |
| ExpectedImprovement  | 0.014576   |
| ActualImprovement    | 0.013994   |
| ImprovementRatio     | 0.96008    |
| MeanKL               | 0.0068826  |
| Entropy              | 2.0624     |
| Perplexity           | 7.8647     |
| AveragePolicyStd     | 0.34376    |
| AveragePolicyStd[0]  | 0.35512    |
| AveragePolicyStd[1]  | 0.39869    |
| AveragePolicyStd[2]  | 0.37145    |
| AveragePolicyStd[3]  | 0.35261    |
| AveragePolicyStd[4]  | 0.27391    |
| AveragePolicyStd[5]  | 0.31078    |
| AverageReturn        | 519.37     |
| MinReturn            | 88.519     |
| MaxReturn            | 683.93     |
| StdReturn            | 184.67     |
| AverageEpisodeLength | 798.37     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 301.46     |
| TotalNEpisodes       | 14922      |
| TotalNSamples        | 6.0252e+05 |
| ExplainedVariance    | -0.23102   |
-------------------------------------
[2018-12-22 10:19:13.286281 UTC] Saving snapshot
[2018-12-22 10:19:13.286539 UTC] Starting iteration 122
[2018-12-22 10:19:13.286681 UTC] Start collecting samples
[2018-12-22 10:19:16.369206 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:16.443636 UTC] Performing policy update
[2018-12-22 10:19:16.444216 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:16.532125 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:17.582191 UTC] Performing line search
[2018-12-22 10:19:17.706797 UTC] Updating baseline
[2018-12-22 10:19:19.258284 UTC] Computing logging information
-------------------------------------
| Iteration            | 122        |
| ExpectedImprovement  | 0.017559   |
| ActualImprovement    | 0.016728   |
| ImprovementRatio     | 0.95268    |
| MeanKL               | 0.0067474  |
| Entropy              | 2.0474     |
| Perplexity           | 7.7475     |
| AveragePolicyStd     | 0.34304    |
| AveragePolicyStd[0]  | 0.3544     |
| AveragePolicyStd[1]  | 0.39847    |
| AveragePolicyStd[2]  | 0.37209    |
| AveragePolicyStd[3]  | 0.35167    |
| AveragePolicyStd[4]  | 0.27039    |
| AveragePolicyStd[5]  | 0.31124    |
| AverageReturn        | 518.21     |
| MinReturn            | 88.519     |
| MaxReturn            | 683.93     |
| StdReturn            | 187.45     |
| AverageEpisodeLength | 795.14     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 305.06     |
| TotalNEpisodes       | 14927      |
| TotalNSamples        | 6.0651e+05 |
| ExplainedVariance    | 0.20726    |
-------------------------------------
[2018-12-22 10:19:19.576766 UTC] Saving snapshot
[2018-12-22 10:19:19.577016 UTC] Starting iteration 123
[2018-12-22 10:19:19.577137 UTC] Start collecting samples
[2018-12-22 10:19:22.698469 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:22.773870 UTC] Performing policy update
[2018-12-22 10:19:22.774599 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:22.867356 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:23.909307 UTC] Performing line search
[2018-12-22 10:19:24.035784 UTC] Updating baseline
[2018-12-22 10:19:25.400660 UTC] Computing logging information
-------------------------------------
| Iteration            | 123        |
| ExpectedImprovement  | 0.012865   |
| ActualImprovement    | 0.01252    |
| ImprovementRatio     | 0.97319    |
| MeanKL               | 0.0071628  |
| Entropy              | 2.0556     |
| Perplexity           | 7.8117     |
| AveragePolicyStd     | 0.34355    |
| AveragePolicyStd[0]  | 0.35674    |
| AveragePolicyStd[1]  | 0.39797    |
| AveragePolicyStd[2]  | 0.37189    |
| AveragePolicyStd[3]  | 0.35329    |
| AveragePolicyStd[4]  | 0.26935    |
| AveragePolicyStd[5]  | 0.31208    |
| AverageReturn        | 537.39     |
| MinReturn            | 111.95     |
| MaxReturn            | 683.93     |
| StdReturn            | 176.94     |
| AverageEpisodeLength | 824.9      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 287.45     |
| TotalNEpisodes       | 14935      |
| TotalNSamples        | 6.1387e+05 |
| ExplainedVariance    | 0.17833    |
-------------------------------------
[2018-12-22 10:19:25.719388 UTC] Saving snapshot
[2018-12-22 10:19:25.719655 UTC] Starting iteration 124
[2018-12-22 10:19:25.719780 UTC] Start collecting samples
[2018-12-22 10:19:28.823989 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:28.897362 UTC] Performing policy update
[2018-12-22 10:19:28.897959 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:28.986192 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:30.035824 UTC] Performing line search
[2018-12-22 10:19:30.159496 UTC] Updating baseline
[2018-12-22 10:19:31.355082 UTC] Computing logging information
-------------------------------------
| Iteration            | 124        |
| ExpectedImprovement  | 0.01209    |
| ActualImprovement    | 0.011871   |
| ImprovementRatio     | 0.98188    |
| MeanKL               | 0.0082618  |
| Entropy              | 2.0672     |
| Perplexity           | 7.9025     |
| AveragePolicyStd     | 0.34429    |
| AveragePolicyStd[0]  | 0.35724    |
| AveragePolicyStd[1]  | 0.4015     |
| AveragePolicyStd[2]  | 0.37135    |
| AveragePolicyStd[3]  | 0.35433    |
| AveragePolicyStd[4]  | 0.26983    |
| AveragePolicyStd[5]  | 0.31147    |
| AverageReturn        | 538.72     |
| MinReturn            | 111.95     |
| MaxReturn            | 683.93     |
| StdReturn            | 178.02     |
| AverageEpisodeLength | 824.74     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 287.71     |
| TotalNEpisodes       | 14941      |
| TotalNSamples        | 6.1921e+05 |
| ExplainedVariance    | 0.17196    |
-------------------------------------
[2018-12-22 10:19:31.673265 UTC] Saving snapshot
[2018-12-22 10:19:31.673530 UTC] Starting iteration 125
[2018-12-22 10:19:31.673668 UTC] Start collecting samples
[2018-12-22 10:19:34.743098 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:34.817246 UTC] Performing policy update
[2018-12-22 10:19:34.817994 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:34.908516 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:35.959513 UTC] Performing line search
[2018-12-22 10:19:36.084067 UTC] Updating baseline
[2018-12-22 10:19:37.439694 UTC] Computing logging information
-------------------------------------
| Iteration            | 125        |
| ExpectedImprovement  | 0.012887   |
| ActualImprovement    | 0.01229    |
| ImprovementRatio     | 0.95373    |
| MeanKL               | 0.0067941  |
| Entropy              | 2.0734     |
| Perplexity           | 7.9515     |
| AveragePolicyStd     | 0.34483    |
| AveragePolicyStd[0]  | 0.35785    |
| AveragePolicyStd[1]  | 0.40742    |
| AveragePolicyStd[2]  | 0.37035    |
| AveragePolicyStd[3]  | 0.35392    |
| AveragePolicyStd[4]  | 0.2692     |
| AveragePolicyStd[5]  | 0.31023    |
| AverageReturn        | 541.34     |
| MinReturn            | 111.95     |
| MaxReturn            | 683.93     |
| StdReturn            | 177.76     |
| AverageEpisodeLength | 828.23     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 288.18     |
| TotalNEpisodes       | 14945      |
| TotalNSamples        | 6.2263e+05 |
| ExplainedVariance    | 0.35449    |
-------------------------------------
[2018-12-22 10:19:37.761168 UTC] Saving snapshot
[2018-12-22 10:19:37.761444 UTC] Starting iteration 126
[2018-12-22 10:19:37.761565 UTC] Start collecting samples
[2018-12-22 10:19:40.883746 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:40.961835 UTC] Performing policy update
[2018-12-22 10:19:40.962404 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:41.050964 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:42.086753 UTC] Performing line search
[2018-12-22 10:19:42.210109 UTC] Updating baseline
[2018-12-22 10:19:43.386831 UTC] Computing logging information
-------------------------------------
| Iteration            | 126        |
| ExpectedImprovement  | 0.018077   |
| ActualImprovement    | 0.016807   |
| ImprovementRatio     | 0.92978    |
| MeanKL               | 0.0068489  |
| Entropy              | 2.0811     |
| Perplexity           | 8.013      |
| AveragePolicyStd     | 0.34519    |
| AveragePolicyStd[0]  | 0.35711    |
| AveragePolicyStd[1]  | 0.40833    |
| AveragePolicyStd[2]  | 0.3717     |
| AveragePolicyStd[3]  | 0.35121    |
| AveragePolicyStd[4]  | 0.27067    |
| AveragePolicyStd[5]  | 0.31216    |
| AverageReturn        | 541.93     |
| MinReturn            | 36.795     |
| MaxReturn            | 683.93     |
| StdReturn            | 176.81     |
| AverageEpisodeLength | 827.07     |
| MinEpisodeLength     | 54         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 285.76     |
| TotalNEpisodes       | 14956      |
| TotalNSamples        | 6.3144e+05 |
| ExplainedVariance    | 0.38758    |
-------------------------------------
[2018-12-22 10:19:43.703989 UTC] Saving snapshot
[2018-12-22 10:19:43.704239 UTC] Starting iteration 127
[2018-12-22 10:19:43.704396 UTC] Start collecting samples
[2018-12-22 10:19:46.824523 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:46.900440 UTC] Performing policy update
[2018-12-22 10:19:46.901088 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:46.989972 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:47.924116 UTC] Performing line search
[2018-12-22 10:19:48.036653 UTC] Updating baseline
[2018-12-22 10:19:49.232555 UTC] Computing logging information
-------------------------------------
| Iteration            | 127        |
| ExpectedImprovement  | 0.014782   |
| ActualImprovement    | 0.014936   |
| ImprovementRatio     | 1.0105     |
| MeanKL               | 0.0071936  |
| Entropy              | 2.0508     |
| Perplexity           | 7.7738     |
| AveragePolicyStd     | 0.3435     |
| AveragePolicyStd[0]  | 0.35546    |
| AveragePolicyStd[1]  | 0.4096     |
| AveragePolicyStd[2]  | 0.36721    |
| AveragePolicyStd[3]  | 0.3499     |
| AveragePolicyStd[4]  | 0.27065    |
| AveragePolicyStd[5]  | 0.30817    |
| AverageReturn        | 526.25     |
| MinReturn            | 36.795     |
| MaxReturn            | 683.93     |
| StdReturn            | 186.34     |
| AverageEpisodeLength | 800.27     |
| MinEpisodeLength     | 54         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 300.74     |
| TotalNEpisodes       | 14964      |
| TotalNSamples        | 6.3492e+05 |
| ExplainedVariance    | 0.86931    |
-------------------------------------
[2018-12-22 10:19:49.548653 UTC] Saving snapshot
[2018-12-22 10:19:49.548933 UTC] Starting iteration 128
[2018-12-22 10:19:49.549052 UTC] Start collecting samples
[2018-12-22 10:19:52.652300 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:52.728419 UTC] Performing policy update
[2018-12-22 10:19:52.730501 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:52.821221 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:53.861706 UTC] Performing line search
[2018-12-22 10:19:53.988994 UTC] Updating baseline
[2018-12-22 10:19:55.196502 UTC] Computing logging information
-------------------------------------
| Iteration            | 128        |
| ExpectedImprovement  | 0.015613   |
| ActualImprovement    | 0.015259   |
| ImprovementRatio     | 0.9773     |
| MeanKL               | 0.0073859  |
| Entropy              | 2.048      |
| Perplexity           | 7.752      |
| AveragePolicyStd     | 0.34334    |
| AveragePolicyStd[0]  | 0.35432    |
| AveragePolicyStd[1]  | 0.40832    |
| AveragePolicyStd[2]  | 0.3665     |
| AveragePolicyStd[3]  | 0.35325    |
| AveragePolicyStd[4]  | 0.27026    |
| AveragePolicyStd[5]  | 0.30737    |
| AverageReturn        | 520.84     |
| MinReturn            | 36.795     |
| MaxReturn            | 683.93     |
| StdReturn            | 187.82     |
| AverageEpisodeLength | 792.34     |
| MinEpisodeLength     | 54         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 302.81     |
| TotalNEpisodes       | 14971      |
| TotalNSamples        | 6.3912e+05 |
| ExplainedVariance    | 0.69821    |
-------------------------------------
[2018-12-22 10:19:55.515976 UTC] Saving snapshot
[2018-12-22 10:19:55.516226 UTC] Starting iteration 129
[2018-12-22 10:19:55.516346 UTC] Start collecting samples
[2018-12-22 10:19:58.620760 UTC] Computing input variables for policy optimization
[2018-12-22 10:19:58.697204 UTC] Performing policy update
[2018-12-22 10:19:58.698108 UTC] Computing gradient in Euclidean space
[2018-12-22 10:19:58.787047 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:19:59.835938 UTC] Performing line search
[2018-12-22 10:19:59.963612 UTC] Updating baseline
[2018-12-22 10:20:01.356776 UTC] Computing logging information
-------------------------------------
| Iteration            | 129        |
| ExpectedImprovement  | 0.015647   |
| ActualImprovement    | 0.014125   |
| ImprovementRatio     | 0.9027     |
| MeanKL               | 0.0065867  |
| Entropy              | 2.0292     |
| Perplexity           | 7.6077     |
| AveragePolicyStd     | 0.34209    |
| AveragePolicyStd[0]  | 0.35312    |
| AveragePolicyStd[1]  | 0.40539    |
| AveragePolicyStd[2]  | 0.36333    |
| AveragePolicyStd[3]  | 0.352      |
| AveragePolicyStd[4]  | 0.27161    |
| AveragePolicyStd[5]  | 0.30708    |
| AverageReturn        | 516.22     |
| MinReturn            | 36.795     |
| MaxReturn            | 683.93     |
| StdReturn            | 191.47     |
| AverageEpisodeLength | 784.9      |
| MinEpisodeLength     | 54         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 307.59     |
| TotalNEpisodes       | 14980      |
| TotalNSamples        | 6.4396e+05 |
| ExplainedVariance    | 0.5836     |
-------------------------------------
[2018-12-22 10:20:01.673109 UTC] Saving snapshot
[2018-12-22 10:20:01.673349 UTC] Starting iteration 130
[2018-12-22 10:20:01.673472 UTC] Start collecting samples
[2018-12-22 10:20:04.774907 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:04.849610 UTC] Performing policy update
[2018-12-22 10:20:04.850245 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:04.939046 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:05.990893 UTC] Performing line search
[2018-12-22 10:20:06.113077 UTC] Updating baseline
[2018-12-22 10:20:07.333477 UTC] Computing logging information
-------------------------------------
| Iteration            | 130        |
| ExpectedImprovement  | 0.019956   |
| ActualImprovement    | 0.016709   |
| ImprovementRatio     | 0.8373     |
| MeanKL               | 0.0066839  |
| Entropy              | 2.0369     |
| Perplexity           | 7.6666     |
| AveragePolicyStd     | 0.34253    |
| AveragePolicyStd[0]  | 0.35147    |
| AveragePolicyStd[1]  | 0.40432    |
| AveragePolicyStd[2]  | 0.36538    |
| AveragePolicyStd[3]  | 0.35484    |
| AveragePolicyStd[4]  | 0.27114    |
| AveragePolicyStd[5]  | 0.30803    |
| AverageReturn        | 507.98     |
| MinReturn            | 31.735     |
| MaxReturn            | 683.93     |
| StdReturn            | 197.93     |
| AverageEpisodeLength | 768.65     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 314.5      |
| TotalNEpisodes       | 14987      |
| TotalNSamples        | 6.4909e+05 |
| ExplainedVariance    | 0.22127    |
-------------------------------------
[2018-12-22 10:20:07.650628 UTC] Saving snapshot
[2018-12-22 10:20:07.659036 UTC] Starting iteration 131
[2018-12-22 10:20:07.659240 UTC] Start collecting samples
[2018-12-22 10:20:10.709755 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:10.782689 UTC] Performing policy update
[2018-12-22 10:20:10.783389 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:10.872586 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:11.928008 UTC] Performing line search
[2018-12-22 10:20:12.052779 UTC] Updating baseline
[2018-12-22 10:20:13.264653 UTC] Computing logging information
-------------------------------------
| Iteration            | 131        |
| ExpectedImprovement  | 0.014967   |
| ActualImprovement    | 0.0154     |
| ImprovementRatio     | 1.0289     |
| MeanKL               | 0.0069445  |
| Entropy              | 2.0215     |
| Perplexity           | 7.5494     |
| AveragePolicyStd     | 0.3418     |
| AveragePolicyStd[0]  | 0.35149    |
| AveragePolicyStd[1]  | 0.40808    |
| AveragePolicyStd[2]  | 0.36184    |
| AveragePolicyStd[3]  | 0.35201    |
| AveragePolicyStd[4]  | 0.26881    |
| AveragePolicyStd[5]  | 0.30854    |
| AverageReturn        | 507.32     |
| MinReturn            | 31.735     |
| MaxReturn            | 689.36     |
| StdReturn            | 197.97     |
| AverageEpisodeLength | 766.25     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 313.77     |
| TotalNEpisodes       | 14991      |
| TotalNSamples        | 6.5267e+05 |
| ExplainedVariance    | 0.39811    |
-------------------------------------
[2018-12-22 10:20:13.588418 UTC] Saving snapshot
[2018-12-22 10:20:13.588693 UTC] Starting iteration 132
[2018-12-22 10:20:13.588818 UTC] Start collecting samples
[2018-12-22 10:20:16.801701 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:16.884321 UTC] Performing policy update
[2018-12-22 10:20:16.885146 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:16.978048 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:18.072947 UTC] Performing line search
[2018-12-22 10:20:18.204174 UTC] Updating baseline
[2018-12-22 10:20:19.653183 UTC] Computing logging information
-------------------------------------
| Iteration            | 132        |
| ExpectedImprovement  | 0.016708   |
| ActualImprovement    | 0.016048   |
| ImprovementRatio     | 0.96048    |
| MeanKL               | 0.0070202  |
| Entropy              | 2.0374     |
| Perplexity           | 7.6709     |
| AveragePolicyStd     | 0.34278    |
| AveragePolicyStd[0]  | 0.35359    |
| AveragePolicyStd[1]  | 0.40763    |
| AveragePolicyStd[2]  | 0.36361    |
| AveragePolicyStd[3]  | 0.35516    |
| AveragePolicyStd[4]  | 0.26788    |
| AveragePolicyStd[5]  | 0.30879    |
| AverageReturn        | 505.06     |
| MinReturn            | 31.735     |
| MaxReturn            | 689.36     |
| StdReturn            | 200.12     |
| AverageEpisodeLength | 762.22     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 316.67     |
| TotalNEpisodes       | 15000      |
| TotalNSamples        | 6.6039e+05 |
| ExplainedVariance    | 0.25375    |
-------------------------------------
[2018-12-22 10:20:20.002464 UTC] Saving snapshot
[2018-12-22 10:20:20.002764 UTC] Starting iteration 133
[2018-12-22 10:20:20.002886 UTC] Start collecting samples
[2018-12-22 10:20:23.265196 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:23.344445 UTC] Performing policy update
[2018-12-22 10:20:23.345084 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:23.437388 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:24.533983 UTC] Performing line search
[2018-12-22 10:20:24.663224 UTC] Updating baseline
[2018-12-22 10:20:25.926183 UTC] Computing logging information
-------------------------------------
| Iteration            | 133        |
| ExpectedImprovement  | 0.0131     |
| ActualImprovement    | 0.012422   |
| ImprovementRatio     | 0.94819    |
| MeanKL               | 0.007459   |
| Entropy              | 2.0325     |
| Perplexity           | 7.6333     |
| AveragePolicyStd     | 0.34248    |
| AveragePolicyStd[0]  | 0.35144    |
| AveragePolicyStd[1]  | 0.40724    |
| AveragePolicyStd[2]  | 0.3625     |
| AveragePolicyStd[3]  | 0.35578    |
| AveragePolicyStd[4]  | 0.26686    |
| AveragePolicyStd[5]  | 0.31104    |
| AverageReturn        | 516.71     |
| MinReturn            | 31.735     |
| MaxReturn            | 689.36     |
| StdReturn            | 196.5      |
| AverageEpisodeLength | 778.68     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 309.3      |
| TotalNEpisodes       | 15004      |
| TotalNSamples        | 6.6405e+05 |
| ExplainedVariance    | 0.20429    |
-------------------------------------
[2018-12-22 10:20:26.243318 UTC] Saving snapshot
[2018-12-22 10:20:26.243554 UTC] Starting iteration 134
[2018-12-22 10:20:26.243700 UTC] Start collecting samples
[2018-12-22 10:20:29.278229 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:29.350120 UTC] Performing policy update
[2018-12-22 10:20:29.350784 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:29.439181 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:30.492806 UTC] Performing line search
[2018-12-22 10:20:30.621833 UTC] Updating baseline
[2018-12-22 10:20:31.829912 UTC] Computing logging information
-------------------------------------
| Iteration            | 134        |
| ExpectedImprovement  | 0.01188    |
| ActualImprovement    | 0.011046   |
| ImprovementRatio     | 0.92982    |
| MeanKL               | 0.0073949  |
| Entropy              | 2.0251     |
| Perplexity           | 7.5772     |
| AveragePolicyStd     | 0.34215    |
| AveragePolicyStd[0]  | 0.34957    |
| AveragePolicyStd[1]  | 0.40764    |
| AveragePolicyStd[2]  | 0.3652     |
| AveragePolicyStd[3]  | 0.35439    |
| AveragePolicyStd[4]  | 0.26545    |
| AveragePolicyStd[5]  | 0.31067    |
| AverageReturn        | 521.53     |
| MinReturn            | 31.735     |
| MaxReturn            | 689.36     |
| StdReturn            | 194.31     |
| AverageEpisodeLength | 786.11     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 305.58     |
| TotalNEpisodes       | 15007      |
| TotalNSamples        | 6.6705e+05 |
| ExplainedVariance    | 0.26415    |
-------------------------------------
[2018-12-22 10:20:32.150918 UTC] Saving snapshot
[2018-12-22 10:20:32.151161 UTC] Starting iteration 135
[2018-12-22 10:20:32.151299 UTC] Start collecting samples
[2018-12-22 10:20:35.239690 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:35.313996 UTC] Performing policy update
[2018-12-22 10:20:35.314642 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:35.404012 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:36.458624 UTC] Performing line search
[2018-12-22 10:20:36.582430 UTC] Updating baseline
[2018-12-22 10:20:37.768994 UTC] Computing logging information
-------------------------------------
| Iteration            | 135        |
| ExpectedImprovement  | 0.014012   |
| ActualImprovement    | 0.012664   |
| ImprovementRatio     | 0.90378    |
| MeanKL               | 0.007495   |
| Entropy              | 2.0217     |
| Perplexity           | 7.5515     |
| AveragePolicyStd     | 0.34202    |
| AveragePolicyStd[0]  | 0.34775    |
| AveragePolicyStd[1]  | 0.40852    |
| AveragePolicyStd[2]  | 0.36817    |
| AveragePolicyStd[3]  | 0.35234    |
| AveragePolicyStd[4]  | 0.26497    |
| AveragePolicyStd[5]  | 0.3104     |
| AverageReturn        | 520.7      |
| MinReturn            | 31.735     |
| MaxReturn            | 689.36     |
| StdReturn            | 194.14     |
| AverageEpisodeLength | 784.28     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 304.84     |
| TotalNEpisodes       | 15012      |
| TotalNSamples        | 6.7187e+05 |
| ExplainedVariance    | 0.14453    |
-------------------------------------
[2018-12-22 10:20:38.086899 UTC] Saving snapshot
[2018-12-22 10:20:38.087209 UTC] Starting iteration 136
[2018-12-22 10:20:38.087332 UTC] Start collecting samples
[2018-12-22 10:20:41.189373 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:41.266420 UTC] Performing policy update
[2018-12-22 10:20:41.266989 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:41.355733 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:42.405001 UTC] Performing line search
[2018-12-22 10:20:42.533206 UTC] Updating baseline
[2018-12-22 10:20:43.988938 UTC] Computing logging information
------------------------------------
| Iteration            | 136       |
| ExpectedImprovement  | 0.015829  |
| ActualImprovement    | 0.015768  |
| ImprovementRatio     | 0.99616   |
| MeanKL               | 0.0066299 |
| Entropy              | 2.0118    |
| Perplexity           | 7.4771    |
| AveragePolicyStd     | 0.3415    |
| AveragePolicyStd[0]  | 0.34624   |
| AveragePolicyStd[1]  | 0.40708   |
| AveragePolicyStd[2]  | 0.37012   |
| AveragePolicyStd[3]  | 0.35219   |
| AveragePolicyStd[4]  | 0.26426   |
| AveragePolicyStd[5]  | 0.3091    |
| AverageReturn        | 521.11    |
| MinReturn            | 31.735    |
| MaxReturn            | 689.36    |
| StdReturn            | 194.11    |
| AverageEpisodeLength | 782.89    |
| MinEpisodeLength     | 43        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 302.85    |
| TotalNEpisodes       | 15022     |
| TotalNSamples        | 6.808e+05 |
| ExplainedVariance    | 0.069978  |
------------------------------------
[2018-12-22 10:20:44.307865 UTC] Saving snapshot
[2018-12-22 10:20:44.308100 UTC] Starting iteration 137
[2018-12-22 10:20:44.308224 UTC] Start collecting samples
[2018-12-22 10:20:47.356421 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:47.430864 UTC] Performing policy update
[2018-12-22 10:20:47.431472 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:47.522320 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:48.570981 UTC] Performing line search
[2018-12-22 10:20:48.694140 UTC] Updating baseline
[2018-12-22 10:20:49.987057 UTC] Computing logging information
-------------------------------------
| Iteration            | 137        |
| ExpectedImprovement  | 0.015847   |
| ActualImprovement    | 0.014687   |
| ImprovementRatio     | 0.92679    |
| MeanKL               | 0.0068699  |
| Entropy              | 1.9972     |
| Perplexity           | 7.3684     |
| AveragePolicyStd     | 0.3409     |
| AveragePolicyStd[0]  | 0.34971    |
| AveragePolicyStd[1]  | 0.40562    |
| AveragePolicyStd[2]  | 0.37221    |
| AveragePolicyStd[3]  | 0.35019    |
| AveragePolicyStd[4]  | 0.26002    |
| AveragePolicyStd[5]  | 0.30763    |
| AverageReturn        | 519.41     |
| MinReturn            | 31.735     |
| MaxReturn            | 689.36     |
| StdReturn            | 194.39     |
| AverageEpisodeLength | 780.05     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 303.35     |
| TotalNEpisodes       | 15025      |
| TotalNSamples        | 6.8337e+05 |
| ExplainedVariance    | 0.32276    |
-------------------------------------
[2018-12-22 10:20:50.305234 UTC] Saving snapshot
[2018-12-22 10:20:50.305487 UTC] Starting iteration 138
[2018-12-22 10:20:50.305628 UTC] Start collecting samples
[2018-12-22 10:20:53.339667 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:53.412734 UTC] Performing policy update
[2018-12-22 10:20:53.413304 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:53.501657 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:20:54.553279 UTC] Performing line search
[2018-12-22 10:20:54.676791 UTC] Updating baseline
[2018-12-22 10:20:56.137482 UTC] Computing logging information
-------------------------------------
| Iteration            | 138        |
| ExpectedImprovement  | 0.015303   |
| ActualImprovement    | 0.015277   |
| ImprovementRatio     | 0.99831    |
| MeanKL               | 0.0066819  |
| Entropy              | 1.9834     |
| Perplexity           | 7.2676     |
| AveragePolicyStd     | 0.34006    |
| AveragePolicyStd[0]  | 0.34797    |
| AveragePolicyStd[1]  | 0.40398    |
| AveragePolicyStd[2]  | 0.37167    |
| AveragePolicyStd[3]  | 0.34912    |
| AveragePolicyStd[4]  | 0.25982    |
| AveragePolicyStd[5]  | 0.30779    |
| AverageReturn        | 524.99     |
| MinReturn            | 31.735     |
| MaxReturn            | 689.36     |
| StdReturn            | 190.77     |
| AverageEpisodeLength | 787.24     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 296.59     |
| TotalNEpisodes       | 15029      |
| TotalNSamples        | 6.8724e+05 |
| ExplainedVariance    | 0.11577    |
-------------------------------------
[2018-12-22 10:20:56.456069 UTC] Saving snapshot
[2018-12-22 10:20:56.456306 UTC] Starting iteration 139
[2018-12-22 10:20:56.456423 UTC] Start collecting samples
[2018-12-22 10:20:59.557988 UTC] Computing input variables for policy optimization
[2018-12-22 10:20:59.635540 UTC] Performing policy update
[2018-12-22 10:20:59.636173 UTC] Computing gradient in Euclidean space
[2018-12-22 10:20:59.724247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:00.775985 UTC] Performing line search
[2018-12-22 10:21:00.900597 UTC] Updating baseline
[2018-12-22 10:21:02.143215 UTC] Computing logging information
-------------------------------------
| Iteration            | 139        |
| ExpectedImprovement  | 0.014081   |
| ActualImprovement    | 0.013212   |
| ImprovementRatio     | 0.93833    |
| MeanKL               | 0.0081832  |
| Entropy              | 1.9822     |
| Perplexity           | 7.259      |
| AveragePolicyStd     | 0.33993    |
| AveragePolicyStd[0]  | 0.34898    |
| AveragePolicyStd[1]  | 0.40283    |
| AveragePolicyStd[2]  | 0.37009    |
| AveragePolicyStd[3]  | 0.35001    |
| AveragePolicyStd[4]  | 0.26044    |
| AveragePolicyStd[5]  | 0.30721    |
| AverageReturn        | 525.14     |
| MinReturn            | 31.735     |
| MaxReturn            | 690.64     |
| StdReturn            | 191.96     |
| AverageEpisodeLength | 785.85     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 297.54     |
| TotalNEpisodes       | 15038      |
| TotalNSamples        | 6.9546e+05 |
| ExplainedVariance    | 0.21509    |
-------------------------------------
[2018-12-22 10:21:02.464953 UTC] Saving snapshot
[2018-12-22 10:21:02.465192 UTC] Starting iteration 140
[2018-12-22 10:21:02.465312 UTC] Start collecting samples
[2018-12-22 10:21:05.526287 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:05.601979 UTC] Performing policy update
[2018-12-22 10:21:05.602538 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:05.691897 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:06.738342 UTC] Performing line search
[2018-12-22 10:21:06.863087 UTC] Updating baseline
[2018-12-22 10:21:08.169468 UTC] Computing logging information
-------------------------------------
| Iteration            | 140        |
| ExpectedImprovement  | 0.013545   |
| ActualImprovement    | 0.013461   |
| ImprovementRatio     | 0.99382    |
| MeanKL               | 0.0071005  |
| Entropy              | 1.9637     |
| Perplexity           | 7.1258     |
| AveragePolicyStd     | 0.33891    |
| AveragePolicyStd[0]  | 0.34782    |
| AveragePolicyStd[1]  | 0.40109    |
| AveragePolicyStd[2]  | 0.36754    |
| AveragePolicyStd[3]  | 0.35081    |
| AveragePolicyStd[4]  | 0.25809    |
| AveragePolicyStd[5]  | 0.30808    |
| AverageReturn        | 523.35     |
| MinReturn            | 31.735     |
| MaxReturn            | 690.64     |
| StdReturn            | 194.39     |
| AverageEpisodeLength | 783.16     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 301.84     |
| TotalNEpisodes       | 15043      |
| TotalNSamples        | 6.9894e+05 |
| ExplainedVariance    | 0.44172    |
-------------------------------------
[2018-12-22 10:21:08.492888 UTC] Saving snapshot
[2018-12-22 10:21:08.501126 UTC] Starting iteration 141
[2018-12-22 10:21:08.501329 UTC] Start collecting samples
[2018-12-22 10:21:11.543544 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:11.618696 UTC] Performing policy update
[2018-12-22 10:21:11.619352 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:11.709308 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:12.753213 UTC] Performing line search
[2018-12-22 10:21:12.878225 UTC] Updating baseline
[2018-12-22 10:21:14.333464 UTC] Computing logging information
-------------------------------------
| Iteration            | 141        |
| ExpectedImprovement  | 0.014564   |
| ActualImprovement    | 0.013756   |
| ImprovementRatio     | 0.9445     |
| MeanKL               | 0.0072602  |
| Entropy              | 1.9736     |
| Perplexity           | 7.1962     |
| AveragePolicyStd     | 0.3394     |
| AveragePolicyStd[0]  | 0.34825    |
| AveragePolicyStd[1]  | 0.40105    |
| AveragePolicyStd[2]  | 0.36988    |
| AveragePolicyStd[3]  | 0.34848    |
| AveragePolicyStd[4]  | 0.25918    |
| AveragePolicyStd[5]  | 0.30956    |
| AverageReturn        | 522.67     |
| MinReturn            | 31.735     |
| MaxReturn            | 690.64     |
| StdReturn            | 194.94     |
| AverageEpisodeLength | 779.62     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 301.36     |
| TotalNEpisodes       | 15046      |
| TotalNSamples        | 7.0159e+05 |
| ExplainedVariance    | 0.24207    |
-------------------------------------
[2018-12-22 10:21:14.650747 UTC] Saving snapshot
[2018-12-22 10:21:14.650979 UTC] Starting iteration 142
[2018-12-22 10:21:14.651097 UTC] Start collecting samples
[2018-12-22 10:21:17.777658 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:17.855111 UTC] Performing policy update
[2018-12-22 10:21:17.855783 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:17.949795 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:18.996210 UTC] Performing line search
[2018-12-22 10:21:19.121240 UTC] Updating baseline
[2018-12-22 10:21:20.349085 UTC] Computing logging information
-------------------------------------
| Iteration            | 142        |
| ExpectedImprovement  | 0.012943   |
| ActualImprovement    | 0.0129     |
| ImprovementRatio     | 0.99667    |
| MeanKL               | 0.0068855  |
| Entropy              | 1.9632     |
| Perplexity           | 7.1219     |
| AveragePolicyStd     | 0.33899    |
| AveragePolicyStd[0]  | 0.34819    |
| AveragePolicyStd[1]  | 0.40102    |
| AveragePolicyStd[2]  | 0.37       |
| AveragePolicyStd[3]  | 0.34964    |
| AveragePolicyStd[4]  | 0.2561     |
| AveragePolicyStd[5]  | 0.309      |
| AverageReturn        | 527.25     |
| MinReturn            | 31.735     |
| MaxReturn            | 706.49     |
| StdReturn            | 193        |
| AverageEpisodeLength | 784.44     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 297.7      |
| TotalNEpisodes       | 15057      |
| TotalNSamples        | 7.1002e+05 |
| ExplainedVariance    | 0.37915    |
-------------------------------------
[2018-12-22 10:21:20.670702 UTC] Saving snapshot
[2018-12-22 10:21:20.670958 UTC] Starting iteration 143
[2018-12-22 10:21:20.671078 UTC] Start collecting samples
[2018-12-22 10:21:23.762295 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:23.840765 UTC] Performing policy update
[2018-12-22 10:21:23.841356 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:23.934477 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:24.987152 UTC] Performing line search
[2018-12-22 10:21:25.111819 UTC] Updating baseline
[2018-12-22 10:21:26.425135 UTC] Computing logging information
-------------------------------------
| Iteration            | 143        |
| ExpectedImprovement  | 0.02093    |
| ActualImprovement    | 0.019309   |
| ImprovementRatio     | 0.92254    |
| MeanKL               | 0.0065335  |
| Entropy              | 1.9733     |
| Perplexity           | 7.194      |
| AveragePolicyStd     | 0.33966    |
| AveragePolicyStd[0]  | 0.34959    |
| AveragePolicyStd[1]  | 0.40055    |
| AveragePolicyStd[2]  | 0.37096    |
| AveragePolicyStd[3]  | 0.35103    |
| AveragePolicyStd[4]  | 0.25377    |
| AveragePolicyStd[5]  | 0.31205    |
| AverageReturn        | 532.77     |
| MinReturn            | 14.275     |
| MaxReturn            | 706.49     |
| StdReturn            | 203.16     |
| AverageEpisodeLength | 793.14     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 309.71     |
| TotalNEpisodes       | 15067      |
| TotalNSamples        | 7.1585e+05 |
| ExplainedVariance    | 0.16055    |
-------------------------------------
[2018-12-22 10:21:26.747081 UTC] Saving snapshot
[2018-12-22 10:21:26.747375 UTC] Starting iteration 144
[2018-12-22 10:21:26.747497 UTC] Start collecting samples
[2018-12-22 10:21:29.807622 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:29.879797 UTC] Performing policy update
[2018-12-22 10:21:29.880431 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:29.972972 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:31.013896 UTC] Performing line search
[2018-12-22 10:21:31.137356 UTC] Updating baseline
[2018-12-22 10:21:32.342594 UTC] Computing logging information
-------------------------------------
| Iteration            | 144        |
| ExpectedImprovement  | 0.015168   |
| ActualImprovement    | 0.014075   |
| ImprovementRatio     | 0.92795    |
| MeanKL               | 0.007095   |
| Entropy              | 1.9762     |
| Perplexity           | 7.2152     |
| AveragePolicyStd     | 0.33974    |
| AveragePolicyStd[0]  | 0.3512     |
| AveragePolicyStd[1]  | 0.39886    |
| AveragePolicyStd[2]  | 0.37024    |
| AveragePolicyStd[3]  | 0.35237    |
| AveragePolicyStd[4]  | 0.25491    |
| AveragePolicyStd[5]  | 0.31086    |
| AverageReturn        | 533.19     |
| MinReturn            | 14.275     |
| MaxReturn            | 706.49     |
| StdReturn            | 203.69     |
| AverageEpisodeLength | 792.86     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 310.03     |
| TotalNEpisodes       | 15069      |
| TotalNSamples        | 7.1728e+05 |
| ExplainedVariance    | 0.44714    |
-------------------------------------
[2018-12-22 10:21:32.663072 UTC] Saving snapshot
[2018-12-22 10:21:32.663314 UTC] Starting iteration 145
[2018-12-22 10:21:32.663437 UTC] Start collecting samples
[2018-12-22 10:21:35.793060 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:35.870170 UTC] Performing policy update
[2018-12-22 10:21:35.870846 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:35.963042 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:37.005256 UTC] Performing line search
[2018-12-22 10:21:37.128801 UTC] Updating baseline
[2018-12-22 10:21:38.319410 UTC] Computing logging information
-------------------------------------
| Iteration            | 145        |
| ExpectedImprovement  | 0.016707   |
| ActualImprovement    | 0.017315   |
| ImprovementRatio     | 1.0364     |
| MeanKL               | 0.0068328  |
| Entropy              | 1.9877     |
| Perplexity           | 7.299      |
| AveragePolicyStd     | 0.34044    |
| AveragePolicyStd[0]  | 0.3513     |
| AveragePolicyStd[1]  | 0.39658    |
| AveragePolicyStd[2]  | 0.37409    |
| AveragePolicyStd[3]  | 0.35392    |
| AveragePolicyStd[4]  | 0.2538     |
| AveragePolicyStd[5]  | 0.31294    |
| AverageReturn        | 552.42     |
| MinReturn            | 14.275     |
| MaxReturn            | 711.98     |
| StdReturn            | 187.65     |
| AverageEpisodeLength | 818.17     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 285.02     |
| TotalNEpisodes       | 15080      |
| TotalNSamples        | 7.2578e+05 |
| ExplainedVariance    | 0.66272    |
-------------------------------------
[2018-12-22 10:21:38.634914 UTC] Saving snapshot
[2018-12-22 10:21:38.635193 UTC] Starting iteration 146
[2018-12-22 10:21:38.635312 UTC] Start collecting samples
[2018-12-22 10:21:41.698709 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:41.777276 UTC] Performing policy update
[2018-12-22 10:21:41.777885 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:41.865229 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:42.905159 UTC] Performing line search
[2018-12-22 10:21:43.027273 UTC] Updating baseline
[2018-12-22 10:21:44.244911 UTC] Computing logging information
-------------------------------------
| Iteration            | 146        |
| ExpectedImprovement  | 0.014974   |
| ActualImprovement    | 0.014642   |
| ImprovementRatio     | 0.97781    |
| MeanKL               | 0.0069155  |
| Entropy              | 1.9856     |
| Perplexity           | 7.2832     |
| AveragePolicyStd     | 0.34035    |
| AveragePolicyStd[0]  | 0.35005    |
| AveragePolicyStd[1]  | 0.39632    |
| AveragePolicyStd[2]  | 0.373      |
| AveragePolicyStd[3]  | 0.35574    |
| AveragePolicyStd[4]  | 0.25252    |
| AveragePolicyStd[5]  | 0.31447    |
| AverageReturn        | 561.16     |
| MinReturn            | 14.275     |
| MaxReturn            | 727.16     |
| StdReturn            | 182.14     |
| AverageEpisodeLength | 827.65     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.27     |
| TotalNEpisodes       | 15086      |
| TotalNSamples        | 7.3086e+05 |
| ExplainedVariance    | 0.69381    |
-------------------------------------
[2018-12-22 10:21:44.592689 UTC] Saving snapshot
[2018-12-22 10:21:44.592955 UTC] Starting iteration 147
[2018-12-22 10:21:44.593078 UTC] Start collecting samples
[2018-12-22 10:21:47.870251 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:47.949857 UTC] Performing policy update
[2018-12-22 10:21:47.950653 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:48.043673 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:49.140252 UTC] Performing line search
[2018-12-22 10:21:49.271381 UTC] Updating baseline
[2018-12-22 10:21:50.527769 UTC] Computing logging information
-------------------------------------
| Iteration            | 147        |
| ExpectedImprovement  | 0.01386    |
| ActualImprovement    | 0.014126   |
| ImprovementRatio     | 1.0192     |
| MeanKL               | 0.0069399  |
| Entropy              | 1.9781     |
| Perplexity           | 7.229      |
| AveragePolicyStd     | 0.33997    |
| AveragePolicyStd[0]  | 0.34807    |
| AveragePolicyStd[1]  | 0.3927     |
| AveragePolicyStd[2]  | 0.37507    |
| AveragePolicyStd[3]  | 0.35791    |
| AveragePolicyStd[4]  | 0.25098    |
| AveragePolicyStd[5]  | 0.31506    |
| AverageReturn        | 549.86     |
| MinReturn            | 14.275     |
| MaxReturn            | 727.16     |
| StdReturn            | 186.9      |
| AverageEpisodeLength | 810.52     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 283.97     |
| TotalNEpisodes       | 15090      |
| TotalNSamples        | 7.3286e+05 |
| ExplainedVariance    | 0.9085     |
-------------------------------------
[2018-12-22 10:21:50.848477 UTC] Saving snapshot
[2018-12-22 10:21:50.848735 UTC] Starting iteration 148
[2018-12-22 10:21:50.848865 UTC] Start collecting samples
[2018-12-22 10:21:53.869468 UTC] Computing input variables for policy optimization
[2018-12-22 10:21:53.945565 UTC] Performing policy update
[2018-12-22 10:21:53.946446 UTC] Computing gradient in Euclidean space
[2018-12-22 10:21:54.034357 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:21:55.082035 UTC] Performing line search
[2018-12-22 10:21:55.207123 UTC] Updating baseline
[2018-12-22 10:21:57.093763 UTC] Computing logging information
-------------------------------------
| Iteration            | 148        |
| ExpectedImprovement  | 0.012773   |
| ActualImprovement    | 0.012466   |
| ImprovementRatio     | 0.97593    |
| MeanKL               | 0.0076289  |
| Entropy              | 1.9606     |
| Perplexity           | 7.1037     |
| AveragePolicyStd     | 0.33902    |
| AveragePolicyStd[0]  | 0.34703    |
| AveragePolicyStd[1]  | 0.39146    |
| AveragePolicyStd[2]  | 0.37535    |
| AveragePolicyStd[3]  | 0.35698    |
| AveragePolicyStd[4]  | 0.2501     |
| AveragePolicyStd[5]  | 0.31318    |
| AverageReturn        | 553.56     |
| MinReturn            | 14.275     |
| MaxReturn            | 727.16     |
| StdReturn            | 188.61     |
| AverageEpisodeLength | 812.43     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 284.47     |
| TotalNEpisodes       | 15096      |
| TotalNSamples        | 7.3864e+05 |
| ExplainedVariance    | 0.14842    |
-------------------------------------
[2018-12-22 10:21:57.414052 UTC] Saving snapshot
[2018-12-22 10:21:57.414288 UTC] Starting iteration 149
[2018-12-22 10:21:57.414418 UTC] Start collecting samples
[2018-12-22 10:22:00.461521 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:00.536935 UTC] Performing policy update
[2018-12-22 10:22:00.537505 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:00.627841 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:01.668506 UTC] Performing line search
[2018-12-22 10:22:01.792953 UTC] Updating baseline
[2018-12-22 10:22:03.006651 UTC] Computing logging information
-------------------------------------
| Iteration            | 149        |
| ExpectedImprovement  | 0.010356   |
| ActualImprovement    | 0.0099445  |
| ImprovementRatio     | 0.96029    |
| MeanKL               | 0.0071406  |
| Entropy              | 1.9532     |
| Perplexity           | 7.0511     |
| AveragePolicyStd     | 0.33848    |
| AveragePolicyStd[0]  | 0.34603    |
| AveragePolicyStd[1]  | 0.39144    |
| AveragePolicyStd[2]  | 0.37462    |
| AveragePolicyStd[3]  | 0.35476    |
| AveragePolicyStd[4]  | 0.25171    |
| AveragePolicyStd[5]  | 0.31233    |
| AverageReturn        | 562.48     |
| MinReturn            | 14.275     |
| MaxReturn            | 727.16     |
| StdReturn            | 188        |
| AverageEpisodeLength | 822.54     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 281.44     |
| TotalNEpisodes       | 15101      |
| TotalNSamples        | 7.4364e+05 |
| ExplainedVariance    | -0.0082349 |
-------------------------------------
[2018-12-22 10:22:03.326957 UTC] Saving snapshot
[2018-12-22 10:22:03.327191 UTC] Starting iteration 150
[2018-12-22 10:22:03.327307 UTC] Start collecting samples
[2018-12-22 10:22:06.385758 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:06.459517 UTC] Performing policy update
[2018-12-22 10:22:06.460170 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:06.549557 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:07.598159 UTC] Performing line search
[2018-12-22 10:22:07.722742 UTC] Updating baseline
[2018-12-22 10:22:09.026812 UTC] Computing logging information
-------------------------------------
| Iteration            | 150        |
| ExpectedImprovement  | 0.024449   |
| ActualImprovement    | 0.017664   |
| ImprovementRatio     | 0.72249    |
| MeanKL               | 0.0065939  |
| Entropy              | 1.9435     |
| Perplexity           | 6.9832     |
| AveragePolicyStd     | 0.33801    |
| AveragePolicyStd[0]  | 0.34303    |
| AveragePolicyStd[1]  | 0.39357    |
| AveragePolicyStd[2]  | 0.37404    |
| AveragePolicyStd[3]  | 0.35609    |
| AveragePolicyStd[4]  | 0.25201    |
| AveragePolicyStd[5]  | 0.30929    |
| AverageReturn        | 557.17     |
| MinReturn            | 10.85      |
| MaxReturn            | 727.16     |
| StdReturn            | 196.3      |
| AverageEpisodeLength | 812.59     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 291.95     |
| TotalNEpisodes       | 15107      |
| TotalNSamples        | 7.4831e+05 |
| ExplainedVariance    | 0.35515    |
-------------------------------------
[2018-12-22 10:22:09.347983 UTC] Saving snapshot
[2018-12-22 10:22:09.356112 UTC] Starting iteration 151
[2018-12-22 10:22:09.356312 UTC] Start collecting samples
[2018-12-22 10:22:12.422200 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:12.496512 UTC] Performing policy update
[2018-12-22 10:22:12.497322 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:12.587380 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:13.640242 UTC] Performing line search
[2018-12-22 10:22:13.768750 UTC] Updating baseline
[2018-12-22 10:22:14.903614 UTC] Computing logging information
-------------------------------------
| Iteration            | 151        |
| ExpectedImprovement  | 0.013594   |
| ActualImprovement    | 0.013711   |
| ImprovementRatio     | 1.0086     |
| MeanKL               | 0.0074076  |
| Entropy              | 1.9318     |
| Perplexity           | 6.9018     |
| AveragePolicyStd     | 0.33733    |
| AveragePolicyStd[0]  | 0.34394    |
| AveragePolicyStd[1]  | 0.39374    |
| AveragePolicyStd[2]  | 0.37059    |
| AveragePolicyStd[3]  | 0.35556    |
| AveragePolicyStd[4]  | 0.25162    |
| AveragePolicyStd[5]  | 0.30852    |
| AverageReturn        | 555.71     |
| MinReturn            | 10.85      |
| MaxReturn            | 727.16     |
| StdReturn            | 200.57     |
| AverageEpisodeLength | 807.04     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 297.05     |
| TotalNEpisodes       | 15112      |
| TotalNSamples        | 7.5257e+05 |
| ExplainedVariance    | 0.14306    |
-------------------------------------
[2018-12-22 10:22:15.228103 UTC] Saving snapshot
[2018-12-22 10:22:15.228393 UTC] Starting iteration 152
[2018-12-22 10:22:15.228515 UTC] Start collecting samples
[2018-12-22 10:22:18.348426 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:18.426105 UTC] Performing policy update
[2018-12-22 10:22:18.426842 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:18.516900 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:19.560684 UTC] Performing line search
[2018-12-22 10:22:19.686457 UTC] Updating baseline
[2018-12-22 10:22:21.323831 UTC] Computing logging information
-------------------------------------
| Iteration            | 152        |
| ExpectedImprovement  | 0.014479   |
| ActualImprovement    | 0.014111   |
| ImprovementRatio     | 0.97461    |
| MeanKL               | 0.0068751  |
| Entropy              | 1.9392     |
| Perplexity           | 6.9533     |
| AveragePolicyStd     | 0.33787    |
| AveragePolicyStd[0]  | 0.344      |
| AveragePolicyStd[1]  | 0.39635    |
| AveragePolicyStd[2]  | 0.37088    |
| AveragePolicyStd[3]  | 0.35629    |
| AveragePolicyStd[4]  | 0.25064    |
| AveragePolicyStd[5]  | 0.30905    |
| AverageReturn        | 554.31     |
| MinReturn            | 10.85      |
| MaxReturn            | 729.64     |
| StdReturn            | 203.57     |
| AverageEpisodeLength | 800.25     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 299.09     |
| TotalNEpisodes       | 15122      |
| TotalNSamples        | 7.6083e+05 |
| ExplainedVariance    | 0.18358    |
-------------------------------------
[2018-12-22 10:22:21.645228 UTC] Saving snapshot
[2018-12-22 10:22:21.645464 UTC] Starting iteration 153
[2018-12-22 10:22:21.645598 UTC] Start collecting samples
[2018-12-22 10:22:24.710984 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:24.786594 UTC] Performing policy update
[2018-12-22 10:22:24.787314 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:24.876705 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:25.818216 UTC] Performing line search
[2018-12-22 10:22:25.932531 UTC] Updating baseline
[2018-12-22 10:22:27.235448 UTC] Computing logging information
-------------------------------------
| Iteration            | 153        |
| ExpectedImprovement  | 0.017847   |
| ActualImprovement    | 0.016856   |
| ImprovementRatio     | 0.94445    |
| MeanKL               | 0.0067281  |
| Entropy              | 1.9531     |
| Perplexity           | 7.0503     |
| AveragePolicyStd     | 0.33869    |
| AveragePolicyStd[0]  | 0.34781    |
| AveragePolicyStd[1]  | 0.3987     |
| AveragePolicyStd[2]  | 0.36778    |
| AveragePolicyStd[3]  | 0.36014    |
| AveragePolicyStd[4]  | 0.25216    |
| AveragePolicyStd[5]  | 0.30554    |
| AverageReturn        | 541.61     |
| MinReturn            | 10.85      |
| MaxReturn            | 729.64     |
| StdReturn            | 209.76     |
| AverageEpisodeLength | 780.9      |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 308.43     |
| TotalNEpisodes       | 15129      |
| TotalNSamples        | 7.6532e+05 |
| ExplainedVariance    | 0.63679    |
-------------------------------------
[2018-12-22 10:22:27.556199 UTC] Saving snapshot
[2018-12-22 10:22:27.556450 UTC] Starting iteration 154
[2018-12-22 10:22:27.556587 UTC] Start collecting samples
[2018-12-22 10:22:30.604741 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:30.678458 UTC] Performing policy update
[2018-12-22 10:22:30.680410 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:30.769961 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:31.816642 UTC] Performing line search
[2018-12-22 10:22:31.943707 UTC] Updating baseline
[2018-12-22 10:22:33.151823 UTC] Computing logging information
-------------------------------------
| Iteration            | 154        |
| ExpectedImprovement  | 0.014137   |
| ActualImprovement    | 0.012791   |
| ImprovementRatio     | 0.9048     |
| MeanKL               | 0.0071906  |
| Entropy              | 1.9325     |
| Perplexity           | 6.9069     |
| AveragePolicyStd     | 0.33748    |
| AveragePolicyStd[0]  | 0.34673    |
| AveragePolicyStd[1]  | 0.39591    |
| AveragePolicyStd[2]  | 0.36389    |
| AveragePolicyStd[3]  | 0.36147    |
| AveragePolicyStd[4]  | 0.25095    |
| AveragePolicyStd[5]  | 0.30595    |
| AverageReturn        | 537.32     |
| MinReturn            | 10.85      |
| MaxReturn            | 729.64     |
| StdReturn            | 210.88     |
| AverageEpisodeLength | 771.95     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 308.49     |
| TotalNEpisodes       | 15133      |
| TotalNSamples        | 7.6843e+05 |
| ExplainedVariance    | 0.48236    |
-------------------------------------
[2018-12-22 10:22:33.475664 UTC] Saving snapshot
[2018-12-22 10:22:33.475898 UTC] Starting iteration 155
[2018-12-22 10:22:33.476018 UTC] Start collecting samples
[2018-12-22 10:22:36.510594 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:36.585063 UTC] Performing policy update
[2018-12-22 10:22:36.585659 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:36.673454 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:37.721639 UTC] Performing line search
[2018-12-22 10:22:37.850481 UTC] Updating baseline
[2018-12-22 10:22:39.144786 UTC] Computing logging information
-------------------------------------
| Iteration            | 155        |
| ExpectedImprovement  | 0.016186   |
| ActualImprovement    | 0.016201   |
| ImprovementRatio     | 1.0009     |
| MeanKL               | 0.0069381  |
| Entropy              | 1.9224     |
| Perplexity           | 6.8372     |
| AveragePolicyStd     | 0.3369     |
| AveragePolicyStd[0]  | 0.34487    |
| AveragePolicyStd[1]  | 0.39418    |
| AveragePolicyStd[2]  | 0.36432    |
| AveragePolicyStd[3]  | 0.36157    |
| AveragePolicyStd[4]  | 0.25029    |
| AveragePolicyStd[5]  | 0.30619    |
| AverageReturn        | 541.36     |
| MinReturn            | 10.85      |
| MaxReturn            | 729.64     |
| StdReturn            | 210.34     |
| AverageEpisodeLength | 775.32     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 305.6      |
| TotalNEpisodes       | 15138      |
| TotalNSamples        | 7.7299e+05 |
| ExplainedVariance    | 0.031978   |
-------------------------------------
[2018-12-22 10:22:39.466979 UTC] Saving snapshot
[2018-12-22 10:22:39.467217 UTC] Starting iteration 156
[2018-12-22 10:22:39.467350 UTC] Start collecting samples
[2018-12-22 10:22:42.523558 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:42.599713 UTC] Performing policy update
[2018-12-22 10:22:42.600294 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:42.692117 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:43.742979 UTC] Performing line search
[2018-12-22 10:22:43.870629 UTC] Updating baseline
[2018-12-22 10:22:45.336808 UTC] Computing logging information
-------------------------------------
| Iteration            | 156        |
| ExpectedImprovement  | 0.015618   |
| ActualImprovement    | 0.015959   |
| ImprovementRatio     | 1.0219     |
| MeanKL               | 0.0068945  |
| Entropy              | 1.9235     |
| Perplexity           | 6.8452     |
| AveragePolicyStd     | 0.33694    |
| AveragePolicyStd[0]  | 0.34488    |
| AveragePolicyStd[1]  | 0.39425    |
| AveragePolicyStd[2]  | 0.36537    |
| AveragePolicyStd[3]  | 0.3604     |
| AveragePolicyStd[4]  | 0.25089    |
| AveragePolicyStd[5]  | 0.30587    |
| AverageReturn        | 553.97     |
| MinReturn            | 10.85      |
| MaxReturn            | 730.58     |
| StdReturn            | 206.68     |
| AverageEpisodeLength | 790.47     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 297.4      |
| TotalNEpisodes       | 15144      |
| TotalNSamples        | 7.7899e+05 |
| ExplainedVariance    | 0.15473    |
-------------------------------------
[2018-12-22 10:22:45.659977 UTC] Saving snapshot
[2018-12-22 10:22:45.660225 UTC] Starting iteration 157
[2018-12-22 10:22:45.660350 UTC] Start collecting samples
[2018-12-22 10:22:48.706872 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:48.780691 UTC] Performing policy update
[2018-12-22 10:22:48.781298 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:48.870779 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:49.928139 UTC] Performing line search
[2018-12-22 10:22:50.052593 UTC] Updating baseline
[2018-12-22 10:22:51.580655 UTC] Computing logging information
-------------------------------------
| Iteration            | 157        |
| ExpectedImprovement  | 0.012776   |
| ActualImprovement    | 0.01234    |
| ImprovementRatio     | 0.96589    |
| MeanKL               | 0.0070629  |
| Entropy              | 1.9171     |
| Perplexity           | 6.8011     |
| AveragePolicyStd     | 0.33684    |
| AveragePolicyStd[0]  | 0.34518    |
| AveragePolicyStd[1]  | 0.39528    |
| AveragePolicyStd[2]  | 0.36786    |
| AveragePolicyStd[3]  | 0.36072    |
| AveragePolicyStd[4]  | 0.2481     |
| AveragePolicyStd[5]  | 0.3039     |
| AverageReturn        | 557.96     |
| MinReturn            | 10.85      |
| MaxReturn            | 730.58     |
| StdReturn            | 207.53     |
| AverageEpisodeLength | 794.7      |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 298.05     |
| TotalNEpisodes       | 15149      |
| TotalNSamples        | 7.8394e+05 |
| ExplainedVariance    | 0.15425    |
-------------------------------------
[2018-12-22 10:22:51.904079 UTC] Saving snapshot
[2018-12-22 10:22:51.904351 UTC] Starting iteration 158
[2018-12-22 10:22:51.904482 UTC] Start collecting samples
[2018-12-22 10:22:54.923393 UTC] Computing input variables for policy optimization
[2018-12-22 10:22:54.996408 UTC] Performing policy update
[2018-12-22 10:22:54.996999 UTC] Computing gradient in Euclidean space
[2018-12-22 10:22:55.086199 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:22:56.139767 UTC] Performing line search
[2018-12-22 10:22:56.265717 UTC] Updating baseline
[2018-12-22 10:22:57.711661 UTC] Computing logging information
-------------------------------------
| Iteration            | 158        |
| ExpectedImprovement  | 0.013752   |
| ActualImprovement    | 0.013206   |
| ImprovementRatio     | 0.96028    |
| MeanKL               | 0.0072416  |
| Entropy              | 1.9373     |
| Perplexity           | 6.9402     |
| AveragePolicyStd     | 0.33817    |
| AveragePolicyStd[0]  | 0.34354    |
| AveragePolicyStd[1]  | 0.40396    |
| AveragePolicyStd[2]  | 0.36897    |
| AveragePolicyStd[3]  | 0.36105    |
| AveragePolicyStd[4]  | 0.24938    |
| AveragePolicyStd[5]  | 0.30216    |
| AverageReturn        | 564.04     |
| MinReturn            | 10.85      |
| MaxReturn            | 730.58     |
| StdReturn            | 204.61     |
| AverageEpisodeLength | 802.53     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 293.01     |
| TotalNEpisodes       | 15152      |
| TotalNSamples        | 7.8694e+05 |
| ExplainedVariance    | -0.040215  |
-------------------------------------
[2018-12-22 10:22:58.028696 UTC] Saving snapshot
[2018-12-22 10:22:58.028942 UTC] Starting iteration 159
[2018-12-22 10:22:58.029063 UTC] Start collecting samples
[2018-12-22 10:23:01.102474 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:01.179791 UTC] Performing policy update
[2018-12-22 10:23:01.180507 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:01.270593 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:02.323403 UTC] Performing line search
[2018-12-22 10:23:02.448200 UTC] Updating baseline
[2018-12-22 10:23:04.012114 UTC] Computing logging information
-------------------------------------
| Iteration            | 159        |
| ExpectedImprovement  | 0.012644   |
| ActualImprovement    | 0.012021   |
| ImprovementRatio     | 0.95072    |
| MeanKL               | 0.007328   |
| Entropy              | 1.9276     |
| Perplexity           | 6.8728     |
| AveragePolicyStd     | 0.33774    |
| AveragePolicyStd[0]  | 0.34131    |
| AveragePolicyStd[1]  | 0.40603    |
| AveragePolicyStd[2]  | 0.36958    |
| AveragePolicyStd[3]  | 0.3594     |
| AveragePolicyStd[4]  | 0.24837    |
| AveragePolicyStd[5]  | 0.30173    |
| AverageReturn        | 576.8      |
| MinReturn            | 10.85      |
| MaxReturn            | 730.58     |
| StdReturn            | 199.73     |
| AverageEpisodeLength | 819.83     |
| MinEpisodeLength     | 22         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 285.25     |
| TotalNEpisodes       | 15159      |
| TotalNSamples        | 7.9334e+05 |
| ExplainedVariance    | 0.1931     |
-------------------------------------
[2018-12-22 10:23:04.335047 UTC] Saving snapshot
[2018-12-22 10:23:04.335292 UTC] Starting iteration 160
[2018-12-22 10:23:04.335410 UTC] Start collecting samples
[2018-12-22 10:23:07.398028 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:07.473943 UTC] Performing policy update
[2018-12-22 10:23:07.474518 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:07.563446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:08.626066 UTC] Performing line search
[2018-12-22 10:23:08.751324 UTC] Updating baseline
[2018-12-22 10:23:10.229579 UTC] Computing logging information
-------------------------------------
| Iteration            | 160        |
| ExpectedImprovement  | 0.013729   |
| ActualImprovement    | 0.013405   |
| ImprovementRatio     | 0.97635    |
| MeanKL               | 0.007027   |
| Entropy              | 1.9134     |
| Perplexity           | 6.7764     |
| AveragePolicyStd     | 0.33693    |
| AveragePolicyStd[0]  | 0.33976    |
| AveragePolicyStd[1]  | 0.40587    |
| AveragePolicyStd[2]  | 0.36755    |
| AveragePolicyStd[3]  | 0.35775    |
| AveragePolicyStd[4]  | 0.24714    |
| AveragePolicyStd[5]  | 0.30351    |
| AverageReturn        | 594.86     |
| MinReturn            | 10.85      |
| MaxReturn            | 730.58     |
| StdReturn            | 181.51     |
| AverageEpisodeLength | 844.62     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 259.24     |
| TotalNEpisodes       | 15166      |
| TotalNSamples        | 8.0028e+05 |
| ExplainedVariance    | 0.21519    |
-------------------------------------
[2018-12-22 10:23:10.551986 UTC] Saving snapshot
[2018-12-22 10:23:10.560197 UTC] Starting iteration 161
[2018-12-22 10:23:10.560410 UTC] Start collecting samples
[2018-12-22 10:23:13.621377 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:13.696685 UTC] Performing policy update
[2018-12-22 10:23:13.697392 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:13.785108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:14.830650 UTC] Performing line search
[2018-12-22 10:23:14.893398 UTC] Updating baseline
[2018-12-22 10:23:16.292842 UTC] Computing logging information
-------------------------------------
| Iteration            | 161        |
| ExpectedImprovement  | 0.035871   |
| ActualImprovement    | 0.028487   |
| ImprovementRatio     | 0.79415    |
| MeanKL               | 0.0099692  |
| Entropy              | 1.9073     |
| Perplexity           | 6.7351     |
| AveragePolicyStd     | 0.33656    |
| AveragePolicyStd[0]  | 0.34061    |
| AveragePolicyStd[1]  | 0.40289    |
| AveragePolicyStd[2]  | 0.36733    |
| AveragePolicyStd[3]  | 0.35842    |
| AveragePolicyStd[4]  | 0.24612    |
| AveragePolicyStd[5]  | 0.304      |
| AverageReturn        | 600.97     |
| MinReturn            | 10.85      |
| MaxReturn            | 730.58     |
| StdReturn            | 177.78     |
| AverageEpisodeLength | 852.68     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.65     |
| TotalNEpisodes       | 15171      |
| TotalNSamples        | 8.0375e+05 |
| ExplainedVariance    | 0.16136    |
-------------------------------------
[2018-12-22 10:23:16.614537 UTC] Saving snapshot
[2018-12-22 10:23:16.614786 UTC] Starting iteration 162
[2018-12-22 10:23:16.614915 UTC] Start collecting samples
[2018-12-22 10:23:19.657024 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:19.731841 UTC] Performing policy update
[2018-12-22 10:23:19.732535 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:19.821865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:20.864500 UTC] Performing line search
[2018-12-22 10:23:20.989812 UTC] Updating baseline
[2018-12-22 10:23:22.217330 UTC] Computing logging information
-------------------------------------
| Iteration            | 162        |
| ExpectedImprovement  | 0.020725   |
| ActualImprovement    | 0.019096   |
| ImprovementRatio     | 0.9214     |
| MeanKL               | 0.0066199  |
| Entropy              | 1.9039     |
| Perplexity           | 6.7121     |
| AveragePolicyStd     | 0.33637    |
| AveragePolicyStd[0]  | 0.34037    |
| AveragePolicyStd[1]  | 0.40382    |
| AveragePolicyStd[2]  | 0.36579    |
| AveragePolicyStd[3]  | 0.35846    |
| AveragePolicyStd[4]  | 0.24646    |
| AveragePolicyStd[5]  | 0.3033     |
| AverageReturn        | 606.82     |
| MinReturn            | 10.85      |
| MaxReturn            | 730.58     |
| StdReturn            | 173.59     |
| AverageEpisodeLength | 861.3      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 247        |
| TotalNEpisodes       | 15175      |
| TotalNSamples        | 8.0726e+05 |
| ExplainedVariance    | 0.28509    |
-------------------------------------
[2018-12-22 10:23:22.537991 UTC] Saving snapshot
[2018-12-22 10:23:22.538232 UTC] Starting iteration 163
[2018-12-22 10:23:22.538355 UTC] Start collecting samples
[2018-12-22 10:23:25.590145 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:25.666177 UTC] Performing policy update
[2018-12-22 10:23:25.666967 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:25.760862 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:26.814549 UTC] Performing line search
[2018-12-22 10:23:26.938774 UTC] Updating baseline
[2018-12-22 10:23:28.132909 UTC] Computing logging information
-------------------------------------
| Iteration            | 163        |
| ExpectedImprovement  | 0.015202   |
| ActualImprovement    | 0.014603   |
| ImprovementRatio     | 0.96059    |
| MeanKL               | 0.0074819  |
| Entropy              | 1.8858     |
| Perplexity           | 6.5915     |
| AveragePolicyStd     | 0.33545    |
| AveragePolicyStd[0]  | 0.33881    |
| AveragePolicyStd[1]  | 0.40431    |
| AveragePolicyStd[2]  | 0.36547    |
| AveragePolicyStd[3]  | 0.35765    |
| AveragePolicyStd[4]  | 0.24558    |
| AveragePolicyStd[5]  | 0.30088    |
| AverageReturn        | 608.35     |
| MinReturn            | 10.85      |
| MaxReturn            | 734.77     |
| StdReturn            | 175.74     |
| AverageEpisodeLength | 860.65     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.83     |
| TotalNEpisodes       | 15182      |
| TotalNSamples        | 8.1384e+05 |
| ExplainedVariance    | 0.18319    |
-------------------------------------
[2018-12-22 10:23:28.458156 UTC] Saving snapshot
[2018-12-22 10:23:28.458410 UTC] Starting iteration 164
[2018-12-22 10:23:28.458544 UTC] Start collecting samples
[2018-12-22 10:23:31.524961 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:31.600779 UTC] Performing policy update
[2018-12-22 10:23:31.601407 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:31.689914 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:32.737495 UTC] Performing line search
[2018-12-22 10:23:32.861942 UTC] Updating baseline
[2018-12-22 10:23:34.184477 UTC] Computing logging information
-------------------------------------
| Iteration            | 164        |
| ExpectedImprovement  | 0.016279   |
| ActualImprovement    | 0.015825   |
| ImprovementRatio     | 0.97211    |
| MeanKL               | 0.0068648  |
| Entropy              | 1.8771     |
| Perplexity           | 6.5344     |
| AveragePolicyStd     | 0.33503    |
| AveragePolicyStd[0]  | 0.33782    |
| AveragePolicyStd[1]  | 0.40634    |
| AveragePolicyStd[2]  | 0.36327    |
| AveragePolicyStd[3]  | 0.35742    |
| AveragePolicyStd[4]  | 0.24504    |
| AveragePolicyStd[5]  | 0.3003     |
| AverageReturn        | 615.5      |
| MinReturn            | 10.85      |
| MaxReturn            | 734.77     |
| StdReturn            | 172.55     |
| AverageEpisodeLength | 870.25     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 244.55     |
| TotalNEpisodes       | 15189      |
| TotalNSamples        | 8.1929e+05 |
| ExplainedVariance    | 0.19923    |
-------------------------------------
[2018-12-22 10:23:34.506424 UTC] Saving snapshot
[2018-12-22 10:23:34.506745 UTC] Starting iteration 165
[2018-12-22 10:23:34.506879 UTC] Start collecting samples
[2018-12-22 10:23:37.556853 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:37.631666 UTC] Performing policy update
[2018-12-22 10:23:37.632306 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:37.722795 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:38.772894 UTC] Performing line search
[2018-12-22 10:23:38.897193 UTC] Updating baseline
[2018-12-22 10:23:40.286426 UTC] Computing logging information
-------------------------------------
| Iteration            | 165        |
| ExpectedImprovement  | 0.018441   |
| ActualImprovement    | 0.016905   |
| ImprovementRatio     | 0.91668    |
| MeanKL               | 0.0069591  |
| Entropy              | 1.8619     |
| Perplexity           | 6.4358     |
| AveragePolicyStd     | 0.33405    |
| AveragePolicyStd[0]  | 0.33497    |
| AveragePolicyStd[1]  | 0.40292    |
| AveragePolicyStd[2]  | 0.35974    |
| AveragePolicyStd[3]  | 0.35977    |
| AveragePolicyStd[4]  | 0.24461    |
| AveragePolicyStd[5]  | 0.30231    |
| AverageReturn        | 613.09     |
| MinReturn            | 10.85      |
| MaxReturn            | 734.77     |
| StdReturn            | 174.63     |
| AverageEpisodeLength | 865.96     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 247.9      |
| TotalNEpisodes       | 15194      |
| TotalNSamples        | 8.2324e+05 |
| ExplainedVariance    | 0.42301    |
-------------------------------------
[2018-12-22 10:23:40.602194 UTC] Saving snapshot
[2018-12-22 10:23:40.602474 UTC] Starting iteration 166
[2018-12-22 10:23:40.602610 UTC] Start collecting samples
[2018-12-22 10:23:43.688770 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:43.767850 UTC] Performing policy update
[2018-12-22 10:23:43.768477 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:43.856491 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:44.900553 UTC] Performing line search
[2018-12-22 10:23:45.026637 UTC] Updating baseline
[2018-12-22 10:23:46.262590 UTC] Computing logging information
-------------------------------------
| Iteration            | 166        |
| ExpectedImprovement  | 0.015872   |
| ActualImprovement    | 0.015587   |
| ImprovementRatio     | 0.982      |
| MeanKL               | 0.0069782  |
| Entropy              | 1.8392     |
| Perplexity           | 6.2913     |
| AveragePolicyStd     | 0.33271    |
| AveragePolicyStd[0]  | 0.33423    |
| AveragePolicyStd[1]  | 0.39941    |
| AveragePolicyStd[2]  | 0.35887    |
| AveragePolicyStd[3]  | 0.3573     |
| AveragePolicyStd[4]  | 0.24361    |
| AveragePolicyStd[5]  | 0.30282    |
| AverageReturn        | 607.48     |
| MinReturn            | 10.85      |
| MaxReturn            | 734.77     |
| StdReturn            | 181.12     |
| AverageEpisodeLength | 857.5      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 257.07     |
| TotalNEpisodes       | 15200      |
| TotalNSamples        | 8.2839e+05 |
| ExplainedVariance    | 0.44103    |
-------------------------------------
[2018-12-22 10:23:46.584857 UTC] Saving snapshot
[2018-12-22 10:23:46.585131 UTC] Starting iteration 167
[2018-12-22 10:23:46.585258 UTC] Start collecting samples
[2018-12-22 10:23:49.676527 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:49.753365 UTC] Performing policy update
[2018-12-22 10:23:49.754169 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:49.842711 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:50.887742 UTC] Performing line search
[2018-12-22 10:23:51.011023 UTC] Updating baseline
[2018-12-22 10:23:52.462725 UTC] Computing logging information
-------------------------------------
| Iteration            | 167        |
| ExpectedImprovement  | 0.015223   |
| ActualImprovement    | 0.014528   |
| ImprovementRatio     | 0.95439    |
| MeanKL               | 0.0070587  |
| Entropy              | 1.8174     |
| Perplexity           | 6.1558     |
| AveragePolicyStd     | 0.33162    |
| AveragePolicyStd[0]  | 0.3319     |
| AveragePolicyStd[1]  | 0.39822    |
| AveragePolicyStd[2]  | 0.35977    |
| AveragePolicyStd[3]  | 0.35689    |
| AveragePolicyStd[4]  | 0.24173    |
| AveragePolicyStd[5]  | 0.3012     |
| AverageReturn        | 617.1      |
| MinReturn            | 65.598     |
| MaxReturn            | 734.77     |
| StdReturn            | 170.9      |
| AverageEpisodeLength | 870.82     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 242.72     |
| TotalNEpisodes       | 15206      |
| TotalNSamples        | 8.3439e+05 |
| ExplainedVariance    | 0.17219    |
-------------------------------------
[2018-12-22 10:23:52.786040 UTC] Saving snapshot
[2018-12-22 10:23:52.786311 UTC] Starting iteration 168
[2018-12-22 10:23:52.786432 UTC] Start collecting samples
[2018-12-22 10:23:55.856843 UTC] Computing input variables for policy optimization
[2018-12-22 10:23:55.935924 UTC] Performing policy update
[2018-12-22 10:23:55.936526 UTC] Computing gradient in Euclidean space
[2018-12-22 10:23:56.026029 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:23:57.061375 UTC] Performing line search
[2018-12-22 10:23:57.185239 UTC] Updating baseline
[2018-12-22 10:23:58.397283 UTC] Computing logging information
-------------------------------------
| Iteration            | 168        |
| ExpectedImprovement  | 0.017626   |
| ActualImprovement    | 0.016873   |
| ImprovementRatio     | 0.95729    |
| MeanKL               | 0.0069382  |
| Entropy              | 1.8051     |
| Perplexity           | 6.0805     |
| AveragePolicyStd     | 0.33095    |
| AveragePolicyStd[0]  | 0.3348     |
| AveragePolicyStd[1]  | 0.39927    |
| AveragePolicyStd[2]  | 0.35891    |
| AveragePolicyStd[3]  | 0.35038    |
| AveragePolicyStd[4]  | 0.24067    |
| AveragePolicyStd[5]  | 0.30166    |
| AverageReturn        | 614.52     |
| MinReturn            | 65.598     |
| MaxReturn            | 734.77     |
| StdReturn            | 172.82     |
| AverageEpisodeLength | 867.02     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 245.05     |
| TotalNEpisodes       | 15212      |
| TotalNSamples        | 8.3928e+05 |
| ExplainedVariance    | 0.35262    |
-------------------------------------
[2018-12-22 10:23:58.718328 UTC] Saving snapshot
[2018-12-22 10:23:58.718609 UTC] Starting iteration 169
[2018-12-22 10:23:58.718729 UTC] Start collecting samples
[2018-12-22 10:24:01.783291 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:01.860843 UTC] Performing policy update
[2018-12-22 10:24:01.861475 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:01.955556 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:03.008123 UTC] Performing line search
[2018-12-22 10:24:03.131726 UTC] Updating baseline
[2018-12-22 10:24:04.375904 UTC] Computing logging information
-------------------------------------
| Iteration            | 169        |
| ExpectedImprovement  | 0.013941   |
| ActualImprovement    | 0.013249   |
| ImprovementRatio     | 0.95035    |
| MeanKL               | 0.0070331  |
| Entropy              | 1.7862     |
| Perplexity           | 5.9669     |
| AveragePolicyStd     | 0.32998    |
| AveragePolicyStd[0]  | 0.33465    |
| AveragePolicyStd[1]  | 0.39832    |
| AveragePolicyStd[2]  | 0.35662    |
| AveragePolicyStd[3]  | 0.35026    |
| AveragePolicyStd[4]  | 0.23866    |
| AveragePolicyStd[5]  | 0.30139    |
| AverageReturn        | 610.08     |
| MinReturn            | 65.598     |
| MaxReturn            | 734.77     |
| StdReturn            | 179.04     |
| AverageEpisodeLength | 859.64     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.38     |
| TotalNEpisodes       | 15219      |
| TotalNSamples        | 8.4497e+05 |
| ExplainedVariance    | 0.24803    |
-------------------------------------
[2018-12-22 10:24:04.703088 UTC] Saving snapshot
[2018-12-22 10:24:04.703321 UTC] Starting iteration 170
[2018-12-22 10:24:04.703450 UTC] Start collecting samples
[2018-12-22 10:24:07.749754 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:07.825057 UTC] Performing policy update
[2018-12-22 10:24:07.825701 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:07.920674 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:08.967769 UTC] Performing line search
[2018-12-22 10:24:09.093778 UTC] Updating baseline
[2018-12-22 10:24:10.424035 UTC] Computing logging information
-------------------------------------
| Iteration            | 170        |
| ExpectedImprovement  | 0.016107   |
| ActualImprovement    | 0.01524    |
| ImprovementRatio     | 0.94617    |
| MeanKL               | 0.0072286  |
| Entropy              | 1.7901     |
| Perplexity           | 5.9898     |
| AveragePolicyStd     | 0.33011    |
| AveragePolicyStd[0]  | 0.33759    |
| AveragePolicyStd[1]  | 0.39648    |
| AveragePolicyStd[2]  | 0.35616    |
| AveragePolicyStd[3]  | 0.35016    |
| AveragePolicyStd[4]  | 0.23923    |
| AveragePolicyStd[5]  | 0.30106    |
| AverageReturn        | 615.08     |
| MinReturn            | 65.598     |
| MaxReturn            | 734.77     |
| StdReturn            | 176.47     |
| AverageEpisodeLength | 865.28     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.16     |
| TotalNEpisodes       | 15223      |
| TotalNSamples        | 8.4836e+05 |
| ExplainedVariance    | 0.21454    |
-------------------------------------
[2018-12-22 10:24:10.747813 UTC] Saving snapshot
[2018-12-22 10:24:10.756010 UTC] Starting iteration 171
[2018-12-22 10:24:10.756230 UTC] Start collecting samples
[2018-12-22 10:24:13.799883 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:13.874562 UTC] Performing policy update
[2018-12-22 10:24:13.875261 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:13.967437 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:15.019432 UTC] Performing line search
[2018-12-22 10:24:15.145182 UTC] Updating baseline
[2018-12-22 10:24:16.590126 UTC] Computing logging information
-------------------------------------
| Iteration            | 171        |
| ExpectedImprovement  | 0.014605   |
| ActualImprovement    | 0.013522   |
| ImprovementRatio     | 0.92584    |
| MeanKL               | 0.0072334  |
| Entropy              | 1.7835     |
| Perplexity           | 5.9507     |
| AveragePolicyStd     | 0.32971    |
| AveragePolicyStd[0]  | 0.33708    |
| AveragePolicyStd[1]  | 0.39653    |
| AveragePolicyStd[2]  | 0.3542     |
| AveragePolicyStd[3]  | 0.34913    |
| AveragePolicyStd[4]  | 0.23916    |
| AveragePolicyStd[5]  | 0.30213    |
| AverageReturn        | 628.82     |
| MinReturn            | 65.598     |
| MaxReturn            | 734.77     |
| StdReturn            | 166.63     |
| AverageEpisodeLength | 884.06     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 235.53     |
| TotalNEpisodes       | 15227      |
| TotalNSamples        | 8.5236e+05 |
| ExplainedVariance    | 0.077589   |
-------------------------------------
[2018-12-22 10:24:16.914757 UTC] Saving snapshot
[2018-12-22 10:24:16.915033 UTC] Starting iteration 172
[2018-12-22 10:24:16.915152 UTC] Start collecting samples
[2018-12-22 10:24:20.028047 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:20.106234 UTC] Performing policy update
[2018-12-22 10:24:20.106886 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:20.196873 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:21.239733 UTC] Performing line search
[2018-12-22 10:24:21.363258 UTC] Updating baseline
[2018-12-22 10:24:22.680444 UTC] Computing logging information
-------------------------------------
| Iteration            | 172        |
| ExpectedImprovement  | 0.014492   |
| ActualImprovement    | 0.013571   |
| ImprovementRatio     | 0.93648    |
| MeanKL               | 0.0069338  |
| Entropy              | 1.7564     |
| Perplexity           | 5.7915     |
| AveragePolicyStd     | 0.32825    |
| AveragePolicyStd[0]  | 0.33536    |
| AveragePolicyStd[1]  | 0.39124    |
| AveragePolicyStd[2]  | 0.35441    |
| AveragePolicyStd[3]  | 0.3519     |
| AveragePolicyStd[4]  | 0.23757    |
| AveragePolicyStd[5]  | 0.29902    |
| AverageReturn        | 629.58     |
| MinReturn            | 65.598     |
| MaxReturn            | 734.77     |
| StdReturn            | 170.02     |
| AverageEpisodeLength | 885.07     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.86     |
| TotalNEpisodes       | 15237      |
| TotalNSamples        | 8.6094e+05 |
| ExplainedVariance    | 0.21737    |
-------------------------------------
[2018-12-22 10:24:23.001109 UTC] Saving snapshot
[2018-12-22 10:24:23.001401 UTC] Starting iteration 173
[2018-12-22 10:24:23.001523 UTC] Start collecting samples
[2018-12-22 10:24:26.094288 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:26.169496 UTC] Performing policy update
[2018-12-22 10:24:26.170213 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:26.260651 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:27.306643 UTC] Performing line search
[2018-12-22 10:24:27.432449 UTC] Updating baseline
[2018-12-22 10:24:28.741663 UTC] Computing logging information
-------------------------------------
| Iteration            | 173        |
| ExpectedImprovement  | 0.017571   |
| ActualImprovement    | 0.016481   |
| ImprovementRatio     | 0.93794    |
| MeanKL               | 0.0067795  |
| Entropy              | 1.7185     |
| Perplexity           | 5.5763     |
| AveragePolicyStd     | 0.32612    |
| AveragePolicyStd[0]  | 0.33124    |
| AveragePolicyStd[1]  | 0.38888    |
| AveragePolicyStd[2]  | 0.35285    |
| AveragePolicyStd[3]  | 0.34961    |
| AveragePolicyStd[4]  | 0.23725    |
| AveragePolicyStd[5]  | 0.29688    |
| AverageReturn        | 624.87     |
| MinReturn            | 54.98      |
| MaxReturn            | 734.77     |
| StdReturn            | 177.44     |
| AverageEpisodeLength | 878.85     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 251.41     |
| TotalNEpisodes       | 15243      |
| TotalNSamples        | 8.6587e+05 |
| ExplainedVariance    | 0.30499    |
-------------------------------------
[2018-12-22 10:24:29.066064 UTC] Saving snapshot
[2018-12-22 10:24:29.066316 UTC] Starting iteration 174
[2018-12-22 10:24:29.066434 UTC] Start collecting samples
[2018-12-22 10:24:32.107075 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:32.181308 UTC] Performing policy update
[2018-12-22 10:24:32.182099 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:32.270923 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:33.311027 UTC] Performing line search
[2018-12-22 10:24:33.434772 UTC] Updating baseline
[2018-12-22 10:24:34.591948 UTC] Computing logging information
-------------------------------------
| Iteration            | 174        |
| ExpectedImprovement  | 0.016974   |
| ActualImprovement    | 0.016983   |
| ImprovementRatio     | 1.0006     |
| MeanKL               | 0.0069792  |
| Entropy              | 1.6939     |
| Perplexity           | 5.4404     |
| AveragePolicyStd     | 0.32475    |
| AveragePolicyStd[0]  | 0.3286     |
| AveragePolicyStd[1]  | 0.38662    |
| AveragePolicyStd[2]  | 0.35076    |
| AveragePolicyStd[3]  | 0.34978    |
| AveragePolicyStd[4]  | 0.23639    |
| AveragePolicyStd[5]  | 0.29636    |
| AverageReturn        | 620.28     |
| MinReturn            | 54.98      |
| MaxReturn            | 734.77     |
| StdReturn            | 181.08     |
| AverageEpisodeLength | 872.33     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.59     |
| TotalNEpisodes       | 15244      |
| TotalNSamples        | 8.6622e+05 |
| ExplainedVariance    | -0.12197   |
-------------------------------------
[2018-12-22 10:24:34.916169 UTC] Saving snapshot
[2018-12-22 10:24:34.916403 UTC] Starting iteration 175
[2018-12-22 10:24:34.916522 UTC] Start collecting samples
[2018-12-22 10:24:38.020088 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:38.098008 UTC] Performing policy update
[2018-12-22 10:24:38.098588 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:38.187108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:39.239387 UTC] Performing line search
[2018-12-22 10:24:39.366377 UTC] Updating baseline
[2018-12-22 10:24:40.702179 UTC] Computing logging information
-------------------------------------
| Iteration            | 175        |
| ExpectedImprovement  | 0.014427   |
| ActualImprovement    | 0.013836   |
| ImprovementRatio     | 0.95903    |
| MeanKL               | 0.0077554  |
| Entropy              | 1.6734     |
| Perplexity           | 5.3305     |
| AveragePolicyStd     | 0.32357    |
| AveragePolicyStd[0]  | 0.32954    |
| AveragePolicyStd[1]  | 0.38407    |
| AveragePolicyStd[2]  | 0.34778    |
| AveragePolicyStd[3]  | 0.34872    |
| AveragePolicyStd[4]  | 0.23608    |
| AveragePolicyStd[5]  | 0.29525    |
| AverageReturn        | 620.34     |
| MinReturn            | 54.98      |
| MaxReturn            | 734.77     |
| StdReturn            | 181.23     |
| AverageEpisodeLength | 872.82     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.78     |
| TotalNEpisodes       | 15253      |
| TotalNSamples        | 8.7522e+05 |
| ExplainedVariance    | 0.10896    |
-------------------------------------
[2018-12-22 10:24:41.026556 UTC] Saving snapshot
[2018-12-22 10:24:41.026827 UTC] Starting iteration 176
[2018-12-22 10:24:41.026955 UTC] Start collecting samples
[2018-12-22 10:24:44.088374 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:44.165652 UTC] Performing policy update
[2018-12-22 10:24:44.166520 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:44.256565 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:45.307133 UTC] Performing line search
[2018-12-22 10:24:45.432818 UTC] Updating baseline
[2018-12-22 10:24:46.924615 UTC] Computing logging information
-------------------------------------
| Iteration            | 176        |
| ExpectedImprovement  | 0.014188   |
| ActualImprovement    | 0.01329    |
| ImprovementRatio     | 0.93666    |
| MeanKL               | 0.0073025  |
| Entropy              | 1.6719     |
| Perplexity           | 5.3225     |
| AveragePolicyStd     | 0.32342    |
| AveragePolicyStd[0]  | 0.33018    |
| AveragePolicyStd[1]  | 0.38302    |
| AveragePolicyStd[2]  | 0.3472     |
| AveragePolicyStd[3]  | 0.34824    |
| AveragePolicyStd[4]  | 0.23665    |
| AveragePolicyStd[5]  | 0.29522    |
| AverageReturn        | 620.39     |
| MinReturn            | 54.98      |
| MaxReturn            | 734.77     |
| StdReturn            | 180.89     |
| AverageEpisodeLength | 872.96     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.53     |
| TotalNEpisodes       | 15258      |
| TotalNSamples        | 8.7964e+05 |
| ExplainedVariance    | 0.3289     |
-------------------------------------
[2018-12-22 10:24:47.247518 UTC] Saving snapshot
[2018-12-22 10:24:47.247784 UTC] Starting iteration 177
[2018-12-22 10:24:47.247903 UTC] Start collecting samples
[2018-12-22 10:24:50.321388 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:50.398510 UTC] Performing policy update
[2018-12-22 10:24:50.399275 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:50.489090 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:51.533167 UTC] Performing line search
[2018-12-22 10:24:51.657631 UTC] Updating baseline
[2018-12-22 10:24:52.982013 UTC] Computing logging information
-------------------------------------
| Iteration            | 177        |
| ExpectedImprovement  | 0.014277   |
| ActualImprovement    | 0.014019   |
| ImprovementRatio     | 0.98192    |
| MeanKL               | 0.0071307  |
| Entropy              | 1.6829     |
| Perplexity           | 5.3809     |
| AveragePolicyStd     | 0.32404    |
| AveragePolicyStd[0]  | 0.33043    |
| AveragePolicyStd[1]  | 0.38394    |
| AveragePolicyStd[2]  | 0.34979    |
| AveragePolicyStd[3]  | 0.34851    |
| AveragePolicyStd[4]  | 0.23745    |
| AveragePolicyStd[5]  | 0.29411    |
| AverageReturn        | 608.69     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 188.71     |
| AverageEpisodeLength | 854.67     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 267.37     |
| TotalNEpisodes       | 15265      |
| TotalNSamples        | 8.8475e+05 |
| ExplainedVariance    | 0.51696    |
-------------------------------------
[2018-12-22 10:24:53.301197 UTC] Saving snapshot
[2018-12-22 10:24:53.301446 UTC] Starting iteration 178
[2018-12-22 10:24:53.301584 UTC] Start collecting samples
[2018-12-22 10:24:56.382138 UTC] Computing input variables for policy optimization
[2018-12-22 10:24:56.457658 UTC] Performing policy update
[2018-12-22 10:24:56.458450 UTC] Computing gradient in Euclidean space
[2018-12-22 10:24:56.547829 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:24:57.594133 UTC] Performing line search
[2018-12-22 10:24:57.720157 UTC] Updating baseline
[2018-12-22 10:24:58.953014 UTC] Computing logging information
-------------------------------------
| Iteration            | 178        |
| ExpectedImprovement  | 0.015289   |
| ActualImprovement    | 0.014896   |
| ImprovementRatio     | 0.97425    |
| MeanKL               | 0.0071352  |
| Entropy              | 1.6799     |
| Perplexity           | 5.3649     |
| AveragePolicyStd     | 0.3239     |
| AveragePolicyStd[0]  | 0.32922    |
| AveragePolicyStd[1]  | 0.38383    |
| AveragePolicyStd[2]  | 0.3503     |
| AveragePolicyStd[3]  | 0.34954    |
| AveragePolicyStd[4]  | 0.23757    |
| AveragePolicyStd[5]  | 0.29294    |
| AverageReturn        | 607.31     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 186.45     |
| AverageEpisodeLength | 852.59     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.29     |
| TotalNEpisodes       | 15271      |
| TotalNSamples        | 8.8901e+05 |
| ExplainedVariance    | 0.58643    |
-------------------------------------
[2018-12-22 10:24:59.279101 UTC] Saving snapshot
[2018-12-22 10:24:59.279351 UTC] Starting iteration 179
[2018-12-22 10:24:59.279472 UTC] Start collecting samples
[2018-12-22 10:25:02.343364 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:02.415541 UTC] Performing policy update
[2018-12-22 10:25:02.416191 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:02.506143 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:03.504161 UTC] Performing line search
[2018-12-22 10:25:03.630442 UTC] Updating baseline
[2018-12-22 10:25:04.998109 UTC] Computing logging information
-------------------------------------
| Iteration            | 179        |
| ExpectedImprovement  | 0.016136   |
| ActualImprovement    | 0.014595   |
| ImprovementRatio     | 0.9045     |
| MeanKL               | 0.0068231  |
| Entropy              | 1.6565     |
| Perplexity           | 5.2408     |
| AveragePolicyStd     | 0.32271    |
| AveragePolicyStd[0]  | 0.32678    |
| AveragePolicyStd[1]  | 0.38372    |
| AveragePolicyStd[2]  | 0.34996    |
| AveragePolicyStd[3]  | 0.34752    |
| AveragePolicyStd[4]  | 0.23606    |
| AveragePolicyStd[5]  | 0.29219    |
| AverageReturn        | 610.4      |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 185.76     |
| AverageEpisodeLength | 856.7      |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 264.35     |
| TotalNEpisodes       | 15273      |
| TotalNSamples        | 8.9101e+05 |
| ExplainedVariance    | 0.036658   |
-------------------------------------
[2018-12-22 10:25:05.318885 UTC] Saving snapshot
[2018-12-22 10:25:05.319133 UTC] Starting iteration 180
[2018-12-22 10:25:05.319251 UTC] Start collecting samples
[2018-12-22 10:25:08.427729 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:08.506283 UTC] Performing policy update
[2018-12-22 10:25:08.506958 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:08.592457 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:09.642809 UTC] Performing line search
[2018-12-22 10:25:09.772064 UTC] Updating baseline
[2018-12-22 10:25:11.188253 UTC] Computing logging information
-------------------------------------
| Iteration            | 180        |
| ExpectedImprovement  | 0.0131     |
| ActualImprovement    | 0.012796   |
| ImprovementRatio     | 0.97676    |
| MeanKL               | 0.0070453  |
| Entropy              | 1.6539     |
| Perplexity           | 5.2272     |
| AveragePolicyStd     | 0.32247    |
| AveragePolicyStd[0]  | 0.32671    |
| AveragePolicyStd[1]  | 0.38266    |
| AveragePolicyStd[2]  | 0.34762    |
| AveragePolicyStd[3]  | 0.34915    |
| AveragePolicyStd[4]  | 0.23723    |
| AveragePolicyStd[5]  | 0.29146    |
| AverageReturn        | 609.37     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 185.34     |
| AverageEpisodeLength | 857.51     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 264.67     |
| TotalNEpisodes       | 15281      |
| TotalNSamples        | 8.9901e+05 |
| ExplainedVariance    | 0.13417    |
-------------------------------------
[2018-12-22 10:25:11.512706 UTC] Saving snapshot
[2018-12-22 10:25:11.520726 UTC] Starting iteration 181
[2018-12-22 10:25:11.520923 UTC] Start collecting samples
[2018-12-22 10:25:14.605628 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:14.682524 UTC] Performing policy update
[2018-12-22 10:25:14.683191 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:14.772750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:15.836250 UTC] Performing line search
[2018-12-22 10:25:15.961751 UTC] Updating baseline
[2018-12-22 10:25:17.613261 UTC] Computing logging information
-------------------------------------
| Iteration            | 181        |
| ExpectedImprovement  | 0.013254   |
| ActualImprovement    | 0.012509   |
| ImprovementRatio     | 0.94374    |
| MeanKL               | 0.0069534  |
| Entropy              | 1.6509     |
| Perplexity           | 5.2117     |
| AveragePolicyStd     | 0.32247    |
| AveragePolicyStd[0]  | 0.32817    |
| AveragePolicyStd[1]  | 0.38161    |
| AveragePolicyStd[2]  | 0.34647    |
| AveragePolicyStd[3]  | 0.35197    |
| AveragePolicyStd[4]  | 0.23456    |
| AveragePolicyStd[5]  | 0.29201    |
| AverageReturn        | 614.25     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 186.45     |
| AverageEpisodeLength | 863.53     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.56     |
| TotalNEpisodes       | 15287      |
| TotalNSamples        | 9.0437e+05 |
| ExplainedVariance    | 0.036259   |
-------------------------------------
[2018-12-22 10:25:17.945343 UTC] Saving snapshot
[2018-12-22 10:25:17.945608 UTC] Starting iteration 182
[2018-12-22 10:25:17.945728 UTC] Start collecting samples
[2018-12-22 10:25:21.028594 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:21.103047 UTC] Performing policy update
[2018-12-22 10:25:21.103836 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:21.192300 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:22.197937 UTC] Performing line search
[2018-12-22 10:25:22.322879 UTC] Updating baseline
[2018-12-22 10:25:23.516972 UTC] Computing logging information
-------------------------------------
| Iteration            | 182        |
| ExpectedImprovement  | 0.013069   |
| ActualImprovement    | 0.012609   |
| ImprovementRatio     | 0.96479    |
| MeanKL               | 0.0072401  |
| Entropy              | 1.6532     |
| Perplexity           | 5.2237     |
| AveragePolicyStd     | 0.32262    |
| AveragePolicyStd[0]  | 0.33121    |
| AveragePolicyStd[1]  | 0.38159    |
| AveragePolicyStd[2]  | 0.34652    |
| AveragePolicyStd[3]  | 0.35203    |
| AveragePolicyStd[4]  | 0.23512    |
| AveragePolicyStd[5]  | 0.28924    |
| AverageReturn        | 621.81     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 178.8      |
| AverageEpisodeLength | 874.76     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 254.34     |
| TotalNEpisodes       | 15291      |
| TotalNSamples        | 9.0813e+05 |
| ExplainedVariance    | 0.32867    |
-------------------------------------
[2018-12-22 10:25:23.839327 UTC] Saving snapshot
[2018-12-22 10:25:23.839599 UTC] Starting iteration 183
[2018-12-22 10:25:23.839721 UTC] Start collecting samples
[2018-12-22 10:25:26.978885 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:27.059223 UTC] Performing policy update
[2018-12-22 10:25:27.059898 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:27.146976 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:28.151522 UTC] Performing line search
[2018-12-22 10:25:28.267671 UTC] Updating baseline
[2018-12-22 10:25:29.580715 UTC] Computing logging information
-------------------------------------
| Iteration            | 183        |
| ExpectedImprovement  | 0.017489   |
| ActualImprovement    | 0.016564   |
| ImprovementRatio     | 0.94712    |
| MeanKL               | 0.0070769  |
| Entropy              | 1.6595     |
| Perplexity           | 5.2565     |
| AveragePolicyStd     | 0.32294    |
| AveragePolicyStd[0]  | 0.32868    |
| AveragePolicyStd[1]  | 0.3825     |
| AveragePolicyStd[2]  | 0.34661    |
| AveragePolicyStd[3]  | 0.3543     |
| AveragePolicyStd[4]  | 0.23596    |
| AveragePolicyStd[5]  | 0.28961    |
| AverageReturn        | 624.74     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 178.1      |
| AverageEpisodeLength | 878.85     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.56     |
| TotalNEpisodes       | 15297      |
| TotalNSamples        | 9.1329e+05 |
| ExplainedVariance    | 0.22552    |
-------------------------------------
[2018-12-22 10:25:29.922191 UTC] Saving snapshot
[2018-12-22 10:25:29.922433 UTC] Starting iteration 184
[2018-12-22 10:25:29.922554 UTC] Start collecting samples
[2018-12-22 10:25:33.201326 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:33.282403 UTC] Performing policy update
[2018-12-22 10:25:33.283128 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:33.376194 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:34.471305 UTC] Performing line search
[2018-12-22 10:25:34.601293 UTC] Updating baseline
[2018-12-22 10:25:35.995750 UTC] Computing logging information
-------------------------------------
| Iteration            | 184        |
| ExpectedImprovement  | 0.014553   |
| ActualImprovement    | 0.014226   |
| ImprovementRatio     | 0.97753    |
| MeanKL               | 0.0073518  |
| Entropy              | 1.6564     |
| Perplexity           | 5.2405     |
| AveragePolicyStd     | 0.32273    |
| AveragePolicyStd[0]  | 0.32706    |
| AveragePolicyStd[1]  | 0.38246    |
| AveragePolicyStd[2]  | 0.34468    |
| AveragePolicyStd[3]  | 0.35514    |
| AveragePolicyStd[4]  | 0.23608    |
| AveragePolicyStd[5]  | 0.29097    |
| AverageReturn        | 625.56     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 178.56     |
| AverageEpisodeLength | 878.96     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.61     |
| TotalNEpisodes       | 15303      |
| TotalNSamples        | 9.1929e+05 |
| ExplainedVariance    | -0.032942  |
-------------------------------------
[2018-12-22 10:25:36.337867 UTC] Saving snapshot
[2018-12-22 10:25:36.338111 UTC] Starting iteration 185
[2018-12-22 10:25:36.338227 UTC] Start collecting samples
[2018-12-22 10:25:39.449767 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:39.526293 UTC] Performing policy update
[2018-12-22 10:25:39.526894 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:39.616197 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:40.664377 UTC] Performing line search
[2018-12-22 10:25:40.788275 UTC] Updating baseline
[2018-12-22 10:25:42.000267 UTC] Computing logging information
-------------------------------------
| Iteration            | 185        |
| ExpectedImprovement  | 0.012553   |
| ActualImprovement    | 0.011899   |
| ImprovementRatio     | 0.94787    |
| MeanKL               | 0.0074188  |
| Entropy              | 1.6453     |
| Perplexity           | 5.1825     |
| AveragePolicyStd     | 0.32223    |
| AveragePolicyStd[0]  | 0.32582    |
| AveragePolicyStd[1]  | 0.38421    |
| AveragePolicyStd[2]  | 0.34226    |
| AveragePolicyStd[3]  | 0.3552     |
| AveragePolicyStd[4]  | 0.23489    |
| AveragePolicyStd[5]  | 0.29099    |
| AverageReturn        | 626.01     |
| MinReturn            | 54.98      |
| MaxReturn            | 749.52     |
| StdReturn            | 179.28     |
| AverageEpisodeLength | 878.69     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 254.37     |
| TotalNEpisodes       | 15309      |
| TotalNSamples        | 9.2445e+05 |
| ExplainedVariance    | 0.14584    |
-------------------------------------
[2018-12-22 10:25:42.324395 UTC] Saving snapshot
[2018-12-22 10:25:42.324647 UTC] Starting iteration 186
[2018-12-22 10:25:42.324782 UTC] Start collecting samples
[2018-12-22 10:25:45.413347 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:45.492224 UTC] Performing policy update
[2018-12-22 10:25:45.492878 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:45.580863 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:46.625208 UTC] Performing line search
[2018-12-22 10:25:46.749404 UTC] Updating baseline
[2018-12-22 10:25:48.035524 UTC] Computing logging information
-------------------------------------
| Iteration            | 186        |
| ExpectedImprovement  | 0.020687   |
| ActualImprovement    | 0.019148   |
| ImprovementRatio     | 0.9256     |
| MeanKL               | 0.0067263  |
| Entropy              | 1.6356     |
| Perplexity           | 5.1325     |
| AveragePolicyStd     | 0.32157    |
| AveragePolicyStd[0]  | 0.32494    |
| AveragePolicyStd[1]  | 0.38151    |
| AveragePolicyStd[2]  | 0.34288    |
| AveragePolicyStd[3]  | 0.35387    |
| AveragePolicyStd[4]  | 0.23594    |
| AveragePolicyStd[5]  | 0.29028    |
| AverageReturn        | 621.42     |
| MinReturn            | 41.703     |
| MaxReturn            | 749.52     |
| StdReturn            | 185.19     |
| AverageEpisodeLength | 871.16     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 262.74     |
| TotalNEpisodes       | 15317      |
| TotalNSamples        | 9.3008e+05 |
| ExplainedVariance    | 0.28637    |
-------------------------------------
[2018-12-22 10:25:48.353310 UTC] Saving snapshot
[2018-12-22 10:25:48.353610 UTC] Starting iteration 187
[2018-12-22 10:25:48.353757 UTC] Start collecting samples
[2018-12-22 10:25:51.376309 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:51.450030 UTC] Performing policy update
[2018-12-22 10:25:51.450814 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:51.539725 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:52.596471 UTC] Performing line search
[2018-12-22 10:25:52.722230 UTC] Updating baseline
[2018-12-22 10:25:54.301080 UTC] Computing logging information
-------------------------------------
| Iteration            | 187        |
| ExpectedImprovement  | 0.019511   |
| ActualImprovement    | 0.018616   |
| ImprovementRatio     | 0.95412    |
| MeanKL               | 0.006793   |
| Entropy              | 1.645      |
| Perplexity           | 5.1809     |
| AveragePolicyStd     | 0.32218    |
| AveragePolicyStd[0]  | 0.32597    |
| AveragePolicyStd[1]  | 0.38328    |
| AveragePolicyStd[2]  | 0.34588    |
| AveragePolicyStd[3]  | 0.35256    |
| AveragePolicyStd[4]  | 0.23519    |
| AveragePolicyStd[5]  | 0.2902     |
| AverageReturn        | 620.66     |
| MinReturn            | 41.703     |
| MaxReturn            | 749.52     |
| StdReturn            | 184.96     |
| AverageEpisodeLength | 869.54     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 262.44     |
| TotalNEpisodes       | 15320      |
| TotalNSamples        | 9.3292e+05 |
| ExplainedVariance    | 0.23131    |
-------------------------------------
[2018-12-22 10:25:54.625114 UTC] Saving snapshot
[2018-12-22 10:25:54.625366 UTC] Starting iteration 188
[2018-12-22 10:25:54.625487 UTC] Start collecting samples
[2018-12-22 10:25:57.679115 UTC] Computing input variables for policy optimization
[2018-12-22 10:25:57.756286 UTC] Performing policy update
[2018-12-22 10:25:57.757111 UTC] Computing gradient in Euclidean space
[2018-12-22 10:25:57.847247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:25:58.888503 UTC] Performing line search
[2018-12-22 10:25:59.013660 UTC] Updating baseline
[2018-12-22 10:26:00.422766 UTC] Computing logging information
-------------------------------------
| Iteration            | 188        |
| ExpectedImprovement  | 0.0143     |
| ActualImprovement    | 0.013925   |
| ImprovementRatio     | 0.97381    |
| MeanKL               | 0.0070937  |
| Entropy              | 1.6462     |
| Perplexity           | 5.1871     |
| AveragePolicyStd     | 0.32224    |
| AveragePolicyStd[0]  | 0.32583    |
| AveragePolicyStd[1]  | 0.38558    |
| AveragePolicyStd[2]  | 0.34376    |
| AveragePolicyStd[3]  | 0.35268    |
| AveragePolicyStd[4]  | 0.23611    |
| AveragePolicyStd[5]  | 0.2895     |
| AverageReturn        | 623.99     |
| MinReturn            | 41.703     |
| MaxReturn            | 749.52     |
| StdReturn            | 182.82     |
| AverageEpisodeLength | 872.76     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 258.45     |
| TotalNEpisodes       | 15326      |
| TotalNSamples        | 9.3863e+05 |
| ExplainedVariance    | 0.20717    |
-------------------------------------
[2018-12-22 10:26:00.742429 UTC] Saving snapshot
[2018-12-22 10:26:00.742695 UTC] Starting iteration 189
[2018-12-22 10:26:00.742816 UTC] Start collecting samples
[2018-12-22 10:26:03.810051 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:03.887084 UTC] Performing policy update
[2018-12-22 10:26:03.887775 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:03.980325 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:05.021644 UTC] Performing line search
[2018-12-22 10:26:05.148117 UTC] Updating baseline
[2018-12-22 10:26:06.466824 UTC] Computing logging information
-------------------------------------
| Iteration            | 189        |
| ExpectedImprovement  | 0.018849   |
| ActualImprovement    | 0.015719   |
| ImprovementRatio     | 0.83393    |
| MeanKL               | 0.0068777  |
| Entropy              | 1.6524     |
| Perplexity           | 5.2193     |
| AveragePolicyStd     | 0.32244    |
| AveragePolicyStd[0]  | 0.32453    |
| AveragePolicyStd[1]  | 0.38107    |
| AveragePolicyStd[2]  | 0.34434    |
| AveragePolicyStd[3]  | 0.3569     |
| AveragePolicyStd[4]  | 0.23714    |
| AveragePolicyStd[5]  | 0.29066    |
| AverageReturn        | 626.33     |
| MinReturn            | 41.703     |
| MaxReturn            | 749.52     |
| StdReturn            | 179.31     |
| AverageEpisodeLength | 875.62     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 251.51     |
| TotalNEpisodes       | 15333      |
| TotalNSamples        | 9.4503e+05 |
| ExplainedVariance    | 0.090055   |
-------------------------------------
[2018-12-22 10:26:06.790292 UTC] Saving snapshot
[2018-12-22 10:26:06.790564 UTC] Starting iteration 190
[2018-12-22 10:26:06.790707 UTC] Start collecting samples
[2018-12-22 10:26:09.869963 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:09.950528 UTC] Performing policy update
[2018-12-22 10:26:09.951418 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:10.043969 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:11.083885 UTC] Performing line search
[2018-12-22 10:26:11.207653 UTC] Updating baseline
[2018-12-22 10:26:12.546996 UTC] Computing logging information
-------------------------------------
| Iteration            | 190        |
| ExpectedImprovement  | 0.013916   |
| ActualImprovement    | 0.01348    |
| ImprovementRatio     | 0.96865    |
| MeanKL               | 0.007573   |
| Entropy              | 1.6403     |
| Perplexity           | 5.157      |
| AveragePolicyStd     | 0.32171    |
| AveragePolicyStd[0]  | 0.32522    |
| AveragePolicyStd[1]  | 0.37966    |
| AveragePolicyStd[2]  | 0.34225    |
| AveragePolicyStd[3]  | 0.35532    |
| AveragePolicyStd[4]  | 0.23731    |
| AveragePolicyStd[5]  | 0.29048    |
| AverageReturn        | 619.77     |
| MinReturn            | 41.703     |
| MaxReturn            | 749.52     |
| StdReturn            | 185.26     |
| AverageEpisodeLength | 864.26     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 260.75     |
| TotalNEpisodes       | 15340      |
| TotalNSamples        | 9.4945e+05 |
| ExplainedVariance    | 0.4405     |
-------------------------------------
[2018-12-22 10:26:12.874379 UTC] Saving snapshot
[2018-12-22 10:26:12.882587 UTC] Starting iteration 191
[2018-12-22 10:26:12.882797 UTC] Start collecting samples
[2018-12-22 10:26:15.940267 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:16.015928 UTC] Performing policy update
[2018-12-22 10:26:16.016612 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:16.105773 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:17.158655 UTC] Performing line search
[2018-12-22 10:26:17.283004 UTC] Updating baseline
[2018-12-22 10:26:18.724895 UTC] Computing logging information
-------------------------------------
| Iteration            | 191        |
| ExpectedImprovement  | 0.018906   |
| ActualImprovement    | 0.017592   |
| ImprovementRatio     | 0.9305     |
| MeanKL               | 0.0068458  |
| Entropy              | 1.6224     |
| Perplexity           | 5.0653     |
| AveragePolicyStd     | 0.32077    |
| AveragePolicyStd[0]  | 0.32339    |
| AveragePolicyStd[1]  | 0.37844    |
| AveragePolicyStd[2]  | 0.34036    |
| AveragePolicyStd[3]  | 0.35634    |
| AveragePolicyStd[4]  | 0.23675    |
| AveragePolicyStd[5]  | 0.2893     |
| AverageReturn        | 624.09     |
| MinReturn            | 41.703     |
| MaxReturn            | 749.52     |
| StdReturn            | 182.03     |
| AverageEpisodeLength | 869.71     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.16     |
| TotalNEpisodes       | 15344      |
| TotalNSamples        | 9.5319e+05 |
| ExplainedVariance    | 0.00020142 |
-------------------------------------
[2018-12-22 10:26:19.051821 UTC] Saving snapshot
[2018-12-22 10:26:19.052060 UTC] Starting iteration 192
[2018-12-22 10:26:19.052178 UTC] Start collecting samples
[2018-12-22 10:26:22.113821 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:22.191443 UTC] Performing policy update
[2018-12-22 10:26:22.192155 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:22.281148 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:23.328813 UTC] Performing line search
[2018-12-22 10:26:23.453186 UTC] Updating baseline
[2018-12-22 10:26:24.998059 UTC] Computing logging information
------------------------------------
| Iteration            | 192       |
| ExpectedImprovement  | 0.013453  |
| ActualImprovement    | 0.013168  |
| ImprovementRatio     | 0.97877   |
| MeanKL               | 0.0071353 |
| Entropy              | 1.6241    |
| Perplexity           | 5.0739    |
| AveragePolicyStd     | 0.32077   |
| AveragePolicyStd[0]  | 0.32348   |
| AveragePolicyStd[1]  | 0.37851   |
| AveragePolicyStd[2]  | 0.34155   |
| AveragePolicyStd[3]  | 0.35265   |
| AveragePolicyStd[4]  | 0.23688   |
| AveragePolicyStd[5]  | 0.29152   |
| AverageReturn        | 619.76    |
| MinReturn            | 41.703    |
| MaxReturn            | 753.99    |
| StdReturn            | 190.66    |
| AverageEpisodeLength | 860.74    |
| MinEpisodeLength     | 59        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 266.92    |
| TotalNEpisodes       | 15350     |
| TotalNSamples        | 9.583e+05 |
| ExplainedVariance    | -0.026113 |
------------------------------------
[2018-12-22 10:26:25.324117 UTC] Saving snapshot
[2018-12-22 10:26:25.324395 UTC] Starting iteration 193
[2018-12-22 10:26:25.324522 UTC] Start collecting samples
[2018-12-22 10:26:28.421111 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:28.501454 UTC] Performing policy update
[2018-12-22 10:26:28.502325 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:28.592601 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:29.650200 UTC] Performing line search
[2018-12-22 10:26:29.779371 UTC] Updating baseline
[2018-12-22 10:26:31.192710 UTC] Computing logging information
-------------------------------------
| Iteration            | 193        |
| ExpectedImprovement  | 0.015275   |
| ActualImprovement    | 0.014407   |
| ImprovementRatio     | 0.94321    |
| MeanKL               | 0.0069752  |
| Entropy              | 1.6377     |
| Perplexity           | 5.1432     |
| AveragePolicyStd     | 0.32143    |
| AveragePolicyStd[0]  | 0.32383    |
| AveragePolicyStd[1]  | 0.37792    |
| AveragePolicyStd[2]  | 0.34267    |
| AveragePolicyStd[3]  | 0.3546     |
| AveragePolicyStd[4]  | 0.23858    |
| AveragePolicyStd[5]  | 0.29097    |
| AverageReturn        | 608.59     |
| MinReturn            | 40.862     |
| MaxReturn            | 765.2      |
| StdReturn            | 206.03     |
| AverageEpisodeLength | 842.65     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 286.96     |
| TotalNEpisodes       | 15359      |
| TotalNSamples        | 9.6490e+05 |
| ExplainedVariance    | 0.18531    |
-------------------------------------
[2018-12-22 10:26:31.519468 UTC] Saving snapshot
[2018-12-22 10:26:31.519744 UTC] Starting iteration 194
[2018-12-22 10:26:31.519888 UTC] Start collecting samples
[2018-12-22 10:26:34.578184 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:34.652998 UTC] Performing policy update
[2018-12-22 10:26:34.653817 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:34.743621 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:35.790074 UTC] Performing line search
[2018-12-22 10:26:35.914368 UTC] Updating baseline
[2018-12-22 10:26:37.154967 UTC] Computing logging information
-------------------------------------
| Iteration            | 194        |
| ExpectedImprovement  | 0.015674   |
| ActualImprovement    | 0.014643   |
| ImprovementRatio     | 0.93418    |
| MeanKL               | 0.0078132  |
| Entropy              | 1.6334     |
| Perplexity           | 5.1214     |
| AveragePolicyStd     | 0.32115    |
| AveragePolicyStd[0]  | 0.32443    |
| AveragePolicyStd[1]  | 0.37622    |
| AveragePolicyStd[2]  | 0.34212    |
| AveragePolicyStd[3]  | 0.35457    |
| AveragePolicyStd[4]  | 0.23874    |
| AveragePolicyStd[5]  | 0.29081    |
| AverageReturn        | 612.75     |
| MinReturn            | 40.862     |
| MaxReturn            | 765.2      |
| StdReturn            | 206.13     |
| AverageEpisodeLength | 846.58     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 286.51     |
| TotalNEpisodes       | 15363      |
| TotalNSamples        | 9.6832e+05 |
| ExplainedVariance    | 0.19722    |
-------------------------------------
[2018-12-22 10:26:37.477327 UTC] Saving snapshot
[2018-12-22 10:26:37.477650 UTC] Starting iteration 195
[2018-12-22 10:26:37.477889 UTC] Start collecting samples
[2018-12-22 10:26:40.581634 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:40.659143 UTC] Performing policy update
[2018-12-22 10:26:40.659747 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:40.750704 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:41.807922 UTC] Performing line search
[2018-12-22 10:26:41.936610 UTC] Updating baseline
[2018-12-22 10:26:43.332805 UTC] Computing logging information
-------------------------------------
| Iteration            | 195        |
| ExpectedImprovement  | 0.013454   |
| ActualImprovement    | 0.013336   |
| ImprovementRatio     | 0.99125    |
| MeanKL               | 0.0070441  |
| Entropy              | 1.6123     |
| Perplexity           | 5.0142     |
| AveragePolicyStd     | 0.32003    |
| AveragePolicyStd[0]  | 0.32267    |
| AveragePolicyStd[1]  | 0.37581    |
| AveragePolicyStd[2]  | 0.34183    |
| AveragePolicyStd[3]  | 0.35209    |
| AveragePolicyStd[4]  | 0.23777    |
| AveragePolicyStd[5]  | 0.29003    |
| AverageReturn        | 618.49     |
| MinReturn            | 40.862     |
| MaxReturn            | 765.2      |
| StdReturn            | 204.62     |
| AverageEpisodeLength | 852.49     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 283.14     |
| TotalNEpisodes       | 15371      |
| TotalNSamples        | 9.7426e+05 |
| ExplainedVariance    | 0.50183    |
-------------------------------------
[2018-12-22 10:26:43.659472 UTC] Saving snapshot
[2018-12-22 10:26:43.659737 UTC] Starting iteration 196
[2018-12-22 10:26:43.659859 UTC] Start collecting samples
[2018-12-22 10:26:46.728414 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:46.806427 UTC] Performing policy update
[2018-12-22 10:26:46.807281 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:46.896592 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:47.948915 UTC] Performing line search
[2018-12-22 10:26:48.074096 UTC] Updating baseline
[2018-12-22 10:26:49.554670 UTC] Computing logging information
-------------------------------------
| Iteration            | 196        |
| ExpectedImprovement  | 0.013855   |
| ActualImprovement    | 0.013307   |
| ImprovementRatio     | 0.96042    |
| MeanKL               | 0.0074268  |
| Entropy              | 1.6145     |
| Perplexity           | 5.0254     |
| AveragePolicyStd     | 0.32018    |
| AveragePolicyStd[0]  | 0.32567    |
| AveragePolicyStd[1]  | 0.37621    |
| AveragePolicyStd[2]  | 0.33936    |
| AveragePolicyStd[3]  | 0.35267    |
| AveragePolicyStd[4]  | 0.23736    |
| AveragePolicyStd[5]  | 0.28981    |
| AverageReturn        | 610.29     |
| MinReturn            | 40.862     |
| MaxReturn            | 765.2      |
| StdReturn            | 216.96     |
| AverageEpisodeLength | 835.78     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 298.19     |
| TotalNEpisodes       | 15379      |
| TotalNSamples        | 9.8058e+05 |
| ExplainedVariance    | 0.24423    |
-------------------------------------
[2018-12-22 10:26:49.881776 UTC] Saving snapshot
[2018-12-22 10:26:49.882035 UTC] Starting iteration 197
[2018-12-22 10:26:49.882168 UTC] Start collecting samples
[2018-12-22 10:26:53.136997 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:53.217073 UTC] Performing policy update
[2018-12-22 10:26:53.217886 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:53.313706 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:26:54.435748 UTC] Performing line search
[2018-12-22 10:26:54.566472 UTC] Updating baseline
[2018-12-22 10:26:55.855384 UTC] Computing logging information
-------------------------------------
| Iteration            | 197        |
| ExpectedImprovement  | 0.011363   |
| ActualImprovement    | 0.01095    |
| ImprovementRatio     | 0.96362    |
| MeanKL               | 0.0074632  |
| Entropy              | 1.6047     |
| Perplexity           | 4.9764     |
| AveragePolicyStd     | 0.31972    |
| AveragePolicyStd[0]  | 0.32581    |
| AveragePolicyStd[1]  | 0.37484    |
| AveragePolicyStd[2]  | 0.33915    |
| AveragePolicyStd[3]  | 0.35355    |
| AveragePolicyStd[4]  | 0.23634    |
| AveragePolicyStd[5]  | 0.28861    |
| AverageReturn        | 610.22     |
| MinReturn            | 40.862     |
| MaxReturn            | 765.2      |
| StdReturn            | 219.76     |
| AverageEpisodeLength | 834.05     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 300.75     |
| TotalNEpisodes       | 15383      |
| TotalNSamples        | 9.8378e+05 |
| ExplainedVariance    | 0.40891    |
-------------------------------------
[2018-12-22 10:26:56.211352 UTC] Saving snapshot
[2018-12-22 10:26:56.211665 UTC] Starting iteration 198
[2018-12-22 10:26:56.211828 UTC] Start collecting samples
[2018-12-22 10:26:59.295782 UTC] Computing input variables for policy optimization
[2018-12-22 10:26:59.371500 UTC] Performing policy update
[2018-12-22 10:26:59.372439 UTC] Computing gradient in Euclidean space
[2018-12-22 10:26:59.463965 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:00.521534 UTC] Performing line search
[2018-12-22 10:27:00.646971 UTC] Updating baseline
[2018-12-22 10:27:01.981059 UTC] Computing logging information
-------------------------------------
| Iteration            | 198        |
| ExpectedImprovement  | 0.015526   |
| ActualImprovement    | 0.014922   |
| ImprovementRatio     | 0.9611     |
| MeanKL               | 0.0075029  |
| Entropy              | 1.6204     |
| Perplexity           | 5.0552     |
| AveragePolicyStd     | 0.32063    |
| AveragePolicyStd[0]  | 0.32364    |
| AveragePolicyStd[1]  | 0.37667    |
| AveragePolicyStd[2]  | 0.34289    |
| AveragePolicyStd[3]  | 0.35454    |
| AveragePolicyStd[4]  | 0.23629    |
| AveragePolicyStd[5]  | 0.28975    |
| AverageReturn        | 611.57     |
| MinReturn            | 40.862     |
| MaxReturn            | 783.85     |
| StdReturn            | 220.61     |
| AverageEpisodeLength | 834.05     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 300.75     |
| TotalNEpisodes       | 15387      |
| TotalNSamples        | 9.8778e+05 |
| ExplainedVariance    | -0.012281  |
-------------------------------------
[2018-12-22 10:27:02.311168 UTC] Saving snapshot
[2018-12-22 10:27:02.311444 UTC] Starting iteration 199
[2018-12-22 10:27:02.311563 UTC] Start collecting samples
[2018-12-22 10:27:05.400639 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:05.479945 UTC] Performing policy update
[2018-12-22 10:27:05.480715 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:05.570286 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:06.631326 UTC] Performing line search
[2018-12-22 10:27:06.756834 UTC] Updating baseline
[2018-12-22 10:27:07.994749 UTC] Computing logging information
-------------------------------------
| Iteration            | 199        |
| ExpectedImprovement  | 0.014984   |
| ActualImprovement    | 0.013903   |
| ImprovementRatio     | 0.92786    |
| MeanKL               | 0.0073307  |
| Entropy              | 1.6106     |
| Perplexity           | 5.0059     |
| AveragePolicyStd     | 0.3202     |
| AveragePolicyStd[0]  | 0.32416    |
| AveragePolicyStd[1]  | 0.37485    |
| AveragePolicyStd[2]  | 0.34326    |
| AveragePolicyStd[3]  | 0.35433    |
| AveragePolicyStd[4]  | 0.23364    |
| AveragePolicyStd[5]  | 0.29097    |
| AverageReturn        | 616.16     |
| MinReturn            | 40.862     |
| MaxReturn            | 790.46     |
| StdReturn            | 222.3      |
| AverageEpisodeLength | 836.86     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 301.29     |
| TotalNEpisodes       | 15393      |
| TotalNSamples        | 9.9378e+05 |
| ExplainedVariance    | 0.21229    |
-------------------------------------
[2018-12-22 10:27:08.327201 UTC] Saving snapshot
[2018-12-22 10:27:08.327485 UTC] Starting iteration 200
[2018-12-22 10:27:08.327620 UTC] Start collecting samples
[2018-12-22 10:27:11.395011 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:11.471738 UTC] Performing policy update
[2018-12-22 10:27:11.472460 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:11.561603 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:12.620143 UTC] Performing line search
[2018-12-22 10:27:12.744867 UTC] Updating baseline
[2018-12-22 10:27:14.141457 UTC] Computing logging information
-------------------------------------
| Iteration            | 200        |
| ExpectedImprovement  | 0.015149   |
| ActualImprovement    | 0.014374   |
| ImprovementRatio     | 0.94884    |
| MeanKL               | 0.0069556  |
| Entropy              | 1.615      |
| Perplexity           | 5.0279     |
| AveragePolicyStd     | 0.32047    |
| AveragePolicyStd[0]  | 0.32464    |
| AveragePolicyStd[1]  | 0.37535    |
| AveragePolicyStd[2]  | 0.34316    |
| AveragePolicyStd[3]  | 0.3557     |
| AveragePolicyStd[4]  | 0.23392    |
| AveragePolicyStd[5]  | 0.29006    |
| AverageReturn        | 612.62     |
| MinReturn            | 40.862     |
| MaxReturn            | 790.97     |
| StdReturn            | 229.05     |
| AverageEpisodeLength | 828.3      |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 308.64     |
| TotalNEpisodes       | 15400      |
| TotalNSamples        | 9.9912e+05 |
| ExplainedVariance    | 0.3771     |
-------------------------------------
[2018-12-22 10:27:14.469794 UTC] Saving snapshot
[2018-12-22 10:27:14.477886 UTC] Starting iteration 201
[2018-12-22 10:27:14.478079 UTC] Start collecting samples
[2018-12-22 10:27:17.528666 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:17.606030 UTC] Performing policy update
[2018-12-22 10:27:17.606615 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:17.697960 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:18.751476 UTC] Performing line search
[2018-12-22 10:27:18.878101 UTC] Updating baseline
[2018-12-22 10:27:20.271353 UTC] Computing logging information
-------------------------------------
| Iteration            | 201        |
| ExpectedImprovement  | 0.016495   |
| ActualImprovement    | 0.012857   |
| ImprovementRatio     | 0.77943    |
| MeanKL               | 0.0068831  |
| Entropy              | 1.6067     |
| Perplexity           | 4.9861     |
| AveragePolicyStd     | 0.3201     |
| AveragePolicyStd[0]  | 0.32652    |
| AveragePolicyStd[1]  | 0.37355    |
| AveragePolicyStd[2]  | 0.34366    |
| AveragePolicyStd[3]  | 0.35544    |
| AveragePolicyStd[4]  | 0.2324     |
| AveragePolicyStd[5]  | 0.28903    |
| AverageReturn        | 606.79     |
| MinReturn            | 16.685     |
| MaxReturn            | 790.97     |
| StdReturn            | 237.05     |
| AverageEpisodeLength | 818.63     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 318.12     |
| TotalNEpisodes       | 15406      |
| TotalNSamples        | 1.0042e+06 |
| ExplainedVariance    | 0.027322   |
-------------------------------------
[2018-12-22 10:27:20.595785 UTC] Saving snapshot
[2018-12-22 10:27:20.596035 UTC] Starting iteration 202
[2018-12-22 10:27:20.596154 UTC] Start collecting samples
[2018-12-22 10:27:23.631233 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:23.706326 UTC] Performing policy update
[2018-12-22 10:27:23.707010 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:23.800050 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:24.855333 UTC] Performing line search
[2018-12-22 10:27:24.979776 UTC] Updating baseline
[2018-12-22 10:27:26.373460 UTC] Computing logging information
-------------------------------------
| Iteration            | 202        |
| ExpectedImprovement  | 0.014154   |
| ActualImprovement    | 0.013916   |
| ImprovementRatio     | 0.98314    |
| MeanKL               | 0.0073991  |
| Entropy              | 1.6069     |
| Perplexity           | 4.9872     |
| AveragePolicyStd     | 0.32005    |
| AveragePolicyStd[0]  | 0.32495    |
| AveragePolicyStd[1]  | 0.37335    |
| AveragePolicyStd[2]  | 0.34402    |
| AveragePolicyStd[3]  | 0.35535    |
| AveragePolicyStd[4]  | 0.23307    |
| AveragePolicyStd[5]  | 0.28958    |
| AverageReturn        | 621.9      |
| MinReturn            | 16.685     |
| MaxReturn            | 790.97     |
| StdReturn            | 232.08     |
| AverageEpisodeLength | 835.95     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 309.52     |
| TotalNEpisodes       | 15411      |
| TotalNSamples        | 1.0092e+06 |
| ExplainedVariance    | -0.025782  |
-------------------------------------
[2018-12-22 10:27:26.698599 UTC] Saving snapshot
[2018-12-22 10:27:26.698845 UTC] Starting iteration 203
[2018-12-22 10:27:26.699152 UTC] Start collecting samples
[2018-12-22 10:27:29.740324 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:29.818789 UTC] Performing policy update
[2018-12-22 10:27:29.819361 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:29.906988 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:30.953647 UTC] Performing line search
[2018-12-22 10:27:31.077292 UTC] Updating baseline
[2018-12-22 10:27:32.745396 UTC] Computing logging information
-------------------------------------
| Iteration            | 203        |
| ExpectedImprovement  | 0.01635    |
| ActualImprovement    | 0.01606    |
| ImprovementRatio     | 0.98228    |
| MeanKL               | 0.0068147  |
| Entropy              | 1.5882     |
| Perplexity           | 4.8948     |
| AveragePolicyStd     | 0.31906    |
| AveragePolicyStd[0]  | 0.32326    |
| AveragePolicyStd[1]  | 0.37199    |
| AveragePolicyStd[2]  | 0.34389    |
| AveragePolicyStd[3]  | 0.35401    |
| AveragePolicyStd[4]  | 0.23229    |
| AveragePolicyStd[5]  | 0.28892    |
| AverageReturn        | 629.36     |
| MinReturn            | 16.685     |
| MaxReturn            | 790.97     |
| StdReturn            | 226.26     |
| AverageEpisodeLength | 844.49     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 301.13     |
| TotalNEpisodes       | 15417      |
| TotalNSamples        | 1.0145e+06 |
| ExplainedVariance    | 0.039188   |
-------------------------------------
[2018-12-22 10:27:33.069196 UTC] Saving snapshot
[2018-12-22 10:27:33.069432 UTC] Starting iteration 204
[2018-12-22 10:27:33.069550 UTC] Start collecting samples
[2018-12-22 10:27:36.100804 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:36.177810 UTC] Performing policy update
[2018-12-22 10:27:36.178668 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:36.268579 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:37.320280 UTC] Performing line search
[2018-12-22 10:27:37.446558 UTC] Updating baseline
[2018-12-22 10:27:38.977899 UTC] Computing logging information
-------------------------------------
| Iteration            | 204        |
| ExpectedImprovement  | 0.013873   |
| ActualImprovement    | 0.01341    |
| ImprovementRatio     | 0.96662    |
| MeanKL               | 0.0075515  |
| Entropy              | 1.5837     |
| Perplexity           | 4.8727     |
| AveragePolicyStd     | 0.3189     |
| AveragePolicyStd[0]  | 0.32275    |
| AveragePolicyStd[1]  | 0.3739     |
| AveragePolicyStd[2]  | 0.3442     |
| AveragePolicyStd[3]  | 0.35238    |
| AveragePolicyStd[4]  | 0.23144    |
| AveragePolicyStd[5]  | 0.28871    |
| AverageReturn        | 627.99     |
| MinReturn            | 16.685     |
| MaxReturn            | 790.97     |
| StdReturn            | 233.74     |
| AverageEpisodeLength | 839.96     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 310.38     |
| TotalNEpisodes       | 15422      |
| TotalNSamples        | 1.0186e+06 |
| ExplainedVariance    | 0.14754    |
-------------------------------------
[2018-12-22 10:27:39.307252 UTC] Saving snapshot
[2018-12-22 10:27:39.307504 UTC] Starting iteration 205
[2018-12-22 10:27:39.307657 UTC] Start collecting samples
[2018-12-22 10:27:42.369535 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:42.446584 UTC] Performing policy update
[2018-12-22 10:27:42.447283 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:42.536509 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:43.586196 UTC] Performing line search
[2018-12-22 10:27:43.711189 UTC] Updating baseline
[2018-12-22 10:27:45.388962 UTC] Computing logging information
-------------------------------------
| Iteration            | 205        |
| ExpectedImprovement  | 0.012106   |
| ActualImprovement    | 0.011568   |
| ImprovementRatio     | 0.95554    |
| MeanKL               | 0.0071424  |
| Entropy              | 1.5871     |
| Perplexity           | 4.8897     |
| AveragePolicyStd     | 0.31913    |
| AveragePolicyStd[0]  | 0.3234     |
| AveragePolicyStd[1]  | 0.37138    |
| AveragePolicyStd[2]  | 0.34419    |
| AveragePolicyStd[3]  | 0.35625    |
| AveragePolicyStd[4]  | 0.23067    |
| AveragePolicyStd[5]  | 0.28891    |
| AverageReturn        | 623.54     |
| MinReturn            | 16.685     |
| MaxReturn            | 790.97     |
| StdReturn            | 240.64     |
| AverageEpisodeLength | 830.93     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 318.62     |
| TotalNEpisodes       | 15429      |
| TotalNSamples        | 1.0247e+06 |
| ExplainedVariance    | 0.12765    |
-------------------------------------
[2018-12-22 10:27:45.719792 UTC] Saving snapshot
[2018-12-22 10:27:45.720050 UTC] Starting iteration 206
[2018-12-22 10:27:45.720179 UTC] Start collecting samples
[2018-12-22 10:27:48.771711 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:48.847060 UTC] Performing policy update
[2018-12-22 10:27:48.847708 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:48.935482 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:49.993702 UTC] Performing line search
[2018-12-22 10:27:50.118943 UTC] Updating baseline
[2018-12-22 10:27:51.452555 UTC] Computing logging information
-------------------------------------
| Iteration            | 206        |
| ExpectedImprovement  | 0.014334   |
| ActualImprovement    | 0.014013   |
| ImprovementRatio     | 0.97762    |
| MeanKL               | 0.0076617  |
| Entropy              | 1.5883     |
| Perplexity           | 4.8952     |
| AveragePolicyStd     | 0.31928    |
| AveragePolicyStd[0]  | 0.32155    |
| AveragePolicyStd[1]  | 0.37296    |
| AveragePolicyStd[2]  | 0.34603    |
| AveragePolicyStd[3]  | 0.35663    |
| AveragePolicyStd[4]  | 0.23052    |
| AveragePolicyStd[5]  | 0.288      |
| AverageReturn        | 619.91     |
| MinReturn            | 16.685     |
| MaxReturn            | 790.97     |
| StdReturn            | 248.78     |
| AverageEpisodeLength | 824.12     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 330.17     |
| TotalNEpisodes       | 15434      |
| TotalNSamples        | 1.0279e+06 |
| ExplainedVariance    | 0.35436    |
-------------------------------------
[2018-12-22 10:27:51.780727 UTC] Saving snapshot
[2018-12-22 10:27:51.780989 UTC] Starting iteration 207
[2018-12-22 10:27:51.781136 UTC] Start collecting samples
[2018-12-22 10:27:54.835531 UTC] Computing input variables for policy optimization
[2018-12-22 10:27:54.914096 UTC] Performing policy update
[2018-12-22 10:27:54.914731 UTC] Computing gradient in Euclidean space
[2018-12-22 10:27:55.003514 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:27:56.014874 UTC] Performing line search
[2018-12-22 10:27:56.139435 UTC] Updating baseline
[2018-12-22 10:27:57.510071 UTC] Computing logging information
-------------------------------------
| Iteration            | 207        |
| ExpectedImprovement  | 0.015941   |
| ActualImprovement    | 0.016338   |
| ImprovementRatio     | 1.0249     |
| MeanKL               | 0.0071551  |
| Entropy              | 1.5944     |
| Perplexity           | 4.9254     |
| AveragePolicyStd     | 0.31965    |
| AveragePolicyStd[0]  | 0.32271    |
| AveragePolicyStd[1]  | 0.37372    |
| AveragePolicyStd[2]  | 0.34673    |
| AveragePolicyStd[3]  | 0.35575    |
| AveragePolicyStd[4]  | 0.22984    |
| AveragePolicyStd[5]  | 0.28914    |
| AverageReturn        | 630.97     |
| MinReturn            | 16.685     |
| MaxReturn            | 795.62     |
| StdReturn            | 242.75     |
| AverageEpisodeLength | 837.4      |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 321.29     |
| TotalNEpisodes       | 15441      |
| TotalNSamples        | 1.0342e+06 |
| ExplainedVariance    | 0.26413    |
-------------------------------------
[2018-12-22 10:27:57.838143 UTC] Saving snapshot
[2018-12-22 10:27:57.838495 UTC] Starting iteration 208
[2018-12-22 10:27:57.838635 UTC] Start collecting samples
[2018-12-22 10:28:00.899801 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:00.976400 UTC] Performing policy update
[2018-12-22 10:28:00.977032 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:01.062997 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:02.045105 UTC] Performing line search
[2018-12-22 10:28:02.157690 UTC] Updating baseline
[2018-12-22 10:28:03.450500 UTC] Computing logging information
-------------------------------------
| Iteration            | 208        |
| ExpectedImprovement  | 0.013802   |
| ActualImprovement    | 0.013475   |
| ImprovementRatio     | 0.97631    |
| MeanKL               | 0.0069292  |
| Entropy              | 1.5881     |
| Perplexity           | 4.8943     |
| AveragePolicyStd     | 0.31921    |
| AveragePolicyStd[0]  | 0.32237    |
| AveragePolicyStd[1]  | 0.37282    |
| AveragePolicyStd[2]  | 0.34516    |
| AveragePolicyStd[3]  | 0.3554     |
| AveragePolicyStd[4]  | 0.23085    |
| AveragePolicyStd[5]  | 0.28863    |
| AverageReturn        | 639.01     |
| MinReturn            | 16.685     |
| MaxReturn            | 795.62     |
| StdReturn            | 238.89     |
| AverageEpisodeLength | 844.05     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 314.67     |
| TotalNEpisodes       | 15448      |
| TotalNSamples        | 1.0407e+06 |
| ExplainedVariance    | 0.10923    |
-------------------------------------
[2018-12-22 10:28:03.772725 UTC] Saving snapshot
[2018-12-22 10:28:03.773038 UTC] Starting iteration 209
[2018-12-22 10:28:03.773159 UTC] Start collecting samples
[2018-12-22 10:28:06.813557 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:06.888389 UTC] Performing policy update
[2018-12-22 10:28:06.888984 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:06.971752 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:07.976317 UTC] Performing line search
[2018-12-22 10:28:08.099122 UTC] Updating baseline
[2018-12-22 10:28:09.498062 UTC] Computing logging information
-------------------------------------
| Iteration            | 209        |
| ExpectedImprovement  | 0.019637   |
| ActualImprovement    | 0.015464   |
| ImprovementRatio     | 0.78747    |
| MeanKL               | 0.0067059  |
| Entropy              | 1.5877     |
| Perplexity           | 4.8924     |
| AveragePolicyStd     | 0.31932    |
| AveragePolicyStd[0]  | 0.32412    |
| AveragePolicyStd[1]  | 0.37298    |
| AveragePolicyStd[2]  | 0.34655    |
| AveragePolicyStd[3]  | 0.35548    |
| AveragePolicyStd[4]  | 0.22963    |
| AveragePolicyStd[5]  | 0.28714    |
| AverageReturn        | 627.39     |
| MinReturn            | 16.685     |
| MaxReturn            | 795.62     |
| StdReturn            | 248.37     |
| AverageEpisodeLength | 828.24     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 326.26     |
| TotalNEpisodes       | 15451      |
| TotalNSamples        | 1.0421e+06 |
| ExplainedVariance    | 0.41689    |
-------------------------------------
[2018-12-22 10:28:09.830472 UTC] Saving snapshot
[2018-12-22 10:28:09.830731 UTC] Starting iteration 210
[2018-12-22 10:28:09.830861 UTC] Start collecting samples
[2018-12-22 10:28:12.898680 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:12.977222 UTC] Performing policy update
[2018-12-22 10:28:12.977965 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:13.068782 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:14.116819 UTC] Performing line search
[2018-12-22 10:28:14.242189 UTC] Updating baseline
[2018-12-22 10:28:15.526903 UTC] Computing logging information
-------------------------------------
| Iteration            | 210        |
| ExpectedImprovement  | 0.013876   |
| ActualImprovement    | 0.013243   |
| ImprovementRatio     | 0.95441    |
| MeanKL               | 0.0081996  |
| Entropy              | 1.5632     |
| Perplexity           | 4.774      |
| AveragePolicyStd     | 0.31799    |
| AveragePolicyStd[0]  | 0.32489    |
| AveragePolicyStd[1]  | 0.37089    |
| AveragePolicyStd[2]  | 0.3432     |
| AveragePolicyStd[3]  | 0.35419    |
| AveragePolicyStd[4]  | 0.22852    |
| AveragePolicyStd[5]  | 0.28626    |
| AverageReturn        | 640.86     |
| MinReturn            | 16.685     |
| MaxReturn            | 795.62     |
| StdReturn            | 242.16     |
| AverageEpisodeLength | 842.81     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 316.4      |
| TotalNEpisodes       | 15458      |
| TotalNSamples        | 1.0491e+06 |
| ExplainedVariance    | 0.12298    |
-------------------------------------
[2018-12-22 10:28:15.850098 UTC] Saving snapshot
[2018-12-22 10:28:15.858195 UTC] Starting iteration 211
[2018-12-22 10:28:15.858394 UTC] Start collecting samples
[2018-12-22 10:28:18.912440 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:18.989795 UTC] Performing policy update
[2018-12-22 10:28:18.990596 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:19.079198 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:20.136876 UTC] Performing line search
[2018-12-22 10:28:20.261483 UTC] Updating baseline
[2018-12-22 10:28:21.669931 UTC] Computing logging information
-------------------------------------
| Iteration            | 211        |
| ExpectedImprovement  | 0.015209   |
| ActualImprovement    | 0.014654   |
| ImprovementRatio     | 0.96356    |
| MeanKL               | 0.0070316  |
| Entropy              | 1.5699     |
| Perplexity           | 4.806      |
| AveragePolicyStd     | 0.31845    |
| AveragePolicyStd[0]  | 0.32275    |
| AveragePolicyStd[1]  | 0.37425    |
| AveragePolicyStd[2]  | 0.34449    |
| AveragePolicyStd[3]  | 0.35393    |
| AveragePolicyStd[4]  | 0.22802    |
| AveragePolicyStd[5]  | 0.28725    |
| AverageReturn        | 643.87     |
| MinReturn            | 16.685     |
| MaxReturn            | 799.44     |
| StdReturn            | 245.9      |
| AverageEpisodeLength | 843.88     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 321        |
| TotalNEpisodes       | 15465      |
| TotalNSamples        | 1.0543e+06 |
| ExplainedVariance    | 0.16502    |
-------------------------------------
[2018-12-22 10:28:22.001033 UTC] Saving snapshot
[2018-12-22 10:28:22.001269 UTC] Starting iteration 212
[2018-12-22 10:28:22.001398 UTC] Start collecting samples
[2018-12-22 10:28:24.987141 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:25.060366 UTC] Performing policy update
[2018-12-22 10:28:25.060937 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:25.149650 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:26.199932 UTC] Performing line search
[2018-12-22 10:28:26.324114 UTC] Updating baseline
[2018-12-22 10:28:27.725483 UTC] Computing logging information
-------------------------------------
| Iteration            | 212        |
| ExpectedImprovement  | 0.020219   |
| ActualImprovement    | 0.018647   |
| ImprovementRatio     | 0.92226    |
| MeanKL               | 0.0072116  |
| Entropy              | 1.5677     |
| Perplexity           | 4.7955     |
| AveragePolicyStd     | 0.31826    |
| AveragePolicyStd[0]  | 0.32241    |
| AveragePolicyStd[1]  | 0.37413    |
| AveragePolicyStd[2]  | 0.34329    |
| AveragePolicyStd[3]  | 0.35461    |
| AveragePolicyStd[4]  | 0.22954    |
| AveragePolicyStd[5]  | 0.28557    |
| AverageReturn        | 645.04     |
| MinReturn            | 16.685     |
| MaxReturn            | 807.45     |
| StdReturn            | 246.46     |
| AverageEpisodeLength | 843.88     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 321        |
| TotalNEpisodes       | 15467      |
| TotalNSamples        | 1.0563e+06 |
| ExplainedVariance    | 0.07323    |
-------------------------------------
[2018-12-22 10:28:28.059487 UTC] Saving snapshot
[2018-12-22 10:28:28.059741 UTC] Starting iteration 213
[2018-12-22 10:28:28.059858 UTC] Start collecting samples
[2018-12-22 10:28:31.135648 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:31.213935 UTC] Performing policy update
[2018-12-22 10:28:31.214520 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:31.303858 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:32.355382 UTC] Performing line search
[2018-12-22 10:28:32.481517 UTC] Updating baseline
[2018-12-22 10:28:33.809703 UTC] Computing logging information
-------------------------------------
| Iteration            | 213        |
| ExpectedImprovement  | 0.015353   |
| ActualImprovement    | 0.013625   |
| ImprovementRatio     | 0.8875     |
| MeanKL               | 0.0071693  |
| Entropy              | 1.5476     |
| Perplexity           | 4.7        |
| AveragePolicyStd     | 0.31716    |
| AveragePolicyStd[0]  | 0.32241    |
| AveragePolicyStd[1]  | 0.37373    |
| AveragePolicyStd[2]  | 0.34051    |
| AveragePolicyStd[3]  | 0.35262    |
| AveragePolicyStd[4]  | 0.22904    |
| AveragePolicyStd[5]  | 0.28468    |
| AverageReturn        | 673.94     |
| MinReturn            | 16.685     |
| MaxReturn            | 808.13     |
| StdReturn            | 227.88     |
| AverageEpisodeLength | 877.31     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 295.41     |
| TotalNEpisodes       | 15476      |
| TotalNSamples        | 1.0653e+06 |
| ExplainedVariance    | 0.0095323  |
-------------------------------------
[2018-12-22 10:28:34.140264 UTC] Saving snapshot
[2018-12-22 10:28:34.140507 UTC] Starting iteration 214
[2018-12-22 10:28:34.140645 UTC] Start collecting samples
[2018-12-22 10:28:37.137195 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:37.211268 UTC] Performing policy update
[2018-12-22 10:28:37.211920 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:37.299288 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:38.232143 UTC] Performing line search
[2018-12-22 10:28:38.345112 UTC] Updating baseline
[2018-12-22 10:28:39.655598 UTC] Computing logging information
-------------------------------------
| Iteration            | 214        |
| ExpectedImprovement  | 0.015025   |
| ActualImprovement    | 0.013144   |
| ImprovementRatio     | 0.87476    |
| MeanKL               | 0.0068556  |
| Entropy              | 1.5501     |
| Perplexity           | 4.712      |
| AveragePolicyStd     | 0.31734    |
| AveragePolicyStd[0]  | 0.32131    |
| AveragePolicyStd[1]  | 0.3762     |
| AveragePolicyStd[2]  | 0.33953    |
| AveragePolicyStd[3]  | 0.35261    |
| AveragePolicyStd[4]  | 0.22903    |
| AveragePolicyStd[5]  | 0.28535    |
| AverageReturn        | 675.32     |
| MinReturn            | 16.685     |
| MaxReturn            | 810.75     |
| StdReturn            | 228.5      |
| AverageEpisodeLength | 877.31     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 295.41     |
| TotalNEpisodes       | 15479      |
| TotalNSamples        | 1.0683e+06 |
| ExplainedVariance    | 0.049424   |
-------------------------------------
[2018-12-22 10:28:39.982513 UTC] Saving snapshot
[2018-12-22 10:28:39.982832 UTC] Starting iteration 215
[2018-12-22 10:28:39.982969 UTC] Start collecting samples
[2018-12-22 10:28:43.007262 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:43.083462 UTC] Performing policy update
[2018-12-22 10:28:43.085220 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:43.174191 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:44.220468 UTC] Performing line search
[2018-12-22 10:28:44.346027 UTC] Updating baseline
[2018-12-22 10:28:45.580875 UTC] Computing logging information
-------------------------------------
| Iteration            | 215        |
| ExpectedImprovement  | 0.016111   |
| ActualImprovement    | 0.014912   |
| ImprovementRatio     | 0.92558    |
| MeanKL               | 0.0073474  |
| Entropy              | 1.5358     |
| Perplexity           | 4.6449     |
| AveragePolicyStd     | 0.31651    |
| AveragePolicyStd[0]  | 0.32381    |
| AveragePolicyStd[1]  | 0.37768    |
| AveragePolicyStd[2]  | 0.33578    |
| AveragePolicyStd[3]  | 0.3475     |
| AveragePolicyStd[4]  | 0.22905    |
| AveragePolicyStd[5]  | 0.28523    |
| AverageReturn        | 683.1      |
| MinReturn            | 16.685     |
| MaxReturn            | 810.75     |
| StdReturn            | 223.15     |
| AverageEpisodeLength | 885.38     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 288.21     |
| TotalNEpisodes       | 15483      |
| TotalNSamples        | 1.0723e+06 |
| ExplainedVariance    | 0.24827    |
-------------------------------------
[2018-12-22 10:28:45.900828 UTC] Saving snapshot
[2018-12-22 10:28:45.901072 UTC] Starting iteration 216
[2018-12-22 10:28:45.901189 UTC] Start collecting samples
[2018-12-22 10:28:48.949061 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:49.028365 UTC] Performing policy update
[2018-12-22 10:28:49.029179 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:49.118587 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:50.169035 UTC] Performing line search
[2018-12-22 10:28:50.293804 UTC] Updating baseline
[2018-12-22 10:28:51.763006 UTC] Computing logging information
------------------------------------
| Iteration            | 216       |
| ExpectedImprovement  | 0.015475  |
| ActualImprovement    | 0.014687  |
| ImprovementRatio     | 0.94904   |
| MeanKL               | 0.0074804 |
| Entropy              | 1.5206    |
| Perplexity           | 4.575     |
| AveragePolicyStd     | 0.31565   |
| AveragePolicyStd[0]  | 0.32166   |
| AveragePolicyStd[1]  | 0.37694   |
| AveragePolicyStd[2]  | 0.33338   |
| AveragePolicyStd[3]  | 0.34733   |
| AveragePolicyStd[4]  | 0.2292    |
| AveragePolicyStd[5]  | 0.28536   |
| AverageReturn        | 684.74    |
| MinReturn            | 16.685    |
| MaxReturn            | 828.17    |
| StdReturn            | 224.56    |
| AverageEpisodeLength | 882.2     |
| MinEpisodeLength     | 33        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 288.2     |
| TotalNEpisodes       | 15491     |
| TotalNSamples        | 1.08e+06  |
| ExplainedVariance    | 0.1295    |
------------------------------------
[2018-12-22 10:28:52.093194 UTC] Saving snapshot
[2018-12-22 10:28:52.093437 UTC] Starting iteration 217
[2018-12-22 10:28:52.093552 UTC] Start collecting samples
[2018-12-22 10:28:55.137255 UTC] Computing input variables for policy optimization
[2018-12-22 10:28:55.213279 UTC] Performing policy update
[2018-12-22 10:28:55.214018 UTC] Computing gradient in Euclidean space
[2018-12-22 10:28:55.301648 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:28:56.349960 UTC] Performing line search
[2018-12-22 10:28:56.472942 UTC] Updating baseline
[2018-12-22 10:28:57.880401 UTC] Computing logging information
-------------------------------------
| Iteration            | 217        |
| ExpectedImprovement  | 0.017955   |
| ActualImprovement    | 0.016169   |
| ImprovementRatio     | 0.90052    |
| MeanKL               | 0.0068671  |
| Entropy              | 1.5058     |
| Perplexity           | 4.5079     |
| AveragePolicyStd     | 0.31482    |
| AveragePolicyStd[0]  | 0.32027    |
| AveragePolicyStd[1]  | 0.37748    |
| AveragePolicyStd[2]  | 0.33196    |
| AveragePolicyStd[3]  | 0.34468    |
| AveragePolicyStd[4]  | 0.22948    |
| AveragePolicyStd[5]  | 0.28504    |
| AverageReturn        | 680.33     |
| MinReturn            | 16.685     |
| MaxReturn            | 828.17     |
| StdReturn            | 231.03     |
| AverageEpisodeLength | 874.66     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 294.8      |
| TotalNEpisodes       | 15496      |
| TotalNSamples        | 1.0842e+06 |
| ExplainedVariance    | 0.29259    |
-------------------------------------
[2018-12-22 10:28:58.212871 UTC] Saving snapshot
[2018-12-22 10:28:58.213112 UTC] Starting iteration 218
[2018-12-22 10:28:58.213230 UTC] Start collecting samples
[2018-12-22 10:29:01.243425 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:01.320371 UTC] Performing policy update
[2018-12-22 10:29:01.321128 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:01.411115 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:02.467860 UTC] Performing line search
[2018-12-22 10:29:02.592481 UTC] Updating baseline
[2018-12-22 10:29:04.177776 UTC] Computing logging information
-------------------------------------
| Iteration            | 218        |
| ExpectedImprovement  | 0.014915   |
| ActualImprovement    | 0.014364   |
| ImprovementRatio     | 0.96305    |
| MeanKL               | 0.0071619  |
| Entropy              | 1.4826     |
| Perplexity           | 4.4043     |
| AveragePolicyStd     | 0.31366    |
| AveragePolicyStd[0]  | 0.31769    |
| AveragePolicyStd[1]  | 0.37698    |
| AveragePolicyStd[2]  | 0.33015    |
| AveragePolicyStd[3]  | 0.34456    |
| AveragePolicyStd[4]  | 0.22805    |
| AveragePolicyStd[5]  | 0.28453    |
| AverageReturn        | 689.79     |
| MinReturn            | 16.685     |
| MaxReturn            | 831.97     |
| StdReturn            | 222.26     |
| AverageEpisodeLength | 884.57     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 282.78     |
| TotalNEpisodes       | 15501      |
| TotalNSamples        | 1.0886e+06 |
| ExplainedVariance    | -0.0047035 |
-------------------------------------
[2018-12-22 10:29:04.508968 UTC] Saving snapshot
[2018-12-22 10:29:04.509237 UTC] Starting iteration 219
[2018-12-22 10:29:04.509384 UTC] Start collecting samples
[2018-12-22 10:29:07.572596 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:07.649248 UTC] Performing policy update
[2018-12-22 10:29:07.649935 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:07.741271 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:08.787083 UTC] Performing line search
[2018-12-22 10:29:08.911475 UTC] Updating baseline
[2018-12-22 10:29:10.548842 UTC] Computing logging information
-------------------------------------
| Iteration            | 219        |
| ExpectedImprovement  | 0.016834   |
| ActualImprovement    | 0.015342   |
| ImprovementRatio     | 0.91139    |
| MeanKL               | 0.0067342  |
| Entropy              | 1.4615     |
| Perplexity           | 4.3122     |
| AveragePolicyStd     | 0.31252    |
| AveragePolicyStd[0]  | 0.31827    |
| AveragePolicyStd[1]  | 0.37608    |
| AveragePolicyStd[2]  | 0.32867    |
| AveragePolicyStd[3]  | 0.34157    |
| AveragePolicyStd[4]  | 0.22787    |
| AveragePolicyStd[5]  | 0.28267    |
| AverageReturn        | 690.94     |
| MinReturn            | 69.244     |
| MaxReturn            | 831.97     |
| StdReturn            | 221.02     |
| AverageEpisodeLength | 883.03     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 279.5      |
| TotalNEpisodes       | 15508      |
| TotalNSamples        | 1.0945e+06 |
| ExplainedVariance    | 0.12718    |
-------------------------------------
[2018-12-22 10:29:10.879128 UTC] Saving snapshot
[2018-12-22 10:29:10.879416 UTC] Starting iteration 220
[2018-12-22 10:29:10.879536 UTC] Start collecting samples
[2018-12-22 10:29:13.916578 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:13.994361 UTC] Performing policy update
[2018-12-22 10:29:13.994957 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:14.085851 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:15.129066 UTC] Performing line search
[2018-12-22 10:29:15.253531 UTC] Updating baseline
[2018-12-22 10:29:16.481663 UTC] Computing logging information
-------------------------------------
| Iteration            | 220        |
| ExpectedImprovement  | 0.015862   |
| ActualImprovement    | 0.014937   |
| ImprovementRatio     | 0.94167    |
| MeanKL               | 0.0069342  |
| Entropy              | 1.4573     |
| Perplexity           | 4.2942     |
| AveragePolicyStd     | 0.31225    |
| AveragePolicyStd[0]  | 0.31838    |
| AveragePolicyStd[1]  | 0.37786    |
| AveragePolicyStd[2]  | 0.32647    |
| AveragePolicyStd[3]  | 0.33913    |
| AveragePolicyStd[4]  | 0.22873    |
| AveragePolicyStd[5]  | 0.28291    |
| AverageReturn        | 693.12     |
| MinReturn            | 69.244     |
| MaxReturn            | 835.48     |
| StdReturn            | 222.09     |
| AverageEpisodeLength | 883.03     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 279.5      |
| TotalNEpisodes       | 15513      |
| TotalNSamples        | 1.0995e+06 |
| ExplainedVariance    | 0.1257     |
-------------------------------------
[2018-12-22 10:29:16.803923 UTC] Saving snapshot
[2018-12-22 10:29:16.812035 UTC] Starting iteration 221
[2018-12-22 10:29:16.812226 UTC] Start collecting samples
[2018-12-22 10:29:19.847178 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:19.927014 UTC] Performing policy update
[2018-12-22 10:29:19.927608 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:20.016508 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:21.065874 UTC] Performing line search
[2018-12-22 10:29:21.191103 UTC] Updating baseline
[2018-12-22 10:29:22.424088 UTC] Computing logging information
-------------------------------------
| Iteration            | 221        |
| ExpectedImprovement  | 0.016348   |
| ActualImprovement    | 0.0154     |
| ImprovementRatio     | 0.94205    |
| MeanKL               | 0.0071076  |
| Entropy              | 1.4548     |
| Perplexity           | 4.2837     |
| AveragePolicyStd     | 0.31206    |
| AveragePolicyStd[0]  | 0.31948    |
| AveragePolicyStd[1]  | 0.37661    |
| AveragePolicyStd[2]  | 0.32444    |
| AveragePolicyStd[3]  | 0.34063    |
| AveragePolicyStd[4]  | 0.22976    |
| AveragePolicyStd[5]  | 0.28143    |
| AverageReturn        | 695.41     |
| MinReturn            | 69.244     |
| MaxReturn            | 835.48     |
| StdReturn            | 223.78     |
| AverageEpisodeLength | 882.86     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 279.81     |
| TotalNEpisodes       | 15518      |
| TotalNSamples        | 1.1038e+06 |
| ExplainedVariance    | 0.10685    |
-------------------------------------
[2018-12-22 10:29:22.753316 UTC] Saving snapshot
[2018-12-22 10:29:22.753591 UTC] Starting iteration 222
[2018-12-22 10:29:22.753725 UTC] Start collecting samples
[2018-12-22 10:29:25.802515 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:25.880315 UTC] Performing policy update
[2018-12-22 10:29:25.881285 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:25.972650 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:27.022816 UTC] Performing line search
[2018-12-22 10:29:27.148935 UTC] Updating baseline
[2018-12-22 10:29:28.557036 UTC] Computing logging information
------------------------------------
| Iteration            | 222       |
| ExpectedImprovement  | 0.019871  |
| ActualImprovement    | 0.01729   |
| ImprovementRatio     | 0.87008   |
| MeanKL               | 0.0068746 |
| Entropy              | 1.4492    |
| Perplexity           | 4.2596    |
| AveragePolicyStd     | 0.31188   |
| AveragePolicyStd[0]  | 0.31933   |
| AveragePolicyStd[1]  | 0.37632   |
| AveragePolicyStd[2]  | 0.32491   |
| AveragePolicyStd[3]  | 0.34184   |
| AveragePolicyStd[4]  | 0.22824   |
| AveragePolicyStd[5]  | 0.28065   |
| AverageReturn        | 696.87    |
| MinReturn            | 15.399    |
| MaxReturn            | 835.48    |
| StdReturn            | 221.75    |
| AverageEpisodeLength | 882.53    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 275.16    |
| TotalNEpisodes       | 15525     |
| TotalNSamples        | 1.109e+06 |
| ExplainedVariance    | 0.27308   |
------------------------------------
[2018-12-22 10:29:28.884761 UTC] Saving snapshot
[2018-12-22 10:29:28.885005 UTC] Starting iteration 223
[2018-12-22 10:29:28.885139 UTC] Start collecting samples
[2018-12-22 10:29:31.886354 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:31.960892 UTC] Performing policy update
[2018-12-22 10:29:31.961593 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:32.049208 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:33.096477 UTC] Performing line search
[2018-12-22 10:29:33.222494 UTC] Updating baseline
[2018-12-22 10:29:34.465121 UTC] Computing logging information
------------------------------------
| Iteration            | 223       |
| ExpectedImprovement  | 0.014089  |
| ActualImprovement    | 0.013532  |
| ImprovementRatio     | 0.96046   |
| MeanKL               | 0.0071791 |
| Entropy              | 1.4456    |
| Perplexity           | 4.2442    |
| AveragePolicyStd     | 0.31164   |
| AveragePolicyStd[0]  | 0.319     |
| AveragePolicyStd[1]  | 0.3766    |
| AveragePolicyStd[2]  | 0.32277   |
| AveragePolicyStd[3]  | 0.34122   |
| AveragePolicyStd[4]  | 0.22852   |
| AveragePolicyStd[5]  | 0.28174   |
| AverageReturn        | 698.68    |
| MinReturn            | 15.399    |
| MaxReturn            | 835.48    |
| StdReturn            | 222.49    |
| AverageEpisodeLength | 882.53    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 275.16    |
| TotalNEpisodes       | 15529     |
| TotalNSamples        | 1.113e+06 |
| ExplainedVariance    | 0.16086   |
------------------------------------
[2018-12-22 10:29:34.795656 UTC] Saving snapshot
[2018-12-22 10:29:34.795922 UTC] Starting iteration 224
[2018-12-22 10:29:34.796042 UTC] Start collecting samples
[2018-12-22 10:29:37.815940 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:37.893176 UTC] Performing policy update
[2018-12-22 10:29:37.893875 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:37.984242 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:39.021372 UTC] Performing line search
[2018-12-22 10:29:39.145723 UTC] Updating baseline
[2018-12-22 10:29:40.400292 UTC] Computing logging information
------------------------------------
| Iteration            | 224       |
| ExpectedImprovement  | 0.015408  |
| ActualImprovement    | 0.01438   |
| ImprovementRatio     | 0.93328   |
| MeanKL               | 0.0069565 |
| Entropy              | 1.4574    |
| Perplexity           | 4.2947    |
| AveragePolicyStd     | 0.31218   |
| AveragePolicyStd[0]  | 0.32174   |
| AveragePolicyStd[1]  | 0.37673   |
| AveragePolicyStd[2]  | 0.31936   |
| AveragePolicyStd[3]  | 0.34278   |
| AveragePolicyStd[4]  | 0.23      |
| AveragePolicyStd[5]  | 0.28246   |
| AverageReturn        | 715.21    |
| MinReturn            | 15.399    |
| MaxReturn            | 859.01    |
| StdReturn            | 206.01    |
| AverageEpisodeLength | 900.42    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 252.13    |
| TotalNEpisodes       | 15534     |
| TotalNSamples        | 1.118e+06 |
| ExplainedVariance    | 0.195     |
------------------------------------
[2018-12-22 10:29:40.729045 UTC] Saving snapshot
[2018-12-22 10:29:40.729298 UTC] Starting iteration 225
[2018-12-22 10:29:40.729430 UTC] Start collecting samples
[2018-12-22 10:29:43.776503 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:43.854518 UTC] Performing policy update
[2018-12-22 10:29:43.855246 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:43.947657 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:45.001071 UTC] Performing line search
[2018-12-22 10:29:45.125766 UTC] Updating baseline
[2018-12-22 10:29:46.461512 UTC] Computing logging information
------------------------------------
| Iteration            | 225       |
| ExpectedImprovement  | 0.015162  |
| ActualImprovement    | 0.015161  |
| ImprovementRatio     | 0.99994   |
| MeanKL               | 0.0069826 |
| Entropy              | 1.4527    |
| Perplexity           | 4.2746    |
| AveragePolicyStd     | 0.31192   |
| AveragePolicyStd[0]  | 0.32163   |
| AveragePolicyStd[1]  | 0.37665   |
| AveragePolicyStd[2]  | 0.3191    |
| AveragePolicyStd[3]  | 0.342     |
| AveragePolicyStd[4]  | 0.23021   |
| AveragePolicyStd[5]  | 0.2819    |
| AverageReturn        | 725.52    |
| MinReturn            | 15.399    |
| MaxReturn            | 859.01    |
| StdReturn            | 200.33    |
| AverageEpisodeLength | 907.92    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 243.68    |
| TotalNEpisodes       | 15541     |
| TotalNSamples        | 1.125e+06 |
| ExplainedVariance    | 0.02767   |
------------------------------------
[2018-12-22 10:29:46.791757 UTC] Saving snapshot
[2018-12-22 10:29:46.792010 UTC] Starting iteration 226
[2018-12-22 10:29:46.792141 UTC] Start collecting samples
[2018-12-22 10:29:49.806641 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:49.882021 UTC] Performing policy update
[2018-12-22 10:29:49.882703 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:49.972180 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:51.025033 UTC] Performing line search
[2018-12-22 10:29:51.150840 UTC] Updating baseline
[2018-12-22 10:29:52.327968 UTC] Computing logging information
------------------------------------
| Iteration            | 226       |
| ExpectedImprovement  | 0.014827  |
| ActualImprovement    | 0.01455   |
| ImprovementRatio     | 0.98129   |
| MeanKL               | 0.0074243 |
| Entropy              | 1.4479    |
| Perplexity           | 4.2541    |
| AveragePolicyStd     | 0.31174   |
| AveragePolicyStd[0]  | 0.32276   |
| AveragePolicyStd[1]  | 0.37617   |
| AveragePolicyStd[2]  | 0.31806   |
| AveragePolicyStd[3]  | 0.34256   |
| AveragePolicyStd[4]  | 0.22894   |
| AveragePolicyStd[5]  | 0.28194   |
| AverageReturn        | 731.08    |
| MinReturn            | 15.399    |
| MaxReturn            | 859.01    |
| StdReturn            | 198.28    |
| AverageEpisodeLength | 912.78    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 240.6     |
| TotalNEpisodes       | 15545     |
| TotalNSamples        | 1.129e+06 |
| ExplainedVariance    | 0.029968  |
------------------------------------
[2018-12-22 10:29:52.655026 UTC] Saving snapshot
[2018-12-22 10:29:52.655268 UTC] Starting iteration 227
[2018-12-22 10:29:52.655402 UTC] Start collecting samples
[2018-12-22 10:29:55.656820 UTC] Computing input variables for policy optimization
[2018-12-22 10:29:55.734081 UTC] Performing policy update
[2018-12-22 10:29:55.734771 UTC] Computing gradient in Euclidean space
[2018-12-22 10:29:55.824500 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:29:56.869900 UTC] Performing line search
[2018-12-22 10:29:56.994322 UTC] Updating baseline
[2018-12-22 10:29:58.679188 UTC] Computing logging information
------------------------------------
| Iteration            | 227       |
| ExpectedImprovement  | 0.01411   |
| ActualImprovement    | 0.013672  |
| ImprovementRatio     | 0.96897   |
| MeanKL               | 0.0070692 |
| Entropy              | 1.4433    |
| Perplexity           | 4.2346    |
| AveragePolicyStd     | 0.31147   |
| AveragePolicyStd[0]  | 0.32283   |
| AveragePolicyStd[1]  | 0.37526   |
| AveragePolicyStd[2]  | 0.3178    |
| AveragePolicyStd[3]  | 0.3414    |
| AveragePolicyStd[4]  | 0.22843   |
| AveragePolicyStd[5]  | 0.28309   |
| AverageReturn        | 739.35    |
| MinReturn            | 15.399    |
| MaxReturn            | 859.01    |
| StdReturn            | 187.61    |
| AverageEpisodeLength | 921.42    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 227.72    |
| TotalNEpisodes       | 15549     |
| TotalNSamples        | 1.133e+06 |
| ExplainedVariance    | 0.0024326 |
------------------------------------
[2018-12-22 10:29:59.005223 UTC] Saving snapshot
[2018-12-22 10:29:59.005493 UTC] Starting iteration 228
[2018-12-22 10:29:59.005627 UTC] Start collecting samples
[2018-12-22 10:30:02.060694 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:02.139139 UTC] Performing policy update
[2018-12-22 10:30:02.139919 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:02.230121 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:03.273625 UTC] Performing line search
[2018-12-22 10:30:03.398413 UTC] Updating baseline
[2018-12-22 10:30:04.812751 UTC] Computing logging information
------------------------------------
| Iteration            | 228       |
| ExpectedImprovement  | 0.012282  |
| ActualImprovement    | 0.012116  |
| ImprovementRatio     | 0.98653   |
| MeanKL               | 0.0072795 |
| Entropy              | 1.433     |
| Perplexity           | 4.1912    |
| AveragePolicyStd     | 0.31093   |
| AveragePolicyStd[0]  | 0.3227    |
| AveragePolicyStd[1]  | 0.37442   |
| AveragePolicyStd[2]  | 0.31683   |
| AveragePolicyStd[3]  | 0.34171   |
| AveragePolicyStd[4]  | 0.22861   |
| AveragePolicyStd[5]  | 0.28131   |
| AverageReturn        | 748.53    |
| MinReturn            | 15.399    |
| MaxReturn            | 859.01    |
| StdReturn            | 181.26    |
| AverageEpisodeLength | 928.59    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 218.61    |
| TotalNEpisodes       | 15557     |
| TotalNSamples        | 1.141e+06 |
| ExplainedVariance    | 0.04407   |
------------------------------------
[2018-12-22 10:30:05.201796 UTC] Saving snapshot
[2018-12-22 10:30:05.202053 UTC] Starting iteration 229
[2018-12-22 10:30:05.202171 UTC] Start collecting samples
[2018-12-22 10:30:08.205313 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:08.282456 UTC] Performing policy update
[2018-12-22 10:30:08.283448 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:08.374007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:09.415632 UTC] Performing line search
[2018-12-22 10:30:09.541888 UTC] Updating baseline
[2018-12-22 10:30:10.787067 UTC] Computing logging information
------------------------------------
| Iteration            | 229       |
| ExpectedImprovement  | 0.015775  |
| ActualImprovement    | 0.01529   |
| ImprovementRatio     | 0.96925   |
| MeanKL               | 0.0070309 |
| Entropy              | 1.4575    |
| Perplexity           | 4.2954    |
| AveragePolicyStd     | 0.31231   |
| AveragePolicyStd[0]  | 0.32453   |
| AveragePolicyStd[1]  | 0.37536   |
| AveragePolicyStd[2]  | 0.31906   |
| AveragePolicyStd[3]  | 0.34438   |
| AveragePolicyStd[4]  | 0.22797   |
| AveragePolicyStd[5]  | 0.28255   |
| AverageReturn        | 751.07    |
| MinReturn            | 15.399    |
| MaxReturn            | 859.01    |
| StdReturn            | 182.1     |
| AverageEpisodeLength | 928.59    |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 218.61    |
| TotalNEpisodes       | 15561     |
| TotalNSamples        | 1.145e+06 |
| ExplainedVariance    | -0.039285 |
------------------------------------
[2018-12-22 10:30:11.111606 UTC] Saving snapshot
[2018-12-22 10:30:11.111860 UTC] Starting iteration 230
[2018-12-22 10:30:11.111984 UTC] Start collecting samples
[2018-12-22 10:30:14.103522 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:14.179272 UTC] Performing policy update
[2018-12-22 10:30:14.179870 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:14.268667 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:15.317085 UTC] Performing line search
[2018-12-22 10:30:15.442626 UTC] Updating baseline
[2018-12-22 10:30:16.788873 UTC] Computing logging information
-------------------------------------
| Iteration            | 230        |
| ExpectedImprovement  | 0.016646   |
| ActualImprovement    | 0.015599   |
| ImprovementRatio     | 0.93712    |
| MeanKL               | 0.0068516  |
| Entropy              | 1.4596     |
| Perplexity           | 4.3042     |
| AveragePolicyStd     | 0.31259    |
| AveragePolicyStd[0]  | 0.3243     |
| AveragePolicyStd[1]  | 0.37765    |
| AveragePolicyStd[2]  | 0.3172     |
| AveragePolicyStd[3]  | 0.34788    |
| AveragePolicyStd[4]  | 0.22734    |
| AveragePolicyStd[5]  | 0.28118    |
| AverageReturn        | 759.46     |
| MinReturn            | 15.399     |
| MaxReturn            | 859.01     |
| StdReturn            | 167.37     |
| AverageEpisodeLength | 938.75     |
| MinEpisodeLength     | 39         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.68     |
| TotalNEpisodes       | 15564      |
| TotalNSamples        | 1.1472e+06 |
| ExplainedVariance    | -0.021639  |
-------------------------------------
[2018-12-22 10:30:17.117225 UTC] Saving snapshot
[2018-12-22 10:30:17.125498 UTC] Starting iteration 231
[2018-12-22 10:30:17.125712 UTC] Start collecting samples
[2018-12-22 10:30:20.169578 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:20.247779 UTC] Performing policy update
[2018-12-22 10:30:20.248557 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:20.338008 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:21.386874 UTC] Performing line search
[2018-12-22 10:30:21.511109 UTC] Updating baseline
[2018-12-22 10:30:22.851528 UTC] Computing logging information
-------------------------------------
| Iteration            | 231        |
| ExpectedImprovement  | 0.017442   |
| ActualImprovement    | 0.016087   |
| ImprovementRatio     | 0.92232    |
| MeanKL               | 0.0068066  |
| Entropy              | 1.4565     |
| Perplexity           | 4.291      |
| AveragePolicyStd     | 0.31246    |
| AveragePolicyStd[0]  | 0.32401    |
| AveragePolicyStd[1]  | 0.37704    |
| AveragePolicyStd[2]  | 0.31569    |
| AveragePolicyStd[3]  | 0.34965    |
| AveragePolicyStd[4]  | 0.22689    |
| AveragePolicyStd[5]  | 0.2815     |
| AverageReturn        | 762.28     |
| MinReturn            | 15.399     |
| MaxReturn            | 859.01     |
| StdReturn            | 168.2      |
| AverageEpisodeLength | 938.75     |
| MinEpisodeLength     | 39         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.68     |
| TotalNEpisodes       | 15570      |
| TotalNSamples        | 1.1532e+06 |
| ExplainedVariance    | 0.0091511  |
-------------------------------------
[2018-12-22 10:30:23.178723 UTC] Saving snapshot
[2018-12-22 10:30:23.178955 UTC] Starting iteration 232
[2018-12-22 10:30:23.179074 UTC] Start collecting samples
[2018-12-22 10:30:26.186015 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:26.263751 UTC] Performing policy update
[2018-12-22 10:30:26.264503 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:26.352919 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:27.385721 UTC] Performing line search
[2018-12-22 10:30:27.509872 UTC] Updating baseline
[2018-12-22 10:30:28.990336 UTC] Computing logging information
-------------------------------------
| Iteration            | 232        |
| ExpectedImprovement  | 0.014578   |
| ActualImprovement    | 0.013651   |
| ImprovementRatio     | 0.93644    |
| MeanKL               | 0.0072123  |
| Entropy              | 1.4416     |
| Perplexity           | 4.2275     |
| AveragePolicyStd     | 0.31154    |
| AveragePolicyStd[0]  | 0.32338    |
| AveragePolicyStd[1]  | 0.37559    |
| AveragePolicyStd[2]  | 0.31386    |
| AveragePolicyStd[3]  | 0.34779    |
| AveragePolicyStd[4]  | 0.22835    |
| AveragePolicyStd[5]  | 0.28027    |
| AverageReturn        | 765.07     |
| MinReturn            | 15.399     |
| MaxReturn            | 859.01     |
| StdReturn            | 169.1      |
| AverageEpisodeLength | 938.75     |
| MinEpisodeLength     | 39         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.68     |
| TotalNEpisodes       | 15576      |
| TotalNSamples        | 1.1592e+06 |
| ExplainedVariance    | 0.065846   |
-------------------------------------
[2018-12-22 10:30:29.318144 UTC] Saving snapshot
[2018-12-22 10:30:29.318386 UTC] Starting iteration 233
[2018-12-22 10:30:29.318506 UTC] Start collecting samples
[2018-12-22 10:30:32.343155 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:32.420738 UTC] Performing policy update
[2018-12-22 10:30:32.421514 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:32.511769 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:33.567979 UTC] Performing line search
[2018-12-22 10:30:33.693479 UTC] Updating baseline
[2018-12-22 10:30:35.045614 UTC] Computing logging information
-------------------------------------
| Iteration            | 233        |
| ExpectedImprovement  | 0.025563   |
| ActualImprovement    | 0.021117   |
| ImprovementRatio     | 0.82608    |
| MeanKL               | 0.0067945  |
| Entropy              | 1.4416     |
| Perplexity           | 4.2275     |
| AveragePolicyStd     | 0.31158    |
| AveragePolicyStd[0]  | 0.32401    |
| AveragePolicyStd[1]  | 0.37618    |
| AveragePolicyStd[2]  | 0.31259    |
| AveragePolicyStd[3]  | 0.34753    |
| AveragePolicyStd[4]  | 0.22759    |
| AveragePolicyStd[5]  | 0.28157    |
| AverageReturn        | 758.43     |
| MinReturn            | 13.698     |
| MaxReturn            | 863.93     |
| StdReturn            | 185.32     |
| AverageEpisodeLength | 929.06     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.22     |
| TotalNEpisodes       | 15581      |
| TotalNSamples        | 1.1632e+06 |
| ExplainedVariance    | 0.14892    |
-------------------------------------
[2018-12-22 10:30:35.374034 UTC] Saving snapshot
[2018-12-22 10:30:35.374281 UTC] Starting iteration 234
[2018-12-22 10:30:35.374415 UTC] Start collecting samples
[2018-12-22 10:30:38.462184 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:38.545156 UTC] Performing policy update
[2018-12-22 10:30:38.545904 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:38.637066 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:39.731278 UTC] Performing line search
[2018-12-22 10:30:39.862378 UTC] Updating baseline
[2018-12-22 10:30:41.256994 UTC] Computing logging information
-------------------------------------
| Iteration            | 234        |
| ExpectedImprovement  | 0.01353    |
| ActualImprovement    | 0.013131   |
| ImprovementRatio     | 0.97055    |
| MeanKL               | 0.0072172  |
| Entropy              | 1.4227     |
| Perplexity           | 4.1484     |
| AveragePolicyStd     | 0.31061    |
| AveragePolicyStd[0]  | 0.32127    |
| AveragePolicyStd[1]  | 0.37469    |
| AveragePolicyStd[2]  | 0.3119     |
| AveragePolicyStd[3]  | 0.34794    |
| AveragePolicyStd[4]  | 0.22663    |
| AveragePolicyStd[5]  | 0.28125    |
| AverageReturn        | 762.54     |
| MinReturn            | 13.698     |
| MaxReturn            | 863.93     |
| StdReturn            | 185.06     |
| AverageEpisodeLength | 932.24     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.54     |
| TotalNEpisodes       | 15586      |
| TotalNSamples        | 1.1682e+06 |
| ExplainedVariance    | 0.10504    |
-------------------------------------
[2018-12-22 10:30:41.613877 UTC] Saving snapshot
[2018-12-22 10:30:41.614129 UTC] Starting iteration 235
[2018-12-22 10:30:41.614256 UTC] Start collecting samples
[2018-12-22 10:30:44.882033 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:44.965281 UTC] Performing policy update
[2018-12-22 10:30:44.966020 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:45.059052 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:46.157439 UTC] Performing line search
[2018-12-22 10:30:46.287379 UTC] Updating baseline
[2018-12-22 10:30:47.653346 UTC] Computing logging information
-------------------------------------
| Iteration            | 235        |
| ExpectedImprovement  | 0.01245    |
| ActualImprovement    | 0.012431   |
| ImprovementRatio     | 0.99845    |
| MeanKL               | 0.007703   |
| Entropy              | 1.4178     |
| Perplexity           | 4.1282     |
| AveragePolicyStd     | 0.31043    |
| AveragePolicyStd[0]  | 0.3201     |
| AveragePolicyStd[1]  | 0.37589    |
| AveragePolicyStd[2]  | 0.31084    |
| AveragePolicyStd[3]  | 0.34907    |
| AveragePolicyStd[4]  | 0.22668    |
| AveragePolicyStd[5]  | 0.27998    |
| AverageReturn        | 765.05     |
| MinReturn            | 13.698     |
| MaxReturn            | 871.14     |
| StdReturn            | 187.7      |
| AverageEpisodeLength | 931.18     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.13     |
| TotalNEpisodes       | 15594      |
| TotalNSamples        | 1.1754e+06 |
| ExplainedVariance    | 0.0024129  |
-------------------------------------
[2018-12-22 10:30:48.017059 UTC] Saving snapshot
[2018-12-22 10:30:48.017296 UTC] Starting iteration 236
[2018-12-22 10:30:48.017422 UTC] Start collecting samples
[2018-12-22 10:30:51.054341 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:51.131960 UTC] Performing policy update
[2018-12-22 10:30:51.132548 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:51.222927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:52.263187 UTC] Performing line search
[2018-12-22 10:30:52.388212 UTC] Updating baseline
[2018-12-22 10:30:53.637266 UTC] Computing logging information
-------------------------------------
| Iteration            | 236        |
| ExpectedImprovement  | 0.014594   |
| ActualImprovement    | 0.013928   |
| ImprovementRatio     | 0.95439    |
| MeanKL               | 0.0073532  |
| Entropy              | 1.4201     |
| Perplexity           | 4.1377     |
| AveragePolicyStd     | 0.31055    |
| AveragePolicyStd[0]  | 0.32013    |
| AveragePolicyStd[1]  | 0.37568    |
| AveragePolicyStd[2]  | 0.3089     |
| AveragePolicyStd[3]  | 0.35138    |
| AveragePolicyStd[4]  | 0.22735    |
| AveragePolicyStd[5]  | 0.27984    |
| AverageReturn        | 761.44     |
| MinReturn            | 13.698     |
| MaxReturn            | 871.14     |
| StdReturn            | 193.75     |
| AverageEpisodeLength | 925.01     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.87     |
| TotalNEpisodes       | 15599      |
| TotalNSamples        | 1.1791e+06 |
| ExplainedVariance    | 0.48593    |
-------------------------------------
[2018-12-22 10:30:53.962378 UTC] Saving snapshot
[2018-12-22 10:30:53.962645 UTC] Starting iteration 237
[2018-12-22 10:30:53.962766 UTC] Start collecting samples
[2018-12-22 10:30:56.974051 UTC] Computing input variables for policy optimization
[2018-12-22 10:30:57.050612 UTC] Performing policy update
[2018-12-22 10:30:57.051333 UTC] Computing gradient in Euclidean space
[2018-12-22 10:30:57.140611 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:30:58.197025 UTC] Performing line search
[2018-12-22 10:30:58.321900 UTC] Updating baseline
[2018-12-22 10:30:59.825326 UTC] Computing logging information
-------------------------------------
| Iteration            | 237        |
| ExpectedImprovement  | 0.012132   |
| ActualImprovement    | 0.01178    |
| ImprovementRatio     | 0.97098    |
| MeanKL               | 0.0077289  |
| Entropy              | 1.4094     |
| Perplexity           | 4.0936     |
| AveragePolicyStd     | 0.31007    |
| AveragePolicyStd[0]  | 0.3199     |
| AveragePolicyStd[1]  | 0.37493    |
| AveragePolicyStd[2]  | 0.31015    |
| AveragePolicyStd[3]  | 0.35128    |
| AveragePolicyStd[4]  | 0.2262     |
| AveragePolicyStd[5]  | 0.27797    |
| AverageReturn        | 767.29     |
| MinReturn            | 13.698     |
| MaxReturn            | 871.14     |
| StdReturn            | 184.94     |
| AverageEpisodeLength | 931.12     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.03     |
| TotalNEpisodes       | 15604      |
| TotalNSamples        | 1.1836e+06 |
| ExplainedVariance    | 0.29051    |
-------------------------------------
[2018-12-22 10:31:00.152272 UTC] Saving snapshot
[2018-12-22 10:31:00.152731 UTC] Starting iteration 238
[2018-12-22 10:31:00.152935 UTC] Start collecting samples
[2018-12-22 10:31:03.231494 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:03.310676 UTC] Performing policy update
[2018-12-22 10:31:03.311232 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:03.399481 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:04.451980 UTC] Performing line search
[2018-12-22 10:31:04.577545 UTC] Updating baseline
[2018-12-22 10:31:05.825496 UTC] Computing logging information
------------------------------------
| Iteration            | 238       |
| ExpectedImprovement  | 0.015612  |
| ActualImprovement    | 0.014925  |
| ImprovementRatio     | 0.95599   |
| MeanKL               | 0.0071148 |
| Entropy              | 1.3882    |
| Perplexity           | 4.0076    |
| AveragePolicyStd     | 0.30904   |
| AveragePolicyStd[0]  | 0.31838   |
| AveragePolicyStd[1]  | 0.37442   |
| AveragePolicyStd[2]  | 0.30816   |
| AveragePolicyStd[3]  | 0.35104   |
| AveragePolicyStd[4]  | 0.22506   |
| AveragePolicyStd[5]  | 0.27717   |
| AverageReturn        | 756.57    |
| MinReturn            | 13.698    |
| MaxReturn            | 871.14    |
| StdReturn            | 195.33    |
| AverageEpisodeLength | 915.9     |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 230.71    |
| TotalNEpisodes       | 15613     |
| TotalNSamples        | 1.191e+06 |
| ExplainedVariance    | 0.27569   |
------------------------------------
[2018-12-22 10:31:06.160528 UTC] Saving snapshot
[2018-12-22 10:31:06.160788 UTC] Starting iteration 239
[2018-12-22 10:31:06.160919 UTC] Start collecting samples
[2018-12-22 10:31:09.186536 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:09.263746 UTC] Performing policy update
[2018-12-22 10:31:09.264490 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:09.352287 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:10.410391 UTC] Performing line search
[2018-12-22 10:31:10.535412 UTC] Updating baseline
[2018-12-22 10:31:11.931668 UTC] Computing logging information
-------------------------------------
| Iteration            | 239        |
| ExpectedImprovement  | 0.01447    |
| ActualImprovement    | 0.013931   |
| ImprovementRatio     | 0.96277    |
| MeanKL               | 0.0066984  |
| Entropy              | 1.3557     |
| Perplexity           | 3.8796     |
| AveragePolicyStd     | 0.30733    |
| AveragePolicyStd[0]  | 0.3173     |
| AveragePolicyStd[1]  | 0.37161    |
| AveragePolicyStd[2]  | 0.30455    |
| AveragePolicyStd[3]  | 0.35013    |
| AveragePolicyStd[4]  | 0.22451    |
| AveragePolicyStd[5]  | 0.27586    |
| AverageReturn        | 749.19     |
| MinReturn            | 13.698     |
| MaxReturn            | 871.14     |
| StdReturn            | 208.96     |
| AverageEpisodeLength | 905.44     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248        |
| TotalNEpisodes       | 15618      |
| TotalNSamples        | 1.1944e+06 |
| ExplainedVariance    | 0.20709    |
-------------------------------------
[2018-12-22 10:31:12.260681 UTC] Saving snapshot
[2018-12-22 10:31:12.260941 UTC] Starting iteration 240
[2018-12-22 10:31:12.261060 UTC] Start collecting samples
[2018-12-22 10:31:15.269148 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:15.344583 UTC] Performing policy update
[2018-12-22 10:31:15.345494 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:15.436029 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:16.492734 UTC] Performing line search
[2018-12-22 10:31:16.617327 UTC] Updating baseline
[2018-12-22 10:31:18.023014 UTC] Computing logging information
-------------------------------------
| Iteration            | 240        |
| ExpectedImprovement  | 0.012479   |
| ActualImprovement    | 0.012039   |
| ImprovementRatio     | 0.96478    |
| MeanKL               | 0.0076524  |
| Entropy              | 1.3654     |
| Perplexity           | 3.9174     |
| AveragePolicyStd     | 0.30785    |
| AveragePolicyStd[0]  | 0.3171     |
| AveragePolicyStd[1]  | 0.37302    |
| AveragePolicyStd[2]  | 0.30487    |
| AveragePolicyStd[3]  | 0.35054    |
| AveragePolicyStd[4]  | 0.2244     |
| AveragePolicyStd[5]  | 0.27719    |
| AverageReturn        | 751.86     |
| MinReturn            | 13.698     |
| MaxReturn            | 871.14     |
| StdReturn            | 210.35     |
| AverageEpisodeLength | 907        |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 249.87     |
| TotalNEpisodes       | 15622      |
| TotalNSamples        | 1.1976e+06 |
| ExplainedVariance    | 0.26431    |
-------------------------------------
[2018-12-22 10:31:18.347699 UTC] Saving snapshot
[2018-12-22 10:31:18.355863 UTC] Starting iteration 241
[2018-12-22 10:31:18.356079 UTC] Start collecting samples
[2018-12-22 10:31:21.413522 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:21.491798 UTC] Performing policy update
[2018-12-22 10:31:21.492372 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:21.579872 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:22.620658 UTC] Performing line search
[2018-12-22 10:31:22.745316 UTC] Updating baseline
[2018-12-22 10:31:24.088227 UTC] Computing logging information
------------------------------------
| Iteration            | 241       |
| ExpectedImprovement  | 0.014795  |
| ActualImprovement    | 0.014279  |
| ImprovementRatio     | 0.96514   |
| MeanKL               | 0.0070678 |
| Entropy              | 1.3724    |
| Perplexity           | 3.9448    |
| AveragePolicyStd     | 0.30799   |
| AveragePolicyStd[0]  | 0.31721   |
| AveragePolicyStd[1]  | 0.37009   |
| AveragePolicyStd[2]  | 0.30447   |
| AveragePolicyStd[3]  | 0.35204   |
| AveragePolicyStd[4]  | 0.2272    |
| AveragePolicyStd[5]  | 0.27696   |
| AverageReturn        | 756.81    |
| MinReturn            | 13.698    |
| MaxReturn            | 871.14    |
| StdReturn            | 204.11    |
| AverageEpisodeLength | 909.72    |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 241.75    |
| TotalNEpisodes       | 15630     |
| TotalNSamples        | 1.205e+06 |
| ExplainedVariance    | 0.14233   |
------------------------------------
[2018-12-22 10:31:24.414666 UTC] Saving snapshot
[2018-12-22 10:31:24.414921 UTC] Starting iteration 242
[2018-12-22 10:31:24.415046 UTC] Start collecting samples
[2018-12-22 10:31:27.443219 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:27.520662 UTC] Performing policy update
[2018-12-22 10:31:27.521291 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:27.611087 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:28.664003 UTC] Performing line search
[2018-12-22 10:31:28.788441 UTC] Updating baseline
[2018-12-22 10:31:30.189849 UTC] Computing logging information
-------------------------------------
| Iteration            | 242        |
| ExpectedImprovement  | 0.014252   |
| ActualImprovement    | 0.013917   |
| ImprovementRatio     | 0.97644    |
| MeanKL               | 0.0076192  |
| Entropy              | 1.3585     |
| Perplexity           | 3.8902     |
| AveragePolicyStd     | 0.3071     |
| AveragePolicyStd[0]  | 0.31702    |
| AveragePolicyStd[1]  | 0.36642    |
| AveragePolicyStd[2]  | 0.30179    |
| AveragePolicyStd[3]  | 0.35127    |
| AveragePolicyStd[4]  | 0.22798    |
| AveragePolicyStd[5]  | 0.27813    |
| AverageReturn        | 744.1      |
| MinReturn            | 13.698     |
| MaxReturn            | 878.47     |
| StdReturn            | 224.73     |
| AverageEpisodeLength | 891.93     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 266.02     |
| TotalNEpisodes       | 15637      |
| TotalNSamples        | 1.2102e+06 |
| ExplainedVariance    | 0.29721    |
-------------------------------------
[2018-12-22 10:31:30.519610 UTC] Saving snapshot
[2018-12-22 10:31:30.519860 UTC] Starting iteration 243
[2018-12-22 10:31:30.519995 UTC] Start collecting samples
[2018-12-22 10:31:33.518955 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:33.593951 UTC] Performing policy update
[2018-12-22 10:31:33.594589 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:33.683421 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:34.730168 UTC] Performing line search
[2018-12-22 10:31:34.854696 UTC] Updating baseline
[2018-12-22 10:31:36.245848 UTC] Computing logging information
-------------------------------------
| Iteration            | 243        |
| ExpectedImprovement  | 0.017665   |
| ActualImprovement    | 0.017343   |
| ImprovementRatio     | 0.98181    |
| MeanKL               | 0.0081329  |
| Entropy              | 1.3456     |
| Perplexity           | 3.8403     |
| AveragePolicyStd     | 0.30624    |
| AveragePolicyStd[0]  | 0.31671    |
| AveragePolicyStd[1]  | 0.36558    |
| AveragePolicyStd[2]  | 0.30184    |
| AveragePolicyStd[3]  | 0.34507    |
| AveragePolicyStd[4]  | 0.22847    |
| AveragePolicyStd[5]  | 0.27977    |
| AverageReturn        | 735.44     |
| MinReturn            | 13.698     |
| MaxReturn            | 878.47     |
| StdReturn            | 234.82     |
| AverageEpisodeLength | 881.26     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.71     |
| TotalNEpisodes       | 15641      |
| TotalNSamples        | 1.2131e+06 |
| ExplainedVariance    | 0.22114    |
-------------------------------------
[2018-12-22 10:31:36.571717 UTC] Saving snapshot
[2018-12-22 10:31:36.571986 UTC] Starting iteration 244
[2018-12-22 10:31:36.572105 UTC] Start collecting samples
[2018-12-22 10:31:39.593769 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:39.670025 UTC] Performing policy update
[2018-12-22 10:31:39.670811 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:39.760795 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:40.760013 UTC] Performing line search
[2018-12-22 10:31:40.883855 UTC] Updating baseline
[2018-12-22 10:31:42.400374 UTC] Computing logging information
-------------------------------------
| Iteration            | 244        |
| ExpectedImprovement  | 0.015579   |
| ActualImprovement    | 0.014844   |
| ImprovementRatio     | 0.95279    |
| MeanKL               | 0.0071143  |
| Entropy              | 1.3384     |
| Perplexity           | 3.813      |
| AveragePolicyStd     | 0.30587    |
| AveragePolicyStd[0]  | 0.31515    |
| AveragePolicyStd[1]  | 0.36527    |
| AveragePolicyStd[2]  | 0.30229    |
| AveragePolicyStd[3]  | 0.34453    |
| AveragePolicyStd[4]  | 0.22788    |
| AveragePolicyStd[5]  | 0.28014    |
| AverageReturn        | 737.34     |
| MinReturn            | 13.698     |
| MaxReturn            | 878.47     |
| StdReturn            | 235.62     |
| AverageEpisodeLength | 881.26     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.71     |
| TotalNEpisodes       | 15647      |
| TotalNSamples        | 1.2191e+06 |
| ExplainedVariance    | 0.017443   |
-------------------------------------
[2018-12-22 10:31:42.726477 UTC] Saving snapshot
[2018-12-22 10:31:42.726754 UTC] Starting iteration 245
[2018-12-22 10:31:42.726887 UTC] Start collecting samples
[2018-12-22 10:31:45.835770 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:45.913227 UTC] Performing policy update
[2018-12-22 10:31:45.913945 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:46.002692 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:47.047583 UTC] Performing line search
[2018-12-22 10:31:47.173591 UTC] Updating baseline
[2018-12-22 10:31:48.943964 UTC] Computing logging information
-------------------------------------
| Iteration            | 245        |
| ExpectedImprovement  | 0.013692   |
| ActualImprovement    | 0.013301   |
| ImprovementRatio     | 0.97146    |
| MeanKL               | 0.0074867  |
| Entropy              | 1.3336     |
| Perplexity           | 3.7948     |
| AveragePolicyStd     | 0.3056     |
| AveragePolicyStd[0]  | 0.31461    |
| AveragePolicyStd[1]  | 0.36492    |
| AveragePolicyStd[2]  | 0.30164    |
| AveragePolicyStd[3]  | 0.34535    |
| AveragePolicyStd[4]  | 0.22904    |
| AveragePolicyStd[5]  | 0.27806    |
| AverageReturn        | 739.81     |
| MinReturn            | 13.698     |
| MaxReturn            | 878.69     |
| StdReturn            | 236.55     |
| AverageEpisodeLength | 881.26     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.71     |
| TotalNEpisodes       | 15653      |
| TotalNSamples        | 1.2251e+06 |
| ExplainedVariance    | -0.064876  |
-------------------------------------
[2018-12-22 10:31:49.279742 UTC] Saving snapshot
[2018-12-22 10:31:49.279984 UTC] Starting iteration 246
[2018-12-22 10:31:49.280109 UTC] Start collecting samples
[2018-12-22 10:31:52.250325 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:52.326806 UTC] Performing policy update
[2018-12-22 10:31:52.327765 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:52.415927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:31:53.467854 UTC] Performing line search
[2018-12-22 10:31:53.592592 UTC] Updating baseline
[2018-12-22 10:31:55.472389 UTC] Computing logging information
-------------------------------------
| Iteration            | 246        |
| ExpectedImprovement  | 0.01322    |
| ActualImprovement    | 0.012571   |
| ImprovementRatio     | 0.9509     |
| MeanKL               | 0.0075198  |
| Entropy              | 1.3139     |
| Perplexity           | 3.7207     |
| AveragePolicyStd     | 0.30459    |
| AveragePolicyStd[0]  | 0.31159    |
| AveragePolicyStd[1]  | 0.36427    |
| AveragePolicyStd[2]  | 0.29973    |
| AveragePolicyStd[3]  | 0.34565    |
| AveragePolicyStd[4]  | 0.22915    |
| AveragePolicyStd[5]  | 0.27716    |
| AverageReturn        | 741.43     |
| MinReturn            | 13.698     |
| MaxReturn            | 878.69     |
| StdReturn            | 237.21     |
| AverageEpisodeLength | 881.26     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.71     |
| TotalNEpisodes       | 15656      |
| TotalNSamples        | 1.2281e+06 |
| ExplainedVariance    | 0.021643   |
-------------------------------------
[2018-12-22 10:31:55.801848 UTC] Saving snapshot
[2018-12-22 10:31:55.802093 UTC] Starting iteration 247
[2018-12-22 10:31:55.802213 UTC] Start collecting samples
[2018-12-22 10:31:58.991768 UTC] Computing input variables for policy optimization
[2018-12-22 10:31:59.075484 UTC] Performing policy update
[2018-12-22 10:31:59.076194 UTC] Computing gradient in Euclidean space
[2018-12-22 10:31:59.169409 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:00.288763 UTC] Performing line search
[2018-12-22 10:32:00.421862 UTC] Updating baseline
[2018-12-22 10:32:02.114253 UTC] Computing logging information
-------------------------------------
| Iteration            | 247        |
| ExpectedImprovement  | 0.013665   |
| ActualImprovement    | 0.01307    |
| ImprovementRatio     | 0.95643    |
| MeanKL               | 0.0080115  |
| Entropy              | 1.3047     |
| Perplexity           | 3.6868     |
| AveragePolicyStd     | 0.30408    |
| AveragePolicyStd[0]  | 0.30993    |
| AveragePolicyStd[1]  | 0.36608    |
| AveragePolicyStd[2]  | 0.30036    |
| AveragePolicyStd[3]  | 0.34137    |
| AveragePolicyStd[4]  | 0.22921    |
| AveragePolicyStd[5]  | 0.27752    |
| AverageReturn        | 740.28     |
| MinReturn            | 13.698     |
| MaxReturn            | 878.69     |
| StdReturn            | 235.89     |
| AverageEpisodeLength | 879.06     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 275.78     |
| TotalNEpisodes       | 15663      |
| TotalNSamples        | 1.2341e+06 |
| ExplainedVariance    | 0.16818    |
-------------------------------------
[2018-12-22 10:32:02.479071 UTC] Saving snapshot
[2018-12-22 10:32:02.479318 UTC] Starting iteration 248
[2018-12-22 10:32:02.479440 UTC] Start collecting samples
[2018-12-22 10:32:05.555823 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:05.635484 UTC] Performing policy update
[2018-12-22 10:32:05.636194 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:05.725148 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:06.783301 UTC] Performing line search
[2018-12-22 10:32:06.910394 UTC] Updating baseline
[2018-12-22 10:32:08.254150 UTC] Computing logging information
-------------------------------------
| Iteration            | 248        |
| ExpectedImprovement  | 0.012422   |
| ActualImprovement    | 0.01183    |
| ImprovementRatio     | 0.95233    |
| MeanKL               | 0.0075453  |
| Entropy              | 1.2959     |
| Perplexity           | 3.6544     |
| AveragePolicyStd     | 0.30354    |
| AveragePolicyStd[0]  | 0.30907    |
| AveragePolicyStd[1]  | 0.36506    |
| AveragePolicyStd[2]  | 0.29929    |
| AveragePolicyStd[3]  | 0.34091    |
| AveragePolicyStd[4]  | 0.23054    |
| AveragePolicyStd[5]  | 0.27638    |
| AverageReturn        | 727.44     |
| MinReturn            | 13.698     |
| MaxReturn            | 878.69     |
| StdReturn            | 249.92     |
| AverageEpisodeLength | 862.08     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 292.36     |
| TotalNEpisodes       | 15671      |
| TotalNSamples        | 1.2404e+06 |
| ExplainedVariance    | 0.36144    |
-------------------------------------
[2018-12-22 10:32:08.582988 UTC] Saving snapshot
[2018-12-22 10:32:08.583248 UTC] Starting iteration 249
[2018-12-22 10:32:08.583409 UTC] Start collecting samples
[2018-12-22 10:32:11.606736 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:11.682799 UTC] Performing policy update
[2018-12-22 10:32:11.683628 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:11.774766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:12.813440 UTC] Performing line search
[2018-12-22 10:32:12.939942 UTC] Updating baseline
[2018-12-22 10:32:14.274768 UTC] Computing logging information
-------------------------------------
| Iteration            | 249        |
| ExpectedImprovement  | 0.015782   |
| ActualImprovement    | 0.015204   |
| ImprovementRatio     | 0.96338    |
| MeanKL               | 0.0070209  |
| Entropy              | 1.2983     |
| Perplexity           | 3.6632     |
| AveragePolicyStd     | 0.30375    |
| AveragePolicyStd[0]  | 0.31098    |
| AveragePolicyStd[1]  | 0.3658     |
| AveragePolicyStd[2]  | 0.29911    |
| AveragePolicyStd[3]  | 0.34138    |
| AveragePolicyStd[4]  | 0.2299     |
| AveragePolicyStd[5]  | 0.27533    |
| AverageReturn        | 719.15     |
| MinReturn            | 13.698     |
| MaxReturn            | 878.69     |
| StdReturn            | 255.07     |
| AverageEpisodeLength | 853.43     |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 298.33     |
| TotalNEpisodes       | 15676      |
| TotalNSamples        | 1.2445e+06 |
| ExplainedVariance    | 0.41054    |
-------------------------------------
[2018-12-22 10:32:14.603138 UTC] Saving snapshot
[2018-12-22 10:32:14.603387 UTC] Starting iteration 250
[2018-12-22 10:32:14.603507 UTC] Start collecting samples
[2018-12-22 10:32:17.604624 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:17.680706 UTC] Performing policy update
[2018-12-22 10:32:17.681472 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:17.772467 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:18.710103 UTC] Performing line search
[2018-12-22 10:32:18.822298 UTC] Updating baseline
[2018-12-22 10:32:20.351122 UTC] Computing logging information
-------------------------------------
| Iteration            | 250        |
| ExpectedImprovement  | 0.013092   |
| ActualImprovement    | 0.012917   |
| ImprovementRatio     | 0.98662    |
| MeanKL               | 0.0079664  |
| Entropy              | 1.2845     |
| Perplexity           | 3.613      |
| AveragePolicyStd     | 0.30299    |
| AveragePolicyStd[0]  | 0.31112    |
| AveragePolicyStd[1]  | 0.36467    |
| AveragePolicyStd[2]  | 0.29578    |
| AveragePolicyStd[3]  | 0.3407     |
| AveragePolicyStd[4]  | 0.23049    |
| AveragePolicyStd[5]  | 0.27517    |
| AverageReturn        | 719.65     |
| MinReturn            | 65.922     |
| MaxReturn            | 878.69     |
| StdReturn            | 250.25     |
| AverageEpisodeLength | 852.71     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 292.7      |
| TotalNEpisodes       | 15681      |
| TotalNSamples        | 1.2485e+06 |
| ExplainedVariance    | 0.32644    |
-------------------------------------
[2018-12-22 10:32:20.678755 UTC] Saving snapshot
[2018-12-22 10:32:20.686925 UTC] Starting iteration 251
[2018-12-22 10:32:20.687127 UTC] Start collecting samples
[2018-12-22 10:32:23.738671 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:23.818647 UTC] Performing policy update
[2018-12-22 10:32:23.820363 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:23.909880 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:24.958379 UTC] Performing line search
[2018-12-22 10:32:25.084491 UTC] Updating baseline
[2018-12-22 10:32:26.393460 UTC] Computing logging information
-------------------------------------
| Iteration            | 251        |
| ExpectedImprovement  | 0.014145   |
| ActualImprovement    | 0.013725   |
| ImprovementRatio     | 0.97032    |
| MeanKL               | 0.0072415  |
| Entropy              | 1.2884     |
| Perplexity           | 3.6272     |
| AveragePolicyStd     | 0.30321    |
| AveragePolicyStd[0]  | 0.30961    |
| AveragePolicyStd[1]  | 0.36545    |
| AveragePolicyStd[2]  | 0.29695    |
| AveragePolicyStd[3]  | 0.34117    |
| AveragePolicyStd[4]  | 0.23029    |
| AveragePolicyStd[5]  | 0.27578    |
| AverageReturn        | 714.33     |
| MinReturn            | 65.922     |
| MaxReturn            | 885.27     |
| StdReturn            | 257.71     |
| AverageEpisodeLength | 844.07     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 300.86     |
| TotalNEpisodes       | 15689      |
| TotalNSamples        | 1.2556e+06 |
| ExplainedVariance    | 0.074414   |
-------------------------------------
[2018-12-22 10:32:26.723812 UTC] Saving snapshot
[2018-12-22 10:32:26.724056 UTC] Starting iteration 252
[2018-12-22 10:32:26.724173 UTC] Start collecting samples
[2018-12-22 10:32:29.728525 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:29.805191 UTC] Performing policy update
[2018-12-22 10:32:29.805809 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:29.895259 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:30.947339 UTC] Performing line search
[2018-12-22 10:32:31.071553 UTC] Updating baseline
[2018-12-22 10:32:32.824428 UTC] Computing logging information
-------------------------------------
| Iteration            | 252        |
| ExpectedImprovement  | 0.0143     |
| ActualImprovement    | 0.013615   |
| ImprovementRatio     | 0.95205    |
| MeanKL               | 0.0078769  |
| Entropy              | 1.2789     |
| Perplexity           | 3.5925     |
| AveragePolicyStd     | 0.30282    |
| AveragePolicyStd[0]  | 0.30945    |
| AveragePolicyStd[1]  | 0.36456    |
| AveragePolicyStd[2]  | 0.29637    |
| AveragePolicyStd[3]  | 0.34234    |
| AveragePolicyStd[4]  | 0.22845    |
| AveragePolicyStd[5]  | 0.27576    |
| AverageReturn        | 721.16     |
| MinReturn            | 65.922     |
| MaxReturn            | 886.96     |
| StdReturn            | 251.04     |
| AverageEpisodeLength | 852.67     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 292.8      |
| TotalNEpisodes       | 15693      |
| TotalNSamples        | 1.2596e+06 |
| ExplainedVariance    | 0.07088    |
-------------------------------------
[2018-12-22 10:32:33.151539 UTC] Saving snapshot
[2018-12-22 10:32:33.151802 UTC] Starting iteration 253
[2018-12-22 10:32:33.151940 UTC] Start collecting samples
[2018-12-22 10:32:36.164201 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:36.239734 UTC] Performing policy update
[2018-12-22 10:32:36.240398 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:36.329329 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:37.375664 UTC] Performing line search
[2018-12-22 10:32:37.499304 UTC] Updating baseline
[2018-12-22 10:32:38.935766 UTC] Computing logging information
-------------------------------------
| Iteration            | 253        |
| ExpectedImprovement  | 0.013491   |
| ActualImprovement    | 0.013274   |
| ImprovementRatio     | 0.98392    |
| MeanKL               | 0.0069654  |
| Entropy              | 1.2826     |
| Perplexity           | 3.606      |
| AveragePolicyStd     | 0.30305    |
| AveragePolicyStd[0]  | 0.31087    |
| AveragePolicyStd[1]  | 0.36447    |
| AveragePolicyStd[2]  | 0.29742    |
| AveragePolicyStd[3]  | 0.3418     |
| AveragePolicyStd[4]  | 0.22726    |
| AveragePolicyStd[5]  | 0.2765     |
| AverageReturn        | 709.96     |
| MinReturn            | 40.356     |
| MaxReturn            | 886.96     |
| StdReturn            | 264.82     |
| AverageEpisodeLength | 839.8      |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 307.41     |
| TotalNEpisodes       | 15697      |
| TotalNSamples        | 1.2619e+06 |
| ExplainedVariance    | 0.17104    |
-------------------------------------
[2018-12-22 10:32:39.269663 UTC] Saving snapshot
[2018-12-22 10:32:39.269953 UTC] Starting iteration 254
[2018-12-22 10:32:39.270074 UTC] Start collecting samples
[2018-12-22 10:32:42.364172 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:42.445385 UTC] Performing policy update
[2018-12-22 10:32:42.445978 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:42.534829 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:43.594219 UTC] Performing line search
[2018-12-22 10:32:43.719557 UTC] Updating baseline
[2018-12-22 10:32:44.960536 UTC] Computing logging information
-------------------------------------
| Iteration            | 254        |
| ExpectedImprovement  | 0.015966   |
| ActualImprovement    | 0.014553   |
| ImprovementRatio     | 0.9115     |
| MeanKL               | 0.0072561  |
| Entropy              | 1.2791     |
| Perplexity           | 3.5933     |
| AveragePolicyStd     | 0.30285    |
| AveragePolicyStd[0]  | 0.31135    |
| AveragePolicyStd[1]  | 0.36492    |
| AveragePolicyStd[2]  | 0.29871    |
| AveragePolicyStd[3]  | 0.33939    |
| AveragePolicyStd[4]  | 0.22745    |
| AveragePolicyStd[5]  | 0.27528    |
| AverageReturn        | 706.16     |
| MinReturn            | 40.356     |
| MaxReturn            | 886.96     |
| StdReturn            | 275.32     |
| AverageEpisodeLength | 833.52     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 318.06     |
| TotalNEpisodes       | 15709      |
| TotalNSamples        | 1.2715e+06 |
| ExplainedVariance    | 0.20603    |
-------------------------------------
[2018-12-22 10:32:45.289863 UTC] Saving snapshot
[2018-12-22 10:32:45.290107 UTC] Starting iteration 255
[2018-12-22 10:32:45.290226 UTC] Start collecting samples
[2018-12-22 10:32:48.275986 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:48.352447 UTC] Performing policy update
[2018-12-22 10:32:48.353056 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:48.442227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:49.479409 UTC] Performing line search
[2018-12-22 10:32:49.604799 UTC] Updating baseline
[2018-12-22 10:32:50.843001 UTC] Computing logging information
-------------------------------------
| Iteration            | 255        |
| ExpectedImprovement  | 0.014327   |
| ActualImprovement    | 0.013745   |
| ImprovementRatio     | 0.95932    |
| MeanKL               | 0.007446   |
| Entropy              | 1.2749     |
| Perplexity           | 3.5782     |
| AveragePolicyStd     | 0.30266    |
| AveragePolicyStd[0]  | 0.31192    |
| AveragePolicyStd[1]  | 0.3631     |
| AveragePolicyStd[2]  | 0.29909    |
| AveragePolicyStd[3]  | 0.34081    |
| AveragePolicyStd[4]  | 0.22697    |
| AveragePolicyStd[5]  | 0.2741     |
| AverageReturn        | 712.42     |
| MinReturn            | 40.356     |
| MaxReturn            | 886.96     |
| StdReturn            | 272.96     |
| AverageEpisodeLength | 839.99     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 314.79     |
| TotalNEpisodes       | 15712      |
| TotalNSamples        | 1.2745e+06 |
| ExplainedVariance    | 0.0075336  |
-------------------------------------
[2018-12-22 10:32:51.172444 UTC] Saving snapshot
[2018-12-22 10:32:51.172706 UTC] Starting iteration 256
[2018-12-22 10:32:51.172826 UTC] Start collecting samples
[2018-12-22 10:32:54.171581 UTC] Computing input variables for policy optimization
[2018-12-22 10:32:54.247404 UTC] Performing policy update
[2018-12-22 10:32:54.248127 UTC] Computing gradient in Euclidean space
[2018-12-22 10:32:54.336802 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:32:55.381069 UTC] Performing line search
[2018-12-22 10:32:55.506437 UTC] Updating baseline
[2018-12-22 10:32:56.993060 UTC] Computing logging information
-------------------------------------
| Iteration            | 256        |
| ExpectedImprovement  | 0.016971   |
| ActualImprovement    | 0.015649   |
| ImprovementRatio     | 0.92215    |
| MeanKL               | 0.0071458  |
| Entropy              | 1.2608     |
| Perplexity           | 3.5284     |
| AveragePolicyStd     | 0.30186    |
| AveragePolicyStd[0]  | 0.31112    |
| AveragePolicyStd[1]  | 0.36221    |
| AveragePolicyStd[2]  | 0.29767    |
| AveragePolicyStd[3]  | 0.33864    |
| AveragePolicyStd[4]  | 0.22762    |
| AveragePolicyStd[5]  | 0.27389    |
| AverageReturn        | 723.83     |
| MinReturn            | 40.356     |
| MaxReturn            | 886.96     |
| StdReturn            | 266.82     |
| AverageEpisodeLength | 852.16     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 307.8      |
| TotalNEpisodes       | 15715      |
| TotalNSamples        | 1.2775e+06 |
| ExplainedVariance    | 0.0018549  |
-------------------------------------
[2018-12-22 10:32:57.315868 UTC] Saving snapshot
[2018-12-22 10:32:57.316122 UTC] Starting iteration 257
[2018-12-22 10:32:57.316238 UTC] Start collecting samples
[2018-12-22 10:33:00.376272 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:00.458493 UTC] Performing policy update
[2018-12-22 10:33:00.459186 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:00.549055 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:01.603546 UTC] Performing line search
[2018-12-22 10:33:01.728275 UTC] Updating baseline
[2018-12-22 10:33:03.347753 UTC] Computing logging information
-------------------------------------
| Iteration            | 257        |
| ExpectedImprovement  | 0.013945   |
| ActualImprovement    | 0.013282   |
| ImprovementRatio     | 0.95245    |
| MeanKL               | 0.007249   |
| Entropy              | 1.2562     |
| Perplexity           | 3.512      |
| AveragePolicyStd     | 0.30161    |
| AveragePolicyStd[0]  | 0.30862    |
| AveragePolicyStd[1]  | 0.36327    |
| AveragePolicyStd[2]  | 0.2976     |
| AveragePolicyStd[3]  | 0.33801    |
| AveragePolicyStd[4]  | 0.22804    |
| AveragePolicyStd[5]  | 0.2741     |
| AverageReturn        | 739.03     |
| MinReturn            | 40.356     |
| MaxReturn            | 895.92     |
| StdReturn            | 255.93     |
| AverageEpisodeLength | 868.28     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 293.47     |
| TotalNEpisodes       | 15724      |
| TotalNSamples        | 1.2865e+06 |
| ExplainedVariance    | -0.018302  |
-------------------------------------
[2018-12-22 10:33:03.681373 UTC] Saving snapshot
[2018-12-22 10:33:03.681657 UTC] Starting iteration 258
[2018-12-22 10:33:03.681804 UTC] Start collecting samples
[2018-12-22 10:33:06.673252 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:06.750686 UTC] Performing policy update
[2018-12-22 10:33:06.751254 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:06.841441 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:07.894631 UTC] Performing line search
[2018-12-22 10:33:08.022496 UTC] Updating baseline
[2018-12-22 10:33:09.630388 UTC] Computing logging information
-------------------------------------
| Iteration            | 258        |
| ExpectedImprovement  | 0.011726   |
| ActualImprovement    | 0.011105   |
| ImprovementRatio     | 0.94707    |
| MeanKL               | 0.0075237  |
| Entropy              | 1.2466     |
| Perplexity           | 3.4784     |
| AveragePolicyStd     | 0.30121    |
| AveragePolicyStd[0]  | 0.30742    |
| AveragePolicyStd[1]  | 0.36485    |
| AveragePolicyStd[2]  | 0.29538    |
| AveragePolicyStd[3]  | 0.33921    |
| AveragePolicyStd[4]  | 0.22822    |
| AveragePolicyStd[5]  | 0.27221    |
| AverageReturn        | 739.89     |
| MinReturn            | 40.356     |
| MaxReturn            | 895.92     |
| StdReturn            | 256.32     |
| AverageEpisodeLength | 868.28     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 293.47     |
| TotalNEpisodes       | 15728      |
| TotalNSamples        | 1.2905e+06 |
| ExplainedVariance    | 0.056077   |
-------------------------------------
[2018-12-22 10:33:09.968120 UTC] Saving snapshot
[2018-12-22 10:33:09.968372 UTC] Starting iteration 259
[2018-12-22 10:33:09.968490 UTC] Start collecting samples
[2018-12-22 10:33:12.955946 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:13.032357 UTC] Performing policy update
[2018-12-22 10:33:13.033054 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:13.121677 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:14.173087 UTC] Performing line search
[2018-12-22 10:33:14.297822 UTC] Updating baseline
[2018-12-22 10:33:15.729448 UTC] Computing logging information
-------------------------------------
| Iteration            | 259        |
| ExpectedImprovement  | 0.013727   |
| ActualImprovement    | 0.012798   |
| ImprovementRatio     | 0.93232    |
| MeanKL               | 0.0079245  |
| Entropy              | 1.2622     |
| Perplexity           | 3.5334     |
| AveragePolicyStd     | 0.30204    |
| AveragePolicyStd[0]  | 0.30928    |
| AveragePolicyStd[1]  | 0.36602    |
| AveragePolicyStd[2]  | 0.29631    |
| AveragePolicyStd[3]  | 0.34083    |
| AveragePolicyStd[4]  | 0.22931    |
| AveragePolicyStd[5]  | 0.27053    |
| AverageReturn        | 746.11     |
| MinReturn            | 40.356     |
| MaxReturn            | 895.92     |
| StdReturn            | 252.02     |
| AverageEpisodeLength | 875.17     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 288.34     |
| TotalNEpisodes       | 15730      |
| TotalNSamples        | 1.2925e+06 |
| ExplainedVariance    | 0.054898   |
-------------------------------------
[2018-12-22 10:33:16.061643 UTC] Saving snapshot
[2018-12-22 10:33:16.061918 UTC] Starting iteration 260
[2018-12-22 10:33:16.062038 UTC] Start collecting samples
[2018-12-22 10:33:19.086548 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:19.165985 UTC] Performing policy update
[2018-12-22 10:33:19.166753 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:19.256889 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:20.309074 UTC] Performing line search
[2018-12-22 10:33:20.434481 UTC] Updating baseline
[2018-12-22 10:33:21.615061 UTC] Computing logging information
-------------------------------------
| Iteration            | 260        |
| ExpectedImprovement  | 0.014737   |
| ActualImprovement    | 0.013671   |
| ImprovementRatio     | 0.92763    |
| MeanKL               | 0.0070264  |
| Entropy              | 1.268      |
| Perplexity           | 3.5538     |
| AveragePolicyStd     | 0.30244    |
| AveragePolicyStd[0]  | 0.30951    |
| AveragePolicyStd[1]  | 0.36706    |
| AveragePolicyStd[2]  | 0.29668    |
| AveragePolicyStd[3]  | 0.34212    |
| AveragePolicyStd[4]  | 0.22807    |
| AveragePolicyStd[5]  | 0.27123    |
| AverageReturn        | 761.1      |
| MinReturn            | 40.356     |
| MaxReturn            | 906.99     |
| StdReturn            | 234.88     |
| AverageEpisodeLength | 892.96     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 267.24     |
| TotalNEpisodes       | 15736      |
| TotalNSamples        | 1.2985e+06 |
| ExplainedVariance    | 0.073261   |
-------------------------------------
[2018-12-22 10:33:21.947065 UTC] Saving snapshot
[2018-12-22 10:33:21.955235 UTC] Starting iteration 261
[2018-12-22 10:33:21.955439 UTC] Start collecting samples
[2018-12-22 10:33:25.003336 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:25.080797 UTC] Performing policy update
[2018-12-22 10:33:25.081427 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:25.170810 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:26.229371 UTC] Performing line search
[2018-12-22 10:33:26.354694 UTC] Updating baseline
[2018-12-22 10:33:28.160231 UTC] Computing logging information
-------------------------------------
| Iteration            | 261        |
| ExpectedImprovement  | 0.012051   |
| ActualImprovement    | 0.011674   |
| ImprovementRatio     | 0.96874    |
| MeanKL               | 0.0082998  |
| Entropy              | 1.2675     |
| Perplexity           | 3.552      |
| AveragePolicyStd     | 0.30237    |
| AveragePolicyStd[0]  | 0.30969    |
| AveragePolicyStd[1]  | 0.36626    |
| AveragePolicyStd[2]  | 0.29629    |
| AveragePolicyStd[3]  | 0.34179    |
| AveragePolicyStd[4]  | 0.22804    |
| AveragePolicyStd[5]  | 0.27217    |
| AverageReturn        | 772.7      |
| MinReturn            | 40.356     |
| MaxReturn            | 906.99     |
| StdReturn            | 224.85     |
| AverageEpisodeLength | 903.63     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.67     |
| TotalNEpisodes       | 15743      |
| TotalNSamples        | 1.3055e+06 |
| ExplainedVariance    | 0.016761   |
-------------------------------------
[2018-12-22 10:33:28.492638 UTC] Saving snapshot
[2018-12-22 10:33:28.492884 UTC] Starting iteration 262
[2018-12-22 10:33:28.493003 UTC] Start collecting samples
[2018-12-22 10:33:31.479182 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:31.554676 UTC] Performing policy update
[2018-12-22 10:33:31.555246 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:31.644083 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:32.694912 UTC] Performing line search
[2018-12-22 10:33:32.819683 UTC] Updating baseline
[2018-12-22 10:33:34.185376 UTC] Computing logging information
-------------------------------------
| Iteration            | 262        |
| ExpectedImprovement  | 0.012908   |
| ActualImprovement    | 0.012331   |
| ImprovementRatio     | 0.95531    |
| MeanKL               | 0.0072757  |
| Entropy              | 1.2704     |
| Perplexity           | 3.5623     |
| AveragePolicyStd     | 0.30256    |
| AveragePolicyStd[0]  | 0.31081    |
| AveragePolicyStd[1]  | 0.36722    |
| AveragePolicyStd[2]  | 0.29744    |
| AveragePolicyStd[3]  | 0.34128    |
| AveragePolicyStd[4]  | 0.22843    |
| AveragePolicyStd[5]  | 0.27016    |
| AverageReturn        | 773.36     |
| MinReturn            | 40.356     |
| MaxReturn            | 906.99     |
| StdReturn            | 225.12     |
| AverageEpisodeLength | 903.63     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.67     |
| TotalNEpisodes       | 15745      |
| TotalNSamples        | 1.3075e+06 |
| ExplainedVariance    | -0.012252  |
-------------------------------------
[2018-12-22 10:33:34.513717 UTC] Saving snapshot
[2018-12-22 10:33:34.513993 UTC] Starting iteration 263
[2018-12-22 10:33:34.514115 UTC] Start collecting samples
[2018-12-22 10:33:37.532728 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:37.610155 UTC] Performing policy update
[2018-12-22 10:33:37.610878 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:37.702112 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:38.763043 UTC] Performing line search
[2018-12-22 10:33:38.889966 UTC] Updating baseline
[2018-12-22 10:33:40.070068 UTC] Computing logging information
-------------------------------------
| Iteration            | 263        |
| ExpectedImprovement  | 0.013881   |
| ActualImprovement    | 0.013504   |
| ImprovementRatio     | 0.97281    |
| MeanKL               | 0.0077874  |
| Entropy              | 1.2453     |
| Perplexity           | 3.4741     |
| AveragePolicyStd     | 0.30133    |
| AveragePolicyStd[0]  | 0.31081    |
| AveragePolicyStd[1]  | 0.36775    |
| AveragePolicyStd[2]  | 0.29677    |
| AveragePolicyStd[3]  | 0.33708    |
| AveragePolicyStd[4]  | 0.22737    |
| AveragePolicyStd[5]  | 0.26821    |
| AverageReturn        | 774.8      |
| MinReturn            | 40.356     |
| MaxReturn            | 907.12     |
| StdReturn            | 225.73     |
| AverageEpisodeLength | 903.63     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.67     |
| TotalNEpisodes       | 15751      |
| TotalNSamples        | 1.3135e+06 |
| ExplainedVariance    | 0.023405   |
-------------------------------------
[2018-12-22 10:33:40.403790 UTC] Saving snapshot
[2018-12-22 10:33:40.404034 UTC] Starting iteration 264
[2018-12-22 10:33:40.404151 UTC] Start collecting samples
[2018-12-22 10:33:43.441764 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:43.521933 UTC] Performing policy update
[2018-12-22 10:33:43.522636 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:43.611233 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:44.656284 UTC] Performing line search
[2018-12-22 10:33:44.780342 UTC] Updating baseline
[2018-12-22 10:33:46.514466 UTC] Computing logging information
-------------------------------------
| Iteration            | 264        |
| ExpectedImprovement  | 0.015142   |
| ActualImprovement    | 0.015046   |
| ImprovementRatio     | 0.99365    |
| MeanKL               | 0.0075889  |
| Entropy              | 1.2554     |
| Perplexity           | 3.5092     |
| AveragePolicyStd     | 0.30192    |
| AveragePolicyStd[0]  | 0.3124     |
| AveragePolicyStd[1]  | 0.3675     |
| AveragePolicyStd[2]  | 0.29644    |
| AveragePolicyStd[3]  | 0.33906    |
| AveragePolicyStd[4]  | 0.22571    |
| AveragePolicyStd[5]  | 0.27043    |
| AverageReturn        | 777.4      |
| MinReturn            | 40.356     |
| MaxReturn            | 907.12     |
| StdReturn            | 226.65     |
| AverageEpisodeLength | 903.63     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.67     |
| TotalNEpisodes       | 15758      |
| TotalNSamples        | 1.3205e+06 |
| ExplainedVariance    | -0.022715  |
-------------------------------------
[2018-12-22 10:33:46.842228 UTC] Saving snapshot
[2018-12-22 10:33:46.842478 UTC] Starting iteration 265
[2018-12-22 10:33:46.842612 UTC] Start collecting samples
[2018-12-22 10:33:49.812172 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:49.889569 UTC] Performing policy update
[2018-12-22 10:33:49.890205 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:49.980704 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:51.036017 UTC] Performing line search
[2018-12-22 10:33:51.162804 UTC] Updating baseline
[2018-12-22 10:33:52.683184 UTC] Computing logging information
-------------------------------------
| Iteration            | 265        |
| ExpectedImprovement  | 0.015735   |
| ActualImprovement    | 0.014939   |
| ImprovementRatio     | 0.94943    |
| MeanKL               | 0.007037   |
| Entropy              | 1.2574     |
| Perplexity           | 3.5163     |
| AveragePolicyStd     | 0.302      |
| AveragePolicyStd[0]  | 0.31149    |
| AveragePolicyStd[1]  | 0.36937    |
| AveragePolicyStd[2]  | 0.29577    |
| AveragePolicyStd[3]  | 0.33816    |
| AveragePolicyStd[4]  | 0.22677    |
| AveragePolicyStd[5]  | 0.27045    |
| AverageReturn        | 781.05     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.24     |
| StdReturn            | 226.89     |
| AverageEpisodeLength | 906.28     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.28     |
| TotalNEpisodes       | 15761      |
| TotalNSamples        | 1.3235e+06 |
| ExplainedVariance    | -0.01481   |
-------------------------------------
[2018-12-22 10:33:53.016353 UTC] Saving snapshot
[2018-12-22 10:33:53.016621 UTC] Starting iteration 266
[2018-12-22 10:33:53.016740 UTC] Start collecting samples
[2018-12-22 10:33:56.015754 UTC] Computing input variables for policy optimization
[2018-12-22 10:33:56.092571 UTC] Performing policy update
[2018-12-22 10:33:56.093389 UTC] Computing gradient in Euclidean space
[2018-12-22 10:33:56.182333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:33:57.228580 UTC] Performing line search
[2018-12-22 10:33:57.352623 UTC] Updating baseline
[2018-12-22 10:33:59.050882 UTC] Computing logging information
-------------------------------------
| Iteration            | 266        |
| ExpectedImprovement  | 0.015429   |
| ActualImprovement    | 0.014523   |
| ImprovementRatio     | 0.94131    |
| MeanKL               | 0.0070679  |
| Entropy              | 1.2535     |
| Perplexity           | 3.5025     |
| AveragePolicyStd     | 0.30187    |
| AveragePolicyStd[0]  | 0.31285    |
| AveragePolicyStd[1]  | 0.369      |
| AveragePolicyStd[2]  | 0.29277    |
| AveragePolicyStd[3]  | 0.33999    |
| AveragePolicyStd[4]  | 0.22636    |
| AveragePolicyStd[5]  | 0.27028    |
| AverageReturn        | 788.48     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.24     |
| StdReturn            | 219.99     |
| AverageEpisodeLength | 913.71     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.95     |
| TotalNEpisodes       | 15764      |
| TotalNSamples        | 1.3265e+06 |
| ExplainedVariance    | 0.022498   |
-------------------------------------
[2018-12-22 10:33:59.383401 UTC] Saving snapshot
[2018-12-22 10:33:59.383668 UTC] Starting iteration 267
[2018-12-22 10:33:59.383794 UTC] Start collecting samples
[2018-12-22 10:34:02.457172 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:02.537698 UTC] Performing policy update
[2018-12-22 10:34:02.538469 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:02.626819 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:03.671240 UTC] Performing line search
[2018-12-22 10:34:03.800009 UTC] Updating baseline
[2018-12-22 10:34:05.149049 UTC] Computing logging information
-------------------------------------
| Iteration            | 267        |
| ExpectedImprovement  | 0.01493    |
| ActualImprovement    | 0.013874   |
| ImprovementRatio     | 0.92925    |
| MeanKL               | 0.0072508  |
| Entropy              | 1.2384     |
| Perplexity           | 3.4501     |
| AveragePolicyStd     | 0.30103    |
| AveragePolicyStd[0]  | 0.31258    |
| AveragePolicyStd[1]  | 0.36766    |
| AveragePolicyStd[2]  | 0.29157    |
| AveragePolicyStd[3]  | 0.33688    |
| AveragePolicyStd[4]  | 0.22569    |
| AveragePolicyStd[5]  | 0.27181    |
| AverageReturn        | 806.57     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.35     |
| StdReturn            | 200.59     |
| AverageEpisodeLength | 930.59     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.2      |
| TotalNEpisodes       | 15774      |
| TotalNSamples        | 1.3364e+06 |
| ExplainedVariance    | 0.015885   |
-------------------------------------
[2018-12-22 10:34:05.481165 UTC] Saving snapshot
[2018-12-22 10:34:05.481486 UTC] Starting iteration 268
[2018-12-22 10:34:05.481629 UTC] Start collecting samples
[2018-12-22 10:34:08.479072 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:08.555913 UTC] Performing policy update
[2018-12-22 10:34:08.556580 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:08.644450 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:09.682620 UTC] Performing line search
[2018-12-22 10:34:09.810462 UTC] Updating baseline
[2018-12-22 10:34:11.587268 UTC] Computing logging information
-------------------------------------
| Iteration            | 268        |
| ExpectedImprovement  | 0.015929   |
| ActualImprovement    | 0.015387   |
| ImprovementRatio     | 0.96601    |
| MeanKL               | 0.0071995  |
| Entropy              | 1.2431     |
| Perplexity           | 3.4662     |
| AveragePolicyStd     | 0.30111    |
| AveragePolicyStd[0]  | 0.31265    |
| AveragePolicyStd[1]  | 0.3679     |
| AveragePolicyStd[2]  | 0.291      |
| AveragePolicyStd[3]  | 0.33409    |
| AveragePolicyStd[4]  | 0.22729    |
| AveragePolicyStd[5]  | 0.27371    |
| AverageReturn        | 819.61     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.35     |
| StdReturn            | 184.21     |
| AverageEpisodeLength | 944.26     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.26     |
| TotalNEpisodes       | 15778      |
| TotalNSamples        | 1.3399e+06 |
| ExplainedVariance    | 0.16212    |
-------------------------------------
[2018-12-22 10:34:11.925387 UTC] Saving snapshot
[2018-12-22 10:34:11.925649 UTC] Starting iteration 269
[2018-12-22 10:34:11.925838 UTC] Start collecting samples
[2018-12-22 10:34:14.917817 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:14.994100 UTC] Performing policy update
[2018-12-22 10:34:14.994686 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:15.083193 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:16.134139 UTC] Performing line search
[2018-12-22 10:34:16.259228 UTC] Updating baseline
[2018-12-22 10:34:17.568808 UTC] Computing logging information
-------------------------------------
| Iteration            | 269        |
| ExpectedImprovement  | 0.017274   |
| ActualImprovement    | 0.016423   |
| ImprovementRatio     | 0.95075    |
| MeanKL               | 0.0071274  |
| Entropy              | 1.2404     |
| Perplexity           | 3.457      |
| AveragePolicyStd     | 0.301      |
| AveragePolicyStd[0]  | 0.31238    |
| AveragePolicyStd[1]  | 0.36846    |
| AveragePolicyStd[2]  | 0.28823    |
| AveragePolicyStd[3]  | 0.33591    |
| AveragePolicyStd[4]  | 0.22834    |
| AveragePolicyStd[5]  | 0.27268    |
| AverageReturn        | 810.59     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.35     |
| StdReturn            | 198.17     |
| AverageEpisodeLength | 932.75     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 219.52     |
| TotalNEpisodes       | 15781      |
| TotalNSamples        | 1.3418e+06 |
| ExplainedVariance    | 0.47117    |
-------------------------------------
[2018-12-22 10:34:17.900544 UTC] Saving snapshot
[2018-12-22 10:34:17.900814 UTC] Starting iteration 270
[2018-12-22 10:34:17.900946 UTC] Start collecting samples
[2018-12-22 10:34:20.976002 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:21.056391 UTC] Performing policy update
[2018-12-22 10:34:21.056964 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:21.147472 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:22.200679 UTC] Performing line search
[2018-12-22 10:34:22.326764 UTC] Updating baseline
[2018-12-22 10:34:23.669842 UTC] Computing logging information
-------------------------------------
| Iteration            | 270        |
| ExpectedImprovement  | 0.016037   |
| ActualImprovement    | 0.01538    |
| ImprovementRatio     | 0.95901    |
| MeanKL               | 0.0072006  |
| Entropy              | 1.221      |
| Perplexity           | 3.3905     |
| AveragePolicyStd     | 0.30005    |
| AveragePolicyStd[0]  | 0.31119    |
| AveragePolicyStd[1]  | 0.36819    |
| AveragePolicyStd[2]  | 0.28684    |
| AveragePolicyStd[3]  | 0.33435    |
| AveragePolicyStd[4]  | 0.22755    |
| AveragePolicyStd[5]  | 0.27217    |
| AverageReturn        | 818.28     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.35     |
| StdReturn            | 186.92     |
| AverageEpisodeLength | 941.39     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.48     |
| TotalNEpisodes       | 15790      |
| TotalNSamples        | 1.3508e+06 |
| ExplainedVariance    | 0.031814   |
-------------------------------------
[2018-12-22 10:34:24.007888 UTC] Saving snapshot
[2018-12-22 10:34:24.016168 UTC] Starting iteration 271
[2018-12-22 10:34:24.016378 UTC] Start collecting samples
[2018-12-22 10:34:27.035941 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:27.113033 UTC] Performing policy update
[2018-12-22 10:34:27.113748 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:27.203674 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:28.264283 UTC] Performing line search
[2018-12-22 10:34:28.389869 UTC] Updating baseline
[2018-12-22 10:34:29.650219 UTC] Computing logging information
-------------------------------------
| Iteration            | 271        |
| ExpectedImprovement  | 0.013452   |
| ActualImprovement    | 0.012547   |
| ImprovementRatio     | 0.93273    |
| MeanKL               | 0.0077466  |
| Entropy              | 1.2142     |
| Perplexity           | 3.3675     |
| AveragePolicyStd     | 0.29965    |
| AveragePolicyStd[0]  | 0.31077    |
| AveragePolicyStd[1]  | 0.36642    |
| AveragePolicyStd[2]  | 0.28766    |
| AveragePolicyStd[3]  | 0.33377    |
| AveragePolicyStd[4]  | 0.22733    |
| AveragePolicyStd[5]  | 0.27193    |
| AverageReturn        | 820.75     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.35     |
| StdReturn            | 187.19     |
| AverageEpisodeLength | 941.39     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.48     |
| TotalNEpisodes       | 15794      |
| TotalNSamples        | 1.3548e+06 |
| ExplainedVariance    | 0.011497   |
-------------------------------------
[2018-12-22 10:34:29.987706 UTC] Saving snapshot
[2018-12-22 10:34:29.987972 UTC] Starting iteration 272
[2018-12-22 10:34:29.988088 UTC] Start collecting samples
[2018-12-22 10:34:32.967356 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:33.043075 UTC] Performing policy update
[2018-12-22 10:34:33.044000 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:33.133850 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:34.191959 UTC] Performing line search
[2018-12-22 10:34:34.316763 UTC] Updating baseline
[2018-12-22 10:34:35.545205 UTC] Computing logging information
-------------------------------------
| Iteration            | 272        |
| ExpectedImprovement  | 0.017933   |
| ActualImprovement    | 0.016176   |
| ImprovementRatio     | 0.90202    |
| MeanKL               | 0.0070594  |
| Entropy              | 1.2297     |
| Perplexity           | 3.4202     |
| AveragePolicyStd     | 0.30041    |
| AveragePolicyStd[0]  | 0.31227    |
| AveragePolicyStd[1]  | 0.3652     |
| AveragePolicyStd[2]  | 0.28669    |
| AveragePolicyStd[3]  | 0.33705    |
| AveragePolicyStd[4]  | 0.22777    |
| AveragePolicyStd[5]  | 0.27352    |
| AverageReturn        | 828.61     |
| MinReturn            | 40.356     |
| MaxReturn            | 927.35     |
| StdReturn            | 175.45     |
| AverageEpisodeLength | 948.89     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.38     |
| TotalNEpisodes       | 15796      |
| TotalNSamples        | 1.3568e+06 |
| ExplainedVariance    | 0.078528   |
-------------------------------------
[2018-12-22 10:34:35.878305 UTC] Saving snapshot
[2018-12-22 10:34:35.878589 UTC] Starting iteration 273
[2018-12-22 10:34:35.878713 UTC] Start collecting samples
[2018-12-22 10:34:38.911424 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:38.990511 UTC] Performing policy update
[2018-12-22 10:34:38.991154 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:39.080331 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:40.119869 UTC] Performing line search
[2018-12-22 10:34:40.244025 UTC] Updating baseline
[2018-12-22 10:34:41.496813 UTC] Computing logging information
-------------------------------------
| Iteration            | 273        |
| ExpectedImprovement  | 0.014611   |
| ActualImprovement    | 0.014266   |
| ImprovementRatio     | 0.97641    |
| MeanKL               | 0.0070768  |
| Entropy              | 1.2195     |
| Perplexity           | 3.3856     |
| AveragePolicyStd     | 0.3        |
| AveragePolicyStd[0]  | 0.31291    |
| AveragePolicyStd[1]  | 0.36645    |
| AveragePolicyStd[2]  | 0.28582    |
| AveragePolicyStd[3]  | 0.33598    |
| AveragePolicyStd[4]  | 0.22697    |
| AveragePolicyStd[5]  | 0.27189    |
| AverageReturn        | 855.36     |
| MinReturn            | 90.899     |
| MaxReturn            | 927.35     |
| StdReturn            | 122.09     |
| AverageEpisodeLength | 974        |
| MinEpisodeLength     | 100        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.34     |
| TotalNEpisodes       | 15805      |
| TotalNSamples        | 1.3658e+06 |
| ExplainedVariance    | 0.013408   |
-------------------------------------
[2018-12-22 10:34:41.832265 UTC] Saving snapshot
[2018-12-22 10:34:41.832497 UTC] Starting iteration 274
[2018-12-22 10:34:41.832633 UTC] Start collecting samples
[2018-12-22 10:34:44.864465 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:44.943470 UTC] Performing policy update
[2018-12-22 10:34:44.944051 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:45.032993 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:46.082023 UTC] Performing line search
[2018-12-22 10:34:46.206463 UTC] Updating baseline
[2018-12-22 10:34:47.461775 UTC] Computing logging information
-------------------------------------
| Iteration            | 274        |
| ExpectedImprovement  | 0.017707   |
| ActualImprovement    | 0.015495   |
| ImprovementRatio     | 0.87505    |
| MeanKL               | 0.006968   |
| Entropy              | 1.2173     |
| Perplexity           | 3.3781     |
| AveragePolicyStd     | 0.29991    |
| AveragePolicyStd[0]  | 0.31289    |
| AveragePolicyStd[1]  | 0.36739    |
| AveragePolicyStd[2]  | 0.28662    |
| AveragePolicyStd[3]  | 0.33486    |
| AveragePolicyStd[4]  | 0.22709    |
| AveragePolicyStd[5]  | 0.27062    |
| AverageReturn        | 851.14     |
| MinReturn            | 14.706     |
| MaxReturn            | 932.26     |
| StdReturn            | 135.83     |
| AverageEpisodeLength | 967.77     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.51     |
| TotalNEpisodes       | 15812      |
| TotalNSamples        | 1.3712e+06 |
| ExplainedVariance    | 0.10411    |
-------------------------------------
[2018-12-22 10:34:47.795112 UTC] Saving snapshot
[2018-12-22 10:34:47.795375 UTC] Starting iteration 275
[2018-12-22 10:34:47.795494 UTC] Start collecting samples
[2018-12-22 10:34:50.757126 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:50.832405 UTC] Performing policy update
[2018-12-22 10:34:50.833006 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:50.920899 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:51.969882 UTC] Performing line search
[2018-12-22 10:34:52.093596 UTC] Updating baseline
[2018-12-22 10:34:53.449877 UTC] Computing logging information
-------------------------------------
| Iteration            | 275        |
| ExpectedImprovement  | 0.01658    |
| ActualImprovement    | 0.016521   |
| ImprovementRatio     | 0.99641    |
| MeanKL               | 0.0073104  |
| Entropy              | 1.2123     |
| Perplexity           | 3.3611     |
| AveragePolicyStd     | 0.29955    |
| AveragePolicyStd[0]  | 0.31092    |
| AveragePolicyStd[1]  | 0.36674    |
| AveragePolicyStd[2]  | 0.28538    |
| AveragePolicyStd[3]  | 0.33441    |
| AveragePolicyStd[4]  | 0.22818    |
| AveragePolicyStd[5]  | 0.27169    |
| AverageReturn        | 844.31     |
| MinReturn            | 14.706     |
| MaxReturn            | 932.26     |
| StdReturn            | 153.69     |
| AverageEpisodeLength | 959.14     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.05     |
| TotalNEpisodes       | 15814      |
| TotalNSamples        | 1.3724e+06 |
| ExplainedVariance    | 0.33456    |
-------------------------------------
[2018-12-22 10:34:53.784573 UTC] Saving snapshot
[2018-12-22 10:34:53.784976 UTC] Starting iteration 276
[2018-12-22 10:34:53.785242 UTC] Start collecting samples
[2018-12-22 10:34:56.825548 UTC] Computing input variables for policy optimization
[2018-12-22 10:34:56.903810 UTC] Performing policy update
[2018-12-22 10:34:56.904682 UTC] Computing gradient in Euclidean space
[2018-12-22 10:34:56.994781 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:34:58.053182 UTC] Performing line search
[2018-12-22 10:34:58.178478 UTC] Updating baseline
[2018-12-22 10:34:59.886504 UTC] Computing logging information
-------------------------------------
| Iteration            | 276        |
| ExpectedImprovement  | 0.018615   |
| ActualImprovement    | 0.017225   |
| ImprovementRatio     | 0.92532    |
| MeanKL               | 0.0068146  |
| Entropy              | 1.2192     |
| Perplexity           | 3.3846     |
| AveragePolicyStd     | 0.29988    |
| AveragePolicyStd[0]  | 0.31076    |
| AveragePolicyStd[1]  | 0.36773    |
| AveragePolicyStd[2]  | 0.28651    |
| AveragePolicyStd[3]  | 0.3339     |
| AveragePolicyStd[4]  | 0.22884    |
| AveragePolicyStd[5]  | 0.27155    |
| AverageReturn        | 837.31     |
| MinReturn            | 14.706     |
| MaxReturn            | 932.26     |
| StdReturn            | 174.77     |
| AverageEpisodeLength | 949.48     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.42     |
| TotalNEpisodes       | 15821      |
| TotalNSamples        | 1.3784e+06 |
| ExplainedVariance    | -0.017884  |
-------------------------------------
[2018-12-22 10:35:00.220056 UTC] Saving snapshot
[2018-12-22 10:35:00.220289 UTC] Starting iteration 277
[2018-12-22 10:35:00.220421 UTC] Start collecting samples
[2018-12-22 10:35:03.260725 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:03.340549 UTC] Performing policy update
[2018-12-22 10:35:03.341210 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:03.431804 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:04.492092 UTC] Performing line search
[2018-12-22 10:35:04.617030 UTC] Updating baseline
[2018-12-22 10:35:05.984625 UTC] Computing logging information
-------------------------------------
| Iteration            | 277        |
| ExpectedImprovement  | 0.015568   |
| ActualImprovement    | 0.017043   |
| ImprovementRatio     | 1.0947     |
| MeanKL               | 0.0070136  |
| Entropy              | 1.2163     |
| Perplexity           | 3.3746     |
| AveragePolicyStd     | 0.29965    |
| AveragePolicyStd[0]  | 0.31006    |
| AveragePolicyStd[1]  | 0.36608    |
| AveragePolicyStd[2]  | 0.28662    |
| AveragePolicyStd[3]  | 0.33414    |
| AveragePolicyStd[4]  | 0.22955    |
| AveragePolicyStd[5]  | 0.27144    |
| AverageReturn        | 840.95     |
| MinReturn            | 14.706     |
| MaxReturn            | 942.57     |
| StdReturn            | 176.02     |
| AverageEpisodeLength | 949.48     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.42     |
| TotalNEpisodes       | 15828      |
| TotalNSamples        | 1.3854e+06 |
| ExplainedVariance    | -0.021269  |
-------------------------------------
[2018-12-22 10:35:06.318789 UTC] Saving snapshot
[2018-12-22 10:35:06.319111 UTC] Starting iteration 278
[2018-12-22 10:35:06.319261 UTC] Start collecting samples
[2018-12-22 10:35:09.296096 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:09.373349 UTC] Performing policy update
[2018-12-22 10:35:09.373997 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:09.461806 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:10.506865 UTC] Performing line search
[2018-12-22 10:35:10.631123 UTC] Updating baseline
[2018-12-22 10:35:12.575899 UTC] Computing logging information
-------------------------------------
| Iteration            | 278        |
| ExpectedImprovement  | 0.015063   |
| ActualImprovement    | 0.013754   |
| ImprovementRatio     | 0.91306    |
| MeanKL               | 0.0072083  |
| Entropy              | 1.215      |
| Perplexity           | 3.3703     |
| AveragePolicyStd     | 0.29961    |
| AveragePolicyStd[0]  | 0.30852    |
| AveragePolicyStd[1]  | 0.36766    |
| AveragePolicyStd[2]  | 0.28692    |
| AveragePolicyStd[3]  | 0.33369    |
| AveragePolicyStd[4]  | 0.22975    |
| AveragePolicyStd[5]  | 0.27112    |
| AverageReturn        | 842.88     |
| MinReturn            | 14.706     |
| MaxReturn            | 949.66     |
| StdReturn            | 176.61     |
| AverageEpisodeLength | 949.48     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.42     |
| TotalNEpisodes       | 15831      |
| TotalNSamples        | 1.3884e+06 |
| ExplainedVariance    | 0.017856   |
-------------------------------------
[2018-12-22 10:35:12.905584 UTC] Saving snapshot
[2018-12-22 10:35:12.905863 UTC] Starting iteration 279
[2018-12-22 10:35:12.905988 UTC] Start collecting samples
[2018-12-22 10:35:15.927077 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:16.006479 UTC] Performing policy update
[2018-12-22 10:35:16.007113 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:16.095772 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:17.133927 UTC] Performing line search
[2018-12-22 10:35:17.258808 UTC] Updating baseline
[2018-12-22 10:35:18.854580 UTC] Computing logging information
-------------------------------------
| Iteration            | 279        |
| ExpectedImprovement  | 0.014308   |
| ActualImprovement    | 0.013496   |
| ImprovementRatio     | 0.94324    |
| MeanKL               | 0.0078323  |
| Entropy              | 1.2201     |
| Perplexity           | 3.3874     |
| AveragePolicyStd     | 0.29982    |
| AveragePolicyStd[0]  | 0.30833    |
| AveragePolicyStd[1]  | 0.36845    |
| AveragePolicyStd[2]  | 0.2871     |
| AveragePolicyStd[3]  | 0.33294    |
| AveragePolicyStd[4]  | 0.23054    |
| AveragePolicyStd[5]  | 0.27159    |
| AverageReturn        | 846.57     |
| MinReturn            | 14.706     |
| MaxReturn            | 950.82     |
| StdReturn            | 176.96     |
| AverageEpisodeLength | 949.48     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.42     |
| TotalNEpisodes       | 15837      |
| TotalNSamples        | 1.3944e+06 |
| ExplainedVariance    | 0.020521   |
-------------------------------------
[2018-12-22 10:35:19.190491 UTC] Saving snapshot
[2018-12-22 10:35:19.190775 UTC] Starting iteration 280
[2018-12-22 10:35:19.190903 UTC] Start collecting samples
[2018-12-22 10:35:22.210002 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:22.289947 UTC] Performing policy update
[2018-12-22 10:35:22.290598 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:22.378712 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:23.419834 UTC] Performing line search
[2018-12-22 10:35:23.545848 UTC] Updating baseline
[2018-12-22 10:35:24.983534 UTC] Computing logging information
-------------------------------------
| Iteration            | 280        |
| ExpectedImprovement  | 0.01727    |
| ActualImprovement    | 0.016611   |
| ImprovementRatio     | 0.96181    |
| MeanKL               | 0.0068253  |
| Entropy              | 1.1988     |
| Perplexity           | 3.316      |
| AveragePolicyStd     | 0.29864    |
| AveragePolicyStd[0]  | 0.30689    |
| AveragePolicyStd[1]  | 0.36344    |
| AveragePolicyStd[2]  | 0.28569    |
| AveragePolicyStd[3]  | 0.33402    |
| AveragePolicyStd[4]  | 0.23046    |
| AveragePolicyStd[5]  | 0.27135    |
| AverageReturn        | 848.93     |
| MinReturn            | 14.706     |
| MaxReturn            | 950.82     |
| StdReturn            | 177.62     |
| AverageEpisodeLength | 949.48     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.42     |
| TotalNEpisodes       | 15843      |
| TotalNSamples        | 1.4004e+06 |
| ExplainedVariance    | 0.053331   |
-------------------------------------
[2018-12-22 10:35:25.313367 UTC] Saving snapshot
[2018-12-22 10:35:25.321365 UTC] Starting iteration 281
[2018-12-22 10:35:25.321546 UTC] Start collecting samples
[2018-12-22 10:35:28.316273 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:28.392467 UTC] Performing policy update
[2018-12-22 10:35:28.393077 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:28.482858 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:29.526151 UTC] Performing line search
[2018-12-22 10:35:29.650790 UTC] Updating baseline
[2018-12-22 10:35:31.068636 UTC] Computing logging information
-------------------------------------
| Iteration            | 281        |
| ExpectedImprovement  | 0.014408   |
| ActualImprovement    | 0.013723   |
| ImprovementRatio     | 0.95251    |
| MeanKL               | 0.0070822  |
| Entropy              | 1.2054     |
| Perplexity           | 3.3381     |
| AveragePolicyStd     | 0.29892    |
| AveragePolicyStd[0]  | 0.30802    |
| AveragePolicyStd[1]  | 0.36221    |
| AveragePolicyStd[2]  | 0.28558    |
| AveragePolicyStd[3]  | 0.3345     |
| AveragePolicyStd[4]  | 0.23076    |
| AveragePolicyStd[5]  | 0.27242    |
| AverageReturn        | 850.15     |
| MinReturn            | 14.706     |
| MaxReturn            | 950.82     |
| StdReturn            | 177.98     |
| AverageEpisodeLength | 949.48     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.42     |
| TotalNEpisodes       | 15846      |
| TotalNSamples        | 1.4034e+06 |
| ExplainedVariance    | -0.02146   |
-------------------------------------
[2018-12-22 10:35:31.396186 UTC] Saving snapshot
[2018-12-22 10:35:31.396438 UTC] Starting iteration 282
[2018-12-22 10:35:31.396573 UTC] Start collecting samples
[2018-12-22 10:35:34.402009 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:34.480447 UTC] Performing policy update
[2018-12-22 10:35:34.481052 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:34.569789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:35.623035 UTC] Performing line search
[2018-12-22 10:35:35.748472 UTC] Updating baseline
[2018-12-22 10:35:37.280793 UTC] Computing logging information
-------------------------------------
| Iteration            | 282        |
| ExpectedImprovement  | 0.017818   |
| ActualImprovement    | 0.016671   |
| ImprovementRatio     | 0.93559    |
| MeanKL               | 0.0068956  |
| Entropy              | 1.2227     |
| Perplexity           | 3.3964     |
| AveragePolicyStd     | 0.29992    |
| AveragePolicyStd[0]  | 0.30913    |
| AveragePolicyStd[1]  | 0.36488    |
| AveragePolicyStd[2]  | 0.28621    |
| AveragePolicyStd[3]  | 0.33717    |
| AveragePolicyStd[4]  | 0.23092    |
| AveragePolicyStd[5]  | 0.27121    |
| AverageReturn        | 844.6      |
| MinReturn            | 14.706     |
| MaxReturn            | 950.82     |
| StdReturn            | 190.56     |
| AverageEpisodeLength | 941.33     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.83     |
| TotalNEpisodes       | 15851      |
| TotalNSamples        | 1.4076e+06 |
| ExplainedVariance    | 0.066728   |
-------------------------------------
[2018-12-22 10:35:37.614040 UTC] Saving snapshot
[2018-12-22 10:35:37.614278 UTC] Starting iteration 283
[2018-12-22 10:35:37.614415 UTC] Start collecting samples
[2018-12-22 10:35:40.673379 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:40.754435 UTC] Performing policy update
[2018-12-22 10:35:40.755042 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:40.844411 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:41.894000 UTC] Performing line search
[2018-12-22 10:35:42.021512 UTC] Updating baseline
[2018-12-22 10:35:43.714603 UTC] Computing logging information
-------------------------------------
| Iteration            | 283        |
| ExpectedImprovement  | 0.016731   |
| ActualImprovement    | 0.015673   |
| ImprovementRatio     | 0.93677    |
| MeanKL               | 0.0069871  |
| Entropy              | 1.2057     |
| Perplexity           | 3.3389     |
| AveragePolicyStd     | 0.29899    |
| AveragePolicyStd[0]  | 0.3101     |
| AveragePolicyStd[1]  | 0.36154    |
| AveragePolicyStd[2]  | 0.28543    |
| AveragePolicyStd[3]  | 0.33536    |
| AveragePolicyStd[4]  | 0.23004    |
| AveragePolicyStd[5]  | 0.27147    |
| AverageReturn        | 844.81     |
| MinReturn            | 14.706     |
| MaxReturn            | 950.82     |
| StdReturn            | 191.06     |
| AverageEpisodeLength | 939.83     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.94     |
| TotalNEpisodes       | 15860      |
| TotalNSamples        | 1.4165e+06 |
| ExplainedVariance    | 0.17232    |
-------------------------------------
[2018-12-22 10:35:44.044906 UTC] Saving snapshot
[2018-12-22 10:35:44.045142 UTC] Starting iteration 284
[2018-12-22 10:35:44.045257 UTC] Start collecting samples
[2018-12-22 10:35:47.056907 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:47.135544 UTC] Performing policy update
[2018-12-22 10:35:47.136248 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:47.224181 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:48.270956 UTC] Performing line search
[2018-12-22 10:35:48.395423 UTC] Updating baseline
[2018-12-22 10:35:49.769434 UTC] Computing logging information
-------------------------------------
| Iteration            | 284        |
| ExpectedImprovement  | 0.015793   |
| ActualImprovement    | 0.015205   |
| ImprovementRatio     | 0.9628     |
| MeanKL               | 0.0068954  |
| Entropy              | 1.1981     |
| Perplexity           | 3.314      |
| AveragePolicyStd     | 0.29869    |
| AveragePolicyStd[0]  | 0.30944    |
| AveragePolicyStd[1]  | 0.36409    |
| AveragePolicyStd[2]  | 0.2826     |
| AveragePolicyStd[3]  | 0.3342     |
| AveragePolicyStd[4]  | 0.22975    |
| AveragePolicyStd[5]  | 0.27208    |
| AverageReturn        | 841.4      |
| MinReturn            | 14.706     |
| MaxReturn            | 950.82     |
| StdReturn            | 196.67     |
| AverageEpisodeLength | 934.26     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.66     |
| TotalNEpisodes       | 15864      |
| TotalNSamples        | 1.4199e+06 |
| ExplainedVariance    | 0.2553     |
-------------------------------------
[2018-12-22 10:35:50.123246 UTC] Saving snapshot
[2018-12-22 10:35:50.123506 UTC] Starting iteration 285
[2018-12-22 10:35:50.123693 UTC] Start collecting samples
[2018-12-22 10:35:53.348599 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:53.429547 UTC] Performing policy update
[2018-12-22 10:35:53.430201 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:53.522997 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:35:54.602080 UTC] Performing line search
[2018-12-22 10:35:54.731025 UTC] Updating baseline
[2018-12-22 10:35:56.025273 UTC] Computing logging information
-------------------------------------
| Iteration            | 285        |
| ExpectedImprovement  | 0.015533   |
| ActualImprovement    | 0.014695   |
| ImprovementRatio     | 0.94603    |
| MeanKL               | 0.0069248  |
| Entropy              | 1.1877     |
| Perplexity           | 3.2794     |
| AveragePolicyStd     | 0.29818    |
| AveragePolicyStd[0]  | 0.30959    |
| AveragePolicyStd[1]  | 0.36413    |
| AveragePolicyStd[2]  | 0.28059    |
| AveragePolicyStd[3]  | 0.33358    |
| AveragePolicyStd[4]  | 0.23012    |
| AveragePolicyStd[5]  | 0.27108    |
| AverageReturn        | 839.68     |
| MinReturn            | 14.706     |
| MaxReturn            | 955.39     |
| StdReturn            | 199.14     |
| AverageEpisodeLength | 929.07     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.36     |
| TotalNEpisodes       | 15870      |
| TotalNSamples        | 1.4253e+06 |
| ExplainedVariance    | 0.26945    |
-------------------------------------
[2018-12-22 10:35:56.377358 UTC] Saving snapshot
[2018-12-22 10:35:56.377644 UTC] Starting iteration 286
[2018-12-22 10:35:56.377805 UTC] Start collecting samples
[2018-12-22 10:35:59.595755 UTC] Computing input variables for policy optimization
[2018-12-22 10:35:59.677423 UTC] Performing policy update
[2018-12-22 10:35:59.678055 UTC] Computing gradient in Euclidean space
[2018-12-22 10:35:59.771575 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:00.806371 UTC] Performing line search
[2018-12-22 10:36:00.929466 UTC] Updating baseline
[2018-12-22 10:36:02.332343 UTC] Computing logging information
-------------------------------------
| Iteration            | 286        |
| ExpectedImprovement  | 0.013055   |
| ActualImprovement    | 0.012292   |
| ImprovementRatio     | 0.94151    |
| MeanKL               | 0.0073945  |
| Entropy              | 1.1799     |
| Perplexity           | 3.2541     |
| AveragePolicyStd     | 0.29771    |
| AveragePolicyStd[0]  | 0.3105     |
| AveragePolicyStd[1]  | 0.36237    |
| AveragePolicyStd[2]  | 0.27793    |
| AveragePolicyStd[3]  | 0.33352    |
| AveragePolicyStd[4]  | 0.23196    |
| AveragePolicyStd[5]  | 0.26997    |
| AverageReturn        | 844.88     |
| MinReturn            | 14.706     |
| MaxReturn            | 955.39     |
| StdReturn            | 196.55     |
| AverageEpisodeLength | 933.49     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.21     |
| TotalNEpisodes       | 15876      |
| TotalNSamples        | 1.4313e+06 |
| ExplainedVariance    | 0.045702   |
-------------------------------------
[2018-12-22 10:36:02.662410 UTC] Saving snapshot
[2018-12-22 10:36:02.662666 UTC] Starting iteration 287
[2018-12-22 10:36:02.662788 UTC] Start collecting samples
[2018-12-22 10:36:05.604728 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:05.681157 UTC] Performing policy update
[2018-12-22 10:36:05.681825 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:05.775335 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:06.811509 UTC] Performing line search
[2018-12-22 10:36:06.936241 UTC] Updating baseline
[2018-12-22 10:36:08.288125 UTC] Computing logging information
-------------------------------------
| Iteration            | 287        |
| ExpectedImprovement  | 0.015958   |
| ActualImprovement    | 0.015085   |
| ImprovementRatio     | 0.94528    |
| MeanKL               | 0.0075577  |
| Entropy              | 1.1861     |
| Perplexity           | 3.2743     |
| AveragePolicyStd     | 0.29806    |
| AveragePolicyStd[0]  | 0.3112     |
| AveragePolicyStd[1]  | 0.36135    |
| AveragePolicyStd[2]  | 0.2787     |
| AveragePolicyStd[3]  | 0.33525    |
| AveragePolicyStd[4]  | 0.23046    |
| AveragePolicyStd[5]  | 0.27141    |
| AverageReturn        | 853.61     |
| MinReturn            | 14.706     |
| MaxReturn            | 955.39     |
| StdReturn            | 182.83     |
| AverageEpisodeLength | 942.13     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.58     |
| TotalNEpisodes       | 15879      |
| TotalNSamples        | 1.4343e+06 |
| ExplainedVariance    | 0.053788   |
-------------------------------------
[2018-12-22 10:36:08.613943 UTC] Saving snapshot
[2018-12-22 10:36:08.614187 UTC] Starting iteration 288
[2018-12-22 10:36:08.614306 UTC] Start collecting samples
[2018-12-22 10:36:11.606019 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:11.683211 UTC] Performing policy update
[2018-12-22 10:36:11.683896 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:11.777176 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:12.827585 UTC] Performing line search
[2018-12-22 10:36:12.954741 UTC] Updating baseline
[2018-12-22 10:36:14.573656 UTC] Computing logging information
-------------------------------------
| Iteration            | 288        |
| ExpectedImprovement  | 0.016006   |
| ActualImprovement    | 0.015637   |
| ImprovementRatio     | 0.97693    |
| MeanKL               | 0.0069813  |
| Entropy              | 1.1799     |
| Perplexity           | 3.2541     |
| AveragePolicyStd     | 0.29782    |
| AveragePolicyStd[0]  | 0.31241    |
| AveragePolicyStd[1]  | 0.3614     |
| AveragePolicyStd[2]  | 0.27656    |
| AveragePolicyStd[3]  | 0.33543    |
| AveragePolicyStd[4]  | 0.23019    |
| AveragePolicyStd[5]  | 0.27091    |
| AverageReturn        | 860.05     |
| MinReturn            | 14.706     |
| MaxReturn            | 955.39     |
| StdReturn            | 181.21     |
| AverageEpisodeLength | 945        |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.3      |
| TotalNEpisodes       | 15883      |
| TotalNSamples        | 1.4383e+06 |
| ExplainedVariance    | 0.093397   |
-------------------------------------
[2018-12-22 10:36:14.909055 UTC] Saving snapshot
[2018-12-22 10:36:14.909307 UTC] Starting iteration 289
[2018-12-22 10:36:14.909437 UTC] Start collecting samples
[2018-12-22 10:36:17.946077 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:18.027133 UTC] Performing policy update
[2018-12-22 10:36:18.027746 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:18.116520 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:19.156975 UTC] Performing line search
[2018-12-22 10:36:19.282453 UTC] Updating baseline
[2018-12-22 10:36:20.638852 UTC] Computing logging information
-------------------------------------
| Iteration            | 289        |
| ExpectedImprovement  | 0.013468   |
| ActualImprovement    | 0.012882   |
| ImprovementRatio     | 0.95645    |
| MeanKL               | 0.0078331  |
| Entropy              | 1.1831     |
| Perplexity           | 3.2644     |
| AveragePolicyStd     | 0.29801    |
| AveragePolicyStd[0]  | 0.31239    |
| AveragePolicyStd[1]  | 0.36245    |
| AveragePolicyStd[2]  | 0.27825    |
| AveragePolicyStd[3]  | 0.33505    |
| AveragePolicyStd[4]  | 0.22987    |
| AveragePolicyStd[5]  | 0.27003    |
| AverageReturn        | 858.68     |
| MinReturn            | 14.706     |
| MaxReturn            | 955.39     |
| StdReturn            | 184.71     |
| AverageEpisodeLength | 940.67     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.63     |
| TotalNEpisodes       | 15891      |
| TotalNSamples        | 1.4458e+06 |
| ExplainedVariance    | 0.13859    |
-------------------------------------
[2018-12-22 10:36:20.969696 UTC] Saving snapshot
[2018-12-22 10:36:20.969969 UTC] Starting iteration 290
[2018-12-22 10:36:20.970089 UTC] Start collecting samples
[2018-12-22 10:36:23.981931 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:24.060404 UTC] Performing policy update
[2018-12-22 10:36:24.061051 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:24.148695 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:25.181511 UTC] Performing line search
[2018-12-22 10:36:25.306226 UTC] Updating baseline
[2018-12-22 10:36:26.620366 UTC] Computing logging information
-------------------------------------
| Iteration            | 290        |
| ExpectedImprovement  | 0.013217   |
| ActualImprovement    | 0.012996   |
| ImprovementRatio     | 0.98333    |
| MeanKL               | 0.0072733  |
| Entropy              | 1.1857     |
| Perplexity           | 3.273      |
| AveragePolicyStd     | 0.29823    |
| AveragePolicyStd[0]  | 0.31375    |
| AveragePolicyStd[1]  | 0.36365    |
| AveragePolicyStd[2]  | 0.27829    |
| AveragePolicyStd[3]  | 0.33562    |
| AveragePolicyStd[4]  | 0.22973    |
| AveragePolicyStd[5]  | 0.26834    |
| AverageReturn        | 856.12     |
| MinReturn            | 14.706     |
| MaxReturn            | 958.95     |
| StdReturn            | 190.55     |
| AverageEpisodeLength | 935.16     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.47     |
| TotalNEpisodes       | 15897      |
| TotalNSamples        | 1.4513e+06 |
| ExplainedVariance    | 0.14381    |
-------------------------------------
[2018-12-22 10:36:26.950459 UTC] Saving snapshot
[2018-12-22 10:36:26.958595 UTC] Starting iteration 291
[2018-12-22 10:36:26.958783 UTC] Start collecting samples
[2018-12-22 10:36:29.921551 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:29.997811 UTC] Performing policy update
[2018-12-22 10:36:29.998519 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:30.087054 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:31.139929 UTC] Performing line search
[2018-12-22 10:36:31.269476 UTC] Updating baseline
[2018-12-22 10:36:32.949540 UTC] Computing logging information
-------------------------------------
| Iteration            | 291        |
| ExpectedImprovement  | 0.015727   |
| ActualImprovement    | 0.014233   |
| ImprovementRatio     | 0.90501    |
| MeanKL               | 0.0073575  |
| Entropy              | 1.1978     |
| Perplexity           | 3.3128     |
| AveragePolicyStd     | 0.29884    |
| AveragePolicyStd[0]  | 0.31563    |
| AveragePolicyStd[1]  | 0.36417    |
| AveragePolicyStd[2]  | 0.27846    |
| AveragePolicyStd[3]  | 0.33689    |
| AveragePolicyStd[4]  | 0.23123    |
| AveragePolicyStd[5]  | 0.26669    |
| AverageReturn        | 856.83     |
| MinReturn            | 14.706     |
| MaxReturn            | 958.95     |
| StdReturn            | 190.78     |
| AverageEpisodeLength | 935.16     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.47     |
| TotalNEpisodes       | 15898      |
| TotalNSamples        | 1.4523e+06 |
| ExplainedVariance    | 0.0066544  |
-------------------------------------
[2018-12-22 10:36:33.283925 UTC] Saving snapshot
[2018-12-22 10:36:33.284236 UTC] Starting iteration 292
[2018-12-22 10:36:33.284353 UTC] Start collecting samples
[2018-12-22 10:36:36.301525 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:36.378370 UTC] Performing policy update
[2018-12-22 10:36:36.378959 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:36.465991 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:37.500694 UTC] Performing line search
[2018-12-22 10:36:37.624879 UTC] Updating baseline
[2018-12-22 10:36:39.233839 UTC] Computing logging information
-------------------------------------
| Iteration            | 292        |
| ExpectedImprovement  | 0.014187   |
| ActualImprovement    | 0.013615   |
| ImprovementRatio     | 0.95965    |
| MeanKL               | 0.0079407  |
| Entropy              | 1.1761     |
| Perplexity           | 3.2418     |
| AveragePolicyStd     | 0.29775    |
| AveragePolicyStd[0]  | 0.3137     |
| AveragePolicyStd[1]  | 0.36262    |
| AveragePolicyStd[2]  | 0.27717    |
| AveragePolicyStd[3]  | 0.33653    |
| AveragePolicyStd[4]  | 0.23085    |
| AveragePolicyStd[5]  | 0.26563    |
| AverageReturn        | 859.89     |
| MinReturn            | 14.706     |
| MaxReturn            | 958.95     |
| StdReturn            | 191.81     |
| AverageEpisodeLength | 935.16     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.47     |
| TotalNEpisodes       | 15904      |
| TotalNSamples        | 1.4583e+06 |
| ExplainedVariance    | 0.0035804  |
-------------------------------------
[2018-12-22 10:36:39.565570 UTC] Saving snapshot
[2018-12-22 10:36:39.565855 UTC] Starting iteration 293
[2018-12-22 10:36:39.565989 UTC] Start collecting samples
[2018-12-22 10:36:42.585649 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:42.665169 UTC] Performing policy update
[2018-12-22 10:36:42.665777 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:42.754271 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:43.807965 UTC] Performing line search
[2018-12-22 10:36:43.935269 UTC] Updating baseline
[2018-12-22 10:36:45.281345 UTC] Computing logging information
-------------------------------------
| Iteration            | 293        |
| ExpectedImprovement  | 0.018495   |
| ActualImprovement    | 0.017111   |
| ImprovementRatio     | 0.92517    |
| MeanKL               | 0.0068978  |
| Entropy              | 1.1879     |
| Perplexity           | 3.2802     |
| AveragePolicyStd     | 0.29843    |
| AveragePolicyStd[0]  | 0.31352    |
| AveragePolicyStd[1]  | 0.36456    |
| AveragePolicyStd[2]  | 0.27972    |
| AveragePolicyStd[3]  | 0.3371     |
| AveragePolicyStd[4]  | 0.22952    |
| AveragePolicyStd[5]  | 0.26617    |
| AverageReturn        | 869.18     |
| MinReturn            | 14.706     |
| MaxReturn            | 971.02     |
| StdReturn            | 186.83     |
| AverageEpisodeLength | 940.72     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.51     |
| TotalNEpisodes       | 15911      |
| TotalNSamples        | 1.4653e+06 |
| ExplainedVariance    | -0.016249  |
-------------------------------------
[2018-12-22 10:36:45.618035 UTC] Saving snapshot
[2018-12-22 10:36:45.618296 UTC] Starting iteration 294
[2018-12-22 10:36:45.618444 UTC] Start collecting samples
[2018-12-22 10:36:48.604477 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:48.680970 UTC] Performing policy update
[2018-12-22 10:36:48.681574 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:48.769842 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:49.826693 UTC] Performing line search
[2018-12-22 10:36:49.954171 UTC] Updating baseline
[2018-12-22 10:36:51.386176 UTC] Computing logging information
-------------------------------------
| Iteration            | 294        |
| ExpectedImprovement  | 0.014695   |
| ActualImprovement    | 0.014193   |
| ImprovementRatio     | 0.9659     |
| MeanKL               | 0.0069044  |
| Entropy              | 1.178      |
| Perplexity           | 3.2479     |
| AveragePolicyStd     | 0.29787    |
| AveragePolicyStd[0]  | 0.31269    |
| AveragePolicyStd[1]  | 0.36356    |
| AveragePolicyStd[2]  | 0.27957    |
| AveragePolicyStd[3]  | 0.3357     |
| AveragePolicyStd[4]  | 0.22972    |
| AveragePolicyStd[5]  | 0.26597    |
| AverageReturn        | 887.07     |
| MinReturn            | 15.238     |
| MaxReturn            | 971.02     |
| StdReturn            | 148.36     |
| AverageEpisodeLength | 959.02     |
| MinEpisodeLength     | 34         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.34     |
| TotalNEpisodes       | 15914      |
| TotalNSamples        | 1.4683e+06 |
| ExplainedVariance    | 0.006343   |
-------------------------------------
[2018-12-22 10:36:51.721685 UTC] Saving snapshot
[2018-12-22 10:36:51.721993 UTC] Starting iteration 295
[2018-12-22 10:36:51.722117 UTC] Start collecting samples
[2018-12-22 10:36:54.731859 UTC] Computing input variables for policy optimization
[2018-12-22 10:36:54.810091 UTC] Performing policy update
[2018-12-22 10:36:54.810906 UTC] Computing gradient in Euclidean space
[2018-12-22 10:36:54.900583 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:36:55.952686 UTC] Performing line search
[2018-12-22 10:36:56.078307 UTC] Updating baseline
[2018-12-22 10:36:57.565212 UTC] Computing logging information
-------------------------------------
| Iteration            | 295        |
| ExpectedImprovement  | 0.013623   |
| ActualImprovement    | 0.013543   |
| ImprovementRatio     | 0.99411    |
| MeanKL               | 0.0074486  |
| Entropy              | 1.1644     |
| Perplexity           | 3.2039     |
| AveragePolicyStd     | 0.29724    |
| AveragePolicyStd[0]  | 0.31164    |
| AveragePolicyStd[1]  | 0.36384    |
| AveragePolicyStd[2]  | 0.27942    |
| AveragePolicyStd[3]  | 0.33535    |
| AveragePolicyStd[4]  | 0.22959    |
| AveragePolicyStd[5]  | 0.26363    |
| AverageReturn        | 898.02     |
| MinReturn            | 177.13     |
| MaxReturn            | 971.02     |
| StdReturn            | 120.05     |
| AverageEpisodeLength | 968.68     |
| MinEpisodeLength     | 185        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.73     |
| TotalNEpisodes       | 15919      |
| TotalNSamples        | 1.4733e+06 |
| ExplainedVariance    | 0.0087281  |
-------------------------------------
[2018-12-22 10:36:57.892658 UTC] Saving snapshot
[2018-12-22 10:36:57.892906 UTC] Starting iteration 296
[2018-12-22 10:36:57.893045 UTC] Start collecting samples
[2018-12-22 10:37:00.950567 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:01.031268 UTC] Performing policy update
[2018-12-22 10:37:01.032067 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:01.121518 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:02.173818 UTC] Performing line search
[2018-12-22 10:37:02.298092 UTC] Updating baseline
[2018-12-22 10:37:03.829470 UTC] Computing logging information
-------------------------------------
| Iteration            | 296        |
| ExpectedImprovement  | 0.015465   |
| ActualImprovement    | 0.015328   |
| ImprovementRatio     | 0.99113    |
| MeanKL               | 0.0069937  |
| Entropy              | 1.1606     |
| Perplexity           | 3.1917     |
| AveragePolicyStd     | 0.29698    |
| AveragePolicyStd[0]  | 0.31266    |
| AveragePolicyStd[1]  | 0.36301    |
| AveragePolicyStd[2]  | 0.28141    |
| AveragePolicyStd[3]  | 0.33261    |
| AveragePolicyStd[4]  | 0.22975    |
| AveragePolicyStd[5]  | 0.26247    |
| AverageReturn        | 899.2      |
| MinReturn            | 177.13     |
| MaxReturn            | 971.02     |
| StdReturn            | 120.32     |
| AverageEpisodeLength | 968.68     |
| MinEpisodeLength     | 185        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.73     |
| TotalNEpisodes       | 15927      |
| TotalNSamples        | 1.4813e+06 |
| ExplainedVariance    | 0.018512   |
-------------------------------------
[2018-12-22 10:37:04.206980 UTC] Saving snapshot
[2018-12-22 10:37:04.207223 UTC] Starting iteration 297
[2018-12-22 10:37:04.207339 UTC] Start collecting samples
[2018-12-22 10:37:07.413517 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:07.495207 UTC] Performing policy update
[2018-12-22 10:37:07.496034 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:07.588946 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:08.697521 UTC] Performing line search
[2018-12-22 10:37:08.822192 UTC] Updating baseline
[2018-12-22 10:37:10.344857 UTC] Computing logging information
-------------------------------------
| Iteration            | 297        |
| ExpectedImprovement  | 0.01488    |
| ActualImprovement    | 0.013925   |
| ImprovementRatio     | 0.93579    |
| MeanKL               | 0.0071726  |
| Entropy              | 1.1678     |
| Perplexity           | 3.2148     |
| AveragePolicyStd     | 0.29746    |
| AveragePolicyStd[0]  | 0.31377    |
| AveragePolicyStd[1]  | 0.36518    |
| AveragePolicyStd[2]  | 0.27954    |
| AveragePolicyStd[3]  | 0.33445    |
| AveragePolicyStd[4]  | 0.23052    |
| AveragePolicyStd[5]  | 0.26129    |
| AverageReturn        | 899.93     |
| MinReturn            | 177.13     |
| MaxReturn            | 971.02     |
| StdReturn            | 120.52     |
| AverageEpisodeLength | 968.68     |
| MinEpisodeLength     | 185        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.73     |
| TotalNEpisodes       | 15930      |
| TotalNSamples        | 1.4843e+06 |
| ExplainedVariance    | 0.044514   |
-------------------------------------
[2018-12-22 10:37:10.680636 UTC] Saving snapshot
[2018-12-22 10:37:10.680897 UTC] Starting iteration 298
[2018-12-22 10:37:10.681018 UTC] Start collecting samples
[2018-12-22 10:37:13.667180 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:13.745067 UTC] Performing policy update
[2018-12-22 10:37:13.745976 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:13.839112 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:14.892451 UTC] Performing line search
[2018-12-22 10:37:15.017788 UTC] Updating baseline
[2018-12-22 10:37:16.260247 UTC] Computing logging information
-------------------------------------
| Iteration            | 298        |
| ExpectedImprovement  | 0.013108   |
| ActualImprovement    | 0.012611   |
| ImprovementRatio     | 0.96214    |
| MeanKL               | 0.0075188  |
| Entropy              | 1.1751     |
| Perplexity           | 3.2386     |
| AveragePolicyStd     | 0.29788    |
| AveragePolicyStd[0]  | 0.31244    |
| AveragePolicyStd[1]  | 0.36888    |
| AveragePolicyStd[2]  | 0.27913    |
| AveragePolicyStd[3]  | 0.33352    |
| AveragePolicyStd[4]  | 0.23076    |
| AveragePolicyStd[5]  | 0.26255    |
| AverageReturn        | 901.63     |
| MinReturn            | 177.13     |
| MaxReturn            | 982.56     |
| StdReturn            | 121.12     |
| AverageEpisodeLength | 968.68     |
| MinEpisodeLength     | 185        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.73     |
| TotalNEpisodes       | 15934      |
| TotalNSamples        | 1.4883e+06 |
| ExplainedVariance    | 0.0098401  |
-------------------------------------
[2018-12-22 10:37:16.588163 UTC] Saving snapshot
[2018-12-22 10:37:16.588414 UTC] Starting iteration 299
[2018-12-22 10:37:16.588535 UTC] Start collecting samples
[2018-12-22 10:37:19.604627 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:19.684548 UTC] Performing policy update
[2018-12-22 10:37:19.685222 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:19.775855 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:20.835125 UTC] Performing line search
[2018-12-22 10:37:20.960638 UTC] Updating baseline
[2018-12-22 10:37:22.509272 UTC] Computing logging information
-------------------------------------
| Iteration            | 299        |
| ExpectedImprovement  | 0.014441   |
| ActualImprovement    | 0.013305   |
| ImprovementRatio     | 0.92134    |
| MeanKL               | 0.0074909  |
| Entropy              | 1.1663     |
| Perplexity           | 3.2099     |
| AveragePolicyStd     | 0.29739    |
| AveragePolicyStd[0]  | 0.31001    |
| AveragePolicyStd[1]  | 0.36994    |
| AveragePolicyStd[2]  | 0.27726    |
| AveragePolicyStd[3]  | 0.33255    |
| AveragePolicyStd[4]  | 0.23241    |
| AveragePolicyStd[5]  | 0.26216    |
| AverageReturn        | 904.75     |
| MinReturn            | 177.13     |
| MaxReturn            | 982.56     |
| StdReturn            | 121.86     |
| AverageEpisodeLength | 968.68     |
| MinEpisodeLength     | 185        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.73     |
| TotalNEpisodes       | 15942      |
| TotalNSamples        | 1.4963e+06 |
| ExplainedVariance    | -0.012209  |
-------------------------------------
[2018-12-22 10:37:22.845434 UTC] Saving snapshot
[2018-12-22 10:37:22.845689 UTC] Starting iteration 300
[2018-12-22 10:37:22.845858 UTC] Start collecting samples
[2018-12-22 10:37:25.819818 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:25.897929 UTC] Performing policy update
[2018-12-22 10:37:25.898543 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:25.987893 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:26.985203 UTC] Performing line search
[2018-12-22 10:37:27.109433 UTC] Updating baseline
[2018-12-22 10:37:28.769195 UTC] Computing logging information
--------------------------------------
| Iteration            | 300         |
| ExpectedImprovement  | 0.016383    |
| ActualImprovement    | 0.015224    |
| ImprovementRatio     | 0.92928     |
| MeanKL               | 0.0069278   |
| Entropy              | 1.1525      |
| Perplexity           | 3.166       |
| AveragePolicyStd     | 0.29674     |
| AveragePolicyStd[0]  | 0.31047     |
| AveragePolicyStd[1]  | 0.3679      |
| AveragePolicyStd[2]  | 0.27498     |
| AveragePolicyStd[3]  | 0.33358     |
| AveragePolicyStd[4]  | 0.23115     |
| AveragePolicyStd[5]  | 0.26238     |
| AverageReturn        | 905.7       |
| MinReturn            | 177.13      |
| MaxReturn            | 993.34      |
| StdReturn            | 122.16      |
| AverageEpisodeLength | 968.68      |
| MinEpisodeLength     | 185         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 125.73      |
| TotalNEpisodes       | 15945       |
| TotalNSamples        | 1.4993e+06  |
| ExplainedVariance    | -0.00072381 |
--------------------------------------
[2018-12-22 10:37:29.098390 UTC] Saving snapshot
[2018-12-22 10:37:29.106812 UTC] Starting iteration 301
[2018-12-22 10:37:29.107023 UTC] Start collecting samples
[2018-12-22 10:37:32.118029 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:32.195766 UTC] Performing policy update
[2018-12-22 10:37:32.196375 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:32.282115 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:33.254626 UTC] Performing line search
[2018-12-22 10:37:33.367750 UTC] Updating baseline
[2018-12-22 10:37:34.640091 UTC] Computing logging information
-------------------------------------
| Iteration            | 301        |
| ExpectedImprovement  | 0.015386   |
| ActualImprovement    | 0.014715   |
| ImprovementRatio     | 0.95637    |
| MeanKL               | 0.0070658  |
| Entropy              | 1.1607     |
| Perplexity           | 3.1921     |
| AveragePolicyStd     | 0.29725    |
| AveragePolicyStd[0]  | 0.31151    |
| AveragePolicyStd[1]  | 0.36882    |
| AveragePolicyStd[2]  | 0.27471    |
| AveragePolicyStd[3]  | 0.33598    |
| AveragePolicyStd[4]  | 0.2315     |
| AveragePolicyStd[5]  | 0.26099    |
| AverageReturn        | 915.15     |
| MinReturn            | 396.59     |
| MaxReturn            | 993.34     |
| StdReturn            | 98.295     |
| AverageEpisodeLength | 976.83     |
| MinEpisodeLength     | 443        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 98.035     |
| TotalNEpisodes       | 15950      |
| TotalNSamples        | 1.5043e+06 |
| ExplainedVariance    | 0.014109   |
-------------------------------------
[2018-12-22 10:37:34.971072 UTC] Saving snapshot
[2018-12-22 10:37:34.971303 UTC] Starting iteration 302
[2018-12-22 10:37:34.971436 UTC] Start collecting samples
[2018-12-22 10:37:38.015907 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:38.095307 UTC] Performing policy update
[2018-12-22 10:37:38.096017 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:38.183325 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:39.221725 UTC] Performing line search
[2018-12-22 10:37:39.346681 UTC] Updating baseline
[2018-12-22 10:37:40.865543 UTC] Computing logging information
-------------------------------------
| Iteration            | 302        |
| ExpectedImprovement  | 0.014655   |
| ActualImprovement    | 0.013718   |
| ImprovementRatio     | 0.93607    |
| MeanKL               | 0.0072951  |
| Entropy              | 1.1796     |
| Perplexity           | 3.2531     |
| AveragePolicyStd     | 0.2983     |
| AveragePolicyStd[0]  | 0.31212    |
| AveragePolicyStd[1]  | 0.37312    |
| AveragePolicyStd[2]  | 0.27419    |
| AveragePolicyStd[3]  | 0.33631    |
| AveragePolicyStd[4]  | 0.2323     |
| AveragePolicyStd[5]  | 0.26173    |
| AverageReturn        | 914.19     |
| MinReturn            | 396.59     |
| MaxReturn            | 993.34     |
| StdReturn            | 104.24     |
| AverageEpisodeLength | 973.02     |
| MinEpisodeLength     | 443        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 104.27     |
| TotalNEpisodes       | 15958      |
| TotalNSamples        | 1.5119e+06 |
| ExplainedVariance    | 0.082272   |
-------------------------------------
[2018-12-22 10:37:41.197277 UTC] Saving snapshot
[2018-12-22 10:37:41.197518 UTC] Starting iteration 303
[2018-12-22 10:37:41.197655 UTC] Start collecting samples
[2018-12-22 10:37:44.185938 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:44.263597 UTC] Performing policy update
[2018-12-22 10:37:44.264535 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:44.353805 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:45.396464 UTC] Performing line search
[2018-12-22 10:37:45.521571 UTC] Updating baseline
[2018-12-22 10:37:47.010910 UTC] Computing logging information
-------------------------------------
| Iteration            | 303        |
| ExpectedImprovement  | 0.014717   |
| ActualImprovement    | 0.013422   |
| ImprovementRatio     | 0.91204    |
| MeanKL               | 0.0077151  |
| Entropy              | 1.159      |
| Perplexity           | 3.1868     |
| AveragePolicyStd     | 0.29743    |
| AveragePolicyStd[0]  | 0.31255    |
| AveragePolicyStd[1]  | 0.37304    |
| AveragePolicyStd[2]  | 0.27247    |
| AveragePolicyStd[3]  | 0.33639    |
| AveragePolicyStd[4]  | 0.23069    |
| AveragePolicyStd[5]  | 0.25945    |
| AverageReturn        | 916.64     |
| MinReturn            | 396.59     |
| MaxReturn            | 993.34     |
| StdReturn            | 103.45     |
| AverageEpisodeLength | 974.52     |
| MinEpisodeLength     | 443        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 103.56     |
| TotalNEpisodes       | 15961      |
| TotalNSamples        | 1.5149e+06 |
| ExplainedVariance    | -0.023069  |
-------------------------------------
[2018-12-22 10:37:47.344598 UTC] Saving snapshot
[2018-12-22 10:37:47.344838 UTC] Starting iteration 304
[2018-12-22 10:37:47.344958 UTC] Start collecting samples
[2018-12-22 10:37:50.344299 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:50.420813 UTC] Performing policy update
[2018-12-22 10:37:50.421529 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:50.510344 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:51.550137 UTC] Performing line search
[2018-12-22 10:37:51.674028 UTC] Updating baseline
[2018-12-22 10:37:52.900386 UTC] Computing logging information
-------------------------------------
| Iteration            | 304        |
| ExpectedImprovement  | 0.015064   |
| ActualImprovement    | 0.014431   |
| ImprovementRatio     | 0.95799    |
| MeanKL               | 0.0071974  |
| Entropy              | 1.1569     |
| Perplexity           | 3.1799     |
| AveragePolicyStd     | 0.29739    |
| AveragePolicyStd[0]  | 0.31158    |
| AveragePolicyStd[1]  | 0.37276    |
| AveragePolicyStd[2]  | 0.27241    |
| AveragePolicyStd[3]  | 0.33814    |
| AveragePolicyStd[4]  | 0.22964    |
| AveragePolicyStd[5]  | 0.25978    |
| AverageReturn        | 922.81     |
| MinReturn            | 415.41     |
| MaxReturn            | 993.34     |
| StdReturn            | 89.47      |
| AverageEpisodeLength | 980.09     |
| MinEpisodeLength     | 449        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 88.744     |
| TotalNEpisodes       | 15964      |
| TotalNSamples        | 1.5179e+06 |
| ExplainedVariance    | 0.12954    |
-------------------------------------
[2018-12-22 10:37:53.228026 UTC] Saving snapshot
[2018-12-22 10:37:53.228265 UTC] Starting iteration 305
[2018-12-22 10:37:53.228384 UTC] Start collecting samples
[2018-12-22 10:37:56.261251 UTC] Computing input variables for policy optimization
[2018-12-22 10:37:56.343045 UTC] Performing policy update
[2018-12-22 10:37:56.343639 UTC] Computing gradient in Euclidean space
[2018-12-22 10:37:56.432120 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:37:57.481357 UTC] Performing line search
[2018-12-22 10:37:57.606499 UTC] Updating baseline
[2018-12-22 10:37:59.128007 UTC] Computing logging information
-------------------------------------
| Iteration            | 305        |
| ExpectedImprovement  | 0.015933   |
| ActualImprovement    | 0.015437   |
| ImprovementRatio     | 0.96888    |
| MeanKL               | 0.0075238  |
| Entropy              | 1.1495     |
| Perplexity           | 3.1567     |
| AveragePolicyStd     | 0.29707    |
| AveragePolicyStd[0]  | 0.31106    |
| AveragePolicyStd[1]  | 0.37394    |
| AveragePolicyStd[2]  | 0.26932    |
| AveragePolicyStd[3]  | 0.3377     |
| AveragePolicyStd[4]  | 0.22985    |
| AveragePolicyStd[5]  | 0.26057    |
| AverageReturn        | 932.87     |
| MinReturn            | 415.41     |
| MaxReturn            | 993.34     |
| StdReturn            | 78.265     |
| AverageEpisodeLength | 986.35     |
| MinEpisodeLength     | 449        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.155     |
| TotalNEpisodes       | 15972      |
| TotalNSamples        | 1.5259e+06 |
| ExplainedVariance    | 0.06265    |
-------------------------------------
[2018-12-22 10:37:59.463532 UTC] Saving snapshot
[2018-12-22 10:37:59.463788 UTC] Starting iteration 306
[2018-12-22 10:37:59.463911 UTC] Start collecting samples
[2018-12-22 10:38:02.485877 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:02.563395 UTC] Performing policy update
[2018-12-22 10:38:02.564124 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:02.653130 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:03.699154 UTC] Performing line search
[2018-12-22 10:38:03.827530 UTC] Updating baseline
[2018-12-22 10:38:05.307322 UTC] Computing logging information
-------------------------------------
| Iteration            | 306        |
| ExpectedImprovement  | 0.017053   |
| ActualImprovement    | 0.015846   |
| ImprovementRatio     | 0.92921    |
| MeanKL               | 0.0070825  |
| Entropy              | 1.1269     |
| Perplexity           | 3.0861     |
| AveragePolicyStd     | 0.29602    |
| AveragePolicyStd[0]  | 0.30886    |
| AveragePolicyStd[1]  | 0.37295    |
| AveragePolicyStd[2]  | 0.2682     |
| AveragePolicyStd[3]  | 0.33831    |
| AveragePolicyStd[4]  | 0.22888    |
| AveragePolicyStd[5]  | 0.25893    |
| AverageReturn        | 934.71     |
| MinReturn            | 415.41     |
| MaxReturn            | 993.34     |
| StdReturn            | 78.446     |
| AverageEpisodeLength | 986.35     |
| MinEpisodeLength     | 449        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.155     |
| TotalNEpisodes       | 15977      |
| TotalNSamples        | 1.5309e+06 |
| ExplainedVariance    | 0.032304   |
-------------------------------------
[2018-12-22 10:38:05.642680 UTC] Saving snapshot
[2018-12-22 10:38:05.642932 UTC] Starting iteration 307
[2018-12-22 10:38:05.643049 UTC] Start collecting samples
[2018-12-22 10:38:08.626479 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:08.702640 UTC] Performing policy update
[2018-12-22 10:38:08.703220 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:08.792581 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:09.855920 UTC] Performing line search
[2018-12-22 10:38:09.984040 UTC] Updating baseline
[2018-12-22 10:38:11.600934 UTC] Computing logging information
-------------------------------------
| Iteration            | 307        |
| ExpectedImprovement  | 0.01477    |
| ActualImprovement    | 0.014293   |
| ImprovementRatio     | 0.96771    |
| MeanKL               | 0.0069384  |
| Entropy              | 1.1265     |
| Perplexity           | 3.0849     |
| AveragePolicyStd     | 0.29598    |
| AveragePolicyStd[0]  | 0.3084     |
| AveragePolicyStd[1]  | 0.37281    |
| AveragePolicyStd[2]  | 0.26809    |
| AveragePolicyStd[3]  | 0.33807    |
| AveragePolicyStd[4]  | 0.22886    |
| AveragePolicyStd[5]  | 0.25962    |
| AverageReturn        | 925.96     |
| MinReturn            | 197.84     |
| MaxReturn            | 993.34     |
| StdReturn            | 108.78     |
| AverageEpisodeLength | 976.75     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.28     |
| TotalNEpisodes       | 15980      |
| TotalNSamples        | 1.5329e+06 |
| ExplainedVariance    | 0.47825    |
-------------------------------------
[2018-12-22 10:38:11.946188 UTC] Saving snapshot
[2018-12-22 10:38:11.946450 UTC] Starting iteration 308
[2018-12-22 10:38:11.946586 UTC] Start collecting samples
[2018-12-22 10:38:14.974004 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:15.052999 UTC] Performing policy update
[2018-12-22 10:38:15.053572 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:15.142981 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:16.197864 UTC] Performing line search
[2018-12-22 10:38:16.322605 UTC] Updating baseline
[2018-12-22 10:38:17.763718 UTC] Computing logging information
-------------------------------------
| Iteration            | 308        |
| ExpectedImprovement  | 0.016063   |
| ActualImprovement    | 0.015253   |
| ImprovementRatio     | 0.94958    |
| MeanKL               | 0.0071176  |
| Entropy              | 1.1373     |
| Perplexity           | 3.1183     |
| AveragePolicyStd     | 0.29662    |
| AveragePolicyStd[0]  | 0.30761    |
| AveragePolicyStd[1]  | 0.37583    |
| AveragePolicyStd[2]  | 0.26751    |
| AveragePolicyStd[3]  | 0.33954    |
| AveragePolicyStd[4]  | 0.22931    |
| AveragePolicyStd[5]  | 0.25994    |
| AverageReturn        | 928.19     |
| MinReturn            | 197.84     |
| MaxReturn            | 999.69     |
| StdReturn            | 109.5      |
| AverageEpisodeLength | 976.83     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.29     |
| TotalNEpisodes       | 15986      |
| TotalNSamples        | 1.5389e+06 |
| ExplainedVariance    | -0.0014476 |
-------------------------------------
[2018-12-22 10:38:18.105780 UTC] Saving snapshot
[2018-12-22 10:38:18.106037 UTC] Starting iteration 309
[2018-12-22 10:38:18.106154 UTC] Start collecting samples
[2018-12-22 10:38:21.159697 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:21.239279 UTC] Performing policy update
[2018-12-22 10:38:21.239881 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:21.328731 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:22.390759 UTC] Performing line search
[2018-12-22 10:38:22.518051 UTC] Updating baseline
[2018-12-22 10:38:23.885156 UTC] Computing logging information
------------------------------------
| Iteration            | 309       |
| ExpectedImprovement  | 0.014361  |
| ActualImprovement    | 0.013328  |
| ImprovementRatio     | 0.9281    |
| MeanKL               | 0.0070177 |
| Entropy              | 1.1337    |
| Perplexity           | 3.107     |
| AveragePolicyStd     | 0.29643   |
| AveragePolicyStd[0]  | 0.30783   |
| AveragePolicyStd[1]  | 0.37603   |
| AveragePolicyStd[2]  | 0.26623   |
| AveragePolicyStd[3]  | 0.3388    |
| AveragePolicyStd[4]  | 0.23004   |
| AveragePolicyStd[5]  | 0.25965   |
| AverageReturn        | 931.94    |
| MinReturn            | 197.84    |
| MaxReturn            | 999.69    |
| StdReturn            | 109.84    |
| AverageEpisodeLength | 977.36    |
| MinEpisodeLength     | 239       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 110.93    |
| TotalNEpisodes       | 15994     |
| TotalNSamples        | 1.546e+06 |
| ExplainedVariance    | 0.13624   |
------------------------------------
[2018-12-22 10:38:24.224237 UTC] Saving snapshot
[2018-12-22 10:38:24.224501 UTC] Starting iteration 310
[2018-12-22 10:38:24.224637 UTC] Start collecting samples
[2018-12-22 10:38:27.202885 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:27.281330 UTC] Performing policy update
[2018-12-22 10:38:27.281938 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:27.372044 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:28.442625 UTC] Performing line search
[2018-12-22 10:38:28.569714 UTC] Updating baseline
[2018-12-22 10:38:29.926121 UTC] Computing logging information
-------------------------------------
| Iteration            | 310        |
| ExpectedImprovement  | 0.015669   |
| ActualImprovement    | 0.013713   |
| ImprovementRatio     | 0.87515    |
| MeanKL               | 0.0069696  |
| Entropy              | 1.132      |
| Perplexity           | 3.1018     |
| AveragePolicyStd     | 0.29628    |
| AveragePolicyStd[0]  | 0.30789    |
| AveragePolicyStd[1]  | 0.37407    |
| AveragePolicyStd[2]  | 0.26517    |
| AveragePolicyStd[3]  | 0.33909    |
| AveragePolicyStd[4]  | 0.22935    |
| AveragePolicyStd[5]  | 0.26212    |
| AverageReturn        | 932.64     |
| MinReturn            | 197.84     |
| MaxReturn            | 999.69     |
| StdReturn            | 110.03     |
| AverageEpisodeLength | 977.36     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 110.93     |
| TotalNEpisodes       | 15997      |
| TotalNSamples        | 1.549e+06  |
| ExplainedVariance    | -0.0072688 |
-------------------------------------
[2018-12-22 10:38:30.264053 UTC] Saving snapshot
[2018-12-22 10:38:30.272125 UTC] Starting iteration 311
[2018-12-22 10:38:30.272331 UTC] Start collecting samples
[2018-12-22 10:38:33.283280 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:33.362282 UTC] Performing policy update
[2018-12-22 10:38:33.362892 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:33.451830 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:34.505662 UTC] Performing line search
[2018-12-22 10:38:34.631921 UTC] Updating baseline
[2018-12-22 10:38:36.155829 UTC] Computing logging information
-------------------------------------
| Iteration            | 311        |
| ExpectedImprovement  | 0.013938   |
| ActualImprovement    | 0.013217   |
| ImprovementRatio     | 0.94826    |
| MeanKL               | 0.007828   |
| Entropy              | 1.1322     |
| Perplexity           | 3.1025     |
| AveragePolicyStd     | 0.29615    |
| AveragePolicyStd[0]  | 0.30775    |
| AveragePolicyStd[1]  | 0.37129    |
| AveragePolicyStd[2]  | 0.26522    |
| AveragePolicyStd[3]  | 0.33935    |
| AveragePolicyStd[4]  | 0.22998    |
| AveragePolicyStd[5]  | 0.2633     |
| AverageReturn        | 932.4      |
| MinReturn            | 197.84     |
| MaxReturn            | 1028.7     |
| StdReturn            | 111.93     |
| AverageEpisodeLength | 975.04     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.84     |
| TotalNEpisodes       | 16001      |
| TotalNSamples        | 1.5528e+06 |
| ExplainedVariance    | 0.22948    |
-------------------------------------
[2018-12-22 10:38:36.492690 UTC] Saving snapshot
[2018-12-22 10:38:36.492936 UTC] Starting iteration 312
[2018-12-22 10:38:36.493066 UTC] Start collecting samples
[2018-12-22 10:38:39.513291 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:39.594536 UTC] Performing policy update
[2018-12-22 10:38:39.595134 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:39.683707 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:40.726739 UTC] Performing line search
[2018-12-22 10:38:40.851577 UTC] Updating baseline
[2018-12-22 10:38:42.107360 UTC] Computing logging information
-------------------------------------
| Iteration            | 312        |
| ExpectedImprovement  | 0.016453   |
| ActualImprovement    | 0.015854   |
| ImprovementRatio     | 0.9636     |
| MeanKL               | 0.0077669  |
| Entropy              | 1.1228     |
| Perplexity           | 3.0736     |
| AveragePolicyStd     | 0.29565    |
| AveragePolicyStd[0]  | 0.30665    |
| AveragePolicyStd[1]  | 0.36986    |
| AveragePolicyStd[2]  | 0.26618    |
| AveragePolicyStd[3]  | 0.33937    |
| AveragePolicyStd[4]  | 0.2295     |
| AveragePolicyStd[5]  | 0.26237    |
| AverageReturn        | 925.22     |
| MinReturn            | 197.84     |
| MaxReturn            | 1028.7     |
| StdReturn            | 128.25     |
| AverageEpisodeLength | 964.97     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.4      |
| TotalNEpisodes       | 16011      |
| TotalNSamples        | 1.5618e+06 |
| ExplainedVariance    | 0.31374    |
-------------------------------------
[2018-12-22 10:38:42.442092 UTC] Saving snapshot
[2018-12-22 10:38:42.442329 UTC] Starting iteration 313
[2018-12-22 10:38:42.442449 UTC] Start collecting samples
[2018-12-22 10:38:45.429227 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:45.507300 UTC] Performing policy update
[2018-12-22 10:38:45.508225 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:45.598171 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:46.648764 UTC] Performing line search
[2018-12-22 10:38:46.774547 UTC] Updating baseline
[2018-12-22 10:38:48.037764 UTC] Computing logging information
-------------------------------------
| Iteration            | 313        |
| ExpectedImprovement  | 0.015576   |
| ActualImprovement    | 0.015318   |
| ImprovementRatio     | 0.98342    |
| MeanKL               | 0.0070873  |
| Entropy              | 1.129      |
| Perplexity           | 3.0927     |
| AveragePolicyStd     | 0.29599    |
| AveragePolicyStd[0]  | 0.30684    |
| AveragePolicyStd[1]  | 0.36945    |
| AveragePolicyStd[2]  | 0.26603    |
| AveragePolicyStd[3]  | 0.34169    |
| AveragePolicyStd[4]  | 0.2298     |
| AveragePolicyStd[5]  | 0.26214    |
| AverageReturn        | 922.3      |
| MinReturn            | 197.84     |
| MaxReturn            | 1028.7     |
| StdReturn            | 135.43     |
| AverageEpisodeLength | 960.12     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.81     |
| TotalNEpisodes       | 16015      |
| TotalNSamples        | 1.5653e+06 |
| ExplainedVariance    | -0.032808  |
-------------------------------------
[2018-12-22 10:38:48.376151 UTC] Saving snapshot
[2018-12-22 10:38:48.376408 UTC] Starting iteration 314
[2018-12-22 10:38:48.376546 UTC] Start collecting samples
[2018-12-22 10:38:51.322518 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:51.398231 UTC] Performing policy update
[2018-12-22 10:38:51.398895 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:51.488129 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:52.538847 UTC] Performing line search
[2018-12-22 10:38:52.663196 UTC] Updating baseline
[2018-12-22 10:38:54.149860 UTC] Computing logging information
-------------------------------------
| Iteration            | 314        |
| ExpectedImprovement  | 0.017798   |
| ActualImprovement    | 0.016793   |
| ImprovementRatio     | 0.94357    |
| MeanKL               | 0.007081   |
| Entropy              | 1.1467     |
| Perplexity           | 3.1479     |
| AveragePolicyStd     | 0.29692    |
| AveragePolicyStd[0]  | 0.30894    |
| AveragePolicyStd[1]  | 0.37061    |
| AveragePolicyStd[2]  | 0.26639    |
| AveragePolicyStd[3]  | 0.34294    |
| AveragePolicyStd[4]  | 0.22999    |
| AveragePolicyStd[5]  | 0.26264    |
| AverageReturn        | 922.85     |
| MinReturn            | 197.84     |
| MaxReturn            | 1028.7     |
| StdReturn            | 135.56     |
| AverageEpisodeLength | 960.12     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.81     |
| TotalNEpisodes       | 16017      |
| TotalNSamples        | 1.5673e+06 |
| ExplainedVariance    | -0.014335  |
-------------------------------------
[2018-12-22 10:38:54.489624 UTC] Saving snapshot
[2018-12-22 10:38:54.489925 UTC] Starting iteration 315
[2018-12-22 10:38:54.490061 UTC] Start collecting samples
[2018-12-22 10:38:57.555416 UTC] Computing input variables for policy optimization
[2018-12-22 10:38:57.636513 UTC] Performing policy update
[2018-12-22 10:38:57.637156 UTC] Computing gradient in Euclidean space
[2018-12-22 10:38:57.727534 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:38:58.773515 UTC] Performing line search
[2018-12-22 10:38:58.900905 UTC] Updating baseline
[2018-12-22 10:39:00.259366 UTC] Computing logging information
-------------------------------------
| Iteration            | 315        |
| ExpectedImprovement  | 0.017379   |
| ActualImprovement    | 0.017038   |
| ImprovementRatio     | 0.98039    |
| MeanKL               | 0.0070972  |
| Entropy              | 1.1342     |
| Perplexity           | 3.1087     |
| AveragePolicyStd     | 0.29626    |
| AveragePolicyStd[0]  | 0.30722    |
| AveragePolicyStd[1]  | 0.3709     |
| AveragePolicyStd[2]  | 0.26539    |
| AveragePolicyStd[3]  | 0.34094    |
| AveragePolicyStd[4]  | 0.2303     |
| AveragePolicyStd[5]  | 0.26277    |
| AverageReturn        | 920.34     |
| MinReturn            | 64.048     |
| MaxReturn            | 1028.7     |
| StdReturn            | 161.8      |
| AverageEpisodeLength | 950.87     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.48     |
| TotalNEpisodes       | 16028      |
| TotalNSamples        | 1.5774e+06 |
| ExplainedVariance    | 0.035718   |
-------------------------------------
[2018-12-22 10:39:00.587905 UTC] Saving snapshot
[2018-12-22 10:39:00.588173 UTC] Starting iteration 316
[2018-12-22 10:39:00.588289 UTC] Start collecting samples
[2018-12-22 10:39:03.557946 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:03.633810 UTC] Performing policy update
[2018-12-22 10:39:03.634741 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:03.723085 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:04.770728 UTC] Performing line search
[2018-12-22 10:39:04.896537 UTC] Updating baseline
[2018-12-22 10:39:06.905166 UTC] Computing logging information
-------------------------------------
| Iteration            | 316        |
| ExpectedImprovement  | 0.015453   |
| ActualImprovement    | 0.014716   |
| ImprovementRatio     | 0.95231    |
| MeanKL               | 0.0074199  |
| Entropy              | 1.1265     |
| Perplexity           | 3.0849     |
| AveragePolicyStd     | 0.29579    |
| AveragePolicyStd[0]  | 0.30699    |
| AveragePolicyStd[1]  | 0.36829    |
| AveragePolicyStd[2]  | 0.26547    |
| AveragePolicyStd[3]  | 0.34102    |
| AveragePolicyStd[4]  | 0.2302     |
| AveragePolicyStd[5]  | 0.26279    |
| AverageReturn        | 920.86     |
| MinReturn            | 64.048     |
| MaxReturn            | 1028.7     |
| StdReturn            | 161.97     |
| AverageEpisodeLength | 950.87     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.48     |
| TotalNEpisodes       | 16031      |
| TotalNSamples        | 1.5804e+06 |
| ExplainedVariance    | 0.073991   |
-------------------------------------
[2018-12-22 10:39:07.239338 UTC] Saving snapshot
[2018-12-22 10:39:07.239603 UTC] Starting iteration 317
[2018-12-22 10:39:07.239729 UTC] Start collecting samples
[2018-12-22 10:39:10.220875 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:10.296750 UTC] Performing policy update
[2018-12-22 10:39:10.297492 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:10.386931 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:11.439354 UTC] Performing line search
[2018-12-22 10:39:11.564796 UTC] Updating baseline
[2018-12-22 10:39:13.187730 UTC] Computing logging information
-------------------------------------
| Iteration            | 317        |
| ExpectedImprovement  | 0.014727   |
| ActualImprovement    | 0.014049   |
| ImprovementRatio     | 0.95399    |
| MeanKL               | 0.0074664  |
| Entropy              | 1.1071     |
| Perplexity           | 3.0255     |
| AveragePolicyStd     | 0.29483    |
| AveragePolicyStd[0]  | 0.30497    |
| AveragePolicyStd[1]  | 0.36752    |
| AveragePolicyStd[2]  | 0.2661     |
| AveragePolicyStd[3]  | 0.33969    |
| AveragePolicyStd[4]  | 0.22919    |
| AveragePolicyStd[5]  | 0.26151    |
| AverageReturn        | 917.24     |
| MinReturn            | 64.048     |
| MaxReturn            | 1028.7     |
| StdReturn            | 164.37     |
| AverageEpisodeLength | 946.11     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.73     |
| TotalNEpisodes       | 16034      |
| TotalNSamples        | 1.5829e+06 |
| ExplainedVariance    | 0.34273    |
-------------------------------------
[2018-12-22 10:39:13.527195 UTC] Saving snapshot
[2018-12-22 10:39:13.527475 UTC] Starting iteration 318
[2018-12-22 10:39:13.527643 UTC] Start collecting samples
[2018-12-22 10:39:16.577755 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:16.657216 UTC] Performing policy update
[2018-12-22 10:39:16.657811 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:16.748761 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:17.788534 UTC] Performing line search
[2018-12-22 10:39:17.913697 UTC] Updating baseline
[2018-12-22 10:39:19.319411 UTC] Computing logging information
-------------------------------------
| Iteration            | 318        |
| ExpectedImprovement  | 0.01597    |
| ActualImprovement    | 0.014909   |
| ImprovementRatio     | 0.93353    |
| MeanKL               | 0.0070696  |
| Entropy              | 1.1115     |
| Perplexity           | 3.039      |
| AveragePolicyStd     | 0.29514    |
| AveragePolicyStd[0]  | 0.30599    |
| AveragePolicyStd[1]  | 0.36963    |
| AveragePolicyStd[2]  | 0.26599    |
| AveragePolicyStd[3]  | 0.33858    |
| AveragePolicyStd[4]  | 0.22801    |
| AveragePolicyStd[5]  | 0.26263    |
| AverageReturn        | 920.6      |
| MinReturn            | 64.048     |
| MaxReturn            | 1028.7     |
| StdReturn            | 165.56     |
| AverageEpisodeLength | 945.76     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.66     |
| TotalNEpisodes       | 16042      |
| TotalNSamples        | 1.5909e+06 |
| ExplainedVariance    | 0.031297   |
-------------------------------------
[2018-12-22 10:39:19.649460 UTC] Saving snapshot
[2018-12-22 10:39:19.649728 UTC] Starting iteration 319
[2018-12-22 10:39:19.649897 UTC] Start collecting samples
[2018-12-22 10:39:22.652282 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:22.731789 UTC] Performing policy update
[2018-12-22 10:39:22.732782 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:22.822932 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:23.885785 UTC] Performing line search
[2018-12-22 10:39:24.014755 UTC] Updating baseline
[2018-12-22 10:39:25.251198 UTC] Computing logging information
-------------------------------------
| Iteration            | 319        |
| ExpectedImprovement  | 0.014769   |
| ActualImprovement    | 0.014061   |
| ImprovementRatio     | 0.95207    |
| MeanKL               | 0.0069075  |
| Entropy              | 1.0908     |
| Perplexity           | 2.9768     |
| AveragePolicyStd     | 0.29421    |
| AveragePolicyStd[0]  | 0.30527    |
| AveragePolicyStd[1]  | 0.369      |
| AveragePolicyStd[2]  | 0.26474    |
| AveragePolicyStd[3]  | 0.33849    |
| AveragePolicyStd[4]  | 0.22688    |
| AveragePolicyStd[5]  | 0.26089    |
| AverageReturn        | 915.8      |
| MinReturn            | 64.048     |
| MaxReturn            | 1028.7     |
| StdReturn            | 176.43     |
| AverageEpisodeLength | 939.28     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.72     |
| TotalNEpisodes       | 16047      |
| TotalNSamples        | 1.5952e+06 |
| ExplainedVariance    | 0.22008    |
-------------------------------------
[2018-12-22 10:39:25.593273 UTC] Saving snapshot
[2018-12-22 10:39:25.593541 UTC] Starting iteration 320
[2018-12-22 10:39:25.593682 UTC] Start collecting samples
[2018-12-22 10:39:28.567404 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:28.645373 UTC] Performing policy update
[2018-12-22 10:39:28.646271 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:28.734797 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:29.785228 UTC] Performing line search
[2018-12-22 10:39:29.910737 UTC] Updating baseline
[2018-12-22 10:39:31.255779 UTC] Computing logging information
-------------------------------------
| Iteration            | 320        |
| ExpectedImprovement  | 0.020596   |
| ActualImprovement    | 0.018661   |
| ImprovementRatio     | 0.90602    |
| MeanKL               | 0.0070743  |
| Entropy              | 1.0918     |
| Perplexity           | 2.9795     |
| AveragePolicyStd     | 0.29413    |
| AveragePolicyStd[0]  | 0.30526    |
| AveragePolicyStd[1]  | 0.36844    |
| AveragePolicyStd[2]  | 0.26329    |
| AveragePolicyStd[3]  | 0.33676    |
| AveragePolicyStd[4]  | 0.22781    |
| AveragePolicyStd[5]  | 0.26324    |
| AverageReturn        | 916.49     |
| MinReturn            | 64.048     |
| MaxReturn            | 1028.7     |
| StdReturn            | 176.64     |
| AverageEpisodeLength | 939.28     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.72     |
| TotalNEpisodes       | 16050      |
| TotalNSamples        | 1.5982e+06 |
| ExplainedVariance    | 0.030642   |
-------------------------------------
[2018-12-22 10:39:31.594052 UTC] Saving snapshot
[2018-12-22 10:39:31.602148 UTC] Starting iteration 321
[2018-12-22 10:39:31.602361 UTC] Start collecting samples
[2018-12-22 10:39:34.620749 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:34.699758 UTC] Performing policy update
[2018-12-22 10:39:34.700515 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:34.789474 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:35.845192 UTC] Performing line search
[2018-12-22 10:39:35.970464 UTC] Updating baseline
[2018-12-22 10:39:37.413638 UTC] Computing logging information
-------------------------------------
| Iteration            | 321        |
| ExpectedImprovement  | 0.015284   |
| ActualImprovement    | 0.014333   |
| ImprovementRatio     | 0.93775    |
| MeanKL               | 0.0081471  |
| Entropy              | 1.0736     |
| Perplexity           | 2.926      |
| AveragePolicyStd     | 0.29337    |
| AveragePolicyStd[0]  | 0.30538    |
| AveragePolicyStd[1]  | 0.36826    |
| AveragePolicyStd[2]  | 0.2621     |
| AveragePolicyStd[3]  | 0.33652    |
| AveragePolicyStd[4]  | 0.22616    |
| AveragePolicyStd[5]  | 0.26179    |
| AverageReturn        | 923.33     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 174.2      |
| AverageEpisodeLength | 943.09     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.85     |
| TotalNEpisodes       | 16056      |
| TotalNSamples        | 1.6042e+06 |
| ExplainedVariance    | -0.011414  |
-------------------------------------
[2018-12-22 10:39:37.750783 UTC] Saving snapshot
[2018-12-22 10:39:37.751036 UTC] Starting iteration 322
[2018-12-22 10:39:37.751183 UTC] Start collecting samples
[2018-12-22 10:39:40.764814 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:40.844216 UTC] Performing policy update
[2018-12-22 10:39:40.845134 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:40.934881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:41.989256 UTC] Performing line search
[2018-12-22 10:39:42.114465 UTC] Updating baseline
[2018-12-22 10:39:44.052713 UTC] Computing logging information
-------------------------------------
| Iteration            | 322        |
| ExpectedImprovement  | 0.01636    |
| ActualImprovement    | 0.015511   |
| ImprovementRatio     | 0.94811    |
| MeanKL               | 0.0072006  |
| Entropy              | 1.0741     |
| Perplexity           | 2.9275     |
| AveragePolicyStd     | 0.29334    |
| AveragePolicyStd[0]  | 0.30584    |
| AveragePolicyStd[1]  | 0.3687     |
| AveragePolicyStd[2]  | 0.26366    |
| AveragePolicyStd[3]  | 0.33366    |
| AveragePolicyStd[4]  | 0.22539    |
| AveragePolicyStd[5]  | 0.26279    |
| AverageReturn        | 926.01     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 175.03     |
| AverageEpisodeLength | 943.09     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.85     |
| TotalNEpisodes       | 16063      |
| TotalNSamples        | 1.6112e+06 |
| ExplainedVariance    | -0.0018055 |
-------------------------------------
[2018-12-22 10:39:44.391493 UTC] Saving snapshot
[2018-12-22 10:39:44.391752 UTC] Starting iteration 323
[2018-12-22 10:39:44.391871 UTC] Start collecting samples
[2018-12-22 10:39:47.374348 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:47.449833 UTC] Performing policy update
[2018-12-22 10:39:47.450614 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:47.539010 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:48.594024 UTC] Performing line search
[2018-12-22 10:39:48.718671 UTC] Updating baseline
[2018-12-22 10:39:51.436203 UTC] Computing logging information
-------------------------------------
| Iteration            | 323        |
| ExpectedImprovement  | 0.013652   |
| ActualImprovement    | 0.012927   |
| ImprovementRatio     | 0.94686    |
| MeanKL               | 0.0077898  |
| Entropy              | 1.0762     |
| Perplexity           | 2.9334     |
| AveragePolicyStd     | 0.29346    |
| AveragePolicyStd[0]  | 0.30764    |
| AveragePolicyStd[1]  | 0.36814    |
| AveragePolicyStd[2]  | 0.26375    |
| AveragePolicyStd[3]  | 0.33378    |
| AveragePolicyStd[4]  | 0.22493    |
| AveragePolicyStd[5]  | 0.26255    |
| AverageReturn        | 926.97     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 175.23     |
| AverageEpisodeLength | 943.09     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.85     |
| TotalNEpisodes       | 16066      |
| TotalNSamples        | 1.6142e+06 |
| ExplainedVariance    | 8.488e-06  |
-------------------------------------
[2018-12-22 10:39:51.769926 UTC] Saving snapshot
[2018-12-22 10:39:51.770168 UTC] Starting iteration 324
[2018-12-22 10:39:51.770287 UTC] Start collecting samples
[2018-12-22 10:39:54.769753 UTC] Computing input variables for policy optimization
[2018-12-22 10:39:54.846465 UTC] Performing policy update
[2018-12-22 10:39:54.848446 UTC] Computing gradient in Euclidean space
[2018-12-22 10:39:54.937796 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:39:55.993849 UTC] Performing line search
[2018-12-22 10:39:56.119427 UTC] Updating baseline
[2018-12-22 10:39:57.884792 UTC] Computing logging information
-------------------------------------
| Iteration            | 324        |
| ExpectedImprovement  | 0.020409   |
| ActualImprovement    | 0.018229   |
| ImprovementRatio     | 0.89321    |
| MeanKL               | 0.0067239  |
| Entropy              | 1.0735     |
| Perplexity           | 2.9257     |
| AveragePolicyStd     | 0.29331    |
| AveragePolicyStd[0]  | 0.30957    |
| AveragePolicyStd[1]  | 0.3677     |
| AveragePolicyStd[2]  | 0.2638     |
| AveragePolicyStd[3]  | 0.33183    |
| AveragePolicyStd[4]  | 0.22488    |
| AveragePolicyStd[5]  | 0.26208    |
| AverageReturn        | 924.24     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 175.87     |
| AverageEpisodeLength | 939.54     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.56     |
| TotalNEpisodes       | 16070      |
| TotalNSamples        | 1.6179e+06 |
| ExplainedVariance    | 0.2208     |
-------------------------------------
[2018-12-22 10:39:58.222397 UTC] Saving snapshot
[2018-12-22 10:39:58.222655 UTC] Starting iteration 325
[2018-12-22 10:39:58.222776 UTC] Start collecting samples
[2018-12-22 10:40:01.254973 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:01.336757 UTC] Performing policy update
[2018-12-22 10:40:01.337415 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:01.426594 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:02.476027 UTC] Performing line search
[2018-12-22 10:40:02.600418 UTC] Updating baseline
[2018-12-22 10:40:04.182742 UTC] Computing logging information
-------------------------------------
| Iteration            | 325        |
| ExpectedImprovement  | 0.014739   |
| ActualImprovement    | 0.013963   |
| ImprovementRatio     | 0.94733    |
| MeanKL               | 0.0074373  |
| Entropy              | 1.0693     |
| Perplexity           | 2.9134     |
| AveragePolicyStd     | 0.29311    |
| AveragePolicyStd[0]  | 0.3108     |
| AveragePolicyStd[1]  | 0.3679     |
| AveragePolicyStd[2]  | 0.26332    |
| AveragePolicyStd[3]  | 0.33035    |
| AveragePolicyStd[4]  | 0.22516    |
| AveragePolicyStd[5]  | 0.2611     |
| AverageReturn        | 935.7      |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 159.5      |
| AverageEpisodeLength | 949.14     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.23     |
| TotalNEpisodes       | 16079      |
| TotalNSamples        | 1.6269e+06 |
| ExplainedVariance    | -0.028406  |
-------------------------------------
[2018-12-22 10:40:04.519682 UTC] Saving snapshot
[2018-12-22 10:40:04.519937 UTC] Starting iteration 326
[2018-12-22 10:40:04.520054 UTC] Start collecting samples
[2018-12-22 10:40:07.476264 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:07.551572 UTC] Performing policy update
[2018-12-22 10:40:07.552312 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:07.641072 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:08.708080 UTC] Performing line search
[2018-12-22 10:40:08.832162 UTC] Updating baseline
[2018-12-22 10:40:10.232503 UTC] Computing logging information
-------------------------------------
| Iteration            | 326        |
| ExpectedImprovement  | 0.014397   |
| ActualImprovement    | 0.01352    |
| ImprovementRatio     | 0.93903    |
| MeanKL               | 0.00689    |
| Entropy              | 1.0535     |
| Perplexity           | 2.8676     |
| AveragePolicyStd     | 0.29246    |
| AveragePolicyStd[0]  | 0.31082    |
| AveragePolicyStd[1]  | 0.36778    |
| AveragePolicyStd[2]  | 0.26199    |
| AveragePolicyStd[3]  | 0.32989    |
| AveragePolicyStd[4]  | 0.22263    |
| AveragePolicyStd[5]  | 0.26166    |
| AverageReturn        | 935.83     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 159.54     |
| AverageEpisodeLength | 949.14     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.23     |
| TotalNEpisodes       | 16081      |
| TotalNSamples        | 1.6289e+06 |
| ExplainedVariance    | 0.013158   |
-------------------------------------
[2018-12-22 10:40:10.567457 UTC] Saving snapshot
[2018-12-22 10:40:10.567722 UTC] Starting iteration 327
[2018-12-22 10:40:10.567845 UTC] Start collecting samples
[2018-12-22 10:40:13.578861 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:13.656783 UTC] Performing policy update
[2018-12-22 10:40:13.657415 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:13.746048 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:14.786175 UTC] Performing line search
[2018-12-22 10:40:14.912327 UTC] Updating baseline
[2018-12-22 10:40:16.321071 UTC] Computing logging information
-------------------------------------
| Iteration            | 327        |
| ExpectedImprovement  | 0.01435    |
| ActualImprovement    | 0.013596   |
| ImprovementRatio     | 0.94742    |
| MeanKL               | 0.0068966  |
| Entropy              | 1.0262     |
| Perplexity           | 2.7904     |
| AveragePolicyStd     | 0.29118    |
| AveragePolicyStd[0]  | 0.31024    |
| AveragePolicyStd[1]  | 0.36792    |
| AveragePolicyStd[2]  | 0.25876    |
| AveragePolicyStd[3]  | 0.32756    |
| AveragePolicyStd[4]  | 0.22303    |
| AveragePolicyStd[5]  | 0.25956    |
| AverageReturn        | 918.52     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 197.91     |
| AverageEpisodeLength | 931.35     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.61     |
| TotalNEpisodes       | 16088      |
| TotalNSamples        | 1.6341e+06 |
| ExplainedVariance    | 0.13968    |
-------------------------------------
[2018-12-22 10:40:16.650414 UTC] Saving snapshot
[2018-12-22 10:40:16.650667 UTC] Starting iteration 328
[2018-12-22 10:40:16.650787 UTC] Start collecting samples
[2018-12-22 10:40:19.686352 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:19.766939 UTC] Performing policy update
[2018-12-22 10:40:19.767650 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:19.855486 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:20.857979 UTC] Performing line search
[2018-12-22 10:40:20.983256 UTC] Updating baseline
[2018-12-22 10:40:22.294600 UTC] Computing logging information
-------------------------------------
| Iteration            | 328        |
| ExpectedImprovement  | 0.014295   |
| ActualImprovement    | 0.013562   |
| ImprovementRatio     | 0.94872    |
| MeanKL               | 0.0077293  |
| Entropy              | 1.0203     |
| Perplexity           | 2.7741     |
| AveragePolicyStd     | 0.29096    |
| AveragePolicyStd[0]  | 0.31061    |
| AveragePolicyStd[1]  | 0.36755    |
| AveragePolicyStd[2]  | 0.25849    |
| AveragePolicyStd[3]  | 0.32843    |
| AveragePolicyStd[4]  | 0.2228     |
| AveragePolicyStd[5]  | 0.25786    |
| AverageReturn        | 927.03     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 187.53     |
| AverageEpisodeLength | 940.58     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.23     |
| TotalNEpisodes       | 16096      |
| TotalNSamples        | 1.6421e+06 |
| ExplainedVariance    | 0.0038559  |
-------------------------------------
[2018-12-22 10:40:22.626512 UTC] Saving snapshot
[2018-12-22 10:40:22.626761 UTC] Starting iteration 329
[2018-12-22 10:40:22.626894 UTC] Start collecting samples
[2018-12-22 10:40:25.601467 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:25.677786 UTC] Performing policy update
[2018-12-22 10:40:25.678743 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:25.764743 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:26.771152 UTC] Performing line search
[2018-12-22 10:40:26.895734 UTC] Updating baseline
[2018-12-22 10:40:28.270434 UTC] Computing logging information
-------------------------------------
| Iteration            | 329        |
| ExpectedImprovement  | 0.014093   |
| ActualImprovement    | 0.013628   |
| ImprovementRatio     | 0.96701    |
| MeanKL               | 0.0074461  |
| Entropy              | 1.0177     |
| Perplexity           | 2.7669     |
| AveragePolicyStd     | 0.29087    |
| AveragePolicyStd[0]  | 0.31144    |
| AveragePolicyStd[1]  | 0.36887    |
| AveragePolicyStd[2]  | 0.25877    |
| AveragePolicyStd[3]  | 0.32662    |
| AveragePolicyStd[4]  | 0.22285    |
| AveragePolicyStd[5]  | 0.25666    |
| AverageReturn        | 926.51     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 187.34     |
| AverageEpisodeLength | 940.58     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.23     |
| TotalNEpisodes       | 16098      |
| TotalNSamples        | 1.6441e+06 |
| ExplainedVariance    | -0.037124  |
-------------------------------------
[2018-12-22 10:40:28.603978 UTC] Saving snapshot
[2018-12-22 10:40:28.604219 UTC] Starting iteration 330
[2018-12-22 10:40:28.604352 UTC] Start collecting samples
[2018-12-22 10:40:31.615442 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:31.692861 UTC] Performing policy update
[2018-12-22 10:40:31.693433 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:31.784179 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:32.786765 UTC] Performing line search
[2018-12-22 10:40:32.912784 UTC] Updating baseline
[2018-12-22 10:40:34.574072 UTC] Computing logging information
-------------------------------------
| Iteration            | 330        |
| ExpectedImprovement  | 0.013577   |
| ActualImprovement    | 0.012425   |
| ImprovementRatio     | 0.91513    |
| MeanKL               | 0.0069613  |
| Entropy              | 1.0346     |
| Perplexity           | 2.8139     |
| AveragePolicyStd     | 0.29184    |
| AveragePolicyStd[0]  | 0.31257    |
| AveragePolicyStd[1]  | 0.37173    |
| AveragePolicyStd[2]  | 0.25862    |
| AveragePolicyStd[3]  | 0.32819    |
| AveragePolicyStd[4]  | 0.22232    |
| AveragePolicyStd[5]  | 0.25759    |
| AverageReturn        | 930.23     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 186.32     |
| AverageEpisodeLength | 944.57     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.27     |
| TotalNEpisodes       | 16103      |
| TotalNSamples        | 1.6491e+06 |
| ExplainedVariance    | 0.017789   |
-------------------------------------
[2018-12-22 10:40:34.909618 UTC] Saving snapshot
[2018-12-22 10:40:34.917700 UTC] Starting iteration 331
[2018-12-22 10:40:34.917930 UTC] Start collecting samples
[2018-12-22 10:40:37.929793 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:38.010141 UTC] Performing policy update
[2018-12-22 10:40:38.010863 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:38.097615 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:39.091848 UTC] Performing line search
[2018-12-22 10:40:39.216236 UTC] Updating baseline
[2018-12-22 10:40:40.531601 UTC] Computing logging information
-------------------------------------
| Iteration            | 331        |
| ExpectedImprovement  | 0.01542    |
| ActualImprovement    | 0.014188   |
| ImprovementRatio     | 0.92012    |
| MeanKL               | 0.0078263  |
| Entropy              | 1.0333     |
| Perplexity           | 2.8104     |
| AveragePolicyStd     | 0.29188    |
| AveragePolicyStd[0]  | 0.31223    |
| AveragePolicyStd[1]  | 0.37333    |
| AveragePolicyStd[2]  | 0.25607    |
| AveragePolicyStd[3]  | 0.32905    |
| AveragePolicyStd[4]  | 0.22281    |
| AveragePolicyStd[5]  | 0.25776    |
| AverageReturn        | 930.61     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 193.62     |
| AverageEpisodeLength | 944.18     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.38     |
| TotalNEpisodes       | 16111      |
| TotalNSamples        | 1.6562e+06 |
| ExplainedVariance    | 0.10374    |
-------------------------------------
[2018-12-22 10:40:40.868226 UTC] Saving snapshot
[2018-12-22 10:40:40.868494 UTC] Starting iteration 332
[2018-12-22 10:40:40.868650 UTC] Start collecting samples
[2018-12-22 10:40:43.852781 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:43.929646 UTC] Performing policy update
[2018-12-22 10:40:43.930290 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:44.016730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:45.071364 UTC] Performing line search
[2018-12-22 10:40:45.196827 UTC] Updating baseline
[2018-12-22 10:40:46.634386 UTC] Computing logging information
-------------------------------------
| Iteration            | 332        |
| ExpectedImprovement  | 0.014622   |
| ActualImprovement    | 0.013965   |
| ImprovementRatio     | 0.95501    |
| MeanKL               | 0.0078352  |
| Entropy              | 1.0453     |
| Perplexity           | 2.8442     |
| AveragePolicyStd     | 0.29243    |
| AveragePolicyStd[0]  | 0.31321    |
| AveragePolicyStd[1]  | 0.37426    |
| AveragePolicyStd[2]  | 0.25716    |
| AveragePolicyStd[3]  | 0.32806    |
| AveragePolicyStd[4]  | 0.22275    |
| AveragePolicyStd[5]  | 0.25915    |
| AverageReturn        | 934.32     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 188.67     |
| AverageEpisodeLength | 949.03     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.6      |
| TotalNEpisodes       | 16115      |
| TotalNSamples        | 1.6602e+06 |
| ExplainedVariance    | -0.022198  |
-------------------------------------
[2018-12-22 10:40:46.969510 UTC] Saving snapshot
[2018-12-22 10:40:46.969790 UTC] Starting iteration 333
[2018-12-22 10:40:46.969933 UTC] Start collecting samples
[2018-12-22 10:40:49.954250 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:50.032406 UTC] Performing policy update
[2018-12-22 10:40:50.033095 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:50.121914 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:51.123597 UTC] Performing line search
[2018-12-22 10:40:51.248617 UTC] Updating baseline
[2018-12-22 10:40:52.602497 UTC] Computing logging information
-------------------------------------
| Iteration            | 333        |
| ExpectedImprovement  | 0.012676   |
| ActualImprovement    | 0.012505   |
| ImprovementRatio     | 0.98648    |
| MeanKL               | 0.007849   |
| Entropy              | 1.0504     |
| Perplexity           | 2.8588     |
| AveragePolicyStd     | 0.29272    |
| AveragePolicyStd[0]  | 0.31447    |
| AveragePolicyStd[1]  | 0.3756     |
| AveragePolicyStd[2]  | 0.2555     |
| AveragePolicyStd[3]  | 0.32722    |
| AveragePolicyStd[4]  | 0.22317    |
| AveragePolicyStd[5]  | 0.26036    |
| AverageReturn        | 934.25     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 188.64     |
| AverageEpisodeLength | 949.03     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.6      |
| TotalNEpisodes       | 16119      |
| TotalNSamples        | 1.6642e+06 |
| ExplainedVariance    | 0.044338   |
-------------------------------------
[2018-12-22 10:40:52.939861 UTC] Saving snapshot
[2018-12-22 10:40:52.940170 UTC] Starting iteration 334
[2018-12-22 10:40:52.940291 UTC] Start collecting samples
[2018-12-22 10:40:55.950876 UTC] Computing input variables for policy optimization
[2018-12-22 10:40:56.029864 UTC] Performing policy update
[2018-12-22 10:40:56.030484 UTC] Computing gradient in Euclidean space
[2018-12-22 10:40:56.115505 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:40:57.158764 UTC] Performing line search
[2018-12-22 10:40:57.284648 UTC] Updating baseline
[2018-12-22 10:40:58.635358 UTC] Computing logging information
-------------------------------------
| Iteration            | 334        |
| ExpectedImprovement  | 0.015371   |
| ActualImprovement    | 0.014731   |
| ImprovementRatio     | 0.95837    |
| MeanKL               | 0.0073613  |
| Entropy              | 1.0584     |
| Perplexity           | 2.8819     |
| AveragePolicyStd     | 0.29319    |
| AveragePolicyStd[0]  | 0.31466    |
| AveragePolicyStd[1]  | 0.3767     |
| AveragePolicyStd[2]  | 0.25673    |
| AveragePolicyStd[3]  | 0.32837    |
| AveragePolicyStd[4]  | 0.22246    |
| AveragePolicyStd[5]  | 0.2602     |
| AverageReturn        | 933.38     |
| MinReturn            | 64.048     |
| MaxReturn            | 1029.7     |
| StdReturn            | 188.4      |
| AverageEpisodeLength | 949.03     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.6      |
| TotalNEpisodes       | 16125      |
| TotalNSamples        | 1.6702e+06 |
| ExplainedVariance    | 0.0041902  |
-------------------------------------
[2018-12-22 10:40:58.971142 UTC] Saving snapshot
[2018-12-22 10:40:58.971382 UTC] Starting iteration 335
[2018-12-22 10:40:58.971499 UTC] Start collecting samples
[2018-12-22 10:41:02.087301 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:02.168971 UTC] Performing policy update
[2018-12-22 10:41:02.169795 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:02.262835 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:03.365223 UTC] Performing line search
[2018-12-22 10:41:03.496452 UTC] Updating baseline
[2018-12-22 10:41:04.818866 UTC] Computing logging information
-------------------------------------
| Iteration            | 335        |
| ExpectedImprovement  | 0.014453   |
| ActualImprovement    | 0.013514   |
| ImprovementRatio     | 0.93504    |
| MeanKL               | 0.0075824  |
| Entropy              | 1.054      |
| Perplexity           | 2.8691     |
| AveragePolicyStd     | 0.29293    |
| AveragePolicyStd[0]  | 0.31372    |
| AveragePolicyStd[1]  | 0.37742    |
| AveragePolicyStd[2]  | 0.25639    |
| AveragePolicyStd[3]  | 0.32695    |
| AveragePolicyStd[4]  | 0.2236     |
| AveragePolicyStd[5]  | 0.25947    |
| AverageReturn        | 942.05     |
| MinReturn            | 90.868     |
| MaxReturn            | 1029.7     |
| StdReturn            | 166.82     |
| AverageEpisodeLength | 958.28     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.08     |
| TotalNEpisodes       | 16130      |
| TotalNSamples        | 1.6752e+06 |
| ExplainedVariance    | -0.012353  |
-------------------------------------
[2018-12-22 10:41:05.187773 UTC] Saving snapshot
[2018-12-22 10:41:05.188055 UTC] Starting iteration 336
[2018-12-22 10:41:05.188176 UTC] Start collecting samples
[2018-12-22 10:41:08.402928 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:08.484415 UTC] Performing policy update
[2018-12-22 10:41:08.485170 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:08.578142 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:09.670182 UTC] Performing line search
[2018-12-22 10:41:09.802639 UTC] Updating baseline
[2018-12-22 10:41:11.276176 UTC] Computing logging information
------------------------------------
| Iteration            | 336       |
| ExpectedImprovement  | 0.018327  |
| ActualImprovement    | 0.016431  |
| ImprovementRatio     | 0.89653   |
| MeanKL               | 0.00715   |
| Entropy              | 1.0506    |
| Perplexity           | 2.8593    |
| AveragePolicyStd     | 0.29277   |
| AveragePolicyStd[0]  | 0.31286   |
| AveragePolicyStd[1]  | 0.37833   |
| AveragePolicyStd[2]  | 0.25653   |
| AveragePolicyStd[3]  | 0.32632   |
| AveragePolicyStd[4]  | 0.2239    |
| AveragePolicyStd[5]  | 0.25869   |
| AverageReturn        | 943.67    |
| MinReturn            | 90.868    |
| MaxReturn            | 1029.7    |
| StdReturn            | 164.9     |
| AverageEpisodeLength | 960.93    |
| MinEpisodeLength     | 108       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 165.36    |
| TotalNEpisodes       | 16134     |
| TotalNSamples        | 1.679e+06 |
| ExplainedVariance    | 0.17714   |
------------------------------------
[2018-12-22 10:41:11.615991 UTC] Saving snapshot
[2018-12-22 10:41:11.616242 UTC] Starting iteration 337
[2018-12-22 10:41:11.616360 UTC] Start collecting samples
[2018-12-22 10:41:14.633445 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:14.710725 UTC] Performing policy update
[2018-12-22 10:41:14.711501 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:14.799880 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:15.851424 UTC] Performing line search
[2018-12-22 10:41:15.977934 UTC] Updating baseline
[2018-12-22 10:41:17.241162 UTC] Computing logging information
------------------------------------
| Iteration            | 337       |
| ExpectedImprovement  | 0.016103  |
| ActualImprovement    | 0.015276  |
| ImprovementRatio     | 0.94864   |
| MeanKL               | 0.006989  |
| Entropy              | 1.0461    |
| Perplexity           | 2.8466    |
| AveragePolicyStd     | 0.2926    |
| AveragePolicyStd[0]  | 0.31197   |
| AveragePolicyStd[1]  | 0.3795    |
| AveragePolicyStd[2]  | 0.2558    |
| AveragePolicyStd[3]  | 0.326     |
| AveragePolicyStd[4]  | 0.22406   |
| AveragePolicyStd[5]  | 0.25829   |
| AverageReturn        | 943.59    |
| MinReturn            | 90.868    |
| MaxReturn            | 1029.7    |
| StdReturn            | 164.91    |
| AverageEpisodeLength | 960.93    |
| MinEpisodeLength     | 108       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 165.36    |
| TotalNEpisodes       | 16139     |
| TotalNSamples        | 1.684e+06 |
| ExplainedVariance    | -0.057007 |
------------------------------------
[2018-12-22 10:41:17.577712 UTC] Saving snapshot
[2018-12-22 10:41:17.578007 UTC] Starting iteration 338
[2018-12-22 10:41:17.578128 UTC] Start collecting samples
[2018-12-22 10:41:20.610993 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:20.691227 UTC] Performing policy update
[2018-12-22 10:41:20.691874 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:20.780795 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:21.829282 UTC] Performing line search
[2018-12-22 10:41:21.958173 UTC] Updating baseline
[2018-12-22 10:41:23.662073 UTC] Computing logging information
-------------------------------------
| Iteration            | 338        |
| ExpectedImprovement  | 0.014505   |
| ActualImprovement    | 0.013894   |
| ImprovementRatio     | 0.95787    |
| MeanKL               | 0.0073745  |
| Entropy              | 1.0292     |
| Perplexity           | 2.7989     |
| AveragePolicyStd     | 0.2918     |
| AveragePolicyStd[0]  | 0.31105    |
| AveragePolicyStd[1]  | 0.37831    |
| AveragePolicyStd[2]  | 0.25299    |
| AveragePolicyStd[3]  | 0.32607    |
| AveragePolicyStd[4]  | 0.22386    |
| AveragePolicyStd[5]  | 0.25852    |
| AverageReturn        | 940.91     |
| MinReturn            | 90.868     |
| MaxReturn            | 1029.7     |
| StdReturn            | 173.09     |
| AverageEpisodeLength | 959.02     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.98     |
| TotalNEpisodes       | 16147      |
| TotalNSamples        | 1.6911e+06 |
| ExplainedVariance    | 0.069748   |
-------------------------------------
[2018-12-22 10:41:24.003018 UTC] Saving snapshot
[2018-12-22 10:41:24.003285 UTC] Starting iteration 339
[2018-12-22 10:41:24.003417 UTC] Start collecting samples
[2018-12-22 10:41:26.971036 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:27.048539 UTC] Performing policy update
[2018-12-22 10:41:27.049300 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:27.138848 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:28.205107 UTC] Performing line search
[2018-12-22 10:41:28.330567 UTC] Updating baseline
[2018-12-22 10:41:29.687111 UTC] Computing logging information
-------------------------------------
| Iteration            | 339        |
| ExpectedImprovement  | 0.01598    |
| ActualImprovement    | 0.015134   |
| ImprovementRatio     | 0.94708    |
| MeanKL               | 0.0077106  |
| Entropy              | 1.0314     |
| Perplexity           | 2.805      |
| AveragePolicyStd     | 0.29188    |
| AveragePolicyStd[0]  | 0.3128     |
| AveragePolicyStd[1]  | 0.37731    |
| AveragePolicyStd[2]  | 0.25297    |
| AveragePolicyStd[3]  | 0.32549    |
| AveragePolicyStd[4]  | 0.22359    |
| AveragePolicyStd[5]  | 0.25911    |
| AverageReturn        | 941.7      |
| MinReturn            | 90.868     |
| MaxReturn            | 1029.7     |
| StdReturn            | 173.35     |
| AverageEpisodeLength | 959.02     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.98     |
| TotalNEpisodes       | 16151      |
| TotalNSamples        | 1.6951e+06 |
| ExplainedVariance    | 0.038471   |
-------------------------------------
[2018-12-22 10:41:30.031140 UTC] Saving snapshot
[2018-12-22 10:41:30.031510 UTC] Starting iteration 340
[2018-12-22 10:41:30.031688 UTC] Start collecting samples
[2018-12-22 10:41:33.029272 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:33.107490 UTC] Performing policy update
[2018-12-22 10:41:33.108205 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:33.197839 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:34.263787 UTC] Performing line search
[2018-12-22 10:41:34.389390 UTC] Updating baseline
[2018-12-22 10:41:36.107004 UTC] Computing logging information
-------------------------------------
| Iteration            | 340        |
| ExpectedImprovement  | 0.014211   |
| ActualImprovement    | 0.012877   |
| ImprovementRatio     | 0.90615    |
| MeanKL               | 0.0074061  |
| Entropy              | 1.021      |
| Perplexity           | 2.7758     |
| AveragePolicyStd     | 0.29148    |
| AveragePolicyStd[0]  | 0.3114     |
| AveragePolicyStd[1]  | 0.37885    |
| AveragePolicyStd[2]  | 0.25192    |
| AveragePolicyStd[3]  | 0.32484    |
| AveragePolicyStd[4]  | 0.22268    |
| AveragePolicyStd[5]  | 0.25917    |
| AverageReturn        | 926.92     |
| MinReturn            | 90.868     |
| MaxReturn            | 1027.7     |
| StdReturn            | 196.72     |
| AverageEpisodeLength | 944.37     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.81     |
| TotalNEpisodes       | 16156      |
| TotalNSamples        | 1.6986e+06 |
| ExplainedVariance    | 0.4718     |
-------------------------------------
[2018-12-22 10:41:36.445711 UTC] Saving snapshot
[2018-12-22 10:41:36.453878 UTC] Starting iteration 341
[2018-12-22 10:41:36.454097 UTC] Start collecting samples
[2018-12-22 10:41:39.484610 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:39.564891 UTC] Performing policy update
[2018-12-22 10:41:39.565713 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:39.654847 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:40.710170 UTC] Performing line search
[2018-12-22 10:41:40.836135 UTC] Updating baseline
[2018-12-22 10:41:42.193646 UTC] Computing logging information
-------------------------------------
| Iteration            | 341        |
| ExpectedImprovement  | 0.015474   |
| ActualImprovement    | 0.014536   |
| ImprovementRatio     | 0.9394     |
| MeanKL               | 0.0071404  |
| Entropy              | 1.0234     |
| Perplexity           | 2.7827     |
| AveragePolicyStd     | 0.29175    |
| AveragePolicyStd[0]  | 0.31209    |
| AveragePolicyStd[1]  | 0.37971    |
| AveragePolicyStd[2]  | 0.25061    |
| AveragePolicyStd[3]  | 0.32651    |
| AveragePolicyStd[4]  | 0.2213     |
| AveragePolicyStd[5]  | 0.26028    |
| AverageReturn        | 927.55     |
| MinReturn            | 90.868     |
| MaxReturn            | 1027.7     |
| StdReturn            | 196.98     |
| AverageEpisodeLength | 944.37     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.81     |
| TotalNEpisodes       | 16164      |
| TotalNSamples        | 1.7066e+06 |
| ExplainedVariance    | -0.0096348 |
-------------------------------------
[2018-12-22 10:41:42.530599 UTC] Saving snapshot
[2018-12-22 10:41:42.530857 UTC] Starting iteration 342
[2018-12-22 10:41:42.530980 UTC] Start collecting samples
[2018-12-22 10:41:45.485385 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:45.562029 UTC] Performing policy update
[2018-12-22 10:41:45.562955 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:45.651907 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:46.703232 UTC] Performing line search
[2018-12-22 10:41:46.828898 UTC] Updating baseline
[2018-12-22 10:41:48.316811 UTC] Computing logging information
-------------------------------------
| Iteration            | 342        |
| ExpectedImprovement  | 0.013659   |
| ActualImprovement    | 0.012948   |
| ImprovementRatio     | 0.94794    |
| MeanKL               | 0.0068555  |
| Entropy              | 1.0176     |
| Perplexity           | 2.7666     |
| AveragePolicyStd     | 0.29156    |
| AveragePolicyStd[0]  | 0.31214    |
| AveragePolicyStd[1]  | 0.38099    |
| AveragePolicyStd[2]  | 0.24952    |
| AveragePolicyStd[3]  | 0.32584    |
| AveragePolicyStd[4]  | 0.22102    |
| AveragePolicyStd[5]  | 0.25984    |
| AverageReturn        | 927.78     |
| MinReturn            | 90.868     |
| MaxReturn            | 1027.7     |
| StdReturn            | 197.06     |
| AverageEpisodeLength | 944.37     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.81     |
| TotalNEpisodes       | 16166      |
| TotalNSamples        | 1.7086e+06 |
| ExplainedVariance    | -0.048009  |
-------------------------------------
[2018-12-22 10:41:48.657957 UTC] Saving snapshot
[2018-12-22 10:41:48.658203 UTC] Starting iteration 343
[2018-12-22 10:41:48.658346 UTC] Start collecting samples
[2018-12-22 10:41:51.623980 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:51.700655 UTC] Performing policy update
[2018-12-22 10:41:51.701587 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:51.789318 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:52.829931 UTC] Performing line search
[2018-12-22 10:41:52.953965 UTC] Updating baseline
[2018-12-22 10:41:54.287217 UTC] Computing logging information
-------------------------------------
| Iteration            | 343        |
| ExpectedImprovement  | 0.01705    |
| ActualImprovement    | 0.016026   |
| ImprovementRatio     | 0.93997    |
| MeanKL               | 0.0075844  |
| Entropy              | 1.027      |
| Perplexity           | 2.7928     |
| AveragePolicyStd     | 0.29204    |
| AveragePolicyStd[0]  | 0.31178    |
| AveragePolicyStd[1]  | 0.38262    |
| AveragePolicyStd[2]  | 0.25237    |
| AveragePolicyStd[3]  | 0.3255     |
| AveragePolicyStd[4]  | 0.22034    |
| AveragePolicyStd[5]  | 0.25962    |
| AverageReturn        | 932.35     |
| MinReturn            | 90.868     |
| MaxReturn            | 1027.7     |
| StdReturn            | 196.98     |
| AverageEpisodeLength | 947.92     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.11     |
| TotalNEpisodes       | 16171      |
| TotalNSamples        | 1.7136e+06 |
| ExplainedVariance    | -0.034804  |
-------------------------------------
[2018-12-22 10:41:54.618897 UTC] Saving snapshot
[2018-12-22 10:41:54.619147 UTC] Starting iteration 344
[2018-12-22 10:41:54.619267 UTC] Start collecting samples
[2018-12-22 10:41:57.616360 UTC] Computing input variables for policy optimization
[2018-12-22 10:41:57.696789 UTC] Performing policy update
[2018-12-22 10:41:57.697439 UTC] Computing gradient in Euclidean space
[2018-12-22 10:41:57.789494 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:41:58.827985 UTC] Performing line search
[2018-12-22 10:41:58.952413 UTC] Updating baseline
[2018-12-22 10:42:00.551949 UTC] Computing logging information
-------------------------------------
| Iteration            | 344        |
| ExpectedImprovement  | 0.014125   |
| ActualImprovement    | 0.013649   |
| ImprovementRatio     | 0.96633    |
| MeanKL               | 0.0075357  |
| Entropy              | 1.0203     |
| Perplexity           | 2.774      |
| AveragePolicyStd     | 0.29185    |
| AveragePolicyStd[0]  | 0.31295    |
| AveragePolicyStd[1]  | 0.38394    |
| AveragePolicyStd[2]  | 0.25204    |
| AveragePolicyStd[3]  | 0.32436    |
| AveragePolicyStd[4]  | 0.21919    |
| AveragePolicyStd[5]  | 0.2586     |
| AverageReturn        | 933.48     |
| MinReturn            | 90.868     |
| MaxReturn            | 1028.2     |
| StdReturn            | 197.35     |
| AverageEpisodeLength | 947.92     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.11     |
| TotalNEpisodes       | 16180      |
| TotalNSamples        | 1.7226e+06 |
| ExplainedVariance    | 0.0089375  |
-------------------------------------
[2018-12-22 10:42:00.888332 UTC] Saving snapshot
[2018-12-22 10:42:00.888593 UTC] Starting iteration 345
[2018-12-22 10:42:00.888714 UTC] Start collecting samples
[2018-12-22 10:42:03.835177 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:03.910798 UTC] Performing policy update
[2018-12-22 10:42:03.911371 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:04.002362 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:05.043203 UTC] Performing line search
[2018-12-22 10:42:05.171260 UTC] Updating baseline
[2018-12-22 10:42:06.864416 UTC] Computing logging information
-------------------------------------
| Iteration            | 345        |
| ExpectedImprovement  | 0.016444   |
| ActualImprovement    | 0.01506    |
| ImprovementRatio     | 0.91581    |
| MeanKL               | 0.0074749  |
| Entropy              | 1.021      |
| Perplexity           | 2.776      |
| AveragePolicyStd     | 0.29174    |
| AveragePolicyStd[0]  | 0.31492    |
| AveragePolicyStd[1]  | 0.38103    |
| AveragePolicyStd[2]  | 0.25149    |
| AveragePolicyStd[3]  | 0.32335    |
| AveragePolicyStd[4]  | 0.21956    |
| AveragePolicyStd[5]  | 0.26006    |
| AverageReturn        | 933.18     |
| MinReturn            | 90.868     |
| MaxReturn            | 1028.2     |
| StdReturn            | 197.28     |
| AverageEpisodeLength | 947.92     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.11     |
| TotalNEpisodes       | 16181      |
| TotalNSamples        | 1.7236e+06 |
| ExplainedVariance    | 0.13277    |
-------------------------------------
[2018-12-22 10:42:07.201121 UTC] Saving snapshot
[2018-12-22 10:42:07.201361 UTC] Starting iteration 346
[2018-12-22 10:42:07.201483 UTC] Start collecting samples
[2018-12-22 10:42:10.304638 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:10.388647 UTC] Performing policy update
[2018-12-22 10:42:10.389266 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:10.485998 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:11.593914 UTC] Performing line search
[2018-12-22 10:42:11.724448 UTC] Updating baseline
[2018-12-22 10:42:13.015497 UTC] Computing logging information
-------------------------------------
| Iteration            | 346        |
| ExpectedImprovement  | 0.015115   |
| ActualImprovement    | 0.013916   |
| ImprovementRatio     | 0.92068    |
| MeanKL               | 0.0070998  |
| Entropy              | 1.0117     |
| Perplexity           | 2.7503     |
| AveragePolicyStd     | 0.29131    |
| AveragePolicyStd[0]  | 0.31267    |
| AveragePolicyStd[1]  | 0.38165    |
| AveragePolicyStd[2]  | 0.24991    |
| AveragePolicyStd[3]  | 0.32304    |
| AveragePolicyStd[4]  | 0.21922    |
| AveragePolicyStd[5]  | 0.26139    |
| AverageReturn        | 942.82     |
| MinReturn            | 90.868     |
| MaxReturn            | 1028.2     |
| StdReturn            | 178.92     |
| AverageEpisodeLength | 956.79     |
| MinEpisodeLength     | 108        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.51     |
| TotalNEpisodes       | 16187      |
| TotalNSamples        | 1.7296e+06 |
| ExplainedVariance    | 0.029064   |
-------------------------------------
[2018-12-22 10:42:13.381202 UTC] Saving snapshot
[2018-12-22 10:42:13.381445 UTC] Starting iteration 347
[2018-12-22 10:42:13.381630 UTC] Start collecting samples
[2018-12-22 10:42:16.498350 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:16.576563 UTC] Performing policy update
[2018-12-22 10:42:16.577267 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:16.665093 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:17.714885 UTC] Performing line search
[2018-12-22 10:42:17.842895 UTC] Updating baseline
[2018-12-22 10:42:19.185090 UTC] Computing logging information
-------------------------------------
| Iteration            | 347        |
| ExpectedImprovement  | 0.014086   |
| ActualImprovement    | 0.013421   |
| ImprovementRatio     | 0.95275    |
| MeanKL               | 0.0073877  |
| Entropy              | 1.0053     |
| Perplexity           | 2.7328     |
| AveragePolicyStd     | 0.29101    |
| AveragePolicyStd[0]  | 0.31084    |
| AveragePolicyStd[1]  | 0.37953    |
| AveragePolicyStd[2]  | 0.24766    |
| AveragePolicyStd[3]  | 0.32635    |
| AveragePolicyStd[4]  | 0.21906    |
| AveragePolicyStd[5]  | 0.2626     |
| AverageReturn        | 953.57     |
| MinReturn            | 94.157     |
| MaxReturn            | 1028.2     |
| StdReturn            | 157.41     |
| AverageEpisodeLength | 965.71     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.98     |
| TotalNEpisodes       | 16194      |
| TotalNSamples        | 1.7366e+06 |
| ExplainedVariance    | 0.036313   |
-------------------------------------
[2018-12-22 10:42:19.525714 UTC] Saving snapshot
[2018-12-22 10:42:19.525995 UTC] Starting iteration 348
[2018-12-22 10:42:19.526112 UTC] Start collecting samples
[2018-12-22 10:42:22.481201 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:22.556812 UTC] Performing policy update
[2018-12-22 10:42:22.557487 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:22.646789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:23.701879 UTC] Performing line search
[2018-12-22 10:42:23.831653 UTC] Updating baseline
[2018-12-22 10:42:29.195004 UTC] Computing logging information
-------------------------------------
| Iteration            | 348        |
| ExpectedImprovement  | 0.016141   |
| ActualImprovement    | 0.015082   |
| ImprovementRatio     | 0.93437    |
| MeanKL               | 0.0079179  |
| Entropy              | 1.0094     |
| Perplexity           | 2.744      |
| AveragePolicyStd     | 0.29134    |
| AveragePolicyStd[0]  | 0.31212    |
| AveragePolicyStd[1]  | 0.38151    |
| AveragePolicyStd[2]  | 0.24743    |
| AveragePolicyStd[3]  | 0.32716    |
| AveragePolicyStd[4]  | 0.21939    |
| AveragePolicyStd[5]  | 0.26044    |
| AverageReturn        | 953.48     |
| MinReturn            | 94.157     |
| MaxReturn            | 1028.2     |
| StdReturn            | 157.39     |
| AverageEpisodeLength | 965.71     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.98     |
| TotalNEpisodes       | 16197      |
| TotalNSamples        | 1.7396e+06 |
| ExplainedVariance    | 0.063605   |
-------------------------------------
[2018-12-22 10:42:29.537708 UTC] Saving snapshot
[2018-12-22 10:42:29.538003 UTC] Starting iteration 349
[2018-12-22 10:42:29.538124 UTC] Start collecting samples
[2018-12-22 10:42:32.558299 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:32.636192 UTC] Performing policy update
[2018-12-22 10:42:32.636906 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:32.726578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:33.786584 UTC] Performing line search
[2018-12-22 10:42:33.913395 UTC] Updating baseline
[2018-12-22 10:42:35.692224 UTC] Computing logging information
-------------------------------------
| Iteration            | 349        |
| ExpectedImprovement  | 0.015792   |
| ActualImprovement    | 0.015169   |
| ImprovementRatio     | 0.96052    |
| MeanKL               | 0.0068147  |
| Entropy              | 0.98941    |
| Perplexity           | 2.6896     |
| AveragePolicyStd     | 0.29037    |
| AveragePolicyStd[0]  | 0.31044    |
| AveragePolicyStd[1]  | 0.38039    |
| AveragePolicyStd[2]  | 0.24729    |
| AveragePolicyStd[3]  | 0.32652    |
| AveragePolicyStd[4]  | 0.21855    |
| AveragePolicyStd[5]  | 0.25905    |
| AverageReturn        | 955.23     |
| MinReturn            | 94.157     |
| MaxReturn            | 1028.2     |
| StdReturn            | 157.83     |
| AverageEpisodeLength | 965.71     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.98     |
| TotalNEpisodes       | 16203      |
| TotalNSamples        | 1.7456e+06 |
| ExplainedVariance    | 0.082184   |
-------------------------------------
[2018-12-22 10:42:36.033027 UTC] Saving snapshot
[2018-12-22 10:42:36.033274 UTC] Starting iteration 350
[2018-12-22 10:42:36.033409 UTC] Start collecting samples
[2018-12-22 10:42:38.993701 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:39.070293 UTC] Performing policy update
[2018-12-22 10:42:39.071065 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:39.160188 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:40.217542 UTC] Performing line search
[2018-12-22 10:42:40.341538 UTC] Updating baseline
[2018-12-22 10:42:42.094960 UTC] Computing logging information
-------------------------------------
| Iteration            | 350        |
| ExpectedImprovement  | 0.013865   |
| ActualImprovement    | 0.013117   |
| ImprovementRatio     | 0.94605    |
| MeanKL               | 0.0074336  |
| Entropy              | 0.98988    |
| Perplexity           | 2.6909     |
| AveragePolicyStd     | 0.29043    |
| AveragePolicyStd[0]  | 0.31029    |
| AveragePolicyStd[1]  | 0.38131    |
| AveragePolicyStd[2]  | 0.24736    |
| AveragePolicyStd[3]  | 0.32615    |
| AveragePolicyStd[4]  | 0.21853    |
| AveragePolicyStd[5]  | 0.25892    |
| AverageReturn        | 965.06     |
| MinReturn            | 94.157     |
| MaxReturn            | 1051.6     |
| StdReturn            | 134.97     |
| AverageEpisodeLength | 974.5      |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.26     |
| TotalNEpisodes       | 16207      |
| TotalNSamples        | 1.7496e+06 |
| ExplainedVariance    | 0.0057937  |
-------------------------------------
[2018-12-22 10:42:42.430607 UTC] Saving snapshot
[2018-12-22 10:42:42.438713 UTC] Starting iteration 351
[2018-12-22 10:42:42.438929 UTC] Start collecting samples
[2018-12-22 10:42:45.410028 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:45.487847 UTC] Performing policy update
[2018-12-22 10:42:45.488414 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:45.576633 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:46.629407 UTC] Performing line search
[2018-12-22 10:42:46.753803 UTC] Updating baseline
[2018-12-22 10:42:48.164434 UTC] Computing logging information
-------------------------------------
| Iteration            | 351        |
| ExpectedImprovement  | 0.016301   |
| ActualImprovement    | 0.015143   |
| ImprovementRatio     | 0.92896    |
| MeanKL               | 0.0073688  |
| Entropy              | 0.98287    |
| Perplexity           | 2.6721     |
| AveragePolicyStd     | 0.29008    |
| AveragePolicyStd[0]  | 0.31067    |
| AveragePolicyStd[1]  | 0.38246    |
| AveragePolicyStd[2]  | 0.24729    |
| AveragePolicyStd[3]  | 0.32242    |
| AveragePolicyStd[4]  | 0.2183     |
| AveragePolicyStd[5]  | 0.25933    |
| AverageReturn        | 967.64     |
| MinReturn            | 94.157     |
| MaxReturn            | 1051.6     |
| StdReturn            | 135.55     |
| AverageEpisodeLength | 974.5      |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.26     |
| TotalNEpisodes       | 16213      |
| TotalNSamples        | 1.7556e+06 |
| ExplainedVariance    | -0.021196  |
-------------------------------------
[2018-12-22 10:42:48.504969 UTC] Saving snapshot
[2018-12-22 10:42:48.505222 UTC] Starting iteration 352
[2018-12-22 10:42:48.505343 UTC] Start collecting samples
[2018-12-22 10:42:51.428299 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:51.502638 UTC] Performing policy update
[2018-12-22 10:42:51.503269 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:51.591582 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:52.626924 UTC] Performing line search
[2018-12-22 10:42:52.752828 UTC] Updating baseline
[2018-12-22 10:42:54.341398 UTC] Computing logging information
-------------------------------------
| Iteration            | 352        |
| ExpectedImprovement  | 0.013096   |
| ActualImprovement    | 0.012548   |
| ImprovementRatio     | 0.9582     |
| MeanKL               | 0.0078924  |
| Entropy              | 0.9662     |
| Perplexity           | 2.6279     |
| AveragePolicyStd     | 0.28915    |
| AveragePolicyStd[0]  | 0.30931    |
| AveragePolicyStd[1]  | 0.37989    |
| AveragePolicyStd[2]  | 0.2461     |
| AveragePolicyStd[3]  | 0.32162    |
| AveragePolicyStd[4]  | 0.21885    |
| AveragePolicyStd[5]  | 0.25914    |
| AverageReturn        | 968.74     |
| MinReturn            | 94.157     |
| MaxReturn            | 1051.6     |
| StdReturn            | 135.68     |
| AverageEpisodeLength | 974.5      |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.26     |
| TotalNEpisodes       | 16215      |
| TotalNSamples        | 1.7576e+06 |
| ExplainedVariance    | 0.054044   |
-------------------------------------
[2018-12-22 10:42:54.672713 UTC] Saving snapshot
[2018-12-22 10:42:54.672969 UTC] Starting iteration 353
[2018-12-22 10:42:54.673086 UTC] Start collecting samples
[2018-12-22 10:42:57.686789 UTC] Computing input variables for policy optimization
[2018-12-22 10:42:57.766327 UTC] Performing policy update
[2018-12-22 10:42:57.767216 UTC] Computing gradient in Euclidean space
[2018-12-22 10:42:57.857293 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:42:58.909040 UTC] Performing line search
[2018-12-22 10:42:59.033295 UTC] Updating baseline
[2018-12-22 10:43:00.726288 UTC] Computing logging information
-------------------------------------
| Iteration            | 353        |
| ExpectedImprovement  | 0.017587   |
| ActualImprovement    | 0.01643    |
| ImprovementRatio     | 0.93419    |
| MeanKL               | 0.0072484  |
| Entropy              | 0.96965    |
| Perplexity           | 2.637      |
| AveragePolicyStd     | 0.28919    |
| AveragePolicyStd[0]  | 0.30804    |
| AveragePolicyStd[1]  | 0.37946    |
| AveragePolicyStd[2]  | 0.24749    |
| AveragePolicyStd[3]  | 0.32054    |
| AveragePolicyStd[4]  | 0.21914    |
| AveragePolicyStd[5]  | 0.26047    |
| AverageReturn        | 962.27     |
| MinReturn            | 79.806     |
| MaxReturn            | 1051.6     |
| StdReturn            | 162.62     |
| AverageEpisodeLength | 965.45     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.39     |
| TotalNEpisodes       | 16223      |
| TotalNSamples        | 1.7647e+06 |
| ExplainedVariance    | 0.075332   |
-------------------------------------
[2018-12-22 10:43:01.067111 UTC] Saving snapshot
[2018-12-22 10:43:01.067363 UTC] Starting iteration 354
[2018-12-22 10:43:01.067484 UTC] Start collecting samples
[2018-12-22 10:43:04.069114 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:04.146910 UTC] Performing policy update
[2018-12-22 10:43:04.147796 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:04.236620 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:05.283491 UTC] Performing line search
[2018-12-22 10:43:05.407744 UTC] Updating baseline
[2018-12-22 10:43:06.835223 UTC] Computing logging information
-------------------------------------
| Iteration            | 354        |
| ExpectedImprovement  | 0.015337   |
| ActualImprovement    | 0.014915   |
| ImprovementRatio     | 0.97246    |
| MeanKL               | 0.0078569  |
| Entropy              | 0.95982    |
| Perplexity           | 2.6112     |
| AveragePolicyStd     | 0.28874    |
| AveragePolicyStd[0]  | 0.3088     |
| AveragePolicyStd[1]  | 0.37796    |
| AveragePolicyStd[2]  | 0.24715    |
| AveragePolicyStd[3]  | 0.32019    |
| AveragePolicyStd[4]  | 0.21783    |
| AveragePolicyStd[5]  | 0.2605     |
| AverageReturn        | 964.87     |
| MinReturn            | 79.806     |
| MaxReturn            | 1051.6     |
| StdReturn            | 163.08     |
| AverageEpisodeLength | 965.45     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.39     |
| TotalNEpisodes       | 16229      |
| TotalNSamples        | 1.7707e+06 |
| ExplainedVariance    | -0.062975  |
-------------------------------------
[2018-12-22 10:43:07.175440 UTC] Saving snapshot
[2018-12-22 10:43:07.175701 UTC] Starting iteration 355
[2018-12-22 10:43:07.175818 UTC] Start collecting samples
[2018-12-22 10:43:10.128282 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:10.205101 UTC] Performing policy update
[2018-12-22 10:43:10.205869 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:10.294121 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:11.337129 UTC] Performing line search
[2018-12-22 10:43:11.464270 UTC] Updating baseline
[2018-12-22 10:43:13.430664 UTC] Computing logging information
-------------------------------------
| Iteration            | 355        |
| ExpectedImprovement  | 0.016037   |
| ActualImprovement    | 0.015007   |
| ImprovementRatio     | 0.93578    |
| MeanKL               | 0.0072463  |
| Entropy              | 0.94865    |
| Perplexity           | 2.5822     |
| AveragePolicyStd     | 0.28825    |
| AveragePolicyStd[0]  | 0.3076     |
| AveragePolicyStd[1]  | 0.37772    |
| AveragePolicyStd[2]  | 0.24648    |
| AveragePolicyStd[3]  | 0.32072    |
| AveragePolicyStd[4]  | 0.21745    |
| AveragePolicyStd[5]  | 0.2595     |
| AverageReturn        | 969.3      |
| MinReturn            | 79.806     |
| MaxReturn            | 1077.3     |
| StdReturn            | 162.38     |
| AverageEpisodeLength | 967.56     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.43     |
| TotalNEpisodes       | 16232      |
| TotalNSamples        | 1.7737e+06 |
| ExplainedVariance    | 0.00028039 |
-------------------------------------
[2018-12-22 10:43:13.769291 UTC] Saving snapshot
[2018-12-22 10:43:13.769595 UTC] Starting iteration 356
[2018-12-22 10:43:13.769717 UTC] Start collecting samples
[2018-12-22 10:43:16.737090 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:16.814284 UTC] Performing policy update
[2018-12-22 10:43:16.814878 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:16.902946 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:17.945727 UTC] Performing line search
[2018-12-22 10:43:18.071130 UTC] Updating baseline
[2018-12-22 10:43:19.323512 UTC] Computing logging information
-------------------------------------
| Iteration            | 356        |
| ExpectedImprovement  | 0.013359   |
| ActualImprovement    | 0.01283    |
| ImprovementRatio     | 0.96043    |
| MeanKL               | 0.0076826  |
| Entropy              | 0.9396     |
| Perplexity           | 2.559      |
| AveragePolicyStd     | 0.28794    |
| AveragePolicyStd[0]  | 0.30617    |
| AveragePolicyStd[1]  | 0.37811    |
| AveragePolicyStd[2]  | 0.24527    |
| AveragePolicyStd[3]  | 0.32274    |
| AveragePolicyStd[4]  | 0.21676    |
| AveragePolicyStd[5]  | 0.25857    |
| AverageReturn        | 971.06     |
| MinReturn            | 79.806     |
| MaxReturn            | 1077.3     |
| StdReturn            | 162.8      |
| AverageEpisodeLength | 967.56     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.43     |
| TotalNEpisodes       | 16237      |
| TotalNSamples        | 1.7787e+06 |
| ExplainedVariance    | 0.0090229  |
-------------------------------------
[2018-12-22 10:43:19.666344 UTC] Saving snapshot
[2018-12-22 10:43:19.666612 UTC] Starting iteration 357
[2018-12-22 10:43:19.666732 UTC] Start collecting samples
[2018-12-22 10:43:22.680246 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:22.759582 UTC] Performing policy update
[2018-12-22 10:43:22.760175 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:22.850166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:23.897696 UTC] Performing line search
[2018-12-22 10:43:24.023262 UTC] Updating baseline
[2018-12-22 10:43:25.472432 UTC] Computing logging information
-------------------------------------
| Iteration            | 357        |
| ExpectedImprovement  | 0.015422   |
| ActualImprovement    | 0.014531   |
| ImprovementRatio     | 0.94224    |
| MeanKL               | 0.007149   |
| Entropy              | 0.94762    |
| Perplexity           | 2.5796     |
| AveragePolicyStd     | 0.28832    |
| AveragePolicyStd[0]  | 0.30617    |
| AveragePolicyStd[1]  | 0.37802    |
| AveragePolicyStd[2]  | 0.24604    |
| AveragePolicyStd[3]  | 0.32385    |
| AveragePolicyStd[4]  | 0.21643    |
| AveragePolicyStd[5]  | 0.25941    |
| AverageReturn        | 982.3      |
| MinReturn            | 79.806     |
| MaxReturn            | 1077.3     |
| StdReturn            | 139.1      |
| AverageEpisodeLength | 976.3      |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.99     |
| TotalNEpisodes       | 16245      |
| TotalNSamples        | 1.7867e+06 |
| ExplainedVariance    | -0.017879  |
-------------------------------------
[2018-12-22 10:43:25.816797 UTC] Saving snapshot
[2018-12-22 10:43:25.817060 UTC] Starting iteration 358
[2018-12-22 10:43:25.817180 UTC] Start collecting samples
[2018-12-22 10:43:28.785432 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:28.861954 UTC] Performing policy update
[2018-12-22 10:43:28.862709 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:28.951439 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:29.882848 UTC] Performing line search
[2018-12-22 10:43:29.997278 UTC] Updating baseline
[2018-12-22 10:43:31.440690 UTC] Computing logging information
-------------------------------------
| Iteration            | 358        |
| ExpectedImprovement  | 0.01769    |
| ActualImprovement    | 0.016727   |
| ImprovementRatio     | 0.94553    |
| MeanKL               | 0.0075631  |
| Entropy              | 0.94363    |
| Perplexity           | 2.5693     |
| AveragePolicyStd     | 0.28803    |
| AveragePolicyStd[0]  | 0.30555    |
| AveragePolicyStd[1]  | 0.37737    |
| AveragePolicyStd[2]  | 0.24777    |
| AveragePolicyStd[3]  | 0.3229     |
| AveragePolicyStd[4]  | 0.21712    |
| AveragePolicyStd[5]  | 0.25746    |
| AverageReturn        | 975.07     |
| MinReturn            | 79.806     |
| MaxReturn            | 1077.3     |
| StdReturn            | 161.64     |
| AverageEpisodeLength | 968.04     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.93     |
| TotalNEpisodes       | 16248      |
| TotalNSamples        | 1.7889e+06 |
| ExplainedVariance    | 0.17467    |
-------------------------------------
[2018-12-22 10:43:31.781083 UTC] Saving snapshot
[2018-12-22 10:43:31.781348 UTC] Starting iteration 359
[2018-12-22 10:43:31.781467 UTC] Start collecting samples
[2018-12-22 10:43:34.774108 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:34.852121 UTC] Performing policy update
[2018-12-22 10:43:34.854008 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:34.942569 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:35.991001 UTC] Performing line search
[2018-12-22 10:43:36.116775 UTC] Updating baseline
[2018-12-22 10:43:37.712739 UTC] Computing logging information
-------------------------------------
| Iteration            | 359        |
| ExpectedImprovement  | 0.015088   |
| ActualImprovement    | 0.013897   |
| ImprovementRatio     | 0.92108    |
| MeanKL               | 0.0070825  |
| Entropy              | 0.94723    |
| Perplexity           | 2.5786     |
| AveragePolicyStd     | 0.28826    |
| AveragePolicyStd[0]  | 0.30676    |
| AveragePolicyStd[1]  | 0.37869    |
| AveragePolicyStd[2]  | 0.2477     |
| AveragePolicyStd[3]  | 0.32181    |
| AveragePolicyStd[4]  | 0.21691    |
| AveragePolicyStd[5]  | 0.25768    |
| AverageReturn        | 985.91     |
| MinReturn            | 79.806     |
| MaxReturn            | 1077.3     |
| StdReturn            | 135.62     |
| AverageEpisodeLength | 976.95     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.87     |
| TotalNEpisodes       | 16253      |
| TotalNSamples        | 1.7939e+06 |
| ExplainedVariance    | -0.019039  |
-------------------------------------
[2018-12-22 10:43:38.061007 UTC] Saving snapshot
[2018-12-22 10:43:38.061251 UTC] Starting iteration 360
[2018-12-22 10:43:38.061374 UTC] Start collecting samples
[2018-12-22 10:43:41.085600 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:41.166875 UTC] Performing policy update
[2018-12-22 10:43:41.167717 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:41.258889 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:42.322939 UTC] Performing line search
[2018-12-22 10:43:42.448689 UTC] Updating baseline
[2018-12-22 10:43:44.733985 UTC] Computing logging information
-------------------------------------
| Iteration            | 360        |
| ExpectedImprovement  | 0.01606    |
| ActualImprovement    | 0.015593   |
| ImprovementRatio     | 0.97093    |
| MeanKL               | 0.0068782  |
| Entropy              | 0.9375     |
| Perplexity           | 2.5536     |
| AveragePolicyStd     | 0.28777    |
| AveragePolicyStd[0]  | 0.30714    |
| AveragePolicyStd[1]  | 0.37577    |
| AveragePolicyStd[2]  | 0.24718    |
| AveragePolicyStd[3]  | 0.32276    |
| AveragePolicyStd[4]  | 0.21554    |
| AveragePolicyStd[5]  | 0.25825    |
| AverageReturn        | 993.44     |
| MinReturn            | 79.806     |
| MaxReturn            | 1081.2     |
| StdReturn            | 127.29     |
| AverageEpisodeLength | 982.69     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.3      |
| TotalNEpisodes       | 16262      |
| TotalNSamples        | 1.8029e+06 |
| ExplainedVariance    | 0.00019911 |
-------------------------------------
[2018-12-22 10:43:45.073824 UTC] Saving snapshot
[2018-12-22 10:43:45.081963 UTC] Starting iteration 361
[2018-12-22 10:43:45.082167 UTC] Start collecting samples
[2018-12-22 10:43:48.035128 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:48.111879 UTC] Performing policy update
[2018-12-22 10:43:48.112560 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:48.202041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:49.248992 UTC] Performing line search
[2018-12-22 10:43:49.378321 UTC] Updating baseline
[2018-12-22 10:43:51.069932 UTC] Computing logging information
-------------------------------------
| Iteration            | 361        |
| ExpectedImprovement  | 0.018587   |
| ActualImprovement    | 0.016732   |
| ImprovementRatio     | 0.9002     |
| MeanKL               | 0.0068567  |
| Entropy              | 0.938      |
| Perplexity           | 2.5549     |
| AveragePolicyStd     | 0.28781    |
| AveragePolicyStd[0]  | 0.30928    |
| AveragePolicyStd[1]  | 0.37541    |
| AveragePolicyStd[2]  | 0.24662    |
| AveragePolicyStd[3]  | 0.3219     |
| AveragePolicyStd[4]  | 0.21563    |
| AveragePolicyStd[5]  | 0.258      |
| AverageReturn        | 985.19     |
| MinReturn            | 79.806     |
| MaxReturn            | 1081.2     |
| StdReturn            | 149.59     |
| AverageEpisodeLength | 974.78     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.64     |
| TotalNEpisodes       | 16264      |
| TotalNSamples        | 1.8041e+06 |
| ExplainedVariance    | 0.20292    |
-------------------------------------
[2018-12-22 10:43:51.408630 UTC] Saving snapshot
[2018-12-22 10:43:51.408901 UTC] Starting iteration 362
[2018-12-22 10:43:51.409040 UTC] Start collecting samples
[2018-12-22 10:43:54.390631 UTC] Computing input variables for policy optimization
[2018-12-22 10:43:54.467708 UTC] Performing policy update
[2018-12-22 10:43:54.468466 UTC] Computing gradient in Euclidean space
[2018-12-22 10:43:54.559618 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:43:55.607445 UTC] Performing line search
[2018-12-22 10:43:55.732441 UTC] Updating baseline
[2018-12-22 10:43:57.052715 UTC] Computing logging information
-------------------------------------
| Iteration            | 362        |
| ExpectedImprovement  | 0.017049   |
| ActualImprovement    | 0.015853   |
| ImprovementRatio     | 0.92986    |
| MeanKL               | 0.0068075  |
| Entropy              | 0.93721    |
| Perplexity           | 2.5528     |
| AveragePolicyStd     | 0.28784    |
| AveragePolicyStd[0]  | 0.30761    |
| AveragePolicyStd[1]  | 0.37714    |
| AveragePolicyStd[2]  | 0.24573    |
| AveragePolicyStd[3]  | 0.32188    |
| AveragePolicyStd[4]  | 0.21526    |
| AveragePolicyStd[5]  | 0.25941    |
| AverageReturn        | 982.45     |
| MinReturn            | 79.806     |
| MaxReturn            | 1081.2     |
| StdReturn            | 154.05     |
| AverageEpisodeLength | 970.94     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.99     |
| TotalNEpisodes       | 16270      |
| TotalNSamples        | 1.8097e+06 |
| ExplainedVariance    | 0.22395    |
-------------------------------------
[2018-12-22 10:43:57.388462 UTC] Saving snapshot
[2018-12-22 10:43:57.388722 UTC] Starting iteration 363
[2018-12-22 10:43:57.388851 UTC] Start collecting samples
[2018-12-22 10:44:00.379976 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:00.458516 UTC] Performing policy update
[2018-12-22 10:44:00.459112 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:00.547403 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:01.592166 UTC] Performing line search
[2018-12-22 10:44:01.715967 UTC] Updating baseline
[2018-12-22 10:44:03.059206 UTC] Computing logging information
------------------------------------
| Iteration            | 363       |
| ExpectedImprovement  | 0.016739  |
| ActualImprovement    | 0.016105  |
| ImprovementRatio     | 0.96213   |
| MeanKL               | 0.0076613 |
| Entropy              | 0.93118   |
| Perplexity           | 2.5375    |
| AveragePolicyStd     | 0.28753   |
| AveragePolicyStd[0]  | 0.30719   |
| AveragePolicyStd[1]  | 0.37611   |
| AveragePolicyStd[2]  | 0.24544   |
| AveragePolicyStd[3]  | 0.32104   |
| AveragePolicyStd[4]  | 0.21419   |
| AveragePolicyStd[5]  | 0.26119   |
| AverageReturn        | 977.49    |
| MinReturn            | 79.806    |
| MaxReturn            | 1097.7    |
| StdReturn            | 169.61    |
| AverageEpisodeLength | 963.95    |
| MinEpisodeLength     | 95        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.27    |
| TotalNEpisodes       | 16277     |
| TotalNSamples        | 1.816e+06 |
| ExplainedVariance    | 0.10189   |
------------------------------------
[2018-12-22 10:44:03.398285 UTC] Saving snapshot
[2018-12-22 10:44:03.398537 UTC] Starting iteration 364
[2018-12-22 10:44:03.398678 UTC] Start collecting samples
[2018-12-22 10:44:06.371277 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:06.446960 UTC] Performing policy update
[2018-12-22 10:44:06.447785 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:06.536131 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:07.581455 UTC] Performing line search
[2018-12-22 10:44:07.706577 UTC] Updating baseline
[2018-12-22 10:44:08.965912 UTC] Computing logging information
------------------------------------
| Iteration            | 364       |
| ExpectedImprovement  | 0.019164  |
| ActualImprovement    | 0.018641  |
| ImprovementRatio     | 0.97273   |
| MeanKL               | 0.0071114 |
| Entropy              | 0.93716   |
| Perplexity           | 2.5527    |
| AveragePolicyStd     | 0.2879    |
| AveragePolicyStd[0]  | 0.3084    |
| AveragePolicyStd[1]  | 0.37718   |
| AveragePolicyStd[2]  | 0.24546   |
| AveragePolicyStd[3]  | 0.32241   |
| AveragePolicyStd[4]  | 0.21458   |
| AveragePolicyStd[5]  | 0.25939   |
| AverageReturn        | 978.92    |
| MinReturn            | 79.806    |
| MaxReturn            | 1097.7    |
| StdReturn            | 169.94    |
| AverageEpisodeLength | 963.95    |
| MinEpisodeLength     | 95        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.27    |
| TotalNEpisodes       | 16280     |
| TotalNSamples        | 1.819e+06 |
| ExplainedVariance    | -0.019146 |
------------------------------------
[2018-12-22 10:44:09.313985 UTC] Saving snapshot
[2018-12-22 10:44:09.314229 UTC] Starting iteration 365
[2018-12-22 10:44:09.314350 UTC] Start collecting samples
[2018-12-22 10:44:12.313398 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:12.392909 UTC] Performing policy update
[2018-12-22 10:44:12.393624 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:12.482981 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:13.542200 UTC] Performing line search
[2018-12-22 10:44:13.669392 UTC] Updating baseline
[2018-12-22 10:44:15.019946 UTC] Computing logging information
------------------------------------
| Iteration            | 365       |
| ExpectedImprovement  | 0.014313  |
| ActualImprovement    | 0.01383   |
| ImprovementRatio     | 0.96621   |
| MeanKL               | 0.0074094 |
| Entropy              | 0.9322    |
| Perplexity           | 2.5401    |
| AveragePolicyStd     | 0.28769   |
| AveragePolicyStd[0]  | 0.30796   |
| AveragePolicyStd[1]  | 0.37782   |
| AveragePolicyStd[2]  | 0.24544   |
| AveragePolicyStd[3]  | 0.32166   |
| AveragePolicyStd[4]  | 0.21433   |
| AveragePolicyStd[5]  | 0.25896   |
| AverageReturn        | 980.76    |
| MinReturn            | 79.806    |
| MaxReturn            | 1097.7    |
| StdReturn            | 170.4     |
| AverageEpisodeLength | 963.32    |
| MinEpisodeLength     | 95        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.25    |
| TotalNEpisodes       | 16286     |
| TotalNSamples        | 1.825e+06 |
| ExplainedVariance    | 0.15028   |
------------------------------------
[2018-12-22 10:44:15.354248 UTC] Saving snapshot
[2018-12-22 10:44:15.354508 UTC] Starting iteration 366
[2018-12-22 10:44:15.354645 UTC] Start collecting samples
[2018-12-22 10:44:18.330497 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:18.409339 UTC] Performing policy update
[2018-12-22 10:44:18.410171 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:18.493881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:19.467103 UTC] Performing line search
[2018-12-22 10:44:19.581918 UTC] Updating baseline
[2018-12-22 10:44:21.002522 UTC] Computing logging information
------------------------------------
| Iteration            | 366       |
| ExpectedImprovement  | 0.016663  |
| ActualImprovement    | 0.015554  |
| ImprovementRatio     | 0.93347   |
| MeanKL               | 0.0072394 |
| Entropy              | 0.92902   |
| Perplexity           | 2.532     |
| AveragePolicyStd     | 0.28757   |
| AveragePolicyStd[0]  | 0.30744   |
| AveragePolicyStd[1]  | 0.37786   |
| AveragePolicyStd[2]  | 0.24572   |
| AveragePolicyStd[3]  | 0.32203   |
| AveragePolicyStd[4]  | 0.21403   |
| AveragePolicyStd[5]  | 0.25832   |
| AverageReturn        | 982.4     |
| MinReturn            | 79.806    |
| MaxReturn            | 1097.7    |
| StdReturn            | 170.62    |
| AverageEpisodeLength | 963.32    |
| MinEpisodeLength     | 95        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.25    |
| TotalNEpisodes       | 16292     |
| TotalNSamples        | 1.831e+06 |
| ExplainedVariance    | 0.079864  |
------------------------------------
[2018-12-22 10:44:21.344406 UTC] Saving snapshot
[2018-12-22 10:44:21.344666 UTC] Starting iteration 367
[2018-12-22 10:44:21.344789 UTC] Start collecting samples
[2018-12-22 10:44:24.316078 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:24.392184 UTC] Performing policy update
[2018-12-22 10:44:24.392949 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:24.482262 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:25.535995 UTC] Performing line search
[2018-12-22 10:44:25.663761 UTC] Updating baseline
[2018-12-22 10:44:27.072082 UTC] Computing logging information
------------------------------------
| Iteration            | 367       |
| ExpectedImprovement  | 0.015431  |
| ActualImprovement    | 0.014821  |
| ImprovementRatio     | 0.96046   |
| MeanKL               | 0.00743   |
| Entropy              | 0.91653   |
| Perplexity           | 2.5006    |
| AveragePolicyStd     | 0.28696   |
| AveragePolicyStd[0]  | 0.306     |
| AveragePolicyStd[1]  | 0.37594   |
| AveragePolicyStd[2]  | 0.24485   |
| AveragePolicyStd[3]  | 0.32282   |
| AveragePolicyStd[4]  | 0.21304   |
| AveragePolicyStd[5]  | 0.25909   |
| AverageReturn        | 985.1     |
| MinReturn            | 79.806    |
| MaxReturn            | 1097.7    |
| StdReturn            | 171.13    |
| AverageEpisodeLength | 963.32    |
| MinEpisodeLength     | 95        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.25    |
| TotalNEpisodes       | 16296     |
| TotalNSamples        | 1.835e+06 |
| ExplainedVariance    | -0.019298 |
------------------------------------
[2018-12-22 10:44:27.411230 UTC] Saving snapshot
[2018-12-22 10:44:27.411534 UTC] Starting iteration 368
[2018-12-22 10:44:27.411676 UTC] Start collecting samples
[2018-12-22 10:44:30.381854 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:30.459611 UTC] Performing policy update
[2018-12-22 10:44:30.460382 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:30.549690 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:31.601066 UTC] Performing line search
[2018-12-22 10:44:31.734848 UTC] Updating baseline
[2018-12-22 10:44:33.064389 UTC] Computing logging information
------------------------------------
| Iteration            | 368       |
| ExpectedImprovement  | 0.014639  |
| ActualImprovement    | 0.014557  |
| ImprovementRatio     | 0.99439   |
| MeanKL               | 0.0076259 |
| Entropy              | 0.90108   |
| Perplexity           | 2.4623    |
| AveragePolicyStd     | 0.28611   |
| AveragePolicyStd[0]  | 0.3043    |
| AveragePolicyStd[1]  | 0.37365   |
| AveragePolicyStd[2]  | 0.24468   |
| AveragePolicyStd[3]  | 0.32229   |
| AveragePolicyStd[4]  | 0.21311   |
| AveragePolicyStd[5]  | 0.25864   |
| AverageReturn        | 987.08    |
| MinReturn            | 79.806    |
| MaxReturn            | 1097.7    |
| StdReturn            | 171.55    |
| AverageEpisodeLength | 963.32    |
| MinEpisodeLength     | 95        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.25    |
| TotalNEpisodes       | 16300     |
| TotalNSamples        | 1.839e+06 |
| ExplainedVariance    | 0.16104   |
------------------------------------
[2018-12-22 10:44:33.406831 UTC] Saving snapshot
[2018-12-22 10:44:33.407137 UTC] Starting iteration 369
[2018-12-22 10:44:33.407260 UTC] Start collecting samples
[2018-12-22 10:44:36.412619 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:36.490567 UTC] Performing policy update
[2018-12-22 10:44:36.491359 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:36.579904 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:37.621480 UTC] Performing line search
[2018-12-22 10:44:37.748446 UTC] Updating baseline
[2018-12-22 10:44:38.999985 UTC] Computing logging information
-------------------------------------
| Iteration            | 369        |
| ExpectedImprovement  | 0.015966   |
| ActualImprovement    | 0.015199   |
| ImprovementRatio     | 0.952      |
| MeanKL               | 0.0068489  |
| Entropy              | 0.90437    |
| Perplexity           | 2.4704     |
| AveragePolicyStd     | 0.28629    |
| AveragePolicyStd[0]  | 0.30416    |
| AveragePolicyStd[1]  | 0.37395    |
| AveragePolicyStd[2]  | 0.24564    |
| AveragePolicyStd[3]  | 0.32278    |
| AveragePolicyStd[4]  | 0.21269    |
| AveragePolicyStd[5]  | 0.25851    |
| AverageReturn        | 983.49     |
| MinReturn            | 79.806     |
| MaxReturn            | 1097.7     |
| StdReturn            | 181.03     |
| AverageEpisodeLength | 957.96     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.63     |
| TotalNEpisodes       | 16308      |
| TotalNSamples        | 1.8464e+06 |
| ExplainedVariance    | 0.15403    |
-------------------------------------
[2018-12-22 10:44:39.342076 UTC] Saving snapshot
[2018-12-22 10:44:39.342339 UTC] Starting iteration 370
[2018-12-22 10:44:39.342473 UTC] Start collecting samples
[2018-12-22 10:44:42.328122 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:42.405576 UTC] Performing policy update
[2018-12-22 10:44:42.406235 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:42.495634 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:43.547769 UTC] Performing line search
[2018-12-22 10:44:43.674135 UTC] Updating baseline
[2018-12-22 10:44:44.848071 UTC] Computing logging information
-------------------------------------
| Iteration            | 370        |
| ExpectedImprovement  | 0.015986   |
| ActualImprovement    | 0.013904   |
| ImprovementRatio     | 0.86971    |
| MeanKL               | 0.0071932  |
| Entropy              | 0.90256    |
| Perplexity           | 2.4659     |
| AveragePolicyStd     | 0.28616    |
| AveragePolicyStd[0]  | 0.30523    |
| AveragePolicyStd[1]  | 0.37252    |
| AveragePolicyStd[2]  | 0.24466    |
| AveragePolicyStd[3]  | 0.32206    |
| AveragePolicyStd[4]  | 0.21249    |
| AveragePolicyStd[5]  | 0.25998    |
| AverageReturn        | 984.27     |
| MinReturn            | 79.806     |
| MaxReturn            | 1097.7     |
| StdReturn            | 181.28     |
| AverageEpisodeLength | 957.96     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.63     |
| TotalNEpisodes       | 16312      |
| TotalNSamples        | 1.8504e+06 |
| ExplainedVariance    | -0.03972   |
-------------------------------------
[2018-12-22 10:44:45.188609 UTC] Saving snapshot
[2018-12-22 10:44:45.196795 UTC] Starting iteration 371
[2018-12-22 10:44:45.197001 UTC] Start collecting samples
[2018-12-22 10:44:48.174070 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:48.250574 UTC] Performing policy update
[2018-12-22 10:44:48.251542 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:48.340071 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:49.392346 UTC] Performing line search
[2018-12-22 10:44:49.518511 UTC] Updating baseline
[2018-12-22 10:44:50.772521 UTC] Computing logging information
------------------------------------
| Iteration            | 371       |
| ExpectedImprovement  | 0.01896   |
| ActualImprovement    | 0.017931  |
| ImprovementRatio     | 0.94575   |
| MeanKL               | 0.0070925 |
| Entropy              | 0.88915   |
| Perplexity           | 2.4331    |
| AveragePolicyStd     | 0.28544   |
| AveragePolicyStd[0]  | 0.30456   |
| AveragePolicyStd[1]  | 0.371     |
| AveragePolicyStd[2]  | 0.24488   |
| AveragePolicyStd[3]  | 0.31918   |
| AveragePolicyStd[4]  | 0.21116   |
| AveragePolicyStd[5]  | 0.26187   |
| AverageReturn        | 980.49    |
| MinReturn            | 79.806    |
| MaxReturn            | 1097.7    |
| StdReturn            | 185.54    |
| AverageEpisodeLength | 953.28    |
| MinEpisodeLength     | 95        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 173.34    |
| TotalNEpisodes       | 16316     |
| TotalNSamples        | 1.854e+06 |
| ExplainedVariance    | 0.40929   |
------------------------------------
[2018-12-22 10:44:51.110338 UTC] Saving snapshot
[2018-12-22 10:44:51.110590 UTC] Starting iteration 372
[2018-12-22 10:44:51.110714 UTC] Start collecting samples
[2018-12-22 10:44:54.084210 UTC] Computing input variables for policy optimization
[2018-12-22 10:44:54.162082 UTC] Performing policy update
[2018-12-22 10:44:54.162711 UTC] Computing gradient in Euclidean space
[2018-12-22 10:44:54.251229 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:44:55.300695 UTC] Performing line search
[2018-12-22 10:44:55.426725 UTC] Updating baseline
[2018-12-22 10:44:56.847434 UTC] Computing logging information
-------------------------------------
| Iteration            | 372        |
| ExpectedImprovement  | 0.015962   |
| ActualImprovement    | 0.01511    |
| ImprovementRatio     | 0.94662    |
| MeanKL               | 0.0068496  |
| Entropy              | 0.88736    |
| Perplexity           | 2.4287     |
| AveragePolicyStd     | 0.28537    |
| AveragePolicyStd[0]  | 0.30358    |
| AveragePolicyStd[1]  | 0.37188    |
| AveragePolicyStd[2]  | 0.24422    |
| AveragePolicyStd[3]  | 0.31917    |
| AveragePolicyStd[4]  | 0.2116     |
| AveragePolicyStd[5]  | 0.26178    |
| AverageReturn        | 984.72     |
| MinReturn            | 158.43     |
| MaxReturn            | 1097.7     |
| StdReturn            | 178.67     |
| AverageEpisodeLength | 954.75     |
| MinEpisodeLength     | 174        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.55     |
| TotalNEpisodes       | 16323      |
| TotalNSamples        | 1.8602e+06 |
| ExplainedVariance    | 0.12865    |
-------------------------------------
[2018-12-22 10:44:57.188667 UTC] Saving snapshot
[2018-12-22 10:44:57.188912 UTC] Starting iteration 373
[2018-12-22 10:44:57.189030 UTC] Start collecting samples
[2018-12-22 10:45:00.177939 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:00.256239 UTC] Performing policy update
[2018-12-22 10:45:00.256843 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:00.344086 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:01.399177 UTC] Performing line search
[2018-12-22 10:45:01.524487 UTC] Updating baseline
[2018-12-22 10:45:02.880237 UTC] Computing logging information
-------------------------------------
| Iteration            | 373        |
| ExpectedImprovement  | 0.017229   |
| ActualImprovement    | 0.015518   |
| ImprovementRatio     | 0.90072    |
| MeanKL               | 0.0071796  |
| Entropy              | 0.8753     |
| Perplexity           | 2.3996     |
| AveragePolicyStd     | 0.28491    |
| AveragePolicyStd[0]  | 0.30419    |
| AveragePolicyStd[1]  | 0.37316    |
| AveragePolicyStd[2]  | 0.24371    |
| AveragePolicyStd[3]  | 0.31692    |
| AveragePolicyStd[4]  | 0.21044    |
| AveragePolicyStd[5]  | 0.26105    |
| AverageReturn        | 977.24     |
| MinReturn            | 114.87     |
| MaxReturn            | 1097.7     |
| StdReturn            | 198.99     |
| AverageEpisodeLength | 946.17     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.07     |
| TotalNEpisodes       | 16330      |
| TotalNSamples        | 1.8664e+06 |
| ExplainedVariance    | 0.13653    |
-------------------------------------
[2018-12-22 10:45:03.222977 UTC] Saving snapshot
[2018-12-22 10:45:03.223221 UTC] Starting iteration 374
[2018-12-22 10:45:03.223362 UTC] Start collecting samples
[2018-12-22 10:45:06.180021 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:06.255621 UTC] Performing policy update
[2018-12-22 10:45:06.256468 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:06.347145 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:07.393017 UTC] Performing line search
[2018-12-22 10:45:07.518312 UTC] Updating baseline
[2018-12-22 10:45:09.458601 UTC] Computing logging information
-------------------------------------
| Iteration            | 374        |
| ExpectedImprovement  | 0.017159   |
| ActualImprovement    | 0.015728   |
| ImprovementRatio     | 0.9166     |
| MeanKL               | 0.0070543  |
| Entropy              | 0.87489    |
| Perplexity           | 2.3986     |
| AveragePolicyStd     | 0.28499    |
| AveragePolicyStd[0]  | 0.30477    |
| AveragePolicyStd[1]  | 0.37378    |
| AveragePolicyStd[2]  | 0.24304    |
| AveragePolicyStd[3]  | 0.31703    |
| AveragePolicyStd[4]  | 0.20953    |
| AveragePolicyStd[5]  | 0.26177    |
| AverageReturn        | 977.43     |
| MinReturn            | 114.87     |
| MaxReturn            | 1097.7     |
| StdReturn            | 199.03     |
| AverageEpisodeLength | 946.17     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.07     |
| TotalNEpisodes       | 16332      |
| TotalNSamples        | 1.8684e+06 |
| ExplainedVariance    | 0.0004017  |
-------------------------------------
[2018-12-22 10:45:09.802715 UTC] Saving snapshot
[2018-12-22 10:45:09.802979 UTC] Starting iteration 375
[2018-12-22 10:45:09.803107 UTC] Start collecting samples
[2018-12-22 10:45:12.798866 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:12.877941 UTC] Performing policy update
[2018-12-22 10:45:12.878520 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:12.969417 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:14.025804 UTC] Performing line search
[2018-12-22 10:45:14.151856 UTC] Updating baseline
[2018-12-22 10:45:15.496070 UTC] Computing logging information
-------------------------------------
| Iteration            | 375        |
| ExpectedImprovement  | 0.017403   |
| ActualImprovement    | 0.016577   |
| ImprovementRatio     | 0.95253    |
| MeanKL               | 0.007761   |
| Entropy              | 0.85218    |
| Perplexity           | 2.3447     |
| AveragePolicyStd     | 0.28398    |
| AveragePolicyStd[0]  | 0.30405    |
| AveragePolicyStd[1]  | 0.37277    |
| AveragePolicyStd[2]  | 0.2425     |
| AveragePolicyStd[3]  | 0.31603    |
| AveragePolicyStd[4]  | 0.20778    |
| AveragePolicyStd[5]  | 0.26078    |
| AverageReturn        | 979.47     |
| MinReturn            | 114.87     |
| MaxReturn            | 1101.4     |
| StdReturn            | 199.83     |
| AverageEpisodeLength | 946.17     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.07     |
| TotalNEpisodes       | 16338      |
| TotalNSamples        | 1.8744e+06 |
| ExplainedVariance    | 0.0088115  |
-------------------------------------
[2018-12-22 10:45:15.844304 UTC] Saving snapshot
[2018-12-22 10:45:15.844580 UTC] Starting iteration 376
[2018-12-22 10:45:15.844811 UTC] Start collecting samples
[2018-12-22 10:45:18.887555 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:18.967203 UTC] Performing policy update
[2018-12-22 10:45:18.967860 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:19.056910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:20.117009 UTC] Performing line search
[2018-12-22 10:45:20.242923 UTC] Updating baseline
[2018-12-22 10:45:21.491498 UTC] Computing logging information
-------------------------------------
| Iteration            | 376        |
| ExpectedImprovement  | 0.016487   |
| ActualImprovement    | 0.015996   |
| ImprovementRatio     | 0.97025    |
| MeanKL               | 0.0071384  |
| Entropy              | 0.84282    |
| Perplexity           | 2.3229     |
| AveragePolicyStd     | 0.28352    |
| AveragePolicyStd[0]  | 0.30391    |
| AveragePolicyStd[1]  | 0.37179    |
| AveragePolicyStd[2]  | 0.2431     |
| AveragePolicyStd[3]  | 0.31459    |
| AveragePolicyStd[4]  | 0.20679    |
| AveragePolicyStd[5]  | 0.26092    |
| AverageReturn        | 973.38     |
| MinReturn            | 114.87     |
| MaxReturn            | 1101.4     |
| StdReturn            | 213.04     |
| AverageEpisodeLength | 937.46     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.13     |
| TotalNEpisodes       | 16348      |
| TotalNSamples        | 1.8827e+06 |
| ExplainedVariance    | 0.23441    |
-------------------------------------
[2018-12-22 10:45:21.836947 UTC] Saving snapshot
[2018-12-22 10:45:21.837176 UTC] Starting iteration 377
[2018-12-22 10:45:21.837290 UTC] Start collecting samples
[2018-12-22 10:45:24.826486 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:24.902917 UTC] Performing policy update
[2018-12-22 10:45:24.903489 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:24.993008 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:26.055118 UTC] Performing line search
[2018-12-22 10:45:26.181624 UTC] Updating baseline
[2018-12-22 10:45:27.524216 UTC] Computing logging information
-------------------------------------
| Iteration            | 377        |
| ExpectedImprovement  | 0.017396   |
| ActualImprovement    | 0.016637   |
| ImprovementRatio     | 0.95636    |
| MeanKL               | 0.0071685  |
| Entropy              | 0.83552    |
| Perplexity           | 2.306      |
| AveragePolicyStd     | 0.28307    |
| AveragePolicyStd[0]  | 0.30362    |
| AveragePolicyStd[1]  | 0.37029    |
| AveragePolicyStd[2]  | 0.24389    |
| AveragePolicyStd[3]  | 0.31265    |
| AveragePolicyStd[4]  | 0.20651    |
| AveragePolicyStd[5]  | 0.26143    |
| AverageReturn        | 960.12     |
| MinReturn            | 114.87     |
| MaxReturn            | 1101.4     |
| StdReturn            | 231.38     |
| AverageEpisodeLength | 924.24     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.13     |
| TotalNEpisodes       | 16352      |
| TotalNSamples        | 1.8853e+06 |
| ExplainedVariance    | 0.42844    |
-------------------------------------
[2018-12-22 10:45:27.872456 UTC] Saving snapshot
[2018-12-22 10:45:27.872730 UTC] Starting iteration 378
[2018-12-22 10:45:27.872853 UTC] Start collecting samples
[2018-12-22 10:45:30.859757 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:30.937480 UTC] Performing policy update
[2018-12-22 10:45:30.938245 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:31.027696 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:32.076178 UTC] Performing line search
[2018-12-22 10:45:32.201198 UTC] Updating baseline
[2018-12-22 10:45:33.454506 UTC] Computing logging information
-------------------------------------
| Iteration            | 378        |
| ExpectedImprovement  | 0.015884   |
| ActualImprovement    | 0.015234   |
| ImprovementRatio     | 0.95906    |
| MeanKL               | 0.0074682  |
| Entropy              | 0.82587    |
| Perplexity           | 2.2839     |
| AveragePolicyStd     | 0.28265    |
| AveragePolicyStd[0]  | 0.30392    |
| AveragePolicyStd[1]  | 0.36927    |
| AveragePolicyStd[2]  | 0.24287    |
| AveragePolicyStd[3]  | 0.31249    |
| AveragePolicyStd[4]  | 0.20558    |
| AveragePolicyStd[5]  | 0.2618     |
| AverageReturn        | 962.56     |
| MinReturn            | 114.87     |
| MaxReturn            | 1117.8     |
| StdReturn            | 232.31     |
| AverageEpisodeLength | 924.24     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.13     |
| TotalNEpisodes       | 16357      |
| TotalNSamples        | 1.8903e+06 |
| ExplainedVariance    | -0.0015758 |
-------------------------------------
[2018-12-22 10:45:33.795702 UTC] Saving snapshot
[2018-12-22 10:45:33.795962 UTC] Starting iteration 379
[2018-12-22 10:45:33.796081 UTC] Start collecting samples
[2018-12-22 10:45:36.791231 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:36.869073 UTC] Performing policy update
[2018-12-22 10:45:36.869727 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:36.958538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:38.016498 UTC] Performing line search
[2018-12-22 10:45:38.142937 UTC] Updating baseline
[2018-12-22 10:45:39.647828 UTC] Computing logging information
-------------------------------------
| Iteration            | 379        |
| ExpectedImprovement  | 0.01684    |
| ActualImprovement    | 0.016135   |
| ImprovementRatio     | 0.95815    |
| MeanKL               | 0.0072793  |
| Entropy              | 0.82024    |
| Perplexity           | 2.271      |
| AveragePolicyStd     | 0.28234    |
| AveragePolicyStd[0]  | 0.30159    |
| AveragePolicyStd[1]  | 0.36943    |
| AveragePolicyStd[2]  | 0.24239    |
| AveragePolicyStd[3]  | 0.31255    |
| AveragePolicyStd[4]  | 0.20625    |
| AveragePolicyStd[5]  | 0.26183    |
| AverageReturn        | 966.19     |
| MinReturn            | 114.87     |
| MaxReturn            | 1117.8     |
| StdReturn            | 233.58     |
| AverageEpisodeLength | 924.24     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.13     |
| TotalNEpisodes       | 16362      |
| TotalNSamples        | 1.8953e+06 |
| ExplainedVariance    | -0.040566  |
-------------------------------------
[2018-12-22 10:45:39.995701 UTC] Saving snapshot
[2018-12-22 10:45:39.996011 UTC] Starting iteration 380
[2018-12-22 10:45:39.996130 UTC] Start collecting samples
[2018-12-22 10:45:42.964476 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:43.041463 UTC] Performing policy update
[2018-12-22 10:45:43.042224 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:43.130841 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:44.135096 UTC] Performing line search
[2018-12-22 10:45:44.260405 UTC] Updating baseline
[2018-12-22 10:45:45.604926 UTC] Computing logging information
-------------------------------------
| Iteration            | 380        |
| ExpectedImprovement  | 0.016866   |
| ActualImprovement    | 0.016053   |
| ImprovementRatio     | 0.95177    |
| MeanKL               | 0.0071145  |
| Entropy              | 0.81612    |
| Perplexity           | 2.2617     |
| AveragePolicyStd     | 0.28207    |
| AveragePolicyStd[0]  | 0.30169    |
| AveragePolicyStd[1]  | 0.36836    |
| AveragePolicyStd[2]  | 0.24219    |
| AveragePolicyStd[3]  | 0.31112    |
| AveragePolicyStd[4]  | 0.20646    |
| AveragePolicyStd[5]  | 0.26258    |
| AverageReturn        | 975.52     |
| MinReturn            | 114.87     |
| MaxReturn            | 1125.7     |
| StdReturn            | 221.57     |
| AverageEpisodeLength | 930.7      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.9      |
| TotalNEpisodes       | 16367      |
| TotalNSamples        | 1.9002e+06 |
| ExplainedVariance    | 0.14193    |
-------------------------------------
[2018-12-22 10:45:45.949936 UTC] Saving snapshot
[2018-12-22 10:45:45.957941 UTC] Starting iteration 381
[2018-12-22 10:45:45.958140 UTC] Start collecting samples
[2018-12-22 10:45:48.932152 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:49.010394 UTC] Performing policy update
[2018-12-22 10:45:49.011159 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:49.100631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:50.158411 UTC] Performing line search
[2018-12-22 10:45:50.284386 UTC] Updating baseline
[2018-12-22 10:45:51.881714 UTC] Computing logging information
-------------------------------------
| Iteration            | 381        |
| ExpectedImprovement  | 0.015737   |
| ActualImprovement    | 0.014874   |
| ImprovementRatio     | 0.94514    |
| MeanKL               | 0.0072223  |
| Entropy              | 0.81902    |
| Perplexity           | 2.2683     |
| AveragePolicyStd     | 0.28217    |
| AveragePolicyStd[0]  | 0.30266    |
| AveragePolicyStd[1]  | 0.36739    |
| AveragePolicyStd[2]  | 0.24199    |
| AveragePolicyStd[3]  | 0.31158    |
| AveragePolicyStd[4]  | 0.20683    |
| AveragePolicyStd[5]  | 0.26255    |
| AverageReturn        | 990.99     |
| MinReturn            | 114.87     |
| MaxReturn            | 1129.1     |
| StdReturn            | 208.71     |
| AverageEpisodeLength | 941.53     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.24     |
| TotalNEpisodes       | 16373      |
| TotalNSamples        | 1.9062e+06 |
| ExplainedVariance    | -0.043174  |
-------------------------------------
[2018-12-22 10:45:52.222299 UTC] Saving snapshot
[2018-12-22 10:45:52.222620 UTC] Starting iteration 382
[2018-12-22 10:45:52.222832 UTC] Start collecting samples
[2018-12-22 10:45:55.190747 UTC] Computing input variables for policy optimization
[2018-12-22 10:45:55.267686 UTC] Performing policy update
[2018-12-22 10:45:55.268249 UTC] Computing gradient in Euclidean space
[2018-12-22 10:45:55.355718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:45:56.367053 UTC] Performing line search
[2018-12-22 10:45:56.492253 UTC] Updating baseline
[2018-12-22 10:45:57.757892 UTC] Computing logging information
-------------------------------------
| Iteration            | 382        |
| ExpectedImprovement  | 0.015405   |
| ActualImprovement    | 0.014555   |
| ImprovementRatio     | 0.94483    |
| MeanKL               | 0.0069449  |
| Entropy              | 0.8074     |
| Perplexity           | 2.2421     |
| AveragePolicyStd     | 0.28164    |
| AveragePolicyStd[0]  | 0.30238    |
| AveragePolicyStd[1]  | 0.36724    |
| AveragePolicyStd[2]  | 0.24173    |
| AveragePolicyStd[3]  | 0.30997    |
| AveragePolicyStd[4]  | 0.20606    |
| AveragePolicyStd[5]  | 0.26247    |
| AverageReturn        | 991.12     |
| MinReturn            | 114.87     |
| MaxReturn            | 1129.1     |
| StdReturn            | 210.32     |
| AverageEpisodeLength | 938.85     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.29     |
| TotalNEpisodes       | 16378      |
| TotalNSamples        | 1.9109e+06 |
| ExplainedVariance    | 0.1458     |
-------------------------------------
[2018-12-22 10:45:58.097935 UTC] Saving snapshot
[2018-12-22 10:45:58.098178 UTC] Starting iteration 383
[2018-12-22 10:45:58.098297 UTC] Start collecting samples
[2018-12-22 10:46:01.040889 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:01.116751 UTC] Performing policy update
[2018-12-22 10:46:01.117409 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:01.204710 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:02.262871 UTC] Performing line search
[2018-12-22 10:46:02.387927 UTC] Updating baseline
[2018-12-22 10:46:03.720407 UTC] Computing logging information
-------------------------------------
| Iteration            | 383        |
| ExpectedImprovement  | 0.014524   |
| ActualImprovement    | 0.013884   |
| ImprovementRatio     | 0.95592    |
| MeanKL               | 0.0078111  |
| Entropy              | 0.81314    |
| Perplexity           | 2.255      |
| AveragePolicyStd     | 0.28199    |
| AveragePolicyStd[0]  | 0.3027     |
| AveragePolicyStd[1]  | 0.36963    |
| AveragePolicyStd[2]  | 0.24158    |
| AveragePolicyStd[3]  | 0.30984    |
| AveragePolicyStd[4]  | 0.2066     |
| AveragePolicyStd[5]  | 0.26159    |
| AverageReturn        | 992.07     |
| MinReturn            | 114.87     |
| MaxReturn            | 1129.1     |
| StdReturn            | 210.72     |
| AverageEpisodeLength | 938.85     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.29     |
| TotalNEpisodes       | 16381      |
| TotalNSamples        | 1.9139e+06 |
| ExplainedVariance    | -0.0033195 |
-------------------------------------
[2018-12-22 10:46:04.069493 UTC] Saving snapshot
[2018-12-22 10:46:04.069768 UTC] Starting iteration 384
[2018-12-22 10:46:04.069908 UTC] Start collecting samples
[2018-12-22 10:46:07.035506 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:07.113405 UTC] Performing policy update
[2018-12-22 10:46:07.114159 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:07.203517 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:08.265496 UTC] Performing line search
[2018-12-22 10:46:08.391819 UTC] Updating baseline
[2018-12-22 10:46:09.650786 UTC] Computing logging information
-------------------------------------
| Iteration            | 384        |
| ExpectedImprovement  | 0.016702   |
| ActualImprovement    | 0.015444   |
| ImprovementRatio     | 0.92467    |
| MeanKL               | 0.0069727  |
| Entropy              | 0.8181     |
| Perplexity           | 2.2662     |
| AveragePolicyStd     | 0.28212    |
| AveragePolicyStd[0]  | 0.30307    |
| AveragePolicyStd[1]  | 0.36819    |
| AveragePolicyStd[2]  | 0.24126    |
| AveragePolicyStd[3]  | 0.31054    |
| AveragePolicyStd[4]  | 0.20771    |
| AveragePolicyStd[5]  | 0.26194    |
| AverageReturn        | 996.07     |
| MinReturn            | 114.87     |
| MaxReturn            | 1145.8     |
| StdReturn            | 211.79     |
| AverageEpisodeLength | 939.48     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.38     |
| TotalNEpisodes       | 16386      |
| TotalNSamples        | 1.9189e+06 |
| ExplainedVariance    | 0.12828    |
-------------------------------------
[2018-12-22 10:46:09.994847 UTC] Saving snapshot
[2018-12-22 10:46:09.995082 UTC] Starting iteration 385
[2018-12-22 10:46:09.995202 UTC] Start collecting samples
[2018-12-22 10:46:13.132715 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:13.217983 UTC] Performing policy update
[2018-12-22 10:46:13.218788 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:13.311295 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:14.411062 UTC] Performing line search
[2018-12-22 10:46:14.542878 UTC] Updating baseline
[2018-12-22 10:46:16.281019 UTC] Computing logging information
-------------------------------------
| Iteration            | 385        |
| ExpectedImprovement  | 0.015158   |
| ActualImprovement    | 0.014219   |
| ImprovementRatio     | 0.93804    |
| MeanKL               | 0.0072839  |
| Entropy              | 0.81885    |
| Perplexity           | 2.2679     |
| AveragePolicyStd     | 0.2822     |
| AveragePolicyStd[0]  | 0.30268    |
| AveragePolicyStd[1]  | 0.36854    |
| AveragePolicyStd[2]  | 0.24094    |
| AveragePolicyStd[3]  | 0.31103    |
| AveragePolicyStd[4]  | 0.20717    |
| AveragePolicyStd[5]  | 0.26285    |
| AverageReturn        | 998.57     |
| MinReturn            | 114.87     |
| MaxReturn            | 1145.8     |
| StdReturn            | 213.43     |
| AverageEpisodeLength | 937.54     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.74     |
| TotalNEpisodes       | 16393      |
| TotalNSamples        | 1.9257e+06 |
| ExplainedVariance    | 0.025049   |
-------------------------------------
[2018-12-22 10:46:16.642033 UTC] Saving snapshot
[2018-12-22 10:46:16.642285 UTC] Starting iteration 386
[2018-12-22 10:46:16.642421 UTC] Start collecting samples
[2018-12-22 10:46:19.865188 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:19.949128 UTC] Performing policy update
[2018-12-22 10:46:19.949792 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:20.045936 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:21.159842 UTC] Performing line search
[2018-12-22 10:46:21.292633 UTC] Updating baseline
[2018-12-22 10:46:22.728512 UTC] Computing logging information
-------------------------------------
| Iteration            | 386        |
| ExpectedImprovement  | 0.016352   |
| ActualImprovement    | 0.015522   |
| ImprovementRatio     | 0.94925    |
| MeanKL               | 0.0070826  |
| Entropy              | 0.81504    |
| Perplexity           | 2.2593     |
| AveragePolicyStd     | 0.28201    |
| AveragePolicyStd[0]  | 0.30458    |
| AveragePolicyStd[1]  | 0.36802    |
| AveragePolicyStd[2]  | 0.24059    |
| AveragePolicyStd[3]  | 0.30852    |
| AveragePolicyStd[4]  | 0.2069     |
| AveragePolicyStd[5]  | 0.26342    |
| AverageReturn        | 991.99     |
| MinReturn            | 114.87     |
| MaxReturn            | 1150.9     |
| StdReturn            | 225.66     |
| AverageEpisodeLength | 929.14     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.72     |
| TotalNEpisodes       | 16399      |
| TotalNSamples        | 1.9309e+06 |
| ExplainedVariance    | 0.27168    |
-------------------------------------
[2018-12-22 10:46:23.071427 UTC] Saving snapshot
[2018-12-22 10:46:23.071722 UTC] Starting iteration 387
[2018-12-22 10:46:23.071843 UTC] Start collecting samples
[2018-12-22 10:46:26.059599 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:26.137624 UTC] Performing policy update
[2018-12-22 10:46:26.138484 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:26.228783 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:27.277334 UTC] Performing line search
[2018-12-22 10:46:27.403811 UTC] Updating baseline
[2018-12-22 10:46:28.679348 UTC] Computing logging information
-------------------------------------
| Iteration            | 387        |
| ExpectedImprovement  | 0.017028   |
| ActualImprovement    | 0.015649   |
| ImprovementRatio     | 0.91902    |
| MeanKL               | 0.0072581  |
| Entropy              | 0.81783    |
| Perplexity           | 2.2656     |
| AveragePolicyStd     | 0.2822     |
| AveragePolicyStd[0]  | 0.30499    |
| AveragePolicyStd[1]  | 0.36888    |
| AveragePolicyStd[2]  | 0.23857    |
| AveragePolicyStd[3]  | 0.30919    |
| AveragePolicyStd[4]  | 0.20754    |
| AveragePolicyStd[5]  | 0.26403    |
| AverageReturn        | 1000.2     |
| MinReturn            | 114.87     |
| MaxReturn            | 1150.9     |
| StdReturn            | 219.1      |
| AverageEpisodeLength | 934.5      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.37     |
| TotalNEpisodes       | 16403      |
| TotalNSamples        | 1.9349e+06 |
| ExplainedVariance    | -0.0038333 |
-------------------------------------
[2018-12-22 10:46:29.017787 UTC] Saving snapshot
[2018-12-22 10:46:29.018039 UTC] Starting iteration 388
[2018-12-22 10:46:29.018158 UTC] Start collecting samples
[2018-12-22 10:46:32.070796 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:32.149484 UTC] Performing policy update
[2018-12-22 10:46:32.150091 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:32.238115 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:33.288871 UTC] Performing line search
[2018-12-22 10:46:33.416190 UTC] Updating baseline
[2018-12-22 10:46:35.026771 UTC] Computing logging information
-------------------------------------
| Iteration            | 388        |
| ExpectedImprovement  | 0.014657   |
| ActualImprovement    | 0.013757   |
| ImprovementRatio     | 0.9386     |
| MeanKL               | 0.0075303  |
| Entropy              | 0.81202    |
| Perplexity           | 2.2525     |
| AveragePolicyStd     | 0.2819     |
| AveragePolicyStd[0]  | 0.30447    |
| AveragePolicyStd[1]  | 0.36851    |
| AveragePolicyStd[2]  | 0.23799    |
| AveragePolicyStd[3]  | 0.3081     |
| AveragePolicyStd[4]  | 0.20752    |
| AveragePolicyStd[5]  | 0.26481    |
| AverageReturn        | 1003.3     |
| MinReturn            | 114.87     |
| MaxReturn            | 1150.9     |
| StdReturn            | 220.23     |
| AverageEpisodeLength | 934.5      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.37     |
| TotalNEpisodes       | 16409      |
| TotalNSamples        | 1.9409e+06 |
| ExplainedVariance    | -0.0023349 |
-------------------------------------
[2018-12-22 10:46:35.373403 UTC] Saving snapshot
[2018-12-22 10:46:35.373671 UTC] Starting iteration 389
[2018-12-22 10:46:35.373821 UTC] Start collecting samples
[2018-12-22 10:46:38.400403 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:38.479716 UTC] Performing policy update
[2018-12-22 10:46:38.480564 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:38.572274 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:39.622636 UTC] Performing line search
[2018-12-22 10:46:39.748231 UTC] Updating baseline
[2018-12-22 10:46:41.353989 UTC] Computing logging information
------------------------------------
| Iteration            | 389       |
| ExpectedImprovement  | 0.016669  |
| ActualImprovement    | 0.015624  |
| ImprovementRatio     | 0.93732   |
| MeanKL               | 0.0073404 |
| Entropy              | 0.80506   |
| Perplexity           | 2.2368    |
| AveragePolicyStd     | 0.28155   |
| AveragePolicyStd[0]  | 0.30246   |
| AveragePolicyStd[1]  | 0.36921   |
| AveragePolicyStd[2]  | 0.23759   |
| AveragePolicyStd[3]  | 0.30716   |
| AveragePolicyStd[4]  | 0.20796   |
| AveragePolicyStd[5]  | 0.2649    |
| AverageReturn        | 1000.3    |
| MinReturn            | 80.276    |
| MaxReturn            | 1150.9    |
| StdReturn            | 235.82    |
| AverageEpisodeLength | 929.33    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 212.42    |
| TotalNEpisodes       | 16414     |
| TotalNSamples        | 1.945e+06 |
| ExplainedVariance    | 0.05464   |
------------------------------------
[2018-12-22 10:46:41.695724 UTC] Saving snapshot
[2018-12-22 10:46:41.696035 UTC] Starting iteration 390
[2018-12-22 10:46:41.696177 UTC] Start collecting samples
[2018-12-22 10:46:44.702571 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:44.781413 UTC] Performing policy update
[2018-12-22 10:46:44.782027 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:44.874728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:45.936264 UTC] Performing line search
[2018-12-22 10:46:46.062835 UTC] Updating baseline
[2018-12-22 10:46:47.407443 UTC] Computing logging information
------------------------------------
| Iteration            | 390       |
| ExpectedImprovement  | 0.01354   |
| ActualImprovement    | 0.01329   |
| ImprovementRatio     | 0.98159   |
| MeanKL               | 0.0075821 |
| Entropy              | 0.80826   |
| Perplexity           | 2.244     |
| AveragePolicyStd     | 0.28187   |
| AveragePolicyStd[0]  | 0.30245   |
| AveragePolicyStd[1]  | 0.37043   |
| AveragePolicyStd[2]  | 0.23846   |
| AveragePolicyStd[3]  | 0.30945   |
| AveragePolicyStd[4]  | 0.20604   |
| AveragePolicyStd[5]  | 0.2644    |
| AverageReturn        | 1002.6    |
| MinReturn            | 80.276    |
| MaxReturn            | 1150.9    |
| StdReturn            | 236.35    |
| AverageEpisodeLength | 929.95    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 212.53    |
| TotalNEpisodes       | 16419     |
| TotalNSamples        | 1.95e+06  |
| ExplainedVariance    | 0.022749  |
------------------------------------
[2018-12-22 10:46:47.755277 UTC] Saving snapshot
[2018-12-22 10:46:47.763813 UTC] Starting iteration 391
[2018-12-22 10:46:47.764029 UTC] Start collecting samples
[2018-12-22 10:46:50.761823 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:50.840787 UTC] Performing policy update
[2018-12-22 10:46:50.841700 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:50.931926 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:52.008703 UTC] Performing line search
[2018-12-22 10:46:52.136639 UTC] Updating baseline
[2018-12-22 10:46:53.382345 UTC] Computing logging information
------------------------------------
| Iteration            | 391       |
| ExpectedImprovement  | 0.01853   |
| ActualImprovement    | 0.01738   |
| ImprovementRatio     | 0.93796   |
| MeanKL               | 0.0068583 |
| Entropy              | 0.81753   |
| Perplexity           | 2.2649    |
| AveragePolicyStd     | 0.28238   |
| AveragePolicyStd[0]  | 0.30538   |
| AveragePolicyStd[1]  | 0.37086   |
| AveragePolicyStd[2]  | 0.23958   |
| AveragePolicyStd[3]  | 0.30824   |
| AveragePolicyStd[4]  | 0.20469   |
| AveragePolicyStd[5]  | 0.26555   |
| AverageReturn        | 1013.5    |
| MinReturn            | 80.276    |
| MaxReturn            | 1153.5    |
| StdReturn            | 224.77    |
| AverageEpisodeLength | 937.53    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 201.07    |
| TotalNEpisodes       | 16424     |
| TotalNSamples        | 1.955e+06 |
| ExplainedVariance    | -0.015717 |
------------------------------------
[2018-12-22 10:46:53.729625 UTC] Saving snapshot
[2018-12-22 10:46:53.729892 UTC] Starting iteration 392
[2018-12-22 10:46:53.730013 UTC] Start collecting samples
[2018-12-22 10:46:56.737174 UTC] Computing input variables for policy optimization
[2018-12-22 10:46:56.814541 UTC] Performing policy update
[2018-12-22 10:46:56.815119 UTC] Computing gradient in Euclidean space
[2018-12-22 10:46:56.903807 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:46:57.965939 UTC] Performing line search
[2018-12-22 10:46:58.093385 UTC] Updating baseline
[2018-12-22 10:46:59.528777 UTC] Computing logging information
-------------------------------------
| Iteration            | 392        |
| ExpectedImprovement  | 0.016467   |
| ActualImprovement    | 0.015673   |
| ImprovementRatio     | 0.95179    |
| MeanKL               | 0.0073735  |
| Entropy              | 0.81356    |
| Perplexity           | 2.2559     |
| AveragePolicyStd     | 0.28225    |
| AveragePolicyStd[0]  | 0.30501    |
| AveragePolicyStd[1]  | 0.37102    |
| AveragePolicyStd[2]  | 0.23945    |
| AveragePolicyStd[3]  | 0.30863    |
| AveragePolicyStd[4]  | 0.204      |
| AveragePolicyStd[5]  | 0.2654     |
| AverageReturn        | 1025.4     |
| MinReturn            | 80.276     |
| MaxReturn            | 1176.9     |
| StdReturn            | 206.88     |
| AverageEpisodeLength | 944.85     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.62     |
| TotalNEpisodes       | 16430      |
| TotalNSamples        | 1.9608e+06 |
| ExplainedVariance    | 0.14154    |
-------------------------------------
[2018-12-22 10:46:59.874772 UTC] Saving snapshot
[2018-12-22 10:46:59.875041 UTC] Starting iteration 393
[2018-12-22 10:46:59.875220 UTC] Start collecting samples
[2018-12-22 10:47:02.870132 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:02.948031 UTC] Performing policy update
[2018-12-22 10:47:02.948824 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:03.039666 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:04.106036 UTC] Performing line search
[2018-12-22 10:47:04.232752 UTC] Updating baseline
[2018-12-22 10:47:05.407460 UTC] Computing logging information
-------------------------------------
| Iteration            | 393        |
| ExpectedImprovement  | 0.014747   |
| ActualImprovement    | 0.014369   |
| ImprovementRatio     | 0.97439    |
| MeanKL               | 0.0074232  |
| Entropy              | 0.81432    |
| Perplexity           | 2.2576     |
| AveragePolicyStd     | 0.28231    |
| AveragePolicyStd[0]  | 0.30374    |
| AveragePolicyStd[1]  | 0.37195    |
| AveragePolicyStd[2]  | 0.23984    |
| AveragePolicyStd[3]  | 0.30871    |
| AveragePolicyStd[4]  | 0.20376    |
| AveragePolicyStd[5]  | 0.26585    |
| AverageReturn        | 1029.3     |
| MinReturn            | 80.276     |
| MaxReturn            | 1176.9     |
| StdReturn            | 207.85     |
| AverageEpisodeLength | 944.85     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.62     |
| TotalNEpisodes       | 16434      |
| TotalNSamples        | 1.9648e+06 |
| ExplainedVariance    | -0.049171  |
-------------------------------------
[2018-12-22 10:47:05.753772 UTC] Saving snapshot
[2018-12-22 10:47:05.754027 UTC] Starting iteration 394
[2018-12-22 10:47:05.754149 UTC] Start collecting samples
[2018-12-22 10:47:08.760482 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:08.840325 UTC] Performing policy update
[2018-12-22 10:47:08.841116 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:08.932129 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:10.002040 UTC] Performing line search
[2018-12-22 10:47:10.129457 UTC] Updating baseline
[2018-12-22 10:47:11.831598 UTC] Computing logging information
-------------------------------------
| Iteration            | 394        |
| ExpectedImprovement  | 0.017059   |
| ActualImprovement    | 0.015913   |
| ImprovementRatio     | 0.9328     |
| MeanKL               | 0.0072772  |
| Entropy              | 0.81554    |
| Perplexity           | 2.2604     |
| AveragePolicyStd     | 0.28232    |
| AveragePolicyStd[0]  | 0.30435    |
| AveragePolicyStd[1]  | 0.37125    |
| AveragePolicyStd[2]  | 0.23954    |
| AveragePolicyStd[3]  | 0.30801    |
| AveragePolicyStd[4]  | 0.204      |
| AveragePolicyStd[5]  | 0.26677    |
| AverageReturn        | 1031.4     |
| MinReturn            | 80.276     |
| MaxReturn            | 1176.9     |
| StdReturn            | 208.54     |
| AverageEpisodeLength | 944.85     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.62     |
| TotalNEpisodes       | 16440      |
| TotalNSamples        | 1.9708e+06 |
| ExplainedVariance    | 0.010753   |
-------------------------------------
[2018-12-22 10:47:12.182677 UTC] Saving snapshot
[2018-12-22 10:47:12.182923 UTC] Starting iteration 395
[2018-12-22 10:47:12.183041 UTC] Start collecting samples
[2018-12-22 10:47:15.200300 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:15.283955 UTC] Performing policy update
[2018-12-22 10:47:15.284691 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:15.379167 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:16.501812 UTC] Performing line search
[2018-12-22 10:47:16.635466 UTC] Updating baseline
[2018-12-22 10:47:18.145011 UTC] Computing logging information
-------------------------------------
| Iteration            | 395        |
| ExpectedImprovement  | 0.016357   |
| ActualImprovement    | 0.015562   |
| ImprovementRatio     | 0.95141    |
| MeanKL               | 0.0073097  |
| Entropy              | 0.80057    |
| Perplexity           | 2.2268     |
| AveragePolicyStd     | 0.28161    |
| AveragePolicyStd[0]  | 0.30215    |
| AveragePolicyStd[1]  | 0.37017    |
| AveragePolicyStd[2]  | 0.23921    |
| AveragePolicyStd[3]  | 0.30775    |
| AveragePolicyStd[4]  | 0.20309    |
| AveragePolicyStd[5]  | 0.26728    |
| AverageReturn        | 1038.2     |
| MinReturn            | 80.276     |
| MaxReturn            | 1176.9     |
| StdReturn            | 200.34     |
| AverageEpisodeLength | 948.3      |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.85     |
| TotalNEpisodes       | 16445      |
| TotalNSamples        | 1.9753e+06 |
| ExplainedVariance    | 0.12877    |
-------------------------------------
[2018-12-22 10:47:18.517142 UTC] Saving snapshot
[2018-12-22 10:47:18.517407 UTC] Starting iteration 396
[2018-12-22 10:47:18.517526 UTC] Start collecting samples
[2018-12-22 10:47:21.667249 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:21.744775 UTC] Performing policy update
[2018-12-22 10:47:21.745405 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:21.837139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:22.899636 UTC] Performing line search
[2018-12-22 10:47:23.028059 UTC] Updating baseline
[2018-12-22 10:47:24.288561 UTC] Computing logging information
-------------------------------------
| Iteration            | 396        |
| ExpectedImprovement  | 0.016181   |
| ActualImprovement    | 0.015118   |
| ImprovementRatio     | 0.93431    |
| MeanKL               | 0.0072947  |
| Entropy              | 0.81781    |
| Perplexity           | 2.2655     |
| AveragePolicyStd     | 0.28246    |
| AveragePolicyStd[0]  | 0.30243    |
| AveragePolicyStd[1]  | 0.37236    |
| AveragePolicyStd[2]  | 0.23985    |
| AveragePolicyStd[3]  | 0.30881    |
| AveragePolicyStd[4]  | 0.20381    |
| AveragePolicyStd[5]  | 0.26749    |
| AverageReturn        | 1060.2     |
| MinReturn            | 80.276     |
| MaxReturn            | 1178.6     |
| StdReturn            | 154.42     |
| AverageEpisodeLength | 965.8      |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.33     |
| TotalNEpisodes       | 16451      |
| TotalNSamples        | 1.9809e+06 |
| ExplainedVariance    | 0.10855    |
-------------------------------------
[2018-12-22 10:47:24.635807 UTC] Saving snapshot
[2018-12-22 10:47:24.636127 UTC] Starting iteration 397
[2018-12-22 10:47:24.636247 UTC] Start collecting samples
[2018-12-22 10:47:27.626603 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:27.703684 UTC] Performing policy update
[2018-12-22 10:47:27.704351 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:27.798685 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:28.855702 UTC] Performing line search
[2018-12-22 10:47:28.982131 UTC] Updating baseline
[2018-12-22 10:47:30.421190 UTC] Computing logging information
-------------------------------------
| Iteration            | 397        |
| ExpectedImprovement  | 0.015134   |
| ActualImprovement    | 0.014577   |
| ImprovementRatio     | 0.96325    |
| MeanKL               | 0.0071339  |
| Entropy              | 0.81989    |
| Perplexity           | 2.2703     |
| AveragePolicyStd     | 0.2825     |
| AveragePolicyStd[0]  | 0.30158    |
| AveragePolicyStd[1]  | 0.37304    |
| AveragePolicyStd[2]  | 0.24051    |
| AveragePolicyStd[3]  | 0.30824    |
| AveragePolicyStd[4]  | 0.20453    |
| AveragePolicyStd[5]  | 0.26713    |
| AverageReturn        | 1063.7     |
| MinReturn            | 80.276     |
| MaxReturn            | 1178.6     |
| StdReturn            | 155.24     |
| AverageEpisodeLength | 965.8      |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.33     |
| TotalNEpisodes       | 16456      |
| TotalNSamples        | 1.9859e+06 |
| ExplainedVariance    | 0.035129   |
-------------------------------------
[2018-12-22 10:47:30.766811 UTC] Saving snapshot
[2018-12-22 10:47:30.767054 UTC] Starting iteration 398
[2018-12-22 10:47:30.767172 UTC] Start collecting samples
[2018-12-22 10:47:33.751630 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:33.832478 UTC] Performing policy update
[2018-12-22 10:47:33.833078 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:33.927631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:34.988515 UTC] Performing line search
[2018-12-22 10:47:35.115867 UTC] Updating baseline
[2018-12-22 10:47:36.547914 UTC] Computing logging information
------------------------------------
| Iteration            | 398       |
| ExpectedImprovement  | 0.017806  |
| ActualImprovement    | 0.016949  |
| ImprovementRatio     | 0.95187   |
| MeanKL               | 0.0070119 |
| Entropy              | 0.822     |
| Perplexity           | 2.275     |
| AveragePolicyStd     | 0.28243   |
| AveragePolicyStd[0]  | 0.30161   |
| AveragePolicyStd[1]  | 0.37082   |
| AveragePolicyStd[2]  | 0.24123   |
| AveragePolicyStd[3]  | 0.30839   |
| AveragePolicyStd[4]  | 0.20563   |
| AveragePolicyStd[5]  | 0.26689   |
| AverageReturn        | 1055.5    |
| MinReturn            | 72.87     |
| MaxReturn            | 1183.8    |
| StdReturn            | 184.62    |
| AverageEpisodeLength | 956.61    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.23    |
| TotalNEpisodes       | 16461     |
| TotalNSamples        | 1.99e+06  |
| ExplainedVariance    | 0.11995   |
------------------------------------
[2018-12-22 10:47:36.892255 UTC] Saving snapshot
[2018-12-22 10:47:36.892494 UTC] Starting iteration 399
[2018-12-22 10:47:36.892634 UTC] Start collecting samples
[2018-12-22 10:47:39.849814 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:39.927874 UTC] Performing policy update
[2018-12-22 10:47:39.928451 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:40.017939 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:41.072809 UTC] Performing line search
[2018-12-22 10:47:41.199605 UTC] Updating baseline
[2018-12-22 10:47:42.546788 UTC] Computing logging information
------------------------------------
| Iteration            | 399       |
| ExpectedImprovement  | 0.016825  |
| ActualImprovement    | 0.015611  |
| ImprovementRatio     | 0.92787   |
| MeanKL               | 0.0073169 |
| Entropy              | 0.81209   |
| Perplexity           | 2.2526    |
| AveragePolicyStd     | 0.282     |
| AveragePolicyStd[0]  | 0.30174   |
| AveragePolicyStd[1]  | 0.37062   |
| AveragePolicyStd[2]  | 0.24088   |
| AveragePolicyStd[3]  | 0.30747   |
| AveragePolicyStd[4]  | 0.20489   |
| AveragePolicyStd[5]  | 0.26642   |
| AverageReturn        | 1059.2    |
| MinReturn            | 72.87     |
| MaxReturn            | 1183.8    |
| StdReturn            | 184.54    |
| AverageEpisodeLength | 958.06    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 161.96    |
| TotalNEpisodes       | 16467     |
| TotalNSamples        | 1.996e+06 |
| ExplainedVariance    | 0.11294   |
------------------------------------
[2018-12-22 10:47:42.892561 UTC] Saving snapshot
[2018-12-22 10:47:42.892817 UTC] Starting iteration 400
[2018-12-22 10:47:42.892936 UTC] Start collecting samples
[2018-12-22 10:47:45.847838 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:45.926424 UTC] Performing policy update
[2018-12-22 10:47:45.927031 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:46.016210 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:47.063906 UTC] Performing line search
[2018-12-22 10:47:47.190269 UTC] Updating baseline
[2018-12-22 10:47:48.613147 UTC] Computing logging information
-------------------------------------
| Iteration            | 400        |
| ExpectedImprovement  | 0.016473   |
| ActualImprovement    | 0.01541    |
| ImprovementRatio     | 0.93545    |
| MeanKL               | 0.0073184  |
| Entropy              | 0.80437    |
| Perplexity           | 2.2353     |
| AveragePolicyStd     | 0.28166    |
| AveragePolicyStd[0]  | 0.30084    |
| AveragePolicyStd[1]  | 0.37065    |
| AveragePolicyStd[2]  | 0.24102    |
| AveragePolicyStd[3]  | 0.3068     |
| AveragePolicyStd[4]  | 0.20425    |
| AveragePolicyStd[5]  | 0.26642    |
| AverageReturn        | 1054.4     |
| MinReturn            | 72.87      |
| MaxReturn            | 1183.8     |
| StdReturn            | 194.27     |
| AverageEpisodeLength | 952.52     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.72     |
| TotalNEpisodes       | 16471      |
| TotalNSamples        | 1.9994e+06 |
| ExplainedVariance    | 0.315      |
-------------------------------------
[2018-12-22 10:47:48.956702 UTC] Saving snapshot
[2018-12-22 10:47:48.964748 UTC] Starting iteration 401
[2018-12-22 10:47:48.964942 UTC] Start collecting samples
[2018-12-22 10:47:51.959979 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:52.036765 UTC] Performing policy update
[2018-12-22 10:47:52.037334 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:52.126163 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:53.181898 UTC] Performing line search
[2018-12-22 10:47:53.310670 UTC] Updating baseline
[2018-12-22 10:47:54.573178 UTC] Computing logging information
------------------------------------
| Iteration            | 401       |
| ExpectedImprovement  | 0.018376  |
| ActualImprovement    | 0.018042  |
| ImprovementRatio     | 0.98183   |
| MeanKL               | 0.007163  |
| Entropy              | 0.80367   |
| Perplexity           | 2.2337    |
| AveragePolicyStd     | 0.28173   |
| AveragePolicyStd[0]  | 0.30108   |
| AveragePolicyStd[1]  | 0.37194   |
| AveragePolicyStd[2]  | 0.23893   |
| AveragePolicyStd[3]  | 0.30714   |
| AveragePolicyStd[4]  | 0.20449   |
| AveragePolicyStd[5]  | 0.2668    |
| AverageReturn        | 1053.4    |
| MinReturn            | 72.87     |
| MaxReturn            | 1183.8    |
| StdReturn            | 197.15    |
| AverageEpisodeLength | 950.75    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 172.91    |
| TotalNEpisodes       | 16477     |
| TotalNSamples        | 2.005e+06 |
| ExplainedVariance    | 0.10189   |
------------------------------------
[2018-12-22 10:47:54.920025 UTC] Saving snapshot
[2018-12-22 10:47:54.920265 UTC] Starting iteration 402
[2018-12-22 10:47:54.920398 UTC] Start collecting samples
[2018-12-22 10:47:57.883726 UTC] Computing input variables for policy optimization
[2018-12-22 10:47:57.964850 UTC] Performing policy update
[2018-12-22 10:47:57.965456 UTC] Computing gradient in Euclidean space
[2018-12-22 10:47:58.054888 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:47:59.118567 UTC] Performing line search
[2018-12-22 10:47:59.248896 UTC] Updating baseline
[2018-12-22 10:48:00.864203 UTC] Computing logging information
-------------------------------------
| Iteration            | 402        |
| ExpectedImprovement  | 0.015599   |
| ActualImprovement    | 0.015049   |
| ImprovementRatio     | 0.96471    |
| MeanKL               | 0.0073549  |
| Entropy              | 0.80123    |
| Perplexity           | 2.2283     |
| AveragePolicyStd     | 0.28151    |
| AveragePolicyStd[0]  | 0.30177    |
| AveragePolicyStd[1]  | 0.36948    |
| AveragePolicyStd[2]  | 0.23986    |
| AveragePolicyStd[3]  | 0.3072     |
| AveragePolicyStd[4]  | 0.20428    |
| AveragePolicyStd[5]  | 0.26649    |
| AverageReturn        | 1056.4     |
| MinReturn            | 72.87      |
| MaxReturn            | 1183.8     |
| StdReturn            | 197.95     |
| AverageEpisodeLength | 950.75     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.91     |
| TotalNEpisodes       | 16483      |
| TotalNSamples        | 2.011e+06  |
| ExplainedVariance    | -0.0022998 |
-------------------------------------
[2018-12-22 10:48:01.211430 UTC] Saving snapshot
[2018-12-22 10:48:01.211711 UTC] Starting iteration 403
[2018-12-22 10:48:01.211832 UTC] Start collecting samples
[2018-12-22 10:48:04.209933 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:04.286069 UTC] Performing policy update
[2018-12-22 10:48:04.286809 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:04.377856 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:05.437040 UTC] Performing line search
[2018-12-22 10:48:05.564670 UTC] Updating baseline
[2018-12-22 10:48:07.006021 UTC] Computing logging information
-------------------------------------
| Iteration            | 403        |
| ExpectedImprovement  | 0.01856    |
| ActualImprovement    | 0.018019   |
| ImprovementRatio     | 0.97088    |
| MeanKL               | 0.0072242  |
| Entropy              | 0.78767    |
| Perplexity           | 2.1983     |
| AveragePolicyStd     | 0.28076    |
| AveragePolicyStd[0]  | 0.30404    |
| AveragePolicyStd[1]  | 0.36648    |
| AveragePolicyStd[2]  | 0.23907    |
| AveragePolicyStd[3]  | 0.30432    |
| AveragePolicyStd[4]  | 0.20445    |
| AveragePolicyStd[5]  | 0.26622    |
| AverageReturn        | 1048.8     |
| MinReturn            | 72.87      |
| MaxReturn            | 1183.8     |
| StdReturn            | 216.56     |
| AverageEpisodeLength | 943.45     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.53     |
| TotalNEpisodes       | 16487      |
| TotalNSamples        | 2.0141e+06 |
| ExplainedVariance    | 0.44705    |
-------------------------------------
[2018-12-22 10:48:07.350859 UTC] Saving snapshot
[2018-12-22 10:48:07.351123 UTC] Starting iteration 404
[2018-12-22 10:48:07.351238 UTC] Start collecting samples
[2018-12-22 10:48:10.347378 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:10.426362 UTC] Performing policy update
[2018-12-22 10:48:10.427258 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:10.517884 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:11.571394 UTC] Performing line search
[2018-12-22 10:48:11.698192 UTC] Updating baseline
[2018-12-22 10:48:12.855830 UTC] Computing logging information
-------------------------------------
| Iteration            | 404        |
| ExpectedImprovement  | 0.017209   |
| ActualImprovement    | 0.016647   |
| ImprovementRatio     | 0.96737    |
| MeanKL               | 0.0072585  |
| Entropy              | 0.78078    |
| Perplexity           | 2.1832     |
| AveragePolicyStd     | 0.28047    |
| AveragePolicyStd[0]  | 0.30367    |
| AveragePolicyStd[1]  | 0.36651    |
| AveragePolicyStd[2]  | 0.23906    |
| AveragePolicyStd[3]  | 0.30389    |
| AveragePolicyStd[4]  | 0.20399    |
| AveragePolicyStd[5]  | 0.26568    |
| AverageReturn        | 1050.5     |
| MinReturn            | 72.87      |
| MaxReturn            | 1183.8     |
| StdReturn            | 217.05     |
| AverageEpisodeLength | 943.45     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.53     |
| TotalNEpisodes       | 16493      |
| TotalNSamples        | 2.0201e+06 |
| ExplainedVariance    | -0.055875  |
-------------------------------------
[2018-12-22 10:48:13.197677 UTC] Saving snapshot
[2018-12-22 10:48:13.197969 UTC] Starting iteration 405
[2018-12-22 10:48:13.198089 UTC] Start collecting samples
[2018-12-22 10:48:16.231204 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:16.310706 UTC] Performing policy update
[2018-12-22 10:48:16.311264 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:16.402415 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:17.462095 UTC] Performing line search
[2018-12-22 10:48:17.589678 UTC] Updating baseline
[2018-12-22 10:48:19.376275 UTC] Computing logging information
-------------------------------------
| Iteration            | 405        |
| ExpectedImprovement  | 0.015533   |
| ActualImprovement    | 0.01502    |
| ImprovementRatio     | 0.96697    |
| MeanKL               | 0.0074462  |
| Entropy              | 0.77829    |
| Perplexity           | 2.1777     |
| AveragePolicyStd     | 0.28028    |
| AveragePolicyStd[0]  | 0.3037     |
| AveragePolicyStd[1]  | 0.3647     |
| AveragePolicyStd[2]  | 0.23946    |
| AveragePolicyStd[3]  | 0.30428    |
| AveragePolicyStd[4]  | 0.20393    |
| AveragePolicyStd[5]  | 0.26559    |
| AverageReturn        | 1052.6     |
| MinReturn            | 72.87      |
| MaxReturn            | 1183.8     |
| StdReturn            | 215.69     |
| AverageEpisodeLength | 944.98     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.04     |
| TotalNEpisodes       | 16500      |
| TotalNSamples        | 2.0264e+06 |
| ExplainedVariance    | 0.12328    |
-------------------------------------
[2018-12-22 10:48:19.725764 UTC] Saving snapshot
[2018-12-22 10:48:19.726015 UTC] Starting iteration 406
[2018-12-22 10:48:19.726147 UTC] Start collecting samples
[2018-12-22 10:48:22.707059 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:22.785330 UTC] Performing policy update
[2018-12-22 10:48:22.786117 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:22.876330 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:23.951533 UTC] Performing line search
[2018-12-22 10:48:24.078636 UTC] Updating baseline
[2018-12-22 10:48:25.253657 UTC] Computing logging information
-------------------------------------
| Iteration            | 406        |
| ExpectedImprovement  | 0.016598   |
| ActualImprovement    | 0.01546    |
| ImprovementRatio     | 0.93149    |
| MeanKL               | 0.0075873  |
| Entropy              | 0.77841    |
| Perplexity           | 2.178      |
| AveragePolicyStd     | 0.28033    |
| AveragePolicyStd[0]  | 0.30318    |
| AveragePolicyStd[1]  | 0.36606    |
| AveragePolicyStd[2]  | 0.23938    |
| AveragePolicyStd[3]  | 0.30457    |
| AveragePolicyStd[4]  | 0.20417    |
| AveragePolicyStd[5]  | 0.26461    |
| AverageReturn        | 1052.1     |
| MinReturn            | 72.87      |
| MaxReturn            | 1183.8     |
| StdReturn            | 215.71     |
| AverageEpisodeLength | 944.19     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.98     |
| TotalNEpisodes       | 16504      |
| TotalNSamples        | 2.0303e+06 |
| ExplainedVariance    | 0.25359    |
-------------------------------------
[2018-12-22 10:48:25.595402 UTC] Saving snapshot
[2018-12-22 10:48:25.595667 UTC] Starting iteration 407
[2018-12-22 10:48:25.595790 UTC] Start collecting samples
[2018-12-22 10:48:28.542008 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:28.618564 UTC] Performing policy update
[2018-12-22 10:48:28.619223 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:28.709646 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:29.775712 UTC] Performing line search
[2018-12-22 10:48:29.903912 UTC] Updating baseline
[2018-12-22 10:48:31.247666 UTC] Computing logging information
-------------------------------------
| Iteration            | 407        |
| ExpectedImprovement  | 0.015289   |
| ActualImprovement    | 0.014495   |
| ImprovementRatio     | 0.94804    |
| MeanKL               | 0.0076228  |
| Entropy              | 0.77602    |
| Perplexity           | 2.1728     |
| AveragePolicyStd     | 0.28018    |
| AveragePolicyStd[0]  | 0.30387    |
| AveragePolicyStd[1]  | 0.36525    |
| AveragePolicyStd[2]  | 0.23888    |
| AveragePolicyStd[3]  | 0.30382    |
| AveragePolicyStd[4]  | 0.20448    |
| AveragePolicyStd[5]  | 0.26477    |
| AverageReturn        | 1052.5     |
| MinReturn            | 72.87      |
| MaxReturn            | 1183.8     |
| StdReturn            | 215.8      |
| AverageEpisodeLength | 944.19     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.98     |
| TotalNEpisodes       | 16508      |
| TotalNSamples        | 2.0343e+06 |
| ExplainedVariance    | 0.01232    |
-------------------------------------
[2018-12-22 10:48:31.596816 UTC] Saving snapshot
[2018-12-22 10:48:31.597086 UTC] Starting iteration 408
[2018-12-22 10:48:31.597202 UTC] Start collecting samples
[2018-12-22 10:48:34.600574 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:34.681510 UTC] Performing policy update
[2018-12-22 10:48:34.682358 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:34.773349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:35.835773 UTC] Performing line search
[2018-12-22 10:48:35.963295 UTC] Updating baseline
[2018-12-22 10:48:37.221316 UTC] Computing logging information
-------------------------------------
| Iteration            | 408        |
| ExpectedImprovement  | 0.017648   |
| ActualImprovement    | 0.016802   |
| ImprovementRatio     | 0.95209    |
| MeanKL               | 0.006838   |
| Entropy              | 0.75744    |
| Perplexity           | 2.1328     |
| AveragePolicyStd     | 0.27922    |
| AveragePolicyStd[0]  | 0.30249    |
| AveragePolicyStd[1]  | 0.36349    |
| AveragePolicyStd[2]  | 0.23965    |
| AveragePolicyStd[3]  | 0.30192    |
| AveragePolicyStd[4]  | 0.20394    |
| AveragePolicyStd[5]  | 0.26386    |
| AverageReturn        | 1057       |
| MinReturn            | 72.87      |
| MaxReturn            | 1195.5     |
| StdReturn            | 205.63     |
| AverageEpisodeLength | 946.74     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.64     |
| TotalNEpisodes       | 16516      |
| TotalNSamples        | 2.0416e+06 |
| ExplainedVariance    | 0.12924    |
-------------------------------------
[2018-12-22 10:48:37.566187 UTC] Saving snapshot
[2018-12-22 10:48:37.566513 UTC] Starting iteration 409
[2018-12-22 10:48:37.566670 UTC] Start collecting samples
[2018-12-22 10:48:40.559565 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:40.639680 UTC] Performing policy update
[2018-12-22 10:48:40.640320 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:40.729162 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:41.789355 UTC] Performing line search
[2018-12-22 10:48:41.915681 UTC] Updating baseline
[2018-12-22 10:48:43.450760 UTC] Computing logging information
-------------------------------------
| Iteration            | 409        |
| ExpectedImprovement  | 0.015038   |
| ActualImprovement    | 0.014323   |
| ImprovementRatio     | 0.95248    |
| MeanKL               | 0.007244   |
| Entropy              | 0.76992    |
| Perplexity           | 2.1596     |
| AveragePolicyStd     | 0.27997    |
| AveragePolicyStd[0]  | 0.30431    |
| AveragePolicyStd[1]  | 0.36529    |
| AveragePolicyStd[2]  | 0.23913    |
| AveragePolicyStd[3]  | 0.30315    |
| AveragePolicyStd[4]  | 0.20321    |
| AveragePolicyStd[5]  | 0.26472    |
| AverageReturn        | 1041.6     |
| MinReturn            | 72.87      |
| MaxReturn            | 1195.5     |
| StdReturn            | 234.99     |
| AverageEpisodeLength | 931.46     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.71     |
| TotalNEpisodes       | 16523      |
| TotalNSamples        | 2.0471e+06 |
| ExplainedVariance    | 0.20717    |
-------------------------------------
[2018-12-22 10:48:43.799275 UTC] Saving snapshot
[2018-12-22 10:48:43.799528 UTC] Starting iteration 410
[2018-12-22 10:48:43.799688 UTC] Start collecting samples
[2018-12-22 10:48:46.731821 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:46.807683 UTC] Performing policy update
[2018-12-22 10:48:46.808321 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:46.898760 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:47.952118 UTC] Performing line search
[2018-12-22 10:48:48.079706 UTC] Updating baseline
[2018-12-22 10:48:49.680064 UTC] Computing logging information
-------------------------------------
| Iteration            | 410        |
| ExpectedImprovement  | 0.017819   |
| ActualImprovement    | 0.01679    |
| ImprovementRatio     | 0.94225    |
| MeanKL               | 0.0071943  |
| Entropy              | 0.7683     |
| Perplexity           | 2.1561     |
| AveragePolicyStd     | 0.27998    |
| AveragePolicyStd[0]  | 0.30232    |
| AveragePolicyStd[1]  | 0.36784    |
| AveragePolicyStd[2]  | 0.23898    |
| AveragePolicyStd[3]  | 0.30324    |
| AveragePolicyStd[4]  | 0.20315    |
| AveragePolicyStd[5]  | 0.26433    |
| AverageReturn        | 1042.8     |
| MinReturn            | 72.87      |
| MaxReturn            | 1195.5     |
| StdReturn            | 235.28     |
| AverageEpisodeLength | 931.46     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.71     |
| TotalNEpisodes       | 16526      |
| TotalNSamples        | 2.0501e+06 |
| ExplainedVariance    | -0.0022525 |
-------------------------------------
[2018-12-22 10:48:50.027988 UTC] Saving snapshot
[2018-12-22 10:48:50.036137 UTC] Starting iteration 411
[2018-12-22 10:48:50.036353 UTC] Start collecting samples
[2018-12-22 10:48:52.991648 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:53.070376 UTC] Performing policy update
[2018-12-22 10:48:53.071035 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:53.161266 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:48:54.230770 UTC] Performing line search
[2018-12-22 10:48:54.358511 UTC] Updating baseline
[2018-12-22 10:48:56.163432 UTC] Computing logging information
-------------------------------------
| Iteration            | 411        |
| ExpectedImprovement  | 0.01528    |
| ActualImprovement    | 0.014217   |
| ImprovementRatio     | 0.93048    |
| MeanKL               | 0.0071596  |
| Entropy              | 0.76523    |
| Perplexity           | 2.1495     |
| AveragePolicyStd     | 0.27975    |
| AveragePolicyStd[0]  | 0.30094    |
| AveragePolicyStd[1]  | 0.36597    |
| AveragePolicyStd[2]  | 0.23933    |
| AveragePolicyStd[3]  | 0.30378    |
| AveragePolicyStd[4]  | 0.20288    |
| AveragePolicyStd[5]  | 0.26558    |
| AverageReturn        | 1044.6     |
| MinReturn            | 72.87      |
| MaxReturn            | 1195.5     |
| StdReturn            | 235.29     |
| AverageEpisodeLength | 932.72     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.74     |
| TotalNEpisodes       | 16531      |
| TotalNSamples        | 2.0551e+06 |
| ExplainedVariance    | -0.0067689 |
-------------------------------------
[2018-12-22 10:48:56.509890 UTC] Saving snapshot
[2018-12-22 10:48:56.510158 UTC] Starting iteration 412
[2018-12-22 10:48:56.510276 UTC] Start collecting samples
[2018-12-22 10:48:59.478807 UTC] Computing input variables for policy optimization
[2018-12-22 10:48:59.558537 UTC] Performing policy update
[2018-12-22 10:48:59.559193 UTC] Computing gradient in Euclidean space
[2018-12-22 10:48:59.648199 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:00.710749 UTC] Performing line search
[2018-12-22 10:49:00.838779 UTC] Updating baseline
[2018-12-22 10:49:02.103875 UTC] Computing logging information
-------------------------------------
| Iteration            | 412        |
| ExpectedImprovement  | 0.01595    |
| ActualImprovement    | 0.015487   |
| ImprovementRatio     | 0.97098    |
| MeanKL               | 0.0071342  |
| Entropy              | 0.76129    |
| Perplexity           | 2.141      |
| AveragePolicyStd     | 0.27952    |
| AveragePolicyStd[0]  | 0.30025    |
| AveragePolicyStd[1]  | 0.36545    |
| AveragePolicyStd[2]  | 0.23831    |
| AveragePolicyStd[3]  | 0.30381    |
| AveragePolicyStd[4]  | 0.20354    |
| AveragePolicyStd[5]  | 0.26576    |
| AverageReturn        | 1043.9     |
| MinReturn            | 72.87      |
| MaxReturn            | 1195.5     |
| StdReturn            | 235        |
| AverageEpisodeLength | 932.72     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.74     |
| TotalNEpisodes       | 16537      |
| TotalNSamples        | 2.0611e+06 |
| ExplainedVariance    | 0.0086382  |
-------------------------------------
[2018-12-22 10:49:02.447824 UTC] Saving snapshot
[2018-12-22 10:49:02.448073 UTC] Starting iteration 413
[2018-12-22 10:49:02.448188 UTC] Start collecting samples
[2018-12-22 10:49:05.408991 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:05.486602 UTC] Performing policy update
[2018-12-22 10:49:05.487460 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:05.576846 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:06.630348 UTC] Performing line search
[2018-12-22 10:49:06.756465 UTC] Updating baseline
[2018-12-22 10:49:08.038293 UTC] Computing logging information
--------------------------------------
| Iteration            | 413         |
| ExpectedImprovement  | 0.01586     |
| ActualImprovement    | 0.015401    |
| ImprovementRatio     | 0.97107     |
| MeanKL               | 0.0077086   |
| Entropy              | 0.77273     |
| Perplexity           | 2.1657      |
| AveragePolicyStd     | 0.28006     |
| AveragePolicyStd[0]  | 0.30105     |
| AveragePolicyStd[1]  | 0.36661     |
| AveragePolicyStd[2]  | 0.23882     |
| AveragePolicyStd[3]  | 0.30389     |
| AveragePolicyStd[4]  | 0.20415     |
| AveragePolicyStd[5]  | 0.26583     |
| AverageReturn        | 1045.1      |
| MinReturn            | 72.87       |
| MaxReturn            | 1195.5      |
| StdReturn            | 235.45      |
| AverageEpisodeLength | 932.72      |
| MinEpisodeLength     | 81          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 203.74      |
| TotalNEpisodes       | 16541       |
| TotalNSamples        | 2.0651e+06  |
| ExplainedVariance    | -0.00069695 |
--------------------------------------
[2018-12-22 10:49:08.385630 UTC] Saving snapshot
[2018-12-22 10:49:08.385920 UTC] Starting iteration 414
[2018-12-22 10:49:08.386041 UTC] Start collecting samples
[2018-12-22 10:49:11.364652 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:11.445341 UTC] Performing policy update
[2018-12-22 10:49:11.446130 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:11.536864 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:12.602248 UTC] Performing line search
[2018-12-22 10:49:12.729260 UTC] Updating baseline
[2018-12-22 10:49:14.169923 UTC] Computing logging information
-------------------------------------
| Iteration            | 414        |
| ExpectedImprovement  | 0.017555   |
| ActualImprovement    | 0.016818   |
| ImprovementRatio     | 0.95796    |
| MeanKL               | 0.0070759  |
| Entropy              | 0.77353    |
| Perplexity           | 2.1674     |
| AveragePolicyStd     | 0.28013    |
| AveragePolicyStd[0]  | 0.30179    |
| AveragePolicyStd[1]  | 0.36676    |
| AveragePolicyStd[2]  | 0.23816    |
| AveragePolicyStd[3]  | 0.3032     |
| AveragePolicyStd[4]  | 0.20382    |
| AveragePolicyStd[5]  | 0.26705    |
| AverageReturn        | 1048.5     |
| MinReturn            | 72.87      |
| MaxReturn            | 1195.5     |
| StdReturn            | 230.65     |
| AverageEpisodeLength | 935.71     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.09     |
| TotalNEpisodes       | 16548      |
| TotalNSamples        | 2.0715e+06 |
| ExplainedVariance    | 0.1782     |
-------------------------------------
[2018-12-22 10:49:14.515971 UTC] Saving snapshot
[2018-12-22 10:49:14.516208 UTC] Starting iteration 415
[2018-12-22 10:49:14.516324 UTC] Start collecting samples
[2018-12-22 10:49:17.462280 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:17.539486 UTC] Performing policy update
[2018-12-22 10:49:17.540209 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:17.631030 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:18.696739 UTC] Performing line search
[2018-12-22 10:49:18.824374 UTC] Updating baseline
[2018-12-22 10:49:20.182031 UTC] Computing logging information
-------------------------------------
| Iteration            | 415        |
| ExpectedImprovement  | 0.01582    |
| ActualImprovement    | 0.015548   |
| ImprovementRatio     | 0.98281    |
| MeanKL               | 0.0077654  |
| Entropy              | 0.77392    |
| Perplexity           | 2.1683     |
| AveragePolicyStd     | 0.2802     |
| AveragePolicyStd[0]  | 0.30314    |
| AveragePolicyStd[1]  | 0.36711    |
| AveragePolicyStd[2]  | 0.23699    |
| AveragePolicyStd[3]  | 0.30319    |
| AveragePolicyStd[4]  | 0.2041     |
| AveragePolicyStd[5]  | 0.26667    |
| AverageReturn        | 1049.5     |
| MinReturn            | 72.87      |
| MaxReturn            | 1195.5     |
| StdReturn            | 230.97     |
| AverageEpisodeLength | 935.71     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.09     |
| TotalNEpisodes       | 16552      |
| TotalNSamples        | 2.0755e+06 |
| ExplainedVariance    | -0.0037076 |
-------------------------------------
[2018-12-22 10:49:20.531811 UTC] Saving snapshot
[2018-12-22 10:49:20.532058 UTC] Starting iteration 416
[2018-12-22 10:49:20.532173 UTC] Start collecting samples
[2018-12-22 10:49:23.489871 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:23.568016 UTC] Performing policy update
[2018-12-22 10:49:23.568903 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:23.660061 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:24.731970 UTC] Performing line search
[2018-12-22 10:49:24.859741 UTC] Updating baseline
[2018-12-22 10:49:26.310947 UTC] Computing logging information
-------------------------------------
| Iteration            | 416        |
| ExpectedImprovement  | 0.01663    |
| ActualImprovement    | 0.015288   |
| ImprovementRatio     | 0.91926    |
| MeanKL               | 0.007239   |
| Entropy              | 0.76932    |
| Perplexity           | 2.1583     |
| AveragePolicyStd     | 0.27995    |
| AveragePolicyStd[0]  | 0.30367    |
| AveragePolicyStd[1]  | 0.36672    |
| AveragePolicyStd[2]  | 0.23696    |
| AveragePolicyStd[3]  | 0.30198    |
| AveragePolicyStd[4]  | 0.20429    |
| AveragePolicyStd[5]  | 0.26611    |
| AverageReturn        | 1050.4     |
| MinReturn            | 60.243     |
| MaxReturn            | 1195.5     |
| StdReturn            | 231.91     |
| AverageEpisodeLength | 935.49     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.05     |
| TotalNEpisodes       | 16557      |
| TotalNSamples        | 2.0796e+06 |
| ExplainedVariance    | 0.074258   |
-------------------------------------
[2018-12-22 10:49:26.655809 UTC] Saving snapshot
[2018-12-22 10:49:26.656047 UTC] Starting iteration 417
[2018-12-22 10:49:26.656164 UTC] Start collecting samples
[2018-12-22 10:49:29.610959 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:29.688486 UTC] Performing policy update
[2018-12-22 10:49:29.689191 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:29.783939 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:30.852714 UTC] Performing line search
[2018-12-22 10:49:30.979762 UTC] Updating baseline
[2018-12-22 10:49:32.501837 UTC] Computing logging information
-------------------------------------
| Iteration            | 417        |
| ExpectedImprovement  | 0.01747    |
| ActualImprovement    | 0.016517   |
| ImprovementRatio     | 0.94543    |
| MeanKL               | 0.0073078  |
| Entropy              | 0.75474    |
| Perplexity           | 2.1271     |
| AveragePolicyStd     | 0.27917    |
| AveragePolicyStd[0]  | 0.30376    |
| AveragePolicyStd[1]  | 0.36279    |
| AveragePolicyStd[2]  | 0.23583    |
| AveragePolicyStd[3]  | 0.30212    |
| AveragePolicyStd[4]  | 0.20408    |
| AveragePolicyStd[5]  | 0.26644    |
| AverageReturn        | 1051.3     |
| MinReturn            | 60.243     |
| MaxReturn            | 1195.5     |
| StdReturn            | 232.21     |
| AverageEpisodeLength | 935.49     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.05     |
| TotalNEpisodes       | 16562      |
| TotalNSamples        | 2.0846e+06 |
| ExplainedVariance    | -0.0043491 |
-------------------------------------
[2018-12-22 10:49:32.848853 UTC] Saving snapshot
[2018-12-22 10:49:32.849099 UTC] Starting iteration 418
[2018-12-22 10:49:32.849217 UTC] Start collecting samples
[2018-12-22 10:49:35.797284 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:35.875734 UTC] Performing policy update
[2018-12-22 10:49:35.876736 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:35.967052 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:37.017822 UTC] Performing line search
[2018-12-22 10:49:37.143706 UTC] Updating baseline
[2018-12-22 10:49:38.422628 UTC] Computing logging information
-------------------------------------
| Iteration            | 418        |
| ExpectedImprovement  | 0.017817   |
| ActualImprovement    | 0.016686   |
| ImprovementRatio     | 0.93655    |
| MeanKL               | 0.0069511  |
| Entropy              | 0.75068    |
| Perplexity           | 2.1184     |
| AveragePolicyStd     | 0.27896    |
| AveragePolicyStd[0]  | 0.30361    |
| AveragePolicyStd[1]  | 0.36315    |
| AveragePolicyStd[2]  | 0.23596    |
| AveragePolicyStd[3]  | 0.30088    |
| AveragePolicyStd[4]  | 0.20445    |
| AveragePolicyStd[5]  | 0.2657     |
| AverageReturn        | 1053.8     |
| MinReturn            | 60.243     |
| MaxReturn            | 1198.2     |
| StdReturn            | 233.17     |
| AverageEpisodeLength | 935.49     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.05     |
| TotalNEpisodes       | 16568      |
| TotalNSamples        | 2.0906e+06 |
| ExplainedVariance    | -0.012794  |
-------------------------------------
[2018-12-22 10:49:38.769162 UTC] Saving snapshot
[2018-12-22 10:49:38.769416 UTC] Starting iteration 419
[2018-12-22 10:49:38.769539 UTC] Start collecting samples
[2018-12-22 10:49:41.695280 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:41.773496 UTC] Performing policy update
[2018-12-22 10:49:41.774453 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:41.862778 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:42.913421 UTC] Performing line search
[2018-12-22 10:49:43.039506 UTC] Updating baseline
[2018-12-22 10:49:44.737775 UTC] Computing logging information
-------------------------------------
| Iteration            | 419        |
| ExpectedImprovement  | 0.016665   |
| ActualImprovement    | 0.015565   |
| ImprovementRatio     | 0.93399    |
| MeanKL               | 0.007105   |
| Entropy              | 0.74712    |
| Perplexity           | 2.1109     |
| AveragePolicyStd     | 0.27887    |
| AveragePolicyStd[0]  | 0.304      |
| AveragePolicyStd[1]  | 0.36448    |
| AveragePolicyStd[2]  | 0.23658    |
| AveragePolicyStd[3]  | 0.29889    |
| AveragePolicyStd[4]  | 0.20346    |
| AveragePolicyStd[5]  | 0.26579    |
| AverageReturn        | 1066.4     |
| MinReturn            | 60.243     |
| MaxReturn            | 1198.2     |
| StdReturn            | 221.93     |
| AverageEpisodeLength | 945.48     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.16     |
| TotalNEpisodes       | 16572      |
| TotalNSamples        | 2.0946e+06 |
| ExplainedVariance    | -0.0091505 |
-------------------------------------
[2018-12-22 10:49:45.082295 UTC] Saving snapshot
[2018-12-22 10:49:45.082558 UTC] Starting iteration 420
[2018-12-22 10:49:45.082690 UTC] Start collecting samples
[2018-12-22 10:49:48.060075 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:48.138234 UTC] Performing policy update
[2018-12-22 10:49:48.139048 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:48.228042 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:49.279198 UTC] Performing line search
[2018-12-22 10:49:49.405766 UTC] Updating baseline
[2018-12-22 10:49:51.014902 UTC] Computing logging information
-------------------------------------
| Iteration            | 420        |
| ExpectedImprovement  | 0.016373   |
| ActualImprovement    | 0.015544   |
| ImprovementRatio     | 0.94936    |
| MeanKL               | 0.007243   |
| Entropy              | 0.74495    |
| Perplexity           | 2.1063     |
| AveragePolicyStd     | 0.27874    |
| AveragePolicyStd[0]  | 0.30318    |
| AveragePolicyStd[1]  | 0.36344    |
| AveragePolicyStd[2]  | 0.23642    |
| AveragePolicyStd[3]  | 0.30031    |
| AveragePolicyStd[4]  | 0.20341    |
| AveragePolicyStd[5]  | 0.26566    |
| AverageReturn        | 1065.3     |
| MinReturn            | 60.243     |
| MaxReturn            | 1198.2     |
| StdReturn            | 225.2      |
| AverageEpisodeLength | 941.62     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.91     |
| TotalNEpisodes       | 16578      |
| TotalNSamples        | 2.1002e+06 |
| ExplainedVariance    | 0.094068   |
-------------------------------------
[2018-12-22 10:49:51.364203 UTC] Saving snapshot
[2018-12-22 10:49:51.372328 UTC] Starting iteration 421
[2018-12-22 10:49:51.372529 UTC] Start collecting samples
[2018-12-22 10:49:54.346740 UTC] Computing input variables for policy optimization
[2018-12-22 10:49:54.428179 UTC] Performing policy update
[2018-12-22 10:49:54.428778 UTC] Computing gradient in Euclidean space
[2018-12-22 10:49:54.517362 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:49:55.574290 UTC] Performing line search
[2018-12-22 10:49:55.700887 UTC] Updating baseline
[2018-12-22 10:49:57.136514 UTC] Computing logging information
-------------------------------------
| Iteration            | 421        |
| ExpectedImprovement  | 0.015835   |
| ActualImprovement    | 0.015607   |
| ImprovementRatio     | 0.98562    |
| MeanKL               | 0.0071788  |
| Entropy              | 0.73927    |
| Perplexity           | 2.0944     |
| AveragePolicyStd     | 0.27838    |
| AveragePolicyStd[0]  | 0.30312    |
| AveragePolicyStd[1]  | 0.36264    |
| AveragePolicyStd[2]  | 0.23649    |
| AveragePolicyStd[3]  | 0.29855    |
| AveragePolicyStd[4]  | 0.20402    |
| AveragePolicyStd[5]  | 0.26549    |
| AverageReturn        | 1065.6     |
| MinReturn            | 60.243     |
| MaxReturn            | 1198.2     |
| StdReturn            | 216.15     |
| AverageEpisodeLength | 941.2      |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.17     |
| TotalNEpisodes       | 16587      |
| TotalNSamples        | 2.1082e+06 |
| ExplainedVariance    | 0.13156    |
-------------------------------------
[2018-12-22 10:49:57.481003 UTC] Saving snapshot
[2018-12-22 10:49:57.481240 UTC] Starting iteration 422
[2018-12-22 10:49:57.481356 UTC] Start collecting samples
[2018-12-22 10:50:00.404161 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:00.478456 UTC] Performing policy update
[2018-12-22 10:50:00.479100 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:00.569842 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:01.643808 UTC] Performing line search
[2018-12-22 10:50:01.771880 UTC] Updating baseline
[2018-12-22 10:50:03.036878 UTC] Computing logging information
-------------------------------------
| Iteration            | 422        |
| ExpectedImprovement  | 0.015765   |
| ActualImprovement    | 0.014918   |
| ImprovementRatio     | 0.94627    |
| MeanKL               | 0.0074941  |
| Entropy              | 0.72365    |
| Perplexity           | 2.0619     |
| AveragePolicyStd     | 0.2778     |
| AveragePolicyStd[0]  | 0.30446    |
| AveragePolicyStd[1]  | 0.36213    |
| AveragePolicyStd[2]  | 0.23541    |
| AveragePolicyStd[3]  | 0.29763    |
| AveragePolicyStd[4]  | 0.20208    |
| AveragePolicyStd[5]  | 0.26511    |
| AverageReturn        | 1065.8     |
| MinReturn            | 60.243     |
| MaxReturn            | 1198.2     |
| StdReturn            | 216.21     |
| AverageEpisodeLength | 941.2      |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.17     |
| TotalNEpisodes       | 16588      |
| TotalNSamples        | 2.1092e+06 |
| ExplainedVariance    | 0.037825   |
-------------------------------------
[2018-12-22 10:50:03.381097 UTC] Saving snapshot
[2018-12-22 10:50:03.381364 UTC] Starting iteration 423
[2018-12-22 10:50:03.381489 UTC] Start collecting samples
[2018-12-22 10:50:06.342054 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:06.419823 UTC] Performing policy update
[2018-12-22 10:50:06.420416 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:06.509825 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:07.574910 UTC] Performing line search
[2018-12-22 10:50:07.701554 UTC] Updating baseline
[2018-12-22 10:50:09.311828 UTC] Computing logging information
-------------------------------------
| Iteration            | 423        |
| ExpectedImprovement  | 0.017319   |
| ActualImprovement    | 0.015789   |
| ImprovementRatio     | 0.91168    |
| MeanKL               | 0.0070637  |
| Entropy              | 0.71785    |
| Perplexity           | 2.05       |
| AveragePolicyStd     | 0.27755    |
| AveragePolicyStd[0]  | 0.30487    |
| AveragePolicyStd[1]  | 0.36138    |
| AveragePolicyStd[2]  | 0.23509    |
| AveragePolicyStd[3]  | 0.29735    |
| AveragePolicyStd[4]  | 0.20157    |
| AveragePolicyStd[5]  | 0.26505    |
| AverageReturn        | 1067       |
| MinReturn            | 60.243     |
| MaxReturn            | 1198.2     |
| StdReturn            | 216.64     |
| AverageEpisodeLength | 941.2      |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.17     |
| TotalNEpisodes       | 16592      |
| TotalNSamples        | 2.1132e+06 |
| ExplainedVariance    | 0.0041623  |
-------------------------------------
[2018-12-22 10:50:09.660590 UTC] Saving snapshot
[2018-12-22 10:50:09.660826 UTC] Starting iteration 424
[2018-12-22 10:50:09.660951 UTC] Start collecting samples
[2018-12-22 10:50:12.659366 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:12.740605 UTC] Performing policy update
[2018-12-22 10:50:12.741402 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:12.831694 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:13.886995 UTC] Performing line search
[2018-12-22 10:50:14.015976 UTC] Updating baseline
[2018-12-22 10:50:15.446758 UTC] Computing logging information
-------------------------------------
| Iteration            | 424        |
| ExpectedImprovement  | 0.01728    |
| ActualImprovement    | 0.016366   |
| ImprovementRatio     | 0.9471     |
| MeanKL               | 0.0072291  |
| Entropy              | 0.71851    |
| Perplexity           | 2.0514     |
| AveragePolicyStd     | 0.27763    |
| AveragePolicyStd[0]  | 0.30554    |
| AveragePolicyStd[1]  | 0.36124    |
| AveragePolicyStd[2]  | 0.23499    |
| AveragePolicyStd[3]  | 0.29777    |
| AveragePolicyStd[4]  | 0.20097    |
| AveragePolicyStd[5]  | 0.26528    |
| AverageReturn        | 1082.6     |
| MinReturn            | 60.243     |
| MaxReturn            | 1241.4     |
| StdReturn            | 205.36     |
| AverageEpisodeLength | 948.86     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.27     |
| TotalNEpisodes       | 16601      |
| TotalNSamples        | 2.1222e+06 |
| ExplainedVariance    | 0.0025444  |
-------------------------------------
[2018-12-22 10:50:15.795652 UTC] Saving snapshot
[2018-12-22 10:50:15.795910 UTC] Starting iteration 425
[2018-12-22 10:50:15.796033 UTC] Start collecting samples
[2018-12-22 10:50:18.762706 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:18.841372 UTC] Performing policy update
[2018-12-22 10:50:18.842103 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:18.931235 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:20.007932 UTC] Performing line search
[2018-12-22 10:50:20.134022 UTC] Updating baseline
[2018-12-22 10:50:21.558410 UTC] Computing logging information
-------------------------------------
| Iteration            | 425        |
| ExpectedImprovement  | 0.018807   |
| ActualImprovement    | 0.01777    |
| ImprovementRatio     | 0.94483    |
| MeanKL               | 0.0070139  |
| Entropy              | 0.71469    |
| Perplexity           | 2.0436     |
| AveragePolicyStd     | 0.27737    |
| AveragePolicyStd[0]  | 0.30646    |
| AveragePolicyStd[1]  | 0.3596     |
| AveragePolicyStd[2]  | 0.23493    |
| AveragePolicyStd[3]  | 0.29695    |
| AveragePolicyStd[4]  | 0.20152    |
| AveragePolicyStd[5]  | 0.26474    |
| AverageReturn        | 1064.9     |
| MinReturn            | 60.243     |
| MaxReturn            | 1241.4     |
| StdReturn            | 243.5      |
| AverageEpisodeLength | 932.01     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.33     |
| TotalNEpisodes       | 16606      |
| TotalNSamples        | 2.1255e+06 |
| ExplainedVariance    | 0.26275    |
-------------------------------------
[2018-12-22 10:50:21.903196 UTC] Saving snapshot
[2018-12-22 10:50:21.903436 UTC] Starting iteration 426
[2018-12-22 10:50:21.903573 UTC] Start collecting samples
[2018-12-22 10:50:24.831146 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:24.908864 UTC] Performing policy update
[2018-12-22 10:50:24.909574 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:24.999694 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:26.067367 UTC] Performing line search
[2018-12-22 10:50:26.193871 UTC] Updating baseline
[2018-12-22 10:50:27.625082 UTC] Computing logging information
-------------------------------------
| Iteration            | 426        |
| ExpectedImprovement  | 0.018251   |
| ActualImprovement    | 0.017428   |
| ImprovementRatio     | 0.95492    |
| MeanKL               | 0.0069248  |
| Entropy              | 0.7163     |
| Perplexity           | 2.0468     |
| AveragePolicyStd     | 0.27746    |
| AveragePolicyStd[0]  | 0.30688    |
| AveragePolicyStd[1]  | 0.36004    |
| AveragePolicyStd[2]  | 0.23434    |
| AveragePolicyStd[3]  | 0.29729    |
| AveragePolicyStd[4]  | 0.20208    |
| AveragePolicyStd[5]  | 0.2641     |
| AverageReturn        | 1068.1     |
| MinReturn            | 60.243     |
| MaxReturn            | 1241.4     |
| StdReturn            | 244.49     |
| AverageEpisodeLength | 932.01     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.33     |
| TotalNEpisodes       | 16610      |
| TotalNSamples        | 2.1295e+06 |
| ExplainedVariance    | 0.048329   |
-------------------------------------
[2018-12-22 10:50:27.979458 UTC] Saving snapshot
[2018-12-22 10:50:27.979713 UTC] Starting iteration 427
[2018-12-22 10:50:27.979831 UTC] Start collecting samples
[2018-12-22 10:50:30.984414 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:31.066718 UTC] Performing policy update
[2018-12-22 10:50:31.067296 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:31.156716 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:32.231989 UTC] Performing line search
[2018-12-22 10:50:32.360423 UTC] Updating baseline
[2018-12-22 10:50:33.802158 UTC] Computing logging information
------------------------------------
| Iteration            | 427       |
| ExpectedImprovement  | 0.015312  |
| ActualImprovement    | 0.014824  |
| ImprovementRatio     | 0.96817   |
| MeanKL               | 0.0070604 |
| Entropy              | 0.71606   |
| Perplexity           | 2.0464    |
| AveragePolicyStd     | 0.27751   |
| AveragePolicyStd[0]  | 0.30744   |
| AveragePolicyStd[1]  | 0.36078   |
| AveragePolicyStd[2]  | 0.23481   |
| AveragePolicyStd[3]  | 0.29725   |
| AveragePolicyStd[4]  | 0.2015    |
| AveragePolicyStd[5]  | 0.2633    |
| AverageReturn        | 1083.5    |
| MinReturn            | 60.243    |
| MaxReturn            | 1241.4    |
| StdReturn            | 222.24    |
| AverageEpisodeLength | 942.16    |
| MinEpisodeLength     | 59        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.48    |
| TotalNEpisodes       | 16619     |
| TotalNSamples        | 2.138e+06 |
| ExplainedVariance    | -0.014861 |
------------------------------------
[2018-12-22 10:50:34.153251 UTC] Saving snapshot
[2018-12-22 10:50:34.153510 UTC] Starting iteration 428
[2018-12-22 10:50:34.153650 UTC] Start collecting samples
[2018-12-22 10:50:37.077276 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:37.152993 UTC] Performing policy update
[2018-12-22 10:50:37.153705 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:37.244370 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:38.311854 UTC] Performing line search
[2018-12-22 10:50:38.438759 UTC] Updating baseline
[2018-12-22 10:50:39.778150 UTC] Computing logging information
-----------------------------------
| Iteration            | 428      |
| ExpectedImprovement  | 0.018157 |
| ActualImprovement    | 0.01642  |
| ImprovementRatio     | 0.90433  |
| MeanKL               | 0.006857 |
| Entropy              | 0.70329  |
| Perplexity           | 2.0204   |
| AveragePolicyStd     | 0.27693  |
| AveragePolicyStd[0]  | 0.30592  |
| AveragePolicyStd[1]  | 0.36056  |
| AveragePolicyStd[2]  | 0.2343   |
| AveragePolicyStd[3]  | 0.29743  |
| AveragePolicyStd[4]  | 0.2014   |
| AveragePolicyStd[5]  | 0.26195  |
| AverageReturn        | 1083.7   |
| MinReturn            | 60.243   |
| MaxReturn            | 1241.4   |
| StdReturn            | 222.3    |
| AverageEpisodeLength | 942.16   |
| MinEpisodeLength     | 59       |
| MaxEpisodeLength     | 1000     |
| StdEpisodeLength     | 187.48   |
| TotalNEpisodes       | 16621    |
| TotalNSamples        | 2.14e+06 |
| ExplainedVariance    | 0.015631 |
-----------------------------------
[2018-12-22 10:50:40.126844 UTC] Saving snapshot
[2018-12-22 10:50:40.127100 UTC] Starting iteration 429
[2018-12-22 10:50:40.127218 UTC] Start collecting samples
[2018-12-22 10:50:43.065943 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:43.144159 UTC] Performing policy update
[2018-12-22 10:50:43.144834 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:43.235295 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:44.295600 UTC] Performing line search
[2018-12-22 10:50:44.423490 UTC] Updating baseline
[2018-12-22 10:50:45.762855 UTC] Computing logging information
-------------------------------------
| Iteration            | 429        |
| ExpectedImprovement  | 0.018914   |
| ActualImprovement    | 0.016948   |
| ImprovementRatio     | 0.89604    |
| MeanKL               | 0.0067523  |
| Entropy              | 0.70214    |
| Perplexity           | 2.0181     |
| AveragePolicyStd     | 0.27684    |
| AveragePolicyStd[0]  | 0.3041     |
| AveragePolicyStd[1]  | 0.36062    |
| AveragePolicyStd[2]  | 0.23427    |
| AveragePolicyStd[3]  | 0.29799    |
| AveragePolicyStd[4]  | 0.20161    |
| AveragePolicyStd[5]  | 0.26243    |
| AverageReturn        | 1090       |
| MinReturn            | 60.243     |
| MaxReturn            | 1241.4     |
| StdReturn            | 212.48     |
| AverageEpisodeLength | 945.43     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.2      |
| TotalNEpisodes       | 16625      |
| TotalNSamples        | 2.1437e+06 |
| ExplainedVariance    | 0.18941    |
-------------------------------------
[2018-12-22 10:50:46.108418 UTC] Saving snapshot
[2018-12-22 10:50:46.108670 UTC] Starting iteration 430
[2018-12-22 10:50:46.108792 UTC] Start collecting samples
[2018-12-22 10:50:49.115338 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:49.198104 UTC] Performing policy update
[2018-12-22 10:50:49.198749 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:49.289148 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:50.358055 UTC] Performing line search
[2018-12-22 10:50:50.485145 UTC] Updating baseline
[2018-12-22 10:50:51.744920 UTC] Computing logging information
------------------------------------
| Iteration            | 430       |
| ExpectedImprovement  | 0.016303  |
| ActualImprovement    | 0.015592  |
| ImprovementRatio     | 0.95636   |
| MeanKL               | 0.0071904 |
| Entropy              | 0.69615   |
| Perplexity           | 2.006     |
| AveragePolicyStd     | 0.27663   |
| AveragePolicyStd[0]  | 0.30378   |
| AveragePolicyStd[1]  | 0.36201   |
| AveragePolicyStd[2]  | 0.2326    |
| AveragePolicyStd[3]  | 0.29775   |
| AveragePolicyStd[4]  | 0.20229   |
| AveragePolicyStd[5]  | 0.26133   |
| AverageReturn        | 1088.7    |
| MinReturn            | 60.243    |
| MaxReturn            | 1241.4    |
| StdReturn            | 228.02    |
| AverageEpisodeLength | 938.36    |
| MinEpisodeLength     | 59        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 190.49    |
| TotalNEpisodes       | 16636     |
| TotalNSamples        | 2.154e+06 |
| ExplainedVariance    | 0.11486   |
------------------------------------
[2018-12-22 10:50:52.097503 UTC] Saving snapshot
[2018-12-22 10:50:52.105634 UTC] Starting iteration 431
[2018-12-22 10:50:52.105863 UTC] Start collecting samples
[2018-12-22 10:50:55.015362 UTC] Computing input variables for policy optimization
[2018-12-22 10:50:55.090907 UTC] Performing policy update
[2018-12-22 10:50:55.091664 UTC] Computing gradient in Euclidean space
[2018-12-22 10:50:55.181774 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:50:56.249463 UTC] Performing line search
[2018-12-22 10:50:56.375455 UTC] Updating baseline
[2018-12-22 10:50:58.055904 UTC] Computing logging information
------------------------------------
| Iteration            | 431       |
| ExpectedImprovement  | 0.017296  |
| ActualImprovement    | 0.01639   |
| ImprovementRatio     | 0.94763   |
| MeanKL               | 0.0074634 |
| Entropy              | 0.70106   |
| Perplexity           | 2.0159    |
| AveragePolicyStd     | 0.27697   |
| AveragePolicyStd[0]  | 0.30521   |
| AveragePolicyStd[1]  | 0.36361   |
| AveragePolicyStd[2]  | 0.23196   |
| AveragePolicyStd[3]  | 0.29765   |
| AveragePolicyStd[4]  | 0.20214   |
| AveragePolicyStd[5]  | 0.26124   |
| AverageReturn        | 1089.7    |
| MinReturn            | 60.243    |
| MaxReturn            | 1241.4    |
| StdReturn            | 228.31    |
| AverageEpisodeLength | 938.36    |
| MinEpisodeLength     | 59        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 190.49    |
| TotalNEpisodes       | 16638     |
| TotalNSamples        | 2.156e+06 |
| ExplainedVariance    | 0.042231  |
------------------------------------
[2018-12-22 10:50:58.402020 UTC] Saving snapshot
[2018-12-22 10:50:58.402303 UTC] Starting iteration 432
[2018-12-22 10:50:58.402427 UTC] Start collecting samples
[2018-12-22 10:51:01.344809 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:01.422256 UTC] Performing policy update
[2018-12-22 10:51:01.422918 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:01.513070 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:02.570289 UTC] Performing line search
[2018-12-22 10:51:02.697253 UTC] Updating baseline
[2018-12-22 10:51:04.124003 UTC] Computing logging information
------------------------------------
| Iteration            | 432       |
| ExpectedImprovement  | 0.015476  |
| ActualImprovement    | 0.014857  |
| ImprovementRatio     | 0.96003   |
| MeanKL               | 0.0072681 |
| Entropy              | 0.69699   |
| Perplexity           | 2.0077    |
| AveragePolicyStd     | 0.2767    |
| AveragePolicyStd[0]  | 0.3059    |
| AveragePolicyStd[1]  | 0.36267   |
| AveragePolicyStd[2]  | 0.23185   |
| AveragePolicyStd[3]  | 0.29619   |
| AveragePolicyStd[4]  | 0.20298   |
| AveragePolicyStd[5]  | 0.26059   |
| AverageReturn        | 1091.2    |
| MinReturn            | 60.243    |
| MaxReturn            | 1241.4    |
| StdReturn            | 228.83    |
| AverageEpisodeLength | 938.36    |
| MinEpisodeLength     | 59        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 190.49    |
| TotalNEpisodes       | 16641     |
| TotalNSamples        | 2.159e+06 |
| ExplainedVariance    | 0.087564  |
------------------------------------
[2018-12-22 10:51:04.471289 UTC] Saving snapshot
[2018-12-22 10:51:04.471587 UTC] Starting iteration 433
[2018-12-22 10:51:04.471713 UTC] Start collecting samples
[2018-12-22 10:51:07.491350 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:07.572262 UTC] Performing policy update
[2018-12-22 10:51:07.572898 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:07.662396 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:08.728377 UTC] Performing line search
[2018-12-22 10:51:08.855086 UTC] Updating baseline
[2018-12-22 10:51:10.542977 UTC] Computing logging information
-------------------------------------
| Iteration            | 433        |
| ExpectedImprovement  | 0.017001   |
| ActualImprovement    | 0.016172   |
| ImprovementRatio     | 0.95123    |
| MeanKL               | 0.0072449  |
| Entropy              | 0.70079    |
| Perplexity           | 2.0154     |
| AveragePolicyStd     | 0.27686    |
| AveragePolicyStd[0]  | 0.30487    |
| AveragePolicyStd[1]  | 0.36311    |
| AveragePolicyStd[2]  | 0.23175    |
| AveragePolicyStd[3]  | 0.29712    |
| AveragePolicyStd[4]  | 0.20335    |
| AveragePolicyStd[5]  | 0.26096    |
| AverageReturn        | 1089.7     |
| MinReturn            | 60.243     |
| MaxReturn            | 1241.4     |
| StdReturn            | 242.52     |
| AverageEpisodeLength | 932.11     |
| MinEpisodeLength     | 59         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.39     |
| TotalNEpisodes       | 16651      |
| TotalNSamples        | 2.1677e+06 |
| ExplainedVariance    | 0.13811    |
-------------------------------------
[2018-12-22 10:51:10.892670 UTC] Saving snapshot
[2018-12-22 10:51:10.892926 UTC] Starting iteration 434
[2018-12-22 10:51:10.893041 UTC] Start collecting samples
[2018-12-22 10:51:13.857990 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:13.936652 UTC] Performing policy update
[2018-12-22 10:51:13.937443 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:14.026811 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:15.093965 UTC] Performing line search
[2018-12-22 10:51:15.221469 UTC] Updating baseline
[2018-12-22 10:51:16.654628 UTC] Computing logging information
-------------------------------------
| Iteration            | 434        |
| ExpectedImprovement  | 0.017594   |
| ActualImprovement    | 0.016552   |
| ImprovementRatio     | 0.94077    |
| MeanKL               | 0.0067505  |
| Entropy              | 0.69902    |
| Perplexity           | 2.0118     |
| AveragePolicyStd     | 0.27695    |
| AveragePolicyStd[0]  | 0.30596    |
| AveragePolicyStd[1]  | 0.36422    |
| AveragePolicyStd[2]  | 0.23002    |
| AveragePolicyStd[3]  | 0.29768    |
| AveragePolicyStd[4]  | 0.20243    |
| AveragePolicyStd[5]  | 0.26142    |
| AverageReturn        | 1085.4     |
| MinReturn            | 78.214     |
| MaxReturn            | 1241.4     |
| StdReturn            | 244.38     |
| AverageEpisodeLength | 927.91     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.72     |
| TotalNEpisodes       | 16657      |
| TotalNSamples        | 2.1723e+06 |
| ExplainedVariance    | 0.28843    |
-------------------------------------
[2018-12-22 10:51:17.002801 UTC] Saving snapshot
[2018-12-22 10:51:17.003040 UTC] Starting iteration 435
[2018-12-22 10:51:17.003156 UTC] Start collecting samples
[2018-12-22 10:51:19.966501 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:20.044336 UTC] Performing policy update
[2018-12-22 10:51:20.044918 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:20.135443 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:21.198827 UTC] Performing line search
[2018-12-22 10:51:21.325716 UTC] Updating baseline
[2018-12-22 10:51:22.619983 UTC] Computing logging information
-------------------------------------
| Iteration            | 435        |
| ExpectedImprovement  | 0.015714   |
| ActualImprovement    | 0.01568    |
| ImprovementRatio     | 0.99781    |
| MeanKL               | 0.0073421  |
| Entropy              | 0.69363    |
| Perplexity           | 2.001      |
| AveragePolicyStd     | 0.27674    |
| AveragePolicyStd[0]  | 0.30354    |
| AveragePolicyStd[1]  | 0.36574    |
| AveragePolicyStd[2]  | 0.22834    |
| AveragePolicyStd[3]  | 0.2979     |
| AveragePolicyStd[4]  | 0.20319    |
| AveragePolicyStd[5]  | 0.26174    |
| AverageReturn        | 1073.3     |
| MinReturn            | 78.214     |
| MaxReturn            | 1241.4     |
| StdReturn            | 254.09     |
| AverageEpisodeLength | 916.58     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.57     |
| TotalNEpisodes       | 16661      |
| TotalNSamples        | 2.1752e+06 |
| ExplainedVariance    | 0.51768    |
-------------------------------------
[2018-12-22 10:51:22.990903 UTC] Saving snapshot
[2018-12-22 10:51:22.991144 UTC] Starting iteration 436
[2018-12-22 10:51:22.991260 UTC] Start collecting samples
[2018-12-22 10:51:26.192128 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:26.277382 UTC] Performing policy update
[2018-12-22 10:51:26.278009 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:26.371290 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:27.430972 UTC] Performing line search
[2018-12-22 10:51:27.563153 UTC] Updating baseline
[2018-12-22 10:51:28.854795 UTC] Computing logging information
-------------------------------------
| Iteration            | 436        |
| ExpectedImprovement  | 0.017003   |
| ActualImprovement    | 0.016082   |
| ImprovementRatio     | 0.94582    |
| MeanKL               | 0.0068382  |
| Entropy              | 0.69002    |
| Perplexity           | 1.9938     |
| AveragePolicyStd     | 0.27656    |
| AveragePolicyStd[0]  | 0.30214    |
| AveragePolicyStd[1]  | 0.36563    |
| AveragePolicyStd[2]  | 0.22757    |
| AveragePolicyStd[3]  | 0.29829    |
| AveragePolicyStd[4]  | 0.20348    |
| AveragePolicyStd[5]  | 0.26227    |
| AverageReturn        | 1077.1     |
| MinReturn            | 78.214     |
| MaxReturn            | 1244.3     |
| StdReturn            | 255.74     |
| AverageEpisodeLength | 916.58     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.57     |
| TotalNEpisodes       | 16668      |
| TotalNSamples        | 2.1822e+06 |
| ExplainedVariance    | 0.0095921  |
-------------------------------------
[2018-12-22 10:51:29.230726 UTC] Saving snapshot
[2018-12-22 10:51:29.230987 UTC] Starting iteration 437
[2018-12-22 10:51:29.231106 UTC] Start collecting samples
[2018-12-22 10:51:32.417453 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:32.496178 UTC] Performing policy update
[2018-12-22 10:51:32.497063 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:32.584066 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:33.644640 UTC] Performing line search
[2018-12-22 10:51:33.772562 UTC] Updating baseline
[2018-12-22 10:51:35.034784 UTC] Computing logging information
-------------------------------------
| Iteration            | 437        |
| ExpectedImprovement  | 0.016757   |
| ActualImprovement    | 0.016361   |
| ImprovementRatio     | 0.97637    |
| MeanKL               | 0.0070609  |
| Entropy              | 0.69137    |
| Perplexity           | 1.9965     |
| AveragePolicyStd     | 0.27655    |
| AveragePolicyStd[0]  | 0.30258    |
| AveragePolicyStd[1]  | 0.3644     |
| AveragePolicyStd[2]  | 0.22761    |
| AveragePolicyStd[3]  | 0.29809    |
| AveragePolicyStd[4]  | 0.20384    |
| AveragePolicyStd[5]  | 0.26278    |
| AverageReturn        | 1063.9     |
| MinReturn            | 78.214     |
| MaxReturn            | 1261.6     |
| StdReturn            | 275.07     |
| AverageEpisodeLength | 902.86     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.75     |
| TotalNEpisodes       | 16674      |
| TotalNSamples        | 2.1868e+06 |
| ExplainedVariance    | 0.2594     |
-------------------------------------
[2018-12-22 10:51:35.382932 UTC] Saving snapshot
[2018-12-22 10:51:35.383175 UTC] Starting iteration 438
[2018-12-22 10:51:35.383300 UTC] Start collecting samples
[2018-12-22 10:51:38.312037 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:38.389138 UTC] Performing policy update
[2018-12-22 10:51:38.390008 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:38.480860 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:39.534060 UTC] Performing line search
[2018-12-22 10:51:39.662104 UTC] Updating baseline
[2018-12-22 10:51:41.098623 UTC] Computing logging information
-------------------------------------
| Iteration            | 438        |
| ExpectedImprovement  | 0.01717    |
| ActualImprovement    | 0.015617   |
| ImprovementRatio     | 0.90957    |
| MeanKL               | 0.0069431  |
| Entropy              | 0.69206    |
| Perplexity           | 1.9978     |
| AveragePolicyStd     | 0.27649    |
| AveragePolicyStd[0]  | 0.30073    |
| AveragePolicyStd[1]  | 0.36326    |
| AveragePolicyStd[2]  | 0.22704    |
| AveragePolicyStd[3]  | 0.29846    |
| AveragePolicyStd[4]  | 0.20437    |
| AveragePolicyStd[5]  | 0.26505    |
| AverageReturn        | 1067.9     |
| MinReturn            | 78.214     |
| MaxReturn            | 1261.6     |
| StdReturn            | 273.9      |
| AverageEpisodeLength | 904.71     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.14     |
| TotalNEpisodes       | 16678      |
| TotalNSamples        | 2.1906e+06 |
| ExplainedVariance    | 0.16677    |
-------------------------------------
[2018-12-22 10:51:41.441032 UTC] Saving snapshot
[2018-12-22 10:51:41.441294 UTC] Starting iteration 439
[2018-12-22 10:51:41.441412 UTC] Start collecting samples
[2018-12-22 10:51:44.349251 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:44.426532 UTC] Performing policy update
[2018-12-22 10:51:44.427360 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:44.516971 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:45.567499 UTC] Performing line search
[2018-12-22 10:51:45.694925 UTC] Updating baseline
[2018-12-22 10:51:47.475058 UTC] Computing logging information
-------------------------------------
| Iteration            | 439        |
| ExpectedImprovement  | 0.020405   |
| ActualImprovement    | 0.018155   |
| ImprovementRatio     | 0.88971    |
| MeanKL               | 0.0069986  |
| Entropy              | 0.67036    |
| Perplexity           | 1.9549     |
| AveragePolicyStd     | 0.27547    |
| AveragePolicyStd[0]  | 0.29866    |
| AveragePolicyStd[1]  | 0.36253    |
| AveragePolicyStd[2]  | 0.22718    |
| AveragePolicyStd[3]  | 0.29633    |
| AveragePolicyStd[4]  | 0.20316    |
| AveragePolicyStd[5]  | 0.26498    |
| AverageReturn        | 1081.2     |
| MinReturn            | 78.214     |
| MaxReturn            | 1261.6     |
| StdReturn            | 266.65     |
| AverageEpisodeLength | 914.37     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.56     |
| TotalNEpisodes       | 16681      |
| TotalNSamples        | 2.1936e+06 |
| ExplainedVariance    | -0.0019447 |
-------------------------------------
[2018-12-22 10:51:47.826108 UTC] Saving snapshot
[2018-12-22 10:51:47.826366 UTC] Starting iteration 440
[2018-12-22 10:51:47.826492 UTC] Start collecting samples
[2018-12-22 10:51:50.803962 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:50.885302 UTC] Performing policy update
[2018-12-22 10:51:50.886052 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:50.974931 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:52.031751 UTC] Performing line search
[2018-12-22 10:51:52.164196 UTC] Updating baseline
[2018-12-22 10:51:53.678633 UTC] Computing logging information
-------------------------------------
| Iteration            | 440        |
| ExpectedImprovement  | 0.016517   |
| ActualImprovement    | 0.015499   |
| ImprovementRatio     | 0.93837    |
| MeanKL               | 0.0070185  |
| Entropy              | 0.65435    |
| Perplexity           | 1.9239     |
| AveragePolicyStd     | 0.27475    |
| AveragePolicyStd[0]  | 0.29697    |
| AveragePolicyStd[1]  | 0.3629     |
| AveragePolicyStd[2]  | 0.22573    |
| AveragePolicyStd[3]  | 0.29505    |
| AveragePolicyStd[4]  | 0.20346    |
| AveragePolicyStd[5]  | 0.26442    |
| AverageReturn        | 1085.5     |
| MinReturn            | 78.214     |
| MaxReturn            | 1261.6     |
| StdReturn            | 268.03     |
| AverageEpisodeLength | 914.37     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.56     |
| TotalNEpisodes       | 16689      |
| TotalNSamples        | 2.2016e+06 |
| ExplainedVariance    | -0.0090675 |
-------------------------------------
[2018-12-22 10:51:54.023723 UTC] Saving snapshot
[2018-12-22 10:51:54.031825 UTC] Starting iteration 441
[2018-12-22 10:51:54.032009 UTC] Start collecting samples
[2018-12-22 10:51:56.964702 UTC] Computing input variables for policy optimization
[2018-12-22 10:51:57.042270 UTC] Performing policy update
[2018-12-22 10:51:57.042905 UTC] Computing gradient in Euclidean space
[2018-12-22 10:51:57.131251 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:51:58.197453 UTC] Performing line search
[2018-12-22 10:51:58.325864 UTC] Updating baseline
[2018-12-22 10:51:59.847559 UTC] Computing logging information
-------------------------------------
| Iteration            | 441        |
| ExpectedImprovement  | 0.017775   |
| ActualImprovement    | 0.016513   |
| ImprovementRatio     | 0.92901    |
| MeanKL               | 0.0069732  |
| Entropy              | 0.65865    |
| Perplexity           | 1.9322     |
| AveragePolicyStd     | 0.27496    |
| AveragePolicyStd[0]  | 0.2967     |
| AveragePolicyStd[1]  | 0.36387    |
| AveragePolicyStd[2]  | 0.22566    |
| AveragePolicyStd[3]  | 0.29549    |
| AveragePolicyStd[4]  | 0.2041     |
| AveragePolicyStd[5]  | 0.26394    |
| AverageReturn        | 1087.1     |
| MinReturn            | 78.214     |
| MaxReturn            | 1266.4     |
| StdReturn            | 268.6      |
| AverageEpisodeLength | 914.37     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.56     |
| TotalNEpisodes       | 16694      |
| TotalNSamples        | 2.2066e+06 |
| ExplainedVariance    | 0.0098569  |
-------------------------------------
[2018-12-22 10:52:00.197139 UTC] Saving snapshot
[2018-12-22 10:52:00.197458 UTC] Starting iteration 442
[2018-12-22 10:52:00.197596 UTC] Start collecting samples
[2018-12-22 10:52:03.129486 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:03.206316 UTC] Performing policy update
[2018-12-22 10:52:03.207076 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:03.297046 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:04.351694 UTC] Performing line search
[2018-12-22 10:52:04.477849 UTC] Updating baseline
[2018-12-22 10:52:06.433487 UTC] Computing logging information
-------------------------------------
| Iteration            | 442        |
| ExpectedImprovement  | 0.019807   |
| ActualImprovement    | 0.018597   |
| ImprovementRatio     | 0.93889    |
| MeanKL               | 0.0068231  |
| Entropy              | 0.65133    |
| Perplexity           | 1.9181     |
| AveragePolicyStd     | 0.27465    |
| AveragePolicyStd[0]  | 0.29684    |
| AveragePolicyStd[1]  | 0.36393    |
| AveragePolicyStd[2]  | 0.22534    |
| AveragePolicyStd[3]  | 0.29427    |
| AveragePolicyStd[4]  | 0.20376    |
| AveragePolicyStd[5]  | 0.26375    |
| AverageReturn        | 1077.5     |
| MinReturn            | 78.214     |
| MaxReturn            | 1266.4     |
| StdReturn            | 286.88     |
| AverageEpisodeLength | 905.19     |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.53     |
| TotalNEpisodes       | 16697      |
| TotalNSamples        | 2.2087e+06 |
| ExplainedVariance    | 0.13271    |
-------------------------------------
[2018-12-22 10:52:06.780272 UTC] Saving snapshot
[2018-12-22 10:52:06.780515 UTC] Starting iteration 443
[2018-12-22 10:52:06.780655 UTC] Start collecting samples
[2018-12-22 10:52:09.757948 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:09.838936 UTC] Performing policy update
[2018-12-22 10:52:09.839724 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:09.932001 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:10.987992 UTC] Performing line search
[2018-12-22 10:52:11.114104 UTC] Updating baseline
[2018-12-22 10:52:12.560581 UTC] Computing logging information
-------------------------------------
| Iteration            | 443        |
| ExpectedImprovement  | 0.015964   |
| ActualImprovement    | 0.015336   |
| ImprovementRatio     | 0.96064    |
| MeanKL               | 0.007416   |
| Entropy              | 0.63755    |
| Perplexity           | 1.8918     |
| AveragePolicyStd     | 0.274      |
| AveragePolicyStd[0]  | 0.29613    |
| AveragePolicyStd[1]  | 0.36273    |
| AveragePolicyStd[2]  | 0.22491    |
| AveragePolicyStd[3]  | 0.29363    |
| AveragePolicyStd[4]  | 0.20331    |
| AveragePolicyStd[5]  | 0.2633     |
| AverageReturn        | 1100.5     |
| MinReturn            | 82.326     |
| MaxReturn            | 1266.4     |
| StdReturn            | 256.05     |
| AverageEpisodeLength | 922.04     |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.78     |
| TotalNEpisodes       | 16706      |
| TotalNSamples        | 2.2177e+06 |
| ExplainedVariance    | 0.011343   |
-------------------------------------
[2018-12-22 10:52:12.911492 UTC] Saving snapshot
[2018-12-22 10:52:12.911755 UTC] Starting iteration 444
[2018-12-22 10:52:12.911879 UTC] Start collecting samples
[2018-12-22 10:52:15.860272 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:15.943271 UTC] Performing policy update
[2018-12-22 10:52:15.944056 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:16.034918 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:17.093277 UTC] Performing line search
[2018-12-22 10:52:17.220146 UTC] Updating baseline
[2018-12-22 10:52:18.918192 UTC] Computing logging information
-------------------------------------
| Iteration            | 444        |
| ExpectedImprovement  | 0.018839   |
| ActualImprovement    | 0.018019   |
| ImprovementRatio     | 0.95646    |
| MeanKL               | 0.0070332  |
| Entropy              | 0.61764    |
| Perplexity           | 1.8546     |
| AveragePolicyStd     | 0.27309    |
| AveragePolicyStd[0]  | 0.29583    |
| AveragePolicyStd[1]  | 0.36087    |
| AveragePolicyStd[2]  | 0.22439    |
| AveragePolicyStd[3]  | 0.29274    |
| AveragePolicyStd[4]  | 0.20231    |
| AveragePolicyStd[5]  | 0.26238    |
| AverageReturn        | 1089.3     |
| MinReturn            | 82.326     |
| MaxReturn            | 1266.4     |
| StdReturn            | 274.77     |
| AverageEpisodeLength | 910.8      |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.14     |
| TotalNEpisodes       | 16711      |
| TotalNSamples        | 2.2216e+06 |
| ExplainedVariance    | 0.14722    |
-------------------------------------
[2018-12-22 10:52:19.267380 UTC] Saving snapshot
[2018-12-22 10:52:19.267636 UTC] Starting iteration 445
[2018-12-22 10:52:19.267760 UTC] Start collecting samples
[2018-12-22 10:52:22.299779 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:22.382147 UTC] Performing policy update
[2018-12-22 10:52:22.382795 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:22.477182 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:23.590856 UTC] Performing line search
[2018-12-22 10:52:23.723738 UTC] Updating baseline
[2018-12-22 10:52:25.325236 UTC] Computing logging information
-------------------------------------
| Iteration            | 445        |
| ExpectedImprovement  | 0.018764   |
| ActualImprovement    | 0.017589   |
| ImprovementRatio     | 0.93734    |
| MeanKL               | 0.0069098  |
| Entropy              | 0.61691    |
| Perplexity           | 1.8532     |
| AveragePolicyStd     | 0.27311    |
| AveragePolicyStd[0]  | 0.29513    |
| AveragePolicyStd[1]  | 0.36159    |
| AveragePolicyStd[2]  | 0.22361    |
| AveragePolicyStd[3]  | 0.29417    |
| AveragePolicyStd[4]  | 0.20246    |
| AveragePolicyStd[5]  | 0.26172    |
| AverageReturn        | 1086.1     |
| MinReturn            | 82.326     |
| MaxReturn            | 1266.4     |
| StdReturn            | 284.22     |
| AverageEpisodeLength | 907.6      |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.11     |
| TotalNEpisodes       | 16714      |
| TotalNSamples        | 2.2238e+06 |
| ExplainedVariance    | 0.22625    |
-------------------------------------
[2018-12-22 10:52:25.706885 UTC] Saving snapshot
[2018-12-22 10:52:25.707132 UTC] Starting iteration 446
[2018-12-22 10:52:25.707249 UTC] Start collecting samples
[2018-12-22 10:52:28.789345 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:28.871520 UTC] Performing policy update
[2018-12-22 10:52:28.872179 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:28.962585 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:30.017707 UTC] Performing line search
[2018-12-22 10:52:30.143974 UTC] Updating baseline
[2018-12-22 10:52:31.482483 UTC] Computing logging information
------------------------------------
| Iteration            | 446       |
| ExpectedImprovement  | 0.016681  |
| ActualImprovement    | 0.016092  |
| ImprovementRatio     | 0.9647    |
| MeanKL               | 0.0075172 |
| Entropy              | 0.61081   |
| Perplexity           | 1.8419    |
| AveragePolicyStd     | 0.27277   |
| AveragePolicyStd[0]  | 0.29425   |
| AveragePolicyStd[1]  | 0.36117   |
| AveragePolicyStd[2]  | 0.22399   |
| AveragePolicyStd[3]  | 0.2937    |
| AveragePolicyStd[4]  | 0.20284   |
| AveragePolicyStd[5]  | 0.2607    |
| AverageReturn        | 1073.5    |
| MinReturn            | 82.326    |
| MaxReturn            | 1271      |
| StdReturn            | 317.14    |
| AverageEpisodeLength | 892.99    |
| MinEpisodeLength     | 82        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 256.42    |
| TotalNEpisodes       | 16725     |
| TotalNSamples        | 2.233e+06 |
| ExplainedVariance    | 0.15221   |
------------------------------------
[2018-12-22 10:52:31.823228 UTC] Saving snapshot
[2018-12-22 10:52:31.823537 UTC] Starting iteration 447
[2018-12-22 10:52:31.823677 UTC] Start collecting samples
[2018-12-22 10:52:34.777259 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:34.856058 UTC] Performing policy update
[2018-12-22 10:52:34.856976 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:34.946578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:36.012608 UTC] Performing line search
[2018-12-22 10:52:36.138303 UTC] Updating baseline
[2018-12-22 10:52:37.493790 UTC] Computing logging information
-------------------------------------
| Iteration            | 447        |
| ExpectedImprovement  | 0.018745   |
| ActualImprovement    | 0.017771   |
| ImprovementRatio     | 0.94805    |
| MeanKL               | 0.0070103  |
| Entropy              | 0.60393    |
| Perplexity           | 1.8293     |
| AveragePolicyStd     | 0.27246    |
| AveragePolicyStd[0]  | 0.29299    |
| AveragePolicyStd[1]  | 0.36068    |
| AveragePolicyStd[2]  | 0.22364    |
| AveragePolicyStd[3]  | 0.29464    |
| AveragePolicyStd[4]  | 0.20284    |
| AveragePolicyStd[5]  | 0.25994    |
| AverageReturn        | 1074.5     |
| MinReturn            | 82.326     |
| MaxReturn            | 1271       |
| StdReturn            | 316.56     |
| AverageEpisodeLength | 892.15     |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.76     |
| TotalNEpisodes       | 16730      |
| TotalNSamples        | 2.2372e+06 |
| ExplainedVariance    | 0.28867    |
-------------------------------------
[2018-12-22 10:52:37.843210 UTC] Saving snapshot
[2018-12-22 10:52:37.843483 UTC] Starting iteration 448
[2018-12-22 10:52:37.843645 UTC] Start collecting samples
[2018-12-22 10:52:40.789700 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:40.868614 UTC] Performing policy update
[2018-12-22 10:52:40.869184 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:40.959468 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:42.018904 UTC] Performing line search
[2018-12-22 10:52:42.145333 UTC] Updating baseline
[2018-12-22 10:52:43.768600 UTC] Computing logging information
-------------------------------------
| Iteration            | 448        |
| ExpectedImprovement  | 0.017252   |
| ActualImprovement    | 0.017154   |
| ImprovementRatio     | 0.99436    |
| MeanKL               | 0.0070724  |
| Entropy              | 0.59956    |
| Perplexity           | 1.8213     |
| AveragePolicyStd     | 0.27234    |
| AveragePolicyStd[0]  | 0.29396    |
| AveragePolicyStd[1]  | 0.36053    |
| AveragePolicyStd[2]  | 0.22313    |
| AveragePolicyStd[3]  | 0.29523    |
| AveragePolicyStd[4]  | 0.20219    |
| AveragePolicyStd[5]  | 0.25897    |
| AverageReturn        | 1061.2     |
| MinReturn            | 54.331     |
| MaxReturn            | 1271       |
| StdReturn            | 333        |
| AverageEpisodeLength | 879.79     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 268.92     |
| TotalNEpisodes       | 16735      |
| TotalNSamples        | 2.2409e+06 |
| ExplainedVariance    | 0.3779     |
-------------------------------------
[2018-12-22 10:52:44.119947 UTC] Saving snapshot
[2018-12-22 10:52:44.120196 UTC] Starting iteration 449
[2018-12-22 10:52:44.120317 UTC] Start collecting samples
[2018-12-22 10:52:47.069494 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:47.147091 UTC] Performing policy update
[2018-12-22 10:52:47.147721 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:47.236794 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:48.307210 UTC] Performing line search
[2018-12-22 10:52:48.434424 UTC] Updating baseline
[2018-12-22 10:52:49.696218 UTC] Computing logging information
-------------------------------------
| Iteration            | 449        |
| ExpectedImprovement  | 0.020757   |
| ActualImprovement    | 0.019412   |
| ImprovementRatio     | 0.93521    |
| MeanKL               | 0.0069437  |
| Entropy              | 0.61101    |
| Perplexity           | 1.8423     |
| AveragePolicyStd     | 0.27274    |
| AveragePolicyStd[0]  | 0.29411    |
| AveragePolicyStd[1]  | 0.35881    |
| AveragePolicyStd[2]  | 0.22403    |
| AveragePolicyStd[3]  | 0.29622    |
| AveragePolicyStd[4]  | 0.20244    |
| AveragePolicyStd[5]  | 0.26083    |
| AverageReturn        | 1041.2     |
| MinReturn            | 54.331     |
| MaxReturn            | 1272.4     |
| StdReturn            | 358.85     |
| AverageEpisodeLength | 862.04     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 289        |
| TotalNEpisodes       | 16739      |
| TotalNSamples        | 2.2432e+06 |
| ExplainedVariance    | 0.38249    |
-------------------------------------
[2018-12-22 10:52:50.055296 UTC] Saving snapshot
[2018-12-22 10:52:50.055586 UTC] Starting iteration 450
[2018-12-22 10:52:50.055711 UTC] Start collecting samples
[2018-12-22 10:52:53.031379 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:53.113206 UTC] Performing policy update
[2018-12-22 10:52:53.114006 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:53.203036 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:52:54.264074 UTC] Performing line search
[2018-12-22 10:52:54.390875 UTC] Updating baseline
[2018-12-22 10:52:55.910827 UTC] Computing logging information
-------------------------------------
| Iteration            | 450        |
| ExpectedImprovement  | 0.015994   |
| ActualImprovement    | 0.015258   |
| ImprovementRatio     | 0.95402    |
| MeanKL               | 0.0076072  |
| Entropy              | 0.60809    |
| Perplexity           | 1.8369     |
| AveragePolicyStd     | 0.27256    |
| AveragePolicyStd[0]  | 0.29401    |
| AveragePolicyStd[1]  | 0.35795    |
| AveragePolicyStd[2]  | 0.22393    |
| AveragePolicyStd[3]  | 0.29562    |
| AveragePolicyStd[4]  | 0.20248    |
| AveragePolicyStd[5]  | 0.26137    |
| AverageReturn        | 1059.3     |
| MinReturn            | 54.331     |
| MaxReturn            | 1281       |
| StdReturn            | 350.57     |
| AverageEpisodeLength | 874.54     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 281.55     |
| TotalNEpisodes       | 16748      |
| TotalNSamples        | 2.2522e+06 |
| ExplainedVariance    | -0.023839  |
-------------------------------------
[2018-12-22 10:52:56.262344 UTC] Saving snapshot
[2018-12-22 10:52:56.270343 UTC] Starting iteration 451
[2018-12-22 10:52:56.270536 UTC] Start collecting samples
[2018-12-22 10:52:59.183843 UTC] Computing input variables for policy optimization
[2018-12-22 10:52:59.261136 UTC] Performing policy update
[2018-12-22 10:52:59.261720 UTC] Computing gradient in Euclidean space
[2018-12-22 10:52:59.352333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:00.418496 UTC] Performing line search
[2018-12-22 10:53:00.544462 UTC] Updating baseline
[2018-12-22 10:53:02.174036 UTC] Computing logging information
-------------------------------------
| Iteration            | 451        |
| ExpectedImprovement  | 0.016432   |
| ActualImprovement    | 0.015022   |
| ImprovementRatio     | 0.91424    |
| MeanKL               | 0.0070339  |
| Entropy              | 0.62033    |
| Perplexity           | 1.8595     |
| AveragePolicyStd     | 0.27315    |
| AveragePolicyStd[0]  | 0.29474    |
| AveragePolicyStd[1]  | 0.35895    |
| AveragePolicyStd[2]  | 0.22441    |
| AveragePolicyStd[3]  | 0.2966     |
| AveragePolicyStd[4]  | 0.20257    |
| AveragePolicyStd[5]  | 0.26165    |
| AverageReturn        | 1057.6     |
| MinReturn            | 54.331     |
| MaxReturn            | 1281       |
| StdReturn            | 351.07     |
| AverageEpisodeLength | 872.11     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 281.5      |
| TotalNEpisodes       | 16751      |
| TotalNSamples        | 2.2549e+06 |
| ExplainedVariance    | 0.25707    |
-------------------------------------
[2018-12-22 10:53:02.523690 UTC] Saving snapshot
[2018-12-22 10:53:02.523951 UTC] Starting iteration 452
[2018-12-22 10:53:02.524072 UTC] Start collecting samples
[2018-12-22 10:53:05.428196 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:05.504421 UTC] Performing policy update
[2018-12-22 10:53:05.505015 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:05.593203 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:06.651598 UTC] Performing line search
[2018-12-22 10:53:06.778692 UTC] Updating baseline
[2018-12-22 10:53:08.370671 UTC] Computing logging information
-------------------------------------
| Iteration            | 452        |
| ExpectedImprovement  | 0.01626    |
| ActualImprovement    | 0.015262   |
| ImprovementRatio     | 0.93861    |
| MeanKL               | 0.0070209  |
| Entropy              | 0.60426    |
| Perplexity           | 1.8299     |
| AveragePolicyStd     | 0.27234    |
| AveragePolicyStd[0]  | 0.2938     |
| AveragePolicyStd[1]  | 0.35836    |
| AveragePolicyStd[2]  | 0.22451    |
| AveragePolicyStd[3]  | 0.29481    |
| AveragePolicyStd[4]  | 0.20322    |
| AveragePolicyStd[5]  | 0.25936    |
| AverageReturn        | 1069       |
| MinReturn            | 54.331     |
| MaxReturn            | 1281       |
| StdReturn            | 339.31     |
| AverageEpisodeLength | 880.81     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 271.71     |
| TotalNEpisodes       | 16754      |
| TotalNSamples        | 2.2579e+06 |
| ExplainedVariance    | -0.013106  |
-------------------------------------
[2018-12-22 10:53:08.720108 UTC] Saving snapshot
[2018-12-22 10:53:08.720346 UTC] Starting iteration 453
[2018-12-22 10:53:08.720464 UTC] Start collecting samples
[2018-12-22 10:53:11.730220 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:11.810869 UTC] Performing policy update
[2018-12-22 10:53:11.811434 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:11.901366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:12.952643 UTC] Performing line search
[2018-12-22 10:53:13.078763 UTC] Updating baseline
[2018-12-22 10:53:14.511171 UTC] Computing logging information
--------------------------------------
| Iteration            | 453         |
| ExpectedImprovement  | 0.019332    |
| ActualImprovement    | 0.018628    |
| ImprovementRatio     | 0.96359     |
| MeanKL               | 0.0071196   |
| Entropy              | 0.58917     |
| Perplexity           | 1.8025      |
| AveragePolicyStd     | 0.27153     |
| AveragePolicyStd[0]  | 0.29336     |
| AveragePolicyStd[1]  | 0.3551      |
| AveragePolicyStd[2]  | 0.22396     |
| AveragePolicyStd[3]  | 0.29365     |
| AveragePolicyStd[4]  | 0.20318     |
| AveragePolicyStd[5]  | 0.25991     |
| AverageReturn        | 1090.8      |
| MinReturn            | 54.331      |
| MaxReturn            | 1281        |
| StdReturn            | 330.03      |
| AverageEpisodeLength | 897.05      |
| MinEpisodeLength     | 57          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 264.57      |
| TotalNEpisodes       | 16763       |
| TotalNSamples        | 2.2669e+06  |
| ExplainedVariance    | -0.00016142 |
--------------------------------------
[2018-12-22 10:53:14.853584 UTC] Saving snapshot
[2018-12-22 10:53:14.853873 UTC] Starting iteration 454
[2018-12-22 10:53:14.854003 UTC] Start collecting samples
[2018-12-22 10:53:17.786626 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:17.865480 UTC] Performing policy update
[2018-12-22 10:53:17.866175 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:17.959314 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:19.023629 UTC] Performing line search
[2018-12-22 10:53:19.151402 UTC] Updating baseline
[2018-12-22 10:53:22.075165 UTC] Computing logging information
-------------------------------------
| Iteration            | 454        |
| ExpectedImprovement  | 0.016433   |
| ActualImprovement    | 0.01566    |
| ImprovementRatio     | 0.95293    |
| MeanKL               | 0.0071917  |
| Entropy              | 0.5868     |
| Perplexity           | 1.7982     |
| AveragePolicyStd     | 0.27141    |
| AveragePolicyStd[0]  | 0.2937     |
| AveragePolicyStd[1]  | 0.3547     |
| AveragePolicyStd[2]  | 0.22382    |
| AveragePolicyStd[3]  | 0.29339    |
| AveragePolicyStd[4]  | 0.20323    |
| AveragePolicyStd[5]  | 0.25961    |
| AverageReturn        | 1091.9     |
| MinReturn            | 54.331     |
| MaxReturn            | 1281       |
| StdReturn            | 330.43     |
| AverageEpisodeLength | 897.05     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 264.57     |
| TotalNEpisodes       | 16767      |
| TotalNSamples        | 2.2709e+06 |
| ExplainedVariance    | -0.016701  |
-------------------------------------
[2018-12-22 10:53:22.425353 UTC] Saving snapshot
[2018-12-22 10:53:22.425616 UTC] Starting iteration 455
[2018-12-22 10:53:22.425770 UTC] Start collecting samples
[2018-12-22 10:53:25.372767 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:25.450132 UTC] Performing policy update
[2018-12-22 10:53:25.450719 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:25.540518 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:26.607114 UTC] Performing line search
[2018-12-22 10:53:26.735238 UTC] Updating baseline
[2018-12-22 10:53:28.785904 UTC] Computing logging information
-------------------------------------
| Iteration            | 455        |
| ExpectedImprovement  | 0.016659   |
| ActualImprovement    | 0.015536   |
| ImprovementRatio     | 0.93259    |
| MeanKL               | 0.007133   |
| Entropy              | 0.58388    |
| Perplexity           | 1.793      |
| AveragePolicyStd     | 0.27123    |
| AveragePolicyStd[0]  | 0.29361    |
| AveragePolicyStd[1]  | 0.35378    |
| AveragePolicyStd[2]  | 0.22381    |
| AveragePolicyStd[3]  | 0.29363    |
| AveragePolicyStd[4]  | 0.20343    |
| AveragePolicyStd[5]  | 0.25916    |
| AverageReturn        | 1091.6     |
| MinReturn            | 54.331     |
| MaxReturn            | 1281       |
| StdReturn            | 330.29     |
| AverageEpisodeLength | 897.05     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 264.57     |
| TotalNEpisodes       | 16770      |
| TotalNSamples        | 2.2739e+06 |
| ExplainedVariance    | 0.00022406 |
-------------------------------------
[2018-12-22 10:53:29.137831 UTC] Saving snapshot
[2018-12-22 10:53:29.138080 UTC] Starting iteration 456
[2018-12-22 10:53:29.138203 UTC] Start collecting samples
[2018-12-22 10:53:32.125260 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:32.205777 UTC] Performing policy update
[2018-12-22 10:53:32.206387 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:32.296323 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:33.349266 UTC] Performing line search
[2018-12-22 10:53:33.478049 UTC] Updating baseline
[2018-12-22 10:53:34.748044 UTC] Computing logging information
-------------------------------------
| Iteration            | 456        |
| ExpectedImprovement  | 0.015709   |
| ActualImprovement    | 0.014665   |
| ImprovementRatio     | 0.93356    |
| MeanKL               | 0.0076366  |
| Entropy              | 0.58031    |
| Perplexity           | 1.7866     |
| AveragePolicyStd     | 0.27108    |
| AveragePolicyStd[0]  | 0.29323    |
| AveragePolicyStd[1]  | 0.35385    |
| AveragePolicyStd[2]  | 0.22378    |
| AveragePolicyStd[3]  | 0.29263    |
| AveragePolicyStd[4]  | 0.20286    |
| AveragePolicyStd[5]  | 0.26016    |
| AverageReturn        | 1112.1     |
| MinReturn            | 54.331     |
| MaxReturn            | 1281       |
| StdReturn            | 314.51     |
| AverageEpisodeLength | 912.78     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 251.9      |
| TotalNEpisodes       | 16777      |
| TotalNSamples        | 2.2809e+06 |
| ExplainedVariance    | 0.0057547  |
-------------------------------------
[2018-12-22 10:53:35.093581 UTC] Saving snapshot
[2018-12-22 10:53:35.093867 UTC] Starting iteration 457
[2018-12-22 10:53:35.093993 UTC] Start collecting samples
[2018-12-22 10:53:38.073593 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:38.154505 UTC] Performing policy update
[2018-12-22 10:53:38.155192 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:38.244631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:39.309144 UTC] Performing line search
[2018-12-22 10:53:39.438337 UTC] Updating baseline
[2018-12-22 10:53:40.795475 UTC] Computing logging information
------------------------------------
| Iteration            | 457       |
| ExpectedImprovement  | 0.01888   |
| ActualImprovement    | 0.016982  |
| ImprovementRatio     | 0.89947   |
| MeanKL               | 0.0067151 |
| Entropy              | 0.57854   |
| Perplexity           | 1.7834    |
| AveragePolicyStd     | 0.27103   |
| AveragePolicyStd[0]  | 0.29207   |
| AveragePolicyStd[1]  | 0.35442   |
| AveragePolicyStd[2]  | 0.22409   |
| AveragePolicyStd[3]  | 0.2929    |
| AveragePolicyStd[4]  | 0.20245   |
| AveragePolicyStd[5]  | 0.26024   |
| AverageReturn        | 1101.3    |
| MinReturn            | 43.621    |
| MaxReturn            | 1281      |
| StdReturn            | 332.04    |
| AverageEpisodeLength | 903.35    |
| MinEpisodeLength     | 57        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 265.73    |
| TotalNEpisodes       | 16783     |
| TotalNSamples        | 2.286e+06 |
| ExplainedVariance    | 0.12736   |
------------------------------------
[2018-12-22 10:53:41.148099 UTC] Saving snapshot
[2018-12-22 10:53:41.148378 UTC] Starting iteration 458
[2018-12-22 10:53:41.148497 UTC] Start collecting samples
[2018-12-22 10:53:44.123397 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:44.201438 UTC] Performing policy update
[2018-12-22 10:53:44.202097 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:44.293562 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:45.354278 UTC] Performing line search
[2018-12-22 10:53:45.482642 UTC] Updating baseline
[2018-12-22 10:53:47.016597 UTC] Computing logging information
-------------------------------------
| Iteration            | 458        |
| ExpectedImprovement  | 0.018779   |
| ActualImprovement    | 0.017521   |
| ImprovementRatio     | 0.93299    |
| MeanKL               | 0.0068925  |
| Entropy              | 0.57248    |
| Perplexity           | 1.7726     |
| AveragePolicyStd     | 0.27084    |
| AveragePolicyStd[0]  | 0.29215    |
| AveragePolicyStd[1]  | 0.35441    |
| AveragePolicyStd[2]  | 0.22323    |
| AveragePolicyStd[3]  | 0.29345    |
| AveragePolicyStd[4]  | 0.20168    |
| AveragePolicyStd[5]  | 0.2601     |
| AverageReturn        | 1101.4     |
| MinReturn            | 43.621     |
| MaxReturn            | 1281       |
| StdReturn            | 332.22     |
| AverageEpisodeLength | 902.48     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.55     |
| TotalNEpisodes       | 16787      |
| TotalNSamples        | 2.2899e+06 |
| ExplainedVariance    | 0.17278    |
-------------------------------------
[2018-12-22 10:53:47.367381 UTC] Saving snapshot
[2018-12-22 10:53:47.367655 UTC] Starting iteration 459
[2018-12-22 10:53:47.367778 UTC] Start collecting samples
[2018-12-22 10:53:50.362767 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:50.442493 UTC] Performing policy update
[2018-12-22 10:53:50.443364 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:50.534561 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:51.599552 UTC] Performing line search
[2018-12-22 10:53:51.728481 UTC] Updating baseline
[2018-12-22 10:53:53.695246 UTC] Computing logging information
-------------------------------------
| Iteration            | 459        |
| ExpectedImprovement  | 0.016458   |
| ActualImprovement    | 0.015737   |
| ImprovementRatio     | 0.95619    |
| MeanKL               | 0.0073538  |
| Entropy              | 0.5681     |
| Perplexity           | 1.7649     |
| AveragePolicyStd     | 0.27062    |
| AveragePolicyStd[0]  | 0.29201    |
| AveragePolicyStd[1]  | 0.35372    |
| AveragePolicyStd[2]  | 0.22334    |
| AveragePolicyStd[3]  | 0.29295    |
| AveragePolicyStd[4]  | 0.20124    |
| AveragePolicyStd[5]  | 0.26048    |
| AverageReturn        | 1103.3     |
| MinReturn            | 43.621     |
| MaxReturn            | 1281       |
| StdReturn            | 332.94     |
| AverageEpisodeLength | 902.48     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.55     |
| TotalNEpisodes       | 16794      |
| TotalNSamples        | 2.2969e+06 |
| ExplainedVariance    | -0.0021034 |
-------------------------------------
[2018-12-22 10:53:54.051029 UTC] Saving snapshot
[2018-12-22 10:53:54.051276 UTC] Starting iteration 460
[2018-12-22 10:53:54.051422 UTC] Start collecting samples
[2018-12-22 10:53:57.014645 UTC] Computing input variables for policy optimization
[2018-12-22 10:53:57.092319 UTC] Performing policy update
[2018-12-22 10:53:57.093201 UTC] Computing gradient in Euclidean space
[2018-12-22 10:53:57.182529 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:53:58.241067 UTC] Performing line search
[2018-12-22 10:53:58.367765 UTC] Updating baseline
[2018-12-22 10:53:59.722573 UTC] Computing logging information
-------------------------------------
| Iteration            | 460        |
| ExpectedImprovement  | 0.018577   |
| ActualImprovement    | 0.017258   |
| ImprovementRatio     | 0.92899    |
| MeanKL               | 0.0072501  |
| Entropy              | 0.55404    |
| Perplexity           | 1.7403     |
| AveragePolicyStd     | 0.27008    |
| AveragePolicyStd[0]  | 0.29148    |
| AveragePolicyStd[1]  | 0.35383    |
| AveragePolicyStd[2]  | 0.22293    |
| AveragePolicyStd[3]  | 0.29208    |
| AveragePolicyStd[4]  | 0.19972    |
| AveragePolicyStd[5]  | 0.26043    |
| AverageReturn        | 1116       |
| MinReturn            | 43.621     |
| MaxReturn            | 1281       |
| StdReturn            | 317.34     |
| AverageEpisodeLength | 911.66     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 252.58     |
| TotalNEpisodes       | 16799      |
| TotalNSamples        | 2.3019e+06 |
| ExplainedVariance    | 0.060149   |
-------------------------------------
[2018-12-22 10:54:00.074342 UTC] Saving snapshot
[2018-12-22 10:54:00.082525 UTC] Starting iteration 461
[2018-12-22 10:54:00.082726 UTC] Start collecting samples
[2018-12-22 10:54:03.060871 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:03.140902 UTC] Performing policy update
[2018-12-22 10:54:03.141722 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:03.231610 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:04.305580 UTC] Performing line search
[2018-12-22 10:54:04.434704 UTC] Updating baseline
[2018-12-22 10:54:05.809297 UTC] Computing logging information
-------------------------------------
| Iteration            | 461        |
| ExpectedImprovement  | 0.016704   |
| ActualImprovement    | 0.015519   |
| ImprovementRatio     | 0.92904    |
| MeanKL               | 0.0075549  |
| Entropy              | 0.54144    |
| Perplexity           | 1.7185     |
| AveragePolicyStd     | 0.26942    |
| AveragePolicyStd[0]  | 0.29001    |
| AveragePolicyStd[1]  | 0.35142    |
| AveragePolicyStd[2]  | 0.22303    |
| AveragePolicyStd[3]  | 0.29204    |
| AveragePolicyStd[4]  | 0.19923    |
| AveragePolicyStd[5]  | 0.2608     |
| AverageReturn        | 1109.9     |
| MinReturn            | 43.621     |
| MaxReturn            | 1281       |
| StdReturn            | 322.15     |
| AverageEpisodeLength | 906.15     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.57     |
| TotalNEpisodes       | 16803      |
| TotalNSamples        | 2.3053e+06 |
| ExplainedVariance    | 0.045757   |
-------------------------------------
[2018-12-22 10:54:06.160880 UTC] Saving snapshot
[2018-12-22 10:54:06.161123 UTC] Starting iteration 462
[2018-12-22 10:54:06.161239 UTC] Start collecting samples
[2018-12-22 10:54:09.131107 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:09.211087 UTC] Performing policy update
[2018-12-22 10:54:09.212000 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:09.301862 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:10.365927 UTC] Performing line search
[2018-12-22 10:54:10.492510 UTC] Updating baseline
[2018-12-22 10:54:12.040447 UTC] Computing logging information
-------------------------------------
| Iteration            | 462        |
| ExpectedImprovement  | 0.015392   |
| ActualImprovement    | 0.01464    |
| ImprovementRatio     | 0.95113    |
| MeanKL               | 0.0071832  |
| Entropy              | 0.53277    |
| Perplexity           | 1.7036     |
| AveragePolicyStd     | 0.26903    |
| AveragePolicyStd[0]  | 0.28958    |
| AveragePolicyStd[1]  | 0.34971    |
| AveragePolicyStd[2]  | 0.22197    |
| AveragePolicyStd[3]  | 0.29311    |
| AveragePolicyStd[4]  | 0.19905    |
| AveragePolicyStd[5]  | 0.26073    |
| AverageReturn        | 1113.3     |
| MinReturn            | 43.621     |
| MaxReturn            | 1281       |
| StdReturn            | 322.14     |
| AverageEpisodeLength | 908.47     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.36     |
| TotalNEpisodes       | 16809      |
| TotalNSamples        | 2.3113e+06 |
| ExplainedVariance    | 0.035319   |
-------------------------------------
[2018-12-22 10:54:12.389077 UTC] Saving snapshot
[2018-12-22 10:54:12.389333 UTC] Starting iteration 463
[2018-12-22 10:54:12.389455 UTC] Start collecting samples
[2018-12-22 10:54:15.372182 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:15.451344 UTC] Performing policy update
[2018-12-22 10:54:15.451977 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:15.541923 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:16.616436 UTC] Performing line search
[2018-12-22 10:54:16.743557 UTC] Updating baseline
[2018-12-22 10:54:18.285852 UTC] Computing logging information
-------------------------------------
| Iteration            | 463        |
| ExpectedImprovement  | 0.017269   |
| ActualImprovement    | 0.016509   |
| ImprovementRatio     | 0.95601    |
| MeanKL               | 0.0073557  |
| Entropy              | 0.52871    |
| Perplexity           | 1.6967     |
| AveragePolicyStd     | 0.26887    |
| AveragePolicyStd[0]  | 0.28913    |
| AveragePolicyStd[1]  | 0.34975    |
| AveragePolicyStd[2]  | 0.22154    |
| AveragePolicyStd[3]  | 0.29335    |
| AveragePolicyStd[4]  | 0.19882    |
| AveragePolicyStd[5]  | 0.26065    |
| AverageReturn        | 1123.2     |
| MinReturn            | 43.621     |
| MaxReturn            | 1281       |
| StdReturn            | 307.3      |
| AverageEpisodeLength | 917.24     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 244.01     |
| TotalNEpisodes       | 16815      |
| TotalNSamples        | 2.3165e+06 |
| ExplainedVariance    | 0.20386    |
-------------------------------------
[2018-12-22 10:54:18.639531 UTC] Saving snapshot
[2018-12-22 10:54:18.639793 UTC] Starting iteration 464
[2018-12-22 10:54:18.639921 UTC] Start collecting samples
[2018-12-22 10:54:21.618187 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:21.696521 UTC] Performing policy update
[2018-12-22 10:54:21.697282 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:21.787037 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:22.856031 UTC] Performing line search
[2018-12-22 10:54:22.984207 UTC] Updating baseline
[2018-12-22 10:54:24.613697 UTC] Computing logging information
-------------------------------------
| Iteration            | 464        |
| ExpectedImprovement  | 0.018415   |
| ActualImprovement    | 0.017483   |
| ImprovementRatio     | 0.94939    |
| MeanKL               | 0.0072538  |
| Entropy              | 0.52094    |
| Perplexity           | 1.6836     |
| AveragePolicyStd     | 0.26846    |
| AveragePolicyStd[0]  | 0.28883    |
| AveragePolicyStd[1]  | 0.34867    |
| AveragePolicyStd[2]  | 0.22163    |
| AveragePolicyStd[3]  | 0.29214    |
| AveragePolicyStd[4]  | 0.19884    |
| AveragePolicyStd[5]  | 0.26063    |
| AverageReturn        | 1127.3     |
| MinReturn            | 43.621     |
| MaxReturn            | 1281       |
| StdReturn            | 298.11     |
| AverageEpisodeLength | 919.98     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.14     |
| TotalNEpisodes       | 16821      |
| TotalNSamples        | 2.3219e+06 |
| ExplainedVariance    | 0.060846   |
-------------------------------------
[2018-12-22 10:54:24.967042 UTC] Saving snapshot
[2018-12-22 10:54:24.967322 UTC] Starting iteration 465
[2018-12-22 10:54:24.967445 UTC] Start collecting samples
[2018-12-22 10:54:27.903886 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:27.984828 UTC] Performing policy update
[2018-12-22 10:54:27.985430 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:28.075500 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:29.126055 UTC] Performing line search
[2018-12-22 10:54:29.254307 UTC] Updating baseline
[2018-12-22 10:54:30.598375 UTC] Computing logging information
-------------------------------------
| Iteration            | 465        |
| ExpectedImprovement  | 0.015861   |
| ActualImprovement    | 0.015091   |
| ImprovementRatio     | 0.95145    |
| MeanKL               | 0.0075352  |
| Entropy              | 0.51123    |
| Perplexity           | 1.6673     |
| AveragePolicyStd     | 0.268      |
| AveragePolicyStd[0]  | 0.29011    |
| AveragePolicyStd[1]  | 0.34741    |
| AveragePolicyStd[2]  | 0.22205    |
| AveragePolicyStd[3]  | 0.29294    |
| AveragePolicyStd[4]  | 0.19918    |
| AveragePolicyStd[5]  | 0.25628    |
| AverageReturn        | 1139       |
| MinReturn            | 43.621     |
| MaxReturn            | 1284       |
| StdReturn            | 279.45     |
| AverageEpisodeLength | 929.08     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.03     |
| TotalNEpisodes       | 16823      |
| TotalNSamples        | 2.3239e+06 |
| ExplainedVariance    | 0.028928   |
-------------------------------------
[2018-12-22 10:54:30.951709 UTC] Saving snapshot
[2018-12-22 10:54:30.952023 UTC] Starting iteration 466
[2018-12-22 10:54:30.952147 UTC] Start collecting samples
[2018-12-22 10:54:33.931634 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:34.010344 UTC] Performing policy update
[2018-12-22 10:54:34.011127 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:34.099126 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:35.049891 UTC] Performing line search
[2018-12-22 10:54:35.166801 UTC] Updating baseline
[2018-12-22 10:54:36.686854 UTC] Computing logging information
-------------------------------------
| Iteration            | 466        |
| ExpectedImprovement  | 0.01626    |
| ActualImprovement    | 0.015523   |
| ImprovementRatio     | 0.95468    |
| MeanKL               | 0.0073248  |
| Entropy              | 0.50634    |
| Perplexity           | 1.6592     |
| AveragePolicyStd     | 0.26778    |
| AveragePolicyStd[0]  | 0.28889    |
| AveragePolicyStd[1]  | 0.3477     |
| AveragePolicyStd[2]  | 0.22104    |
| AveragePolicyStd[3]  | 0.293      |
| AveragePolicyStd[4]  | 0.19959    |
| AveragePolicyStd[5]  | 0.25647    |
| AverageReturn        | 1141.1     |
| MinReturn            | 43.621     |
| MaxReturn            | 1299.7     |
| StdReturn            | 279.88     |
| AverageEpisodeLength | 930.11     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.12     |
| TotalNEpisodes       | 16829      |
| TotalNSamples        | 2.3299e+06 |
| ExplainedVariance    | -0.0040052 |
-------------------------------------
[2018-12-22 10:54:37.032796 UTC] Saving snapshot
[2018-12-22 10:54:37.033062 UTC] Starting iteration 467
[2018-12-22 10:54:37.033192 UTC] Start collecting samples
[2018-12-22 10:54:40.038265 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:40.119285 UTC] Performing policy update
[2018-12-22 10:54:40.121194 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:40.210826 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:41.269570 UTC] Performing line search
[2018-12-22 10:54:41.397831 UTC] Updating baseline
[2018-12-22 10:54:43.555126 UTC] Computing logging information
-------------------------------------
| Iteration            | 467        |
| ExpectedImprovement  | 0.015347   |
| ActualImprovement    | 0.015237   |
| ImprovementRatio     | 0.99287    |
| MeanKL               | 0.0071002  |
| Entropy              | 0.50017    |
| Perplexity           | 1.649      |
| AveragePolicyStd     | 0.26755    |
| AveragePolicyStd[0]  | 0.28938    |
| AveragePolicyStd[1]  | 0.34731    |
| AveragePolicyStd[2]  | 0.21997    |
| AveragePolicyStd[3]  | 0.29272    |
| AveragePolicyStd[4]  | 0.19927    |
| AveragePolicyStd[5]  | 0.25665    |
| AverageReturn        | 1165.4     |
| MinReturn            | 43.621     |
| MaxReturn            | 1299.7     |
| StdReturn            | 242.94     |
| AverageEpisodeLength | 949.42     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.4      |
| TotalNEpisodes       | 16837      |
| TotalNSamples        | 2.3369e+06 |
| ExplainedVariance    | 0.037459   |
-------------------------------------
[2018-12-22 10:54:43.909862 UTC] Saving snapshot
[2018-12-22 10:54:43.910099 UTC] Starting iteration 468
[2018-12-22 10:54:43.910218 UTC] Start collecting samples
[2018-12-22 10:54:46.851589 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:46.931527 UTC] Performing policy update
[2018-12-22 10:54:46.932179 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:47.022605 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:48.077585 UTC] Performing line search
[2018-12-22 10:54:48.203059 UTC] Updating baseline
[2018-12-22 10:54:49.650021 UTC] Computing logging information
-------------------------------------
| Iteration            | 468        |
| ExpectedImprovement  | 0.017209   |
| ActualImprovement    | 0.016228   |
| ImprovementRatio     | 0.94298    |
| MeanKL               | 0.0069271  |
| Entropy              | 0.51911    |
| Perplexity           | 1.6805     |
| AveragePolicyStd     | 0.26838    |
| AveragePolicyStd[0]  | 0.28865    |
| AveragePolicyStd[1]  | 0.3483     |
| AveragePolicyStd[2]  | 0.22138    |
| AveragePolicyStd[3]  | 0.29554    |
| AveragePolicyStd[4]  | 0.2        |
| AveragePolicyStd[5]  | 0.2564     |
| AverageReturn        | 1161.8     |
| MinReturn            | 43.621     |
| MaxReturn            | 1299.7     |
| StdReturn            | 244.19     |
| AverageEpisodeLength | 946.19     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.1      |
| TotalNEpisodes       | 16841      |
| TotalNSamples        | 2.3398e+06 |
| ExplainedVariance    | 0.37638    |
-------------------------------------
[2018-12-22 10:54:50.006577 UTC] Saving snapshot
[2018-12-22 10:54:50.006892 UTC] Starting iteration 469
[2018-12-22 10:54:50.007012 UTC] Start collecting samples
[2018-12-22 10:54:52.953889 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:53.033412 UTC] Performing policy update
[2018-12-22 10:54:53.034257 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:53.125349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:54:54.188125 UTC] Performing line search
[2018-12-22 10:54:54.315631 UTC] Updating baseline
[2018-12-22 10:54:55.773844 UTC] Computing logging information
-------------------------------------
| Iteration            | 469        |
| ExpectedImprovement  | 0.018741   |
| ActualImprovement    | 0.017162   |
| ImprovementRatio     | 0.91573    |
| MeanKL               | 0.0071702  |
| Entropy              | 0.52138    |
| Perplexity           | 1.6843     |
| AveragePolicyStd     | 0.26849    |
| AveragePolicyStd[0]  | 0.28893    |
| AveragePolicyStd[1]  | 0.3488     |
| AveragePolicyStd[2]  | 0.22137    |
| AveragePolicyStd[3]  | 0.29503    |
| AveragePolicyStd[4]  | 0.20008    |
| AveragePolicyStd[5]  | 0.25672    |
| AverageReturn        | 1164.5     |
| MinReturn            | 43.621     |
| MaxReturn            | 1310.5     |
| StdReturn            | 245.23     |
| AverageEpisodeLength | 946.19     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.1      |
| TotalNEpisodes       | 16847      |
| TotalNSamples        | 2.3458e+06 |
| ExplainedVariance    | 0.013797   |
-------------------------------------
[2018-12-22 10:54:56.125843 UTC] Saving snapshot
[2018-12-22 10:54:56.126089 UTC] Starting iteration 470
[2018-12-22 10:54:56.126209 UTC] Start collecting samples
[2018-12-22 10:54:59.075106 UTC] Computing input variables for policy optimization
[2018-12-22 10:54:59.154466 UTC] Performing policy update
[2018-12-22 10:54:59.155231 UTC] Computing gradient in Euclidean space
[2018-12-22 10:54:59.245706 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:00.310643 UTC] Performing line search
[2018-12-22 10:55:00.437012 UTC] Updating baseline
[2018-12-22 10:55:01.995858 UTC] Computing logging information
-------------------------------------
| Iteration            | 470        |
| ExpectedImprovement  | 0.017705   |
| ActualImprovement    | 0.017062   |
| ImprovementRatio     | 0.96368    |
| MeanKL               | 0.0069033  |
| Entropy              | 0.51736    |
| Perplexity           | 1.6776     |
| AveragePolicyStd     | 0.26834    |
| AveragePolicyStd[0]  | 0.28858    |
| AveragePolicyStd[1]  | 0.34874    |
| AveragePolicyStd[2]  | 0.21996    |
| AveragePolicyStd[3]  | 0.2952     |
| AveragePolicyStd[4]  | 0.20035    |
| AveragePolicyStd[5]  | 0.25719    |
| AverageReturn        | 1169.9     |
| MinReturn            | 43.621     |
| MaxReturn            | 1320.8     |
| StdReturn            | 245.04     |
| AverageEpisodeLength | 948.62     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.22     |
| TotalNEpisodes       | 16852      |
| TotalNSamples        | 2.3508e+06 |
| ExplainedVariance    | 0.022217   |
-------------------------------------
[2018-12-22 10:55:02.346779 UTC] Saving snapshot
[2018-12-22 10:55:02.354901 UTC] Starting iteration 471
[2018-12-22 10:55:02.355084 UTC] Start collecting samples
[2018-12-22 10:55:05.344279 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:05.427531 UTC] Performing policy update
[2018-12-22 10:55:05.428132 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:05.517551 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:06.577496 UTC] Performing line search
[2018-12-22 10:55:06.704994 UTC] Updating baseline
[2018-12-22 10:55:07.997522 UTC] Computing logging information
-------------------------------------
| Iteration            | 471        |
| ExpectedImprovement  | 0.019131   |
| ActualImprovement    | 0.018377   |
| ImprovementRatio     | 0.96057    |
| MeanKL               | 0.0066197  |
| Entropy              | 0.52728    |
| Perplexity           | 1.6943     |
| AveragePolicyStd     | 0.2688     |
| AveragePolicyStd[0]  | 0.2895     |
| AveragePolicyStd[1]  | 0.34942    |
| AveragePolicyStd[2]  | 0.21993    |
| AveragePolicyStd[3]  | 0.29578    |
| AveragePolicyStd[4]  | 0.20084    |
| AveragePolicyStd[5]  | 0.25733    |
| AverageReturn        | 1143.4     |
| MinReturn            | 43.621     |
| MaxReturn            | 1320.8     |
| StdReturn            | 288.36     |
| AverageEpisodeLength | 926.7      |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.61     |
| TotalNEpisodes       | 16860      |
| TotalNSamples        | 2.3566e+06 |
| ExplainedVariance    | 0.25121    |
-------------------------------------
[2018-12-22 10:55:08.350714 UTC] Saving snapshot
[2018-12-22 10:55:08.350973 UTC] Starting iteration 472
[2018-12-22 10:55:08.351090 UTC] Start collecting samples
[2018-12-22 10:55:11.301490 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:11.381152 UTC] Performing policy update
[2018-12-22 10:55:11.381937 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:11.470992 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:12.535832 UTC] Performing line search
[2018-12-22 10:55:12.664494 UTC] Updating baseline
[2018-12-22 10:55:13.951217 UTC] Computing logging information
-------------------------------------
| Iteration            | 472        |
| ExpectedImprovement  | 0.016123   |
| ActualImprovement    | 0.015613   |
| ImprovementRatio     | 0.96837    |
| MeanKL               | 0.0078595  |
| Entropy              | 0.50187    |
| Perplexity           | 1.6518     |
| AveragePolicyStd     | 0.26772    |
| AveragePolicyStd[0]  | 0.28805    |
| AveragePolicyStd[1]  | 0.34794    |
| AveragePolicyStd[2]  | 0.21882    |
| AveragePolicyStd[3]  | 0.29647    |
| AveragePolicyStd[4]  | 0.19975    |
| AveragePolicyStd[5]  | 0.25527    |
| AverageReturn        | 1140.5     |
| MinReturn            | 43.621     |
| MaxReturn            | 1331.6     |
| StdReturn            | 292.76     |
| AverageEpisodeLength | 922.05     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.83     |
| TotalNEpisodes       | 16864      |
| TotalNSamples        | 2.3601e+06 |
| ExplainedVariance    | 0.1512     |
-------------------------------------
[2018-12-22 10:55:14.303780 UTC] Saving snapshot
[2018-12-22 10:55:14.304014 UTC] Starting iteration 473
[2018-12-22 10:55:14.304131 UTC] Start collecting samples
[2018-12-22 10:55:17.314005 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:17.395358 UTC] Performing policy update
[2018-12-22 10:55:17.396102 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:17.485774 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:18.554201 UTC] Performing line search
[2018-12-22 10:55:18.682192 UTC] Updating baseline
[2018-12-22 10:55:20.060724 UTC] Computing logging information
-------------------------------------
| Iteration            | 473        |
| ExpectedImprovement  | 0.018667   |
| ActualImprovement    | 0.017807   |
| ImprovementRatio     | 0.95392    |
| MeanKL               | 0.0070021  |
| Entropy              | 0.50145    |
| Perplexity           | 1.6511     |
| AveragePolicyStd     | 0.26769    |
| AveragePolicyStd[0]  | 0.28862    |
| AveragePolicyStd[1]  | 0.3463     |
| AveragePolicyStd[2]  | 0.21856    |
| AveragePolicyStd[3]  | 0.29706    |
| AveragePolicyStd[4]  | 0.199      |
| AveragePolicyStd[5]  | 0.25662    |
| AverageReturn        | 1140.7     |
| MinReturn            | 43.621     |
| MaxReturn            | 1331.6     |
| StdReturn            | 295.04     |
| AverageEpisodeLength | 918.16     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.35     |
| TotalNEpisodes       | 16871      |
| TotalNSamples        | 2.3667e+06 |
| ExplainedVariance    | 0.26672    |
-------------------------------------
[2018-12-22 10:55:20.412197 UTC] Saving snapshot
[2018-12-22 10:55:20.412451 UTC] Starting iteration 474
[2018-12-22 10:55:20.412583 UTC] Start collecting samples
[2018-12-22 10:55:23.376530 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:23.458770 UTC] Performing policy update
[2018-12-22 10:55:23.459680 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:23.550002 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:24.628122 UTC] Performing line search
[2018-12-22 10:55:24.755516 UTC] Updating baseline
[2018-12-22 10:55:26.130923 UTC] Computing logging information
-------------------------------------
| Iteration            | 474        |
| ExpectedImprovement  | 0.018346   |
| ActualImprovement    | 0.017283   |
| ImprovementRatio     | 0.94206    |
| MeanKL               | 0.0066414  |
| Entropy              | 0.49806    |
| Perplexity           | 1.6455     |
| AveragePolicyStd     | 0.26754    |
| AveragePolicyStd[0]  | 0.28843    |
| AveragePolicyStd[1]  | 0.34584    |
| AveragePolicyStd[2]  | 0.21813    |
| AveragePolicyStd[3]  | 0.29746    |
| AveragePolicyStd[4]  | 0.19925    |
| AveragePolicyStd[5]  | 0.2561     |
| AverageReturn        | 1137.9     |
| MinReturn            | 43.621     |
| MaxReturn            | 1360.4     |
| StdReturn            | 301.56     |
| AverageEpisodeLength | 913.04     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.17     |
| TotalNEpisodes       | 16877      |
| TotalNSamples        | 2.3722e+06 |
| ExplainedVariance    | 0.070594   |
-------------------------------------
[2018-12-22 10:55:26.482087 UTC] Saving snapshot
[2018-12-22 10:55:26.482339 UTC] Starting iteration 475
[2018-12-22 10:55:26.482458 UTC] Start collecting samples
[2018-12-22 10:55:29.408219 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:29.488943 UTC] Performing policy update
[2018-12-22 10:55:29.489708 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:29.580451 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:30.649106 UTC] Performing line search
[2018-12-22 10:55:30.778518 UTC] Updating baseline
[2018-12-22 10:55:32.071925 UTC] Computing logging information
-------------------------------------
| Iteration            | 475        |
| ExpectedImprovement  | 0.016116   |
| ActualImprovement    | 0.015563   |
| ImprovementRatio     | 0.96568    |
| MeanKL               | 0.0072164  |
| Entropy              | 0.491      |
| Perplexity           | 1.6339     |
| AveragePolicyStd     | 0.26721    |
| AveragePolicyStd[0]  | 0.28907    |
| AveragePolicyStd[1]  | 0.34569    |
| AveragePolicyStd[2]  | 0.21796    |
| AveragePolicyStd[3]  | 0.29681    |
| AveragePolicyStd[4]  | 0.19964    |
| AveragePolicyStd[5]  | 0.25411    |
| AverageReturn        | 1141.8     |
| MinReturn            | 69.858     |
| MaxReturn            | 1360.4     |
| StdReturn            | 296.59     |
| AverageEpisodeLength | 914.36     |
| MinEpisodeLength     | 79         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.63     |
| TotalNEpisodes       | 16881      |
| TotalNSamples        | 2.3754e+06 |
| ExplainedVariance    | 0.21729    |
-------------------------------------
[2018-12-22 10:55:32.423221 UTC] Saving snapshot
[2018-12-22 10:55:32.423470 UTC] Starting iteration 476
[2018-12-22 10:55:32.423607 UTC] Start collecting samples
[2018-12-22 10:55:35.397019 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:35.479707 UTC] Performing policy update
[2018-12-22 10:55:35.480315 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:35.571738 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:36.636438 UTC] Performing line search
[2018-12-22 10:55:36.764223 UTC] Updating baseline
[2018-12-22 10:55:38.199906 UTC] Computing logging information
-------------------------------------
| Iteration            | 476        |
| ExpectedImprovement  | 0.015533   |
| ActualImprovement    | 0.01497    |
| ImprovementRatio     | 0.96376    |
| MeanKL               | 0.0074049  |
| Entropy              | 0.48812    |
| Perplexity           | 1.6293     |
| AveragePolicyStd     | 0.26702    |
| AveragePolicyStd[0]  | 0.28821    |
| AveragePolicyStd[1]  | 0.34511    |
| AveragePolicyStd[2]  | 0.21844    |
| AveragePolicyStd[3]  | 0.29637    |
| AveragePolicyStd[4]  | 0.19969    |
| AveragePolicyStd[5]  | 0.25433    |
| AverageReturn        | 1148.5     |
| MinReturn            | 69.858     |
| MaxReturn            | 1360.7     |
| StdReturn            | 298.93     |
| AverageEpisodeLength | 914.83     |
| MinEpisodeLength     | 79         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.67     |
| TotalNEpisodes       | 16888      |
| TotalNSamples        | 2.3824e+06 |
| ExplainedVariance    | 0.081203   |
-------------------------------------
[2018-12-22 10:55:38.545721 UTC] Saving snapshot
[2018-12-22 10:55:38.545989 UTC] Starting iteration 477
[2018-12-22 10:55:38.546122 UTC] Start collecting samples
[2018-12-22 10:55:41.471822 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:41.550920 UTC] Performing policy update
[2018-12-22 10:55:41.551675 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:41.641499 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:42.707758 UTC] Performing line search
[2018-12-22 10:55:42.834749 UTC] Updating baseline
[2018-12-22 10:55:44.317686 UTC] Computing logging information
-------------------------------------
| Iteration            | 477        |
| ExpectedImprovement  | 0.017149   |
| ActualImprovement    | 0.015636   |
| ImprovementRatio     | 0.91174    |
| MeanKL               | 0.0071121  |
| Entropy              | 0.48887    |
| Perplexity           | 1.6305     |
| AveragePolicyStd     | 0.26707    |
| AveragePolicyStd[0]  | 0.28902    |
| AveragePolicyStd[1]  | 0.34396    |
| AveragePolicyStd[2]  | 0.21858    |
| AveragePolicyStd[3]  | 0.29795    |
| AveragePolicyStd[4]  | 0.19927    |
| AveragePolicyStd[5]  | 0.25367    |
| AverageReturn        | 1150.8     |
| MinReturn            | 69.858     |
| MaxReturn            | 1360.7     |
| StdReturn            | 299.67     |
| AverageEpisodeLength | 914.83     |
| MinEpisodeLength     | 79         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.67     |
| TotalNEpisodes       | 16893      |
| TotalNSamples        | 2.3874e+06 |
| ExplainedVariance    | 0.02087    |
-------------------------------------
[2018-12-22 10:55:44.672263 UTC] Saving snapshot
[2018-12-22 10:55:44.672505 UTC] Starting iteration 478
[2018-12-22 10:55:44.672644 UTC] Start collecting samples
[2018-12-22 10:55:47.643481 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:47.722380 UTC] Performing policy update
[2018-12-22 10:55:47.723286 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:47.814526 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:48.877409 UTC] Performing line search
[2018-12-22 10:55:49.005709 UTC] Updating baseline
[2018-12-22 10:55:50.384992 UTC] Computing logging information
-------------------------------------
| Iteration            | 478        |
| ExpectedImprovement  | 0.018629   |
| ActualImprovement    | 0.017459   |
| ImprovementRatio     | 0.93723    |
| MeanKL               | 0.0071837  |
| Entropy              | 0.48748    |
| Perplexity           | 1.6282     |
| AveragePolicyStd     | 0.26694    |
| AveragePolicyStd[0]  | 0.28922    |
| AveragePolicyStd[1]  | 0.34326    |
| AveragePolicyStd[2]  | 0.21863    |
| AveragePolicyStd[3]  | 0.29661    |
| AveragePolicyStd[4]  | 0.19955    |
| AveragePolicyStd[5]  | 0.2544     |
| AverageReturn        | 1152.3     |
| MinReturn            | 69.858     |
| MaxReturn            | 1360.7     |
| StdReturn            | 300.25     |
| AverageEpisodeLength | 914.83     |
| MinEpisodeLength     | 79         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.67     |
| TotalNEpisodes       | 16896      |
| TotalNSamples        | 2.3904e+06 |
| ExplainedVariance    | 0.057636   |
-------------------------------------
[2018-12-22 10:55:50.737094 UTC] Saving snapshot
[2018-12-22 10:55:50.737334 UTC] Starting iteration 479
[2018-12-22 10:55:50.737452 UTC] Start collecting samples
[2018-12-22 10:55:53.724611 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:53.805470 UTC] Performing policy update
[2018-12-22 10:55:53.806079 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:53.896308 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:55:54.963231 UTC] Performing line search
[2018-12-22 10:55:55.090670 UTC] Updating baseline
[2018-12-22 10:55:56.391519 UTC] Computing logging information
------------------------------------
| Iteration            | 479       |
| ExpectedImprovement  | 0.019214  |
| ActualImprovement    | 0.017031  |
| ImprovementRatio     | 0.88635   |
| MeanKL               | 0.0070446 |
| Entropy              | 0.47479   |
| Perplexity           | 1.6077    |
| AveragePolicyStd     | 0.26645   |
| AveragePolicyStd[0]  | 0.28993   |
| AveragePolicyStd[1]  | 0.34327   |
| AveragePolicyStd[2]  | 0.21724   |
| AveragePolicyStd[3]  | 0.29513   |
| AveragePolicyStd[4]  | 0.19903   |
| AveragePolicyStd[5]  | 0.25408   |
| AverageReturn        | 1159.5    |
| MinReturn            | 69.858    |
| MaxReturn            | 1360.7    |
| StdReturn            | 298.42    |
| AverageEpisodeLength | 916.28    |
| MinEpisodeLength     | 79        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 224.81    |
| TotalNEpisodes       | 16903     |
| TotalNSamples        | 2.397e+06 |
| ExplainedVariance    | 0.16353   |
------------------------------------
[2018-12-22 10:55:56.743668 UTC] Saving snapshot
[2018-12-22 10:55:56.743929 UTC] Starting iteration 480
[2018-12-22 10:55:56.744051 UTC] Start collecting samples
[2018-12-22 10:55:59.669889 UTC] Computing input variables for policy optimization
[2018-12-22 10:55:59.749188 UTC] Performing policy update
[2018-12-22 10:55:59.749834 UTC] Computing gradient in Euclidean space
[2018-12-22 10:55:59.840041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:00.895792 UTC] Performing line search
[2018-12-22 10:56:01.023022 UTC] Updating baseline
[2018-12-22 10:56:02.307580 UTC] Computing logging information
------------------------------------
| Iteration            | 480       |
| ExpectedImprovement  | 0.016933  |
| ActualImprovement    | 0.017617  |
| ImprovementRatio     | 1.0404    |
| MeanKL               | 0.0075068 |
| Entropy              | 0.46682   |
| Perplexity           | 1.5949    |
| AveragePolicyStd     | 0.26613   |
| AveragePolicyStd[0]  | 0.28981   |
| AveragePolicyStd[1]  | 0.34322   |
| AveragePolicyStd[2]  | 0.21699   |
| AveragePolicyStd[3]  | 0.29453   |
| AveragePolicyStd[4]  | 0.19841   |
| AveragePolicyStd[5]  | 0.25381   |
| AverageReturn        | 1162.1    |
| MinReturn            | 69.858    |
| MaxReturn            | 1360.7    |
| StdReturn            | 299.27    |
| AverageEpisodeLength | 916.28    |
| MinEpisodeLength     | 79        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 224.81    |
| TotalNEpisodes       | 16907     |
| TotalNSamples        | 2.401e+06 |
| ExplainedVariance    | -0.051868 |
------------------------------------
[2018-12-22 10:56:02.664493 UTC] Saving snapshot
[2018-12-22 10:56:02.672491 UTC] Starting iteration 481
[2018-12-22 10:56:02.672708 UTC] Start collecting samples
[2018-12-22 10:56:05.631500 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:05.711243 UTC] Performing policy update
[2018-12-22 10:56:05.711962 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:05.805237 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:06.865988 UTC] Performing line search
[2018-12-22 10:56:06.993431 UTC] Updating baseline
[2018-12-22 10:56:08.530907 UTC] Computing logging information
-------------------------------------
| Iteration            | 481        |
| ExpectedImprovement  | 0.016973   |
| ActualImprovement    | 0.016097   |
| ImprovementRatio     | 0.94839    |
| MeanKL               | 0.0070147  |
| Entropy              | 0.45751    |
| Perplexity           | 1.5801     |
| AveragePolicyStd     | 0.26566    |
| AveragePolicyStd[0]  | 0.28925    |
| AveragePolicyStd[1]  | 0.34198    |
| AveragePolicyStd[2]  | 0.2175     |
| AveragePolicyStd[3]  | 0.29308    |
| AveragePolicyStd[4]  | 0.19758    |
| AveragePolicyStd[5]  | 0.25458    |
| AverageReturn        | 1174.4     |
| MinReturn            | 69.858     |
| MaxReturn            | 1361.2     |
| StdReturn            | 286.28     |
| AverageEpisodeLength | 920.94     |
| MinEpisodeLength     | 79         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.81     |
| TotalNEpisodes       | 16913      |
| TotalNSamples        | 2.4066e+06 |
| ExplainedVariance    | 0.17951    |
-------------------------------------
[2018-12-22 10:56:08.884168 UTC] Saving snapshot
[2018-12-22 10:56:08.884428 UTC] Starting iteration 482
[2018-12-22 10:56:08.884559 UTC] Start collecting samples
[2018-12-22 10:56:11.890164 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:11.975525 UTC] Performing policy update
[2018-12-22 10:56:11.976448 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:12.066943 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:13.139779 UTC] Performing line search
[2018-12-22 10:56:13.267260 UTC] Updating baseline
[2018-12-22 10:56:14.637772 UTC] Computing logging information
-------------------------------------
| Iteration            | 482        |
| ExpectedImprovement  | 0.019924   |
| ActualImprovement    | 0.018254   |
| ImprovementRatio     | 0.9162     |
| MeanKL               | 0.0067587  |
| Entropy              | 0.45716    |
| Perplexity           | 1.5796     |
| AveragePolicyStd     | 0.26567    |
| AveragePolicyStd[0]  | 0.2898     |
| AveragePolicyStd[1]  | 0.34227    |
| AveragePolicyStd[2]  | 0.21647    |
| AveragePolicyStd[3]  | 0.29273    |
| AveragePolicyStd[4]  | 0.19792    |
| AveragePolicyStd[5]  | 0.25487    |
| AverageReturn        | 1161       |
| MinReturn            | 69.858     |
| MaxReturn            | 1361.2     |
| StdReturn            | 314.45     |
| AverageEpisodeLength | 906.59     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 235.51     |
| TotalNEpisodes       | 16920      |
| TotalNSamples        | 2.4115e+06 |
| ExplainedVariance    | 0.20745    |
-------------------------------------
[2018-12-22 10:56:14.992979 UTC] Saving snapshot
[2018-12-22 10:56:14.993220 UTC] Starting iteration 483
[2018-12-22 10:56:14.993338 UTC] Start collecting samples
[2018-12-22 10:56:17.968777 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:18.049618 UTC] Performing policy update
[2018-12-22 10:56:18.050389 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:18.140789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:19.159255 UTC] Performing line search
[2018-12-22 10:56:19.287808 UTC] Updating baseline
[2018-12-22 10:56:20.584440 UTC] Computing logging information
-------------------------------------
| Iteration            | 483        |
| ExpectedImprovement  | 0.016668   |
| ActualImprovement    | 0.015678   |
| ImprovementRatio     | 0.94059    |
| MeanKL               | 0.0069914  |
| Entropy              | 0.44127    |
| Perplexity           | 1.5547     |
| AveragePolicyStd     | 0.26488    |
| AveragePolicyStd[0]  | 0.28861    |
| AveragePolicyStd[1]  | 0.3402     |
| AveragePolicyStd[2]  | 0.21568    |
| AveragePolicyStd[3]  | 0.29188    |
| AveragePolicyStd[4]  | 0.19819    |
| AveragePolicyStd[5]  | 0.25474    |
| AverageReturn        | 1163.7     |
| MinReturn            | 69.858     |
| MaxReturn            | 1361.2     |
| StdReturn            | 315.42     |
| AverageEpisodeLength | 906.59     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 235.51     |
| TotalNEpisodes       | 16925      |
| TotalNSamples        | 2.4165e+06 |
| ExplainedVariance    | -0.022838  |
-------------------------------------
[2018-12-22 10:56:20.939015 UTC] Saving snapshot
[2018-12-22 10:56:20.939258 UTC] Starting iteration 484
[2018-12-22 10:56:20.939407 UTC] Start collecting samples
[2018-12-22 10:56:23.864155 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:23.942581 UTC] Performing policy update
[2018-12-22 10:56:23.943337 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:24.033510 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:25.096782 UTC] Performing line search
[2018-12-22 10:56:25.224350 UTC] Updating baseline
[2018-12-22 10:56:26.891618 UTC] Computing logging information
-------------------------------------
| Iteration            | 484        |
| ExpectedImprovement  | 0.02479    |
| ActualImprovement    | 0.021981   |
| ImprovementRatio     | 0.88667    |
| MeanKL               | 0.0066623  |
| Entropy              | 0.44033    |
| Perplexity           | 1.5532     |
| AveragePolicyStd     | 0.26494    |
| AveragePolicyStd[0]  | 0.28836    |
| AveragePolicyStd[1]  | 0.34085    |
| AveragePolicyStd[2]  | 0.21363    |
| AveragePolicyStd[3]  | 0.29317    |
| AveragePolicyStd[4]  | 0.19845    |
| AveragePolicyStd[5]  | 0.2552     |
| AverageReturn        | 1165.7     |
| MinReturn            | 69.858     |
| MaxReturn            | 1365.8     |
| StdReturn            | 316.23     |
| AverageEpisodeLength | 906.59     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 235.51     |
| TotalNEpisodes       | 16927      |
| TotalNSamples        | 2.4185e+06 |
| ExplainedVariance    | 0.10268    |
-------------------------------------
[2018-12-22 10:56:27.245587 UTC] Saving snapshot
[2018-12-22 10:56:27.245870 UTC] Starting iteration 485
[2018-12-22 10:56:27.245992 UTC] Start collecting samples
[2018-12-22 10:56:30.296424 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:30.379337 UTC] Performing policy update
[2018-12-22 10:56:30.380046 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:30.468570 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:31.424428 UTC] Performing line search
[2018-12-22 10:56:31.541727 UTC] Updating baseline
[2018-12-22 10:56:32.918671 UTC] Computing logging information
-------------------------------------
| Iteration            | 485        |
| ExpectedImprovement  | 0.016079   |
| ActualImprovement    | 0.015567   |
| ImprovementRatio     | 0.96816    |
| MeanKL               | 0.0073417  |
| Entropy              | 0.43282    |
| Perplexity           | 1.5416     |
| AveragePolicyStd     | 0.26464    |
| AveragePolicyStd[0]  | 0.28714    |
| AveragePolicyStd[1]  | 0.34093    |
| AveragePolicyStd[2]  | 0.21311    |
| AveragePolicyStd[3]  | 0.29313    |
| AveragePolicyStd[4]  | 0.19807    |
| AveragePolicyStd[5]  | 0.25545    |
| AverageReturn        | 1169.6     |
| MinReturn            | 71.102     |
| MaxReturn            | 1365.8     |
| StdReturn            | 310.04     |
| AverageEpisodeLength | 905.4      |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 229.02     |
| TotalNEpisodes       | 16937      |
| TotalNSamples        | 2.4275e+06 |
| ExplainedVariance    | 0.16819    |
-------------------------------------
[2018-12-22 10:56:33.299156 UTC] Saving snapshot
[2018-12-22 10:56:33.299457 UTC] Starting iteration 486
[2018-12-22 10:56:33.299655 UTC] Start collecting samples
[2018-12-22 10:56:36.472262 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:36.557104 UTC] Performing policy update
[2018-12-22 10:56:36.559055 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:36.653247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:37.755824 UTC] Performing line search
[2018-12-22 10:56:37.888668 UTC] Updating baseline
[2018-12-22 10:56:39.331037 UTC] Computing logging information
-------------------------------------
| Iteration            | 486        |
| ExpectedImprovement  | 0.018793   |
| ActualImprovement    | 0.017457   |
| ImprovementRatio     | 0.92889    |
| MeanKL               | 0.0069797  |
| Entropy              | 0.42801    |
| Perplexity           | 1.5342     |
| AveragePolicyStd     | 0.26441    |
| AveragePolicyStd[0]  | 0.28738    |
| AveragePolicyStd[1]  | 0.34026    |
| AveragePolicyStd[2]  | 0.21338    |
| AveragePolicyStd[3]  | 0.29262    |
| AveragePolicyStd[4]  | 0.19766    |
| AveragePolicyStd[5]  | 0.25516    |
| AverageReturn        | 1187.5     |
| MinReturn            | 71.102     |
| MaxReturn            | 1371.3     |
| StdReturn            | 292.95     |
| AverageEpisodeLength | 917.1      |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.22     |
| TotalNEpisodes       | 16941      |
| TotalNSamples        | 2.4315e+06 |
| ExplainedVariance    | -0.0048351 |
-------------------------------------
[2018-12-22 10:56:39.705093 UTC] Saving snapshot
[2018-12-22 10:56:39.705419 UTC] Starting iteration 487
[2018-12-22 10:56:39.705592 UTC] Start collecting samples
[2018-12-22 10:56:42.902790 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:42.990156 UTC] Performing policy update
[2018-12-22 10:56:42.990929 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:43.084580 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:44.162930 UTC] Performing line search
[2018-12-22 10:56:44.289716 UTC] Updating baseline
[2018-12-22 10:56:45.851868 UTC] Computing logging information
-------------------------------------
| Iteration            | 487        |
| ExpectedImprovement  | 0.018429   |
| ActualImprovement    | 0.017217   |
| ImprovementRatio     | 0.93424    |
| MeanKL               | 0.0072233  |
| Entropy              | 0.42213    |
| Perplexity           | 1.5252     |
| AveragePolicyStd     | 0.26416    |
| AveragePolicyStd[0]  | 0.28687    |
| AveragePolicyStd[1]  | 0.33978    |
| AveragePolicyStd[2]  | 0.21349    |
| AveragePolicyStd[3]  | 0.29231    |
| AveragePolicyStd[4]  | 0.19682    |
| AveragePolicyStd[5]  | 0.2557     |
| AverageReturn        | 1162.1     |
| MinReturn            | 71.102     |
| MaxReturn            | 1371.3     |
| StdReturn            | 319.31     |
| AverageEpisodeLength | 896.81     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.36     |
| TotalNEpisodes       | 16947      |
| TotalNSamples        | 2.4355e+06 |
| ExplainedVariance    | 0.39588    |
-------------------------------------
[2018-12-22 10:56:46.210062 UTC] Saving snapshot
[2018-12-22 10:56:46.210322 UTC] Starting iteration 488
[2018-12-22 10:56:46.210444 UTC] Start collecting samples
[2018-12-22 10:56:49.166900 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:49.248147 UTC] Performing policy update
[2018-12-22 10:56:49.248817 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:49.339140 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:50.411370 UTC] Performing line search
[2018-12-22 10:56:50.538218 UTC] Updating baseline
[2018-12-22 10:56:52.006732 UTC] Computing logging information
-------------------------------------
| Iteration            | 488        |
| ExpectedImprovement  | 0.016447   |
| ActualImprovement    | 0.015898   |
| ImprovementRatio     | 0.96659    |
| MeanKL               | 0.0073597  |
| Entropy              | 0.41692    |
| Perplexity           | 1.5173     |
| AveragePolicyStd     | 0.26397    |
| AveragePolicyStd[0]  | 0.28761    |
| AveragePolicyStd[1]  | 0.33881    |
| AveragePolicyStd[2]  | 0.21201    |
| AveragePolicyStd[3]  | 0.29232    |
| AveragePolicyStd[4]  | 0.19673    |
| AveragePolicyStd[5]  | 0.25633    |
| AverageReturn        | 1165.2     |
| MinReturn            | 71.102     |
| MaxReturn            | 1371.3     |
| StdReturn            | 320.44     |
| AverageEpisodeLength | 896.81     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.36     |
| TotalNEpisodes       | 16952      |
| TotalNSamples        | 2.4405e+06 |
| ExplainedVariance    | -0.025098  |
-------------------------------------
[2018-12-22 10:56:52.358724 UTC] Saving snapshot
[2018-12-22 10:56:52.359006 UTC] Starting iteration 489
[2018-12-22 10:56:52.359142 UTC] Start collecting samples
[2018-12-22 10:56:55.349425 UTC] Computing input variables for policy optimization
[2018-12-22 10:56:55.432414 UTC] Performing policy update
[2018-12-22 10:56:55.433066 UTC] Computing gradient in Euclidean space
[2018-12-22 10:56:55.523851 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:56:56.592630 UTC] Performing line search
[2018-12-22 10:56:56.720241 UTC] Updating baseline
[2018-12-22 10:56:58.017654 UTC] Computing logging information
-------------------------------------
| Iteration            | 489        |
| ExpectedImprovement  | 0.015878   |
| ActualImprovement    | 0.015206   |
| ImprovementRatio     | 0.95769    |
| MeanKL               | 0.0073798  |
| Entropy              | 0.42037    |
| Perplexity           | 1.5225     |
| AveragePolicyStd     | 0.26413    |
| AveragePolicyStd[0]  | 0.2876     |
| AveragePolicyStd[1]  | 0.33871    |
| AveragePolicyStd[2]  | 0.21296    |
| AveragePolicyStd[3]  | 0.29334    |
| AveragePolicyStd[4]  | 0.19623    |
| AveragePolicyStd[5]  | 0.25592    |
| AverageReturn        | 1185       |
| MinReturn            | 71.102     |
| MaxReturn            | 1371.3     |
| StdReturn            | 303.36     |
| AverageEpisodeLength | 907.44     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.54     |
| TotalNEpisodes       | 16959      |
| TotalNSamples        | 2.4472e+06 |
| ExplainedVariance    | 0.15711    |
-------------------------------------
[2018-12-22 10:56:58.373677 UTC] Saving snapshot
[2018-12-22 10:56:58.373954 UTC] Starting iteration 490
[2018-12-22 10:56:58.374070 UTC] Start collecting samples
[2018-12-22 10:57:01.347706 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:01.428363 UTC] Performing policy update
[2018-12-22 10:57:01.429141 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:01.518172 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:02.588821 UTC] Performing line search
[2018-12-22 10:57:02.716731 UTC] Updating baseline
[2018-12-22 10:57:04.269833 UTC] Computing logging information
-------------------------------------
| Iteration            | 490        |
| ExpectedImprovement  | 0.017747   |
| ActualImprovement    | 0.016768   |
| ImprovementRatio     | 0.94481    |
| MeanKL               | 0.0074082  |
| Entropy              | 0.42362    |
| Perplexity           | 1.5275     |
| AveragePolicyStd     | 0.26429    |
| AveragePolicyStd[0]  | 0.28837    |
| AveragePolicyStd[1]  | 0.33949    |
| AveragePolicyStd[2]  | 0.21362    |
| AveragePolicyStd[3]  | 0.29124    |
| AveragePolicyStd[4]  | 0.19545    |
| AveragePolicyStd[5]  | 0.25755    |
| AverageReturn        | 1199.5     |
| MinReturn            | 71.102     |
| MaxReturn            | 1383.7     |
| StdReturn            | 281.66     |
| AverageEpisodeLength | 916.94     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.82     |
| TotalNEpisodes       | 16964      |
| TotalNSamples        | 2.4518e+06 |
| ExplainedVariance    | 0.28258    |
-------------------------------------
[2018-12-22 10:57:04.628372 UTC] Saving snapshot
[2018-12-22 10:57:04.636633 UTC] Starting iteration 491
[2018-12-22 10:57:04.636832 UTC] Start collecting samples
[2018-12-22 10:57:07.591098 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:07.670489 UTC] Performing policy update
[2018-12-22 10:57:07.671264 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:07.766161 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:08.838248 UTC] Performing line search
[2018-12-22 10:57:08.965558 UTC] Updating baseline
[2018-12-22 10:57:10.884684 UTC] Computing logging information
-------------------------------------
| Iteration            | 491        |
| ExpectedImprovement  | 0.015655   |
| ActualImprovement    | 0.01496    |
| ImprovementRatio     | 0.95562    |
| MeanKL               | 0.0075019  |
| Entropy              | 0.42709    |
| Perplexity           | 1.5328     |
| AveragePolicyStd     | 0.26445    |
| AveragePolicyStd[0]  | 0.28848    |
| AveragePolicyStd[1]  | 0.34062    |
| AveragePolicyStd[2]  | 0.21424    |
| AveragePolicyStd[3]  | 0.29108    |
| AveragePolicyStd[4]  | 0.19569    |
| AveragePolicyStd[5]  | 0.25656    |
| AverageReturn        | 1200.8     |
| MinReturn            | 71.102     |
| MaxReturn            | 1383.7     |
| StdReturn            | 282.18     |
| AverageEpisodeLength | 916.94     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.82     |
| TotalNEpisodes       | 16967      |
| TotalNSamples        | 2.4548e+06 |
| ExplainedVariance    | -0.012025  |
-------------------------------------
[2018-12-22 10:57:11.235298 UTC] Saving snapshot
[2018-12-22 10:57:11.235558 UTC] Starting iteration 492
[2018-12-22 10:57:11.235683 UTC] Start collecting samples
[2018-12-22 10:57:14.268195 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:14.351835 UTC] Performing policy update
[2018-12-22 10:57:14.352462 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:14.444281 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:15.503607 UTC] Performing line search
[2018-12-22 10:57:15.631926 UTC] Updating baseline
[2018-12-22 10:57:17.159961 UTC] Computing logging information
-------------------------------------
| Iteration            | 492        |
| ExpectedImprovement  | 0.016749   |
| ActualImprovement    | 0.016131   |
| ImprovementRatio     | 0.96309    |
| MeanKL               | 0.0070315  |
| Entropy              | 0.43309    |
| Perplexity           | 1.542      |
| AveragePolicyStd     | 0.26477    |
| AveragePolicyStd[0]  | 0.28766    |
| AveragePolicyStd[1]  | 0.34285    |
| AveragePolicyStd[2]  | 0.21439    |
| AveragePolicyStd[3]  | 0.29073    |
| AveragePolicyStd[4]  | 0.19564    |
| AveragePolicyStd[5]  | 0.25735    |
| AverageReturn        | 1194.2     |
| MinReturn            | 71.102     |
| MaxReturn            | 1383.7     |
| StdReturn            | 305.62     |
| AverageEpisodeLength | 910.08     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.59     |
| TotalNEpisodes       | 16976      |
| TotalNSamples        | 2.4622e+06 |
| ExplainedVariance    | 0.19177    |
-------------------------------------
[2018-12-22 10:57:17.514492 UTC] Saving snapshot
[2018-12-22 10:57:17.514778 UTC] Starting iteration 493
[2018-12-22 10:57:17.514910 UTC] Start collecting samples
[2018-12-22 10:57:20.570758 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:20.654570 UTC] Performing policy update
[2018-12-22 10:57:20.655388 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:20.745720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:21.822963 UTC] Performing line search
[2018-12-22 10:57:21.954744 UTC] Updating baseline
[2018-12-22 10:57:23.444058 UTC] Computing logging information
-------------------------------------
| Iteration            | 493        |
| ExpectedImprovement  | 0.017804   |
| ActualImprovement    | 0.016692   |
| ImprovementRatio     | 0.93751    |
| MeanKL               | 0.0069079  |
| Entropy              | 0.42444    |
| Perplexity           | 1.5287     |
| AveragePolicyStd     | 0.26436    |
| AveragePolicyStd[0]  | 0.28849    |
| AveragePolicyStd[1]  | 0.34138    |
| AveragePolicyStd[2]  | 0.21335    |
| AveragePolicyStd[3]  | 0.28957    |
| AveragePolicyStd[4]  | 0.19584    |
| AveragePolicyStd[5]  | 0.25752    |
| AverageReturn        | 1149.1     |
| MinReturn            | 47.269     |
| MaxReturn            | 1383.7     |
| StdReturn            | 373.09     |
| AverageEpisodeLength | 874.66     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 274.67     |
| TotalNEpisodes       | 16987      |
| TotalNSamples        | 2.4688e+06 |
| ExplainedVariance    | 0.23315    |
-------------------------------------
[2018-12-22 10:57:23.806152 UTC] Saving snapshot
[2018-12-22 10:57:23.806425 UTC] Starting iteration 494
[2018-12-22 10:57:23.806559 UTC] Start collecting samples
[2018-12-22 10:57:26.759805 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:26.840254 UTC] Performing policy update
[2018-12-22 10:57:26.840944 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:26.932958 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:28.062879 UTC] Performing line search
[2018-12-22 10:57:28.197936 UTC] Updating baseline
[2018-12-22 10:57:29.653036 UTC] Computing logging information
-------------------------------------
| Iteration            | 494        |
| ExpectedImprovement  | 0.017813   |
| ActualImprovement    | 0.017351   |
| ImprovementRatio     | 0.97404    |
| MeanKL               | 0.0073068  |
| Entropy              | 0.42801    |
| Perplexity           | 1.5342     |
| AveragePolicyStd     | 0.26441    |
| AveragePolicyStd[0]  | 0.29022    |
| AveragePolicyStd[1]  | 0.33973    |
| AveragePolicyStd[2]  | 0.21427    |
| AveragePolicyStd[3]  | 0.28845    |
| AveragePolicyStd[4]  | 0.19609    |
| AveragePolicyStd[5]  | 0.2577     |
| AverageReturn        | 1149.7     |
| MinReturn            | 47.269     |
| MaxReturn            | 1383.7     |
| StdReturn            | 373.33     |
| AverageEpisodeLength | 874.66     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 274.67     |
| TotalNEpisodes       | 16988      |
| TotalNSamples        | 2.4698e+06 |
| ExplainedVariance    | 0.034065   |
-------------------------------------
[2018-12-22 10:57:30.055050 UTC] Saving snapshot
[2018-12-22 10:57:30.055298 UTC] Starting iteration 495
[2018-12-22 10:57:30.055415 UTC] Start collecting samples
[2018-12-22 10:57:33.216827 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:33.297024 UTC] Performing policy update
[2018-12-22 10:57:33.297861 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:33.388180 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:34.451069 UTC] Performing line search
[2018-12-22 10:57:34.578108 UTC] Updating baseline
[2018-12-22 10:57:36.221552 UTC] Computing logging information
-------------------------------------
| Iteration            | 495        |
| ExpectedImprovement  | 0.019037   |
| ActualImprovement    | 0.018553   |
| ImprovementRatio     | 0.97461    |
| MeanKL               | 0.0072907  |
| Entropy              | 0.42577    |
| Perplexity           | 1.5308     |
| AveragePolicyStd     | 0.26438    |
| AveragePolicyStd[0]  | 0.29013    |
| AveragePolicyStd[1]  | 0.34099    |
| AveragePolicyStd[2]  | 0.21426    |
| AveragePolicyStd[3]  | 0.28841    |
| AveragePolicyStd[4]  | 0.19574    |
| AveragePolicyStd[5]  | 0.25676    |
| AverageReturn        | 1152.5     |
| MinReturn            | 47.269     |
| MaxReturn            | 1396.8     |
| StdReturn            | 374.59     |
| AverageEpisodeLength | 874.66     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 274.67     |
| TotalNEpisodes       | 16993      |
| TotalNSamples        | 2.4748e+06 |
| ExplainedVariance    | 0.018734   |
-------------------------------------
[2018-12-22 10:57:36.569402 UTC] Saving snapshot
[2018-12-22 10:57:36.569655 UTC] Starting iteration 496
[2018-12-22 10:57:36.569799 UTC] Start collecting samples
[2018-12-22 10:57:39.583466 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:39.665487 UTC] Performing policy update
[2018-12-22 10:57:39.666155 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:39.756448 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:40.833288 UTC] Performing line search
[2018-12-22 10:57:40.962061 UTC] Updating baseline
[2018-12-22 10:57:42.439915 UTC] Computing logging information
-------------------------------------
| Iteration            | 496        |
| ExpectedImprovement  | 0.017004   |
| ActualImprovement    | 0.016473   |
| ImprovementRatio     | 0.96876    |
| MeanKL               | 0.0072685  |
| Entropy              | 0.4186     |
| Perplexity           | 1.5198     |
| AveragePolicyStd     | 0.26407    |
| AveragePolicyStd[0]  | 0.29019    |
| AveragePolicyStd[1]  | 0.3405     |
| AveragePolicyStd[2]  | 0.21306    |
| AveragePolicyStd[3]  | 0.28806    |
| AveragePolicyStd[4]  | 0.19619    |
| AveragePolicyStd[5]  | 0.25639    |
| AverageReturn        | 1160.9     |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 374.84     |
| AverageEpisodeLength | 878.52     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 273.69     |
| TotalNEpisodes       | 17001      |
| TotalNSamples        | 2.4828e+06 |
| ExplainedVariance    | -0.0096764 |
-------------------------------------
[2018-12-22 10:57:42.797260 UTC] Saving snapshot
[2018-12-22 10:57:42.797510 UTC] Starting iteration 497
[2018-12-22 10:57:42.797656 UTC] Start collecting samples
[2018-12-22 10:57:45.759410 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:45.838419 UTC] Performing policy update
[2018-12-22 10:57:45.839047 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:45.935060 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:46.991313 UTC] Performing line search
[2018-12-22 10:57:47.119475 UTC] Updating baseline
[2018-12-22 10:57:48.501144 UTC] Computing logging information
-------------------------------------
| Iteration            | 497        |
| ExpectedImprovement  | 0.016528   |
| ActualImprovement    | 0.015642   |
| ImprovementRatio     | 0.94639    |
| MeanKL               | 0.0072102  |
| Entropy              | 0.42768    |
| Perplexity           | 1.5337     |
| AveragePolicyStd     | 0.26459    |
| AveragePolicyStd[0]  | 0.29004    |
| AveragePolicyStd[1]  | 0.3421     |
| AveragePolicyStd[2]  | 0.21247    |
| AveragePolicyStd[3]  | 0.2897     |
| AveragePolicyStd[4]  | 0.19581    |
| AveragePolicyStd[5]  | 0.25742    |
| AverageReturn        | 1162.1     |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 375.23     |
| AverageEpisodeLength | 878.72     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 273.77     |
| TotalNEpisodes       | 17004      |
| TotalNSamples        | 2.4858e+06 |
| ExplainedVariance    | 0.0021623  |
-------------------------------------
[2018-12-22 10:57:48.855916 UTC] Saving snapshot
[2018-12-22 10:57:48.856162 UTC] Starting iteration 498
[2018-12-22 10:57:48.856283 UTC] Start collecting samples
[2018-12-22 10:57:51.805872 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:51.885167 UTC] Performing policy update
[2018-12-22 10:57:51.885954 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:51.978919 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:53.042037 UTC] Performing line search
[2018-12-22 10:57:53.170347 UTC] Updating baseline
[2018-12-22 10:57:54.455345 UTC] Computing logging information
-------------------------------------
| Iteration            | 498        |
| ExpectedImprovement  | 0.015814   |
| ActualImprovement    | 0.015012   |
| ImprovementRatio     | 0.94926    |
| MeanKL               | 0.0070473  |
| Entropy              | 0.4228     |
| Perplexity           | 1.5262     |
| AveragePolicyStd     | 0.26442    |
| AveragePolicyStd[0]  | 0.28864    |
| AveragePolicyStd[1]  | 0.3432     |
| AveragePolicyStd[2]  | 0.21249    |
| AveragePolicyStd[3]  | 0.28883    |
| AveragePolicyStd[4]  | 0.19519    |
| AveragePolicyStd[5]  | 0.25814    |
| AverageReturn        | 1164.4     |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 376.32     |
| AverageEpisodeLength | 878.72     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 273.77     |
| TotalNEpisodes       | 17008      |
| TotalNSamples        | 2.4898e+06 |
| ExplainedVariance    | -0.0047712 |
-------------------------------------
[2018-12-22 10:57:54.810318 UTC] Saving snapshot
[2018-12-22 10:57:54.810590 UTC] Starting iteration 499
[2018-12-22 10:57:54.810745 UTC] Start collecting samples
[2018-12-22 10:57:57.814982 UTC] Computing input variables for policy optimization
[2018-12-22 10:57:57.897855 UTC] Performing policy update
[2018-12-22 10:57:57.898508 UTC] Computing gradient in Euclidean space
[2018-12-22 10:57:57.993063 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:57:59.063866 UTC] Performing line search
[2018-12-22 10:57:59.215735 UTC] Updating baseline
[2018-12-22 10:58:00.602327 UTC] Computing logging information
-------------------------------------
| Iteration            | 499        |
| ExpectedImprovement  | 0.0185     |
| ActualImprovement    | 0.017697   |
| ImprovementRatio     | 0.9566     |
| MeanKL               | 0.0071894  |
| Entropy              | 0.41419    |
| Perplexity           | 1.5131     |
| AveragePolicyStd     | 0.26401    |
| AveragePolicyStd[0]  | 0.28825    |
| AveragePolicyStd[1]  | 0.34202    |
| AveragePolicyStd[2]  | 0.21301    |
| AveragePolicyStd[3]  | 0.28782    |
| AveragePolicyStd[4]  | 0.19415    |
| AveragePolicyStd[5]  | 0.25882    |
| AverageReturn        | 1171.5     |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 376.95     |
| AverageEpisodeLength | 883.51     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 274.07     |
| TotalNEpisodes       | 17016      |
| TotalNSamples        | 2.4968e+06 |
| ExplainedVariance    | 0.14181    |
-------------------------------------
[2018-12-22 10:58:00.958334 UTC] Saving snapshot
[2018-12-22 10:58:00.958597 UTC] Starting iteration 500
[2018-12-22 10:58:00.958719 UTC] Start collecting samples
[2018-12-22 10:58:03.920965 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:04.004812 UTC] Performing policy update
[2018-12-22 10:58:04.005510 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:04.096584 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:05.167008 UTC] Performing line search
[2018-12-22 10:58:05.294509 UTC] Updating baseline
[2018-12-22 10:58:06.626412 UTC] Computing logging information
-------------------------------------
| Iteration            | 500        |
| ExpectedImprovement  | 0.017495   |
| ActualImprovement    | 0.016578   |
| ImprovementRatio     | 0.94759    |
| MeanKL               | 0.0071687  |
| Entropy              | 0.40825    |
| Perplexity           | 1.5042     |
| AveragePolicyStd     | 0.26365    |
| AveragePolicyStd[0]  | 0.28742    |
| AveragePolicyStd[1]  | 0.34125    |
| AveragePolicyStd[2]  | 0.21367    |
| AveragePolicyStd[3]  | 0.28623    |
| AveragePolicyStd[4]  | 0.19437    |
| AveragePolicyStd[5]  | 0.25895    |
| AverageReturn        | 1180.5     |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 366.47     |
| AverageEpisodeLength | 887.49     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.02     |
| TotalNEpisodes       | 17021      |
| TotalNSamples        | 2.5013e+06 |
| ExplainedVariance    | 0.28145    |
-------------------------------------
[2018-12-22 10:58:06.982017 UTC] Saving snapshot
[2018-12-22 10:58:06.990200 UTC] Starting iteration 501
[2018-12-22 10:58:06.990412 UTC] Start collecting samples
[2018-12-22 10:58:09.997811 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:10.079534 UTC] Performing policy update
[2018-12-22 10:58:10.080312 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:10.171849 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:11.243129 UTC] Performing line search
[2018-12-22 10:58:11.371381 UTC] Updating baseline
[2018-12-22 10:58:12.758083 UTC] Computing logging information
------------------------------------
| Iteration            | 501       |
| ExpectedImprovement  | 0.018905  |
| ActualImprovement    | 0.018741  |
| ImprovementRatio     | 0.99132   |
| MeanKL               | 0.0071084 |
| Entropy              | 0.40079   |
| Perplexity           | 1.493     |
| AveragePolicyStd     | 0.26329   |
| AveragePolicyStd[0]  | 0.2855    |
| AveragePolicyStd[1]  | 0.34132   |
| AveragePolicyStd[2]  | 0.21346   |
| AveragePolicyStd[3]  | 0.28628   |
| AveragePolicyStd[4]  | 0.19458   |
| AveragePolicyStd[5]  | 0.25862   |
| AverageReturn        | 1152.7    |
| MinReturn            | 47.269    |
| MaxReturn            | 1411.6    |
| StdReturn            | 393.49    |
| AverageEpisodeLength | 864.53    |
| MinEpisodeLength     | 57        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 284.23    |
| TotalNEpisodes       | 17028     |
| TotalNSamples        | 2.506e+06 |
| ExplainedVariance    | 0.22684   |
------------------------------------
[2018-12-22 10:58:13.118496 UTC] Saving snapshot
[2018-12-22 10:58:13.118785 UTC] Starting iteration 502
[2018-12-22 10:58:13.118909 UTC] Start collecting samples
[2018-12-22 10:58:16.152254 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:16.234489 UTC] Performing policy update
[2018-12-22 10:58:16.235190 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:16.326168 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:17.393662 UTC] Performing line search
[2018-12-22 10:58:17.522154 UTC] Updating baseline
[2018-12-22 10:58:19.264078 UTC] Computing logging information
-------------------------------------
| Iteration            | 502        |
| ExpectedImprovement  | 0.018263   |
| ActualImprovement    | 0.017627   |
| ImprovementRatio     | 0.96516    |
| MeanKL               | 0.0070384  |
| Entropy              | 0.39223    |
| Perplexity           | 1.4803     |
| AveragePolicyStd     | 0.26282    |
| AveragePolicyStd[0]  | 0.28421    |
| AveragePolicyStd[1]  | 0.33945    |
| AveragePolicyStd[2]  | 0.21347    |
| AveragePolicyStd[3]  | 0.2861     |
| AveragePolicyStd[4]  | 0.19473    |
| AveragePolicyStd[5]  | 0.25896    |
| AverageReturn        | 1170       |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 386.65     |
| AverageEpisodeLength | 873.86     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 278.66     |
| TotalNEpisodes       | 17035      |
| TotalNSamples        | 2.5129e+06 |
| ExplainedVariance    | 0.056018   |
-------------------------------------
[2018-12-22 10:58:19.620525 UTC] Saving snapshot
[2018-12-22 10:58:19.620867 UTC] Starting iteration 503
[2018-12-22 10:58:19.620996 UTC] Start collecting samples
[2018-12-22 10:58:22.581579 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:22.660125 UTC] Performing policy update
[2018-12-22 10:58:22.660794 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:22.750790 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:23.832698 UTC] Performing line search
[2018-12-22 10:58:23.965231 UTC] Updating baseline
[2018-12-22 10:58:25.183508 UTC] Computing logging information
-------------------------------------
| Iteration            | 503        |
| ExpectedImprovement  | 0.018847   |
| ActualImprovement    | 0.018      |
| ImprovementRatio     | 0.9551     |
| MeanKL               | 0.0070417  |
| Entropy              | 0.39001    |
| Perplexity           | 1.477      |
| AveragePolicyStd     | 0.26267    |
| AveragePolicyStd[0]  | 0.28607    |
| AveragePolicyStd[1]  | 0.33817    |
| AveragePolicyStd[2]  | 0.2137     |
| AveragePolicyStd[3]  | 0.28434    |
| AveragePolicyStd[4]  | 0.19468    |
| AveragePolicyStd[5]  | 0.25906    |
| AverageReturn        | 1165.1     |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 388.54     |
| AverageEpisodeLength | 869.46     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 280.1      |
| TotalNEpisodes       | 17037      |
| TotalNSamples        | 2.5144e+06 |
| ExplainedVariance    | 0.16551    |
-------------------------------------
[2018-12-22 10:58:25.545302 UTC] Saving snapshot
[2018-12-22 10:58:25.545606 UTC] Starting iteration 504
[2018-12-22 10:58:25.545728 UTC] Start collecting samples
[2018-12-22 10:58:28.577213 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:28.662271 UTC] Performing policy update
[2018-12-22 10:58:28.663069 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:28.754620 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:29.835769 UTC] Performing line search
[2018-12-22 10:58:29.968123 UTC] Updating baseline
[2018-12-22 10:58:31.427612 UTC] Computing logging information
-------------------------------------
| Iteration            | 504        |
| ExpectedImprovement  | 0.017574   |
| ActualImprovement    | 0.016375   |
| ImprovementRatio     | 0.93173    |
| MeanKL               | 0.0072005  |
| Entropy              | 0.37388    |
| Perplexity           | 1.4534     |
| AveragePolicyStd     | 0.2621     |
| AveragePolicyStd[0]  | 0.28473    |
| AveragePolicyStd[1]  | 0.33933    |
| AveragePolicyStd[2]  | 0.21238    |
| AveragePolicyStd[3]  | 0.2844     |
| AveragePolicyStd[4]  | 0.19371    |
| AveragePolicyStd[5]  | 0.25804    |
| AverageReturn        | 1174       |
| MinReturn            | 47.269     |
| MaxReturn            | 1411.6     |
| StdReturn            | 388.35     |
| AverageEpisodeLength | 873.18     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 278.92     |
| TotalNEpisodes       | 17044      |
| TotalNSamples        | 2.5205e+06 |
| ExplainedVariance    | 0.066289   |
-------------------------------------
[2018-12-22 10:58:31.788282 UTC] Saving snapshot
[2018-12-22 10:58:31.788529 UTC] Starting iteration 505
[2018-12-22 10:58:31.788667 UTC] Start collecting samples
[2018-12-22 10:58:34.820565 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:34.903178 UTC] Performing policy update
[2018-12-22 10:58:34.903955 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:34.995210 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:36.079941 UTC] Performing line search
[2018-12-22 10:58:36.209172 UTC] Updating baseline
[2018-12-22 10:58:37.407702 UTC] Computing logging information
-------------------------------------
| Iteration            | 505        |
| ExpectedImprovement  | 0.016218   |
| ActualImprovement    | 0.015892   |
| ImprovementRatio     | 0.97988    |
| MeanKL               | 0.0070508  |
| Entropy              | 0.37103    |
| Perplexity           | 1.4492     |
| AveragePolicyStd     | 0.26199    |
| AveragePolicyStd[0]  | 0.28594    |
| AveragePolicyStd[1]  | 0.33873    |
| AveragePolicyStd[2]  | 0.21129    |
| AveragePolicyStd[3]  | 0.2843     |
| AveragePolicyStd[4]  | 0.19417    |
| AveragePolicyStd[5]  | 0.2575     |
| AverageReturn        | 1188.9     |
| MinReturn            | 47.269     |
| MaxReturn            | 1436.5     |
| StdReturn            | 386.62     |
| AverageEpisodeLength | 880.73     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.62     |
| TotalNEpisodes       | 17050      |
| TotalNSamples        | 2.5265e+06 |
| ExplainedVariance    | -0.04757   |
-------------------------------------
[2018-12-22 10:58:37.770786 UTC] Saving snapshot
[2018-12-22 10:58:37.771049 UTC] Starting iteration 506
[2018-12-22 10:58:37.771175 UTC] Start collecting samples
[2018-12-22 10:58:40.755659 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:40.836367 UTC] Performing policy update
[2018-12-22 10:58:40.837034 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:40.928718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:41.996787 UTC] Performing line search
[2018-12-22 10:58:42.125258 UTC] Updating baseline
[2018-12-22 10:58:43.560368 UTC] Computing logging information
-------------------------------------
| Iteration            | 506        |
| ExpectedImprovement  | 0.017984   |
| ActualImprovement    | 0.017187   |
| ImprovementRatio     | 0.95567    |
| MeanKL               | 0.0068814  |
| Entropy              | 0.36687    |
| Perplexity           | 1.4432     |
| AveragePolicyStd     | 0.26174    |
| AveragePolicyStd[0]  | 0.28541    |
| AveragePolicyStd[1]  | 0.33847    |
| AveragePolicyStd[2]  | 0.21166    |
| AveragePolicyStd[3]  | 0.28407    |
| AveragePolicyStd[4]  | 0.19483    |
| AveragePolicyStd[5]  | 0.25597    |
| AverageReturn        | 1187       |
| MinReturn            | 47.269     |
| MaxReturn            | 1436.5     |
| StdReturn            | 387.89     |
| AverageEpisodeLength | 877.8      |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.89     |
| TotalNEpisodes       | 17055      |
| TotalNSamples        | 2.5312e+06 |
| ExplainedVariance    | 0.11107    |
-------------------------------------
[2018-12-22 10:58:43.913222 UTC] Saving snapshot
[2018-12-22 10:58:43.913474 UTC] Starting iteration 507
[2018-12-22 10:58:43.913610 UTC] Start collecting samples
[2018-12-22 10:58:46.909478 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:46.990266 UTC] Performing policy update
[2018-12-22 10:58:46.991042 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:47.080883 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:48.155243 UTC] Performing line search
[2018-12-22 10:58:48.283875 UTC] Updating baseline
[2018-12-22 10:58:49.678782 UTC] Computing logging information
-------------------------------------
| Iteration            | 507        |
| ExpectedImprovement  | 0.018262   |
| ActualImprovement    | 0.017272   |
| ImprovementRatio     | 0.94579    |
| MeanKL               | 0.0069775  |
| Entropy              | 0.36399    |
| Perplexity           | 1.4391     |
| AveragePolicyStd     | 0.26155    |
| AveragePolicyStd[0]  | 0.28558    |
| AveragePolicyStd[1]  | 0.3382     |
| AveragePolicyStd[2]  | 0.21214    |
| AveragePolicyStd[3]  | 0.28246    |
| AveragePolicyStd[4]  | 0.19499    |
| AveragePolicyStd[5]  | 0.25595    |
| AverageReturn        | 1187.9     |
| MinReturn            | 47.269     |
| MaxReturn            | 1436.5     |
| StdReturn            | 396.89     |
| AverageEpisodeLength | 876.41     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 282.66     |
| TotalNEpisodes       | 17061      |
| TotalNSamples        | 2.5365e+06 |
| ExplainedVariance    | 0.09824    |
-------------------------------------
[2018-12-22 10:58:50.039290 UTC] Saving snapshot
[2018-12-22 10:58:50.039568 UTC] Starting iteration 508
[2018-12-22 10:58:50.039690 UTC] Start collecting samples
[2018-12-22 10:58:53.022293 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:53.103984 UTC] Performing policy update
[2018-12-22 10:58:53.104663 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:53.196122 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:58:54.271726 UTC] Performing line search
[2018-12-22 10:58:54.398956 UTC] Updating baseline
[2018-12-22 10:58:55.683216 UTC] Computing logging information
-------------------------------------
| Iteration            | 508        |
| ExpectedImprovement  | 0.016701   |
| ActualImprovement    | 0.016723   |
| ImprovementRatio     | 1.0013     |
| MeanKL               | 0.0072542  |
| Entropy              | 0.35941    |
| Perplexity           | 1.4325     |
| AveragePolicyStd     | 0.26137    |
| AveragePolicyStd[0]  | 0.28711    |
| AveragePolicyStd[1]  | 0.33734    |
| AveragePolicyStd[2]  | 0.21216    |
| AveragePolicyStd[3]  | 0.28135    |
| AveragePolicyStd[4]  | 0.19428    |
| AveragePolicyStd[5]  | 0.25598    |
| AverageReturn        | 1190.6     |
| MinReturn            | 47.269     |
| MaxReturn            | 1436.5     |
| StdReturn            | 398.02     |
| AverageEpisodeLength | 876.83     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 282.81     |
| TotalNEpisodes       | 17066      |
| TotalNSamples        | 2.5415e+06 |
| ExplainedVariance    | -0.025581  |
-------------------------------------
[2018-12-22 10:58:56.045775 UTC] Saving snapshot
[2018-12-22 10:58:56.046033 UTC] Starting iteration 509
[2018-12-22 10:58:56.046151 UTC] Start collecting samples
[2018-12-22 10:58:59.026136 UTC] Computing input variables for policy optimization
[2018-12-22 10:58:59.106368 UTC] Performing policy update
[2018-12-22 10:58:59.107157 UTC] Computing gradient in Euclidean space
[2018-12-22 10:58:59.199240 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:00.274670 UTC] Performing line search
[2018-12-22 10:59:00.402049 UTC] Updating baseline
[2018-12-22 10:59:01.688742 UTC] Computing logging information
-------------------------------------
| Iteration            | 509        |
| ExpectedImprovement  | 0.016055   |
| ActualImprovement    | 0.015037   |
| ImprovementRatio     | 0.93659    |
| MeanKL               | 0.0070436  |
| Entropy              | 0.36042    |
| Perplexity           | 1.4339     |
| AveragePolicyStd     | 0.26145    |
| AveragePolicyStd[0]  | 0.28801    |
| AveragePolicyStd[1]  | 0.33788    |
| AveragePolicyStd[2]  | 0.2118     |
| AveragePolicyStd[3]  | 0.28063    |
| AveragePolicyStd[4]  | 0.19428    |
| AveragePolicyStd[5]  | 0.25613    |
| AverageReturn        | 1205.7     |
| MinReturn            | 47.269     |
| MaxReturn            | 1436.5     |
| StdReturn            | 384.87     |
| AverageEpisodeLength | 885.74     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 272.32     |
| TotalNEpisodes       | 17071      |
| TotalNSamples        | 2.5465e+06 |
| ExplainedVariance    | 0.012878   |
-------------------------------------
[2018-12-22 10:59:02.048675 UTC] Saving snapshot
[2018-12-22 10:59:02.048941 UTC] Starting iteration 510
[2018-12-22 10:59:02.049057 UTC] Start collecting samples
[2018-12-22 10:59:05.057353 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:05.139478 UTC] Performing policy update
[2018-12-22 10:59:05.140068 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:05.230634 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:06.312962 UTC] Performing line search
[2018-12-22 10:59:06.442472 UTC] Updating baseline
[2018-12-22 10:59:08.201238 UTC] Computing logging information
-------------------------------------
| Iteration            | 510        |
| ExpectedImprovement  | 0.020218   |
| ActualImprovement    | 0.01831    |
| ImprovementRatio     | 0.90565    |
| MeanKL               | 0.0068171  |
| Entropy              | 0.35638    |
| Perplexity           | 1.4282     |
| AveragePolicyStd     | 0.26122    |
| AveragePolicyStd[0]  | 0.28743    |
| AveragePolicyStd[1]  | 0.33713    |
| AveragePolicyStd[2]  | 0.21237    |
| AveragePolicyStd[3]  | 0.28011    |
| AveragePolicyStd[4]  | 0.19411    |
| AveragePolicyStd[5]  | 0.25619    |
| AverageReturn        | 1211.7     |
| MinReturn            | 47.269     |
| MaxReturn            | 1436.5     |
| StdReturn            | 378.71     |
| AverageEpisodeLength | 888.63     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 267.67     |
| TotalNEpisodes       | 17077      |
| TotalNSamples        | 2.5513e+06 |
| ExplainedVariance    | 0.1462     |
-------------------------------------
[2018-12-22 10:59:08.560569 UTC] Saving snapshot
[2018-12-22 10:59:08.568635 UTC] Starting iteration 511
[2018-12-22 10:59:08.568835 UTC] Start collecting samples
[2018-12-22 10:59:11.549211 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:11.628921 UTC] Performing policy update
[2018-12-22 10:59:11.629508 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:11.721439 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:12.800978 UTC] Performing line search
[2018-12-22 10:59:12.931511 UTC] Updating baseline
[2018-12-22 10:59:14.313052 UTC] Computing logging information
-------------------------------------
| Iteration            | 511        |
| ExpectedImprovement  | 0.016247   |
| ActualImprovement    | 0.015182   |
| ImprovementRatio     | 0.93442    |
| MeanKL               | 0.0073631  |
| Entropy              | 0.34953    |
| Perplexity           | 1.4184     |
| AveragePolicyStd     | 0.26085    |
| AveragePolicyStd[0]  | 0.28795    |
| AveragePolicyStd[1]  | 0.33575    |
| AveragePolicyStd[2]  | 0.2121     |
| AveragePolicyStd[3]  | 0.27854    |
| AveragePolicyStd[4]  | 0.19446    |
| AveragePolicyStd[5]  | 0.25632    |
| AverageReturn        | 1238.8     |
| MinReturn            | 47.269     |
| MaxReturn            | 1436.5     |
| StdReturn            | 345.33     |
| AverageEpisodeLength | 906.4      |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 243.91     |
| TotalNEpisodes       | 17082      |
| TotalNSamples        | 2.5563e+06 |
| ExplainedVariance    | 0.0019039  |
-------------------------------------
[2018-12-22 10:59:14.674930 UTC] Saving snapshot
[2018-12-22 10:59:14.675184 UTC] Starting iteration 512
[2018-12-22 10:59:14.675332 UTC] Start collecting samples
[2018-12-22 10:59:17.662672 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:17.743785 UTC] Performing policy update
[2018-12-22 10:59:17.744436 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:17.838761 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:18.909237 UTC] Performing line search
[2018-12-22 10:59:19.036231 UTC] Updating baseline
[2018-12-22 10:59:20.249669 UTC] Computing logging information
-------------------------------------
| Iteration            | 512        |
| ExpectedImprovement  | 0.017902   |
| ActualImprovement    | 0.017086   |
| ImprovementRatio     | 0.95442    |
| MeanKL               | 0.0075326  |
| Entropy              | 0.32636    |
| Perplexity           | 1.3859     |
| AveragePolicyStd     | 0.25978    |
| AveragePolicyStd[0]  | 0.28633    |
| AveragePolicyStd[1]  | 0.33346    |
| AveragePolicyStd[2]  | 0.21178    |
| AveragePolicyStd[3]  | 0.27802    |
| AveragePolicyStd[4]  | 0.19391    |
| AveragePolicyStd[5]  | 0.25518    |
| AverageReturn        | 1260.5     |
| MinReturn            | 109.29     |
| MaxReturn            | 1436.5     |
| StdReturn            | 305.15     |
| AverageEpisodeLength | 921.11     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.27     |
| TotalNEpisodes       | 17088      |
| TotalNSamples        | 2.5619e+06 |
| ExplainedVariance    | 0.1598     |
-------------------------------------
[2018-12-22 10:59:20.605770 UTC] Saving snapshot
[2018-12-22 10:59:20.606042 UTC] Starting iteration 513
[2018-12-22 10:59:20.606161 UTC] Start collecting samples
[2018-12-22 10:59:23.602088 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:23.683419 UTC] Performing policy update
[2018-12-22 10:59:23.684033 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:23.779720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:24.848135 UTC] Performing line search
[2018-12-22 10:59:24.977776 UTC] Updating baseline
[2018-12-22 10:59:26.340184 UTC] Computing logging information
-------------------------------------
| Iteration            | 513        |
| ExpectedImprovement  | 0.019323   |
| ActualImprovement    | 0.018206   |
| ImprovementRatio     | 0.94221    |
| MeanKL               | 0.0071453  |
| Entropy              | 0.32549    |
| Perplexity           | 1.3847     |
| AveragePolicyStd     | 0.25977    |
| AveragePolicyStd[0]  | 0.28616    |
| AveragePolicyStd[1]  | 0.33357    |
| AveragePolicyStd[2]  | 0.21168    |
| AveragePolicyStd[3]  | 0.27778    |
| AveragePolicyStd[4]  | 0.19342    |
| AveragePolicyStd[5]  | 0.25601    |
| AverageReturn        | 1256.4     |
| MinReturn            | 109.29     |
| MaxReturn            | 1436.5     |
| StdReturn            | 310.9      |
| AverageEpisodeLength | 916.22     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.96     |
| TotalNEpisodes       | 17094      |
| TotalNSamples        | 2.5675e+06 |
| ExplainedVariance    | 0.083226   |
-------------------------------------
[2018-12-22 10:59:26.700422 UTC] Saving snapshot
[2018-12-22 10:59:26.700698 UTC] Starting iteration 514
[2018-12-22 10:59:26.700819 UTC] Start collecting samples
[2018-12-22 10:59:29.675420 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:29.760146 UTC] Performing policy update
[2018-12-22 10:59:29.760877 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:29.853811 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:30.924608 UTC] Performing line search
[2018-12-22 10:59:31.052268 UTC] Updating baseline
[2018-12-22 10:59:32.594852 UTC] Computing logging information
-------------------------------------
| Iteration            | 514        |
| ExpectedImprovement  | 0.014916   |
| ActualImprovement    | 0.014296   |
| ImprovementRatio     | 0.95845    |
| MeanKL               | 0.0074946  |
| Entropy              | 0.30933    |
| Perplexity           | 1.3625     |
| AveragePolicyStd     | 0.25903    |
| AveragePolicyStd[0]  | 0.28592    |
| AveragePolicyStd[1]  | 0.33245    |
| AveragePolicyStd[2]  | 0.21216    |
| AveragePolicyStd[3]  | 0.27601    |
| AveragePolicyStd[4]  | 0.19281    |
| AveragePolicyStd[5]  | 0.25482    |
| AverageReturn        | 1258.1     |
| MinReturn            | 109.29     |
| MaxReturn            | 1436.5     |
| StdReturn            | 311.43     |
| AverageEpisodeLength | 916.22     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.96     |
| TotalNEpisodes       | 17097      |
| TotalNSamples        | 2.5705e+06 |
| ExplainedVariance    | 0.015187   |
-------------------------------------
[2018-12-22 10:59:32.951247 UTC] Saving snapshot
[2018-12-22 10:59:32.951548 UTC] Starting iteration 515
[2018-12-22 10:59:32.951674 UTC] Start collecting samples
[2018-12-22 10:59:35.946941 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:36.029774 UTC] Performing policy update
[2018-12-22 10:59:36.030698 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:36.122067 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:37.192855 UTC] Performing line search
[2018-12-22 10:59:37.320930 UTC] Updating baseline
[2018-12-22 10:59:38.708880 UTC] Computing logging information
-------------------------------------
| Iteration            | 515        |
| ExpectedImprovement  | 0.017755   |
| ActualImprovement    | 0.016681   |
| ImprovementRatio     | 0.93952    |
| MeanKL               | 0.0069068  |
| Entropy              | 0.31019    |
| Perplexity           | 1.3637     |
| AveragePolicyStd     | 0.25897    |
| AveragePolicyStd[0]  | 0.28574    |
| AveragePolicyStd[1]  | 0.33116    |
| AveragePolicyStd[2]  | 0.21179    |
| AveragePolicyStd[3]  | 0.27701    |
| AveragePolicyStd[4]  | 0.19397    |
| AveragePolicyStd[5]  | 0.25417    |
| AverageReturn        | 1259.1     |
| MinReturn            | 109.29     |
| MaxReturn            | 1436.5     |
| StdReturn            | 312.3      |
| AverageEpisodeLength | 914.93     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.84     |
| TotalNEpisodes       | 17104      |
| TotalNSamples        | 2.5773e+06 |
| ExplainedVariance    | 0.067444   |
-------------------------------------
[2018-12-22 10:59:39.068654 UTC] Saving snapshot
[2018-12-22 10:59:39.068919 UTC] Starting iteration 516
[2018-12-22 10:59:39.069038 UTC] Start collecting samples
[2018-12-22 10:59:42.033194 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:42.113655 UTC] Performing policy update
[2018-12-22 10:59:42.114362 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:42.205829 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:43.275742 UTC] Performing line search
[2018-12-22 10:59:43.403852 UTC] Updating baseline
[2018-12-22 10:59:44.960678 UTC] Computing logging information
-------------------------------------
| Iteration            | 516        |
| ExpectedImprovement  | 0.017236   |
| ActualImprovement    | 0.016202   |
| ImprovementRatio     | 0.94003    |
| MeanKL               | 0.0077745  |
| Entropy              | 0.30779    |
| Perplexity           | 1.3604     |
| AveragePolicyStd     | 0.25888    |
| AveragePolicyStd[0]  | 0.2859     |
| AveragePolicyStd[1]  | 0.33148    |
| AveragePolicyStd[2]  | 0.2112     |
| AveragePolicyStd[3]  | 0.27607    |
| AveragePolicyStd[4]  | 0.19422    |
| AveragePolicyStd[5]  | 0.25444    |
| AverageReturn        | 1250.4     |
| MinReturn            | 109.29     |
| MaxReturn            | 1443.1     |
| StdReturn            | 322.34     |
| AverageEpisodeLength | 908.34     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.02     |
| TotalNEpisodes       | 17109      |
| TotalNSamples        | 2.5817e+06 |
| ExplainedVariance    | 0.12658    |
-------------------------------------
[2018-12-22 10:59:45.315839 UTC] Saving snapshot
[2018-12-22 10:59:45.316083 UTC] Starting iteration 517
[2018-12-22 10:59:45.316199 UTC] Start collecting samples
[2018-12-22 10:59:48.262263 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:48.340284 UTC] Performing policy update
[2018-12-22 10:59:48.341053 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:48.431667 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:49.499472 UTC] Performing line search
[2018-12-22 10:59:49.627633 UTC] Updating baseline
[2018-12-22 10:59:51.097497 UTC] Computing logging information
-------------------------------------
| Iteration            | 517        |
| ExpectedImprovement  | 0.018145   |
| ActualImprovement    | 0.017273   |
| ImprovementRatio     | 0.95193    |
| MeanKL               | 0.0072143  |
| Entropy              | 0.3006     |
| Perplexity           | 1.3507     |
| AveragePolicyStd     | 0.25857    |
| AveragePolicyStd[0]  | 0.28623    |
| AveragePolicyStd[1]  | 0.33292    |
| AveragePolicyStd[2]  | 0.21098    |
| AveragePolicyStd[3]  | 0.27278    |
| AveragePolicyStd[4]  | 0.19496    |
| AveragePolicyStd[5]  | 0.25355    |
| AverageReturn        | 1252.5     |
| MinReturn            | 109.29     |
| MaxReturn            | 1443.1     |
| StdReturn            | 323.13     |
| AverageEpisodeLength | 908.4      |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.04     |
| TotalNEpisodes       | 17113      |
| TotalNSamples        | 2.5857e+06 |
| ExplainedVariance    | 0.0032512  |
-------------------------------------
[2018-12-22 10:59:51.450044 UTC] Saving snapshot
[2018-12-22 10:59:51.450322 UTC] Starting iteration 518
[2018-12-22 10:59:51.450447 UTC] Start collecting samples
[2018-12-22 10:59:54.447281 UTC] Computing input variables for policy optimization
[2018-12-22 10:59:54.529382 UTC] Performing policy update
[2018-12-22 10:59:54.532783 UTC] Computing gradient in Euclidean space
[2018-12-22 10:59:54.622510 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 10:59:55.681828 UTC] Performing line search
[2018-12-22 10:59:55.812969 UTC] Updating baseline
[2018-12-22 10:59:57.366415 UTC] Computing logging information
-------------------------------------
| Iteration            | 518        |
| ExpectedImprovement  | 0.0186     |
| ActualImprovement    | 0.017808   |
| ImprovementRatio     | 0.95742    |
| MeanKL               | 0.0068771  |
| Entropy              | 0.29327    |
| Perplexity           | 1.3408     |
| AveragePolicyStd     | 0.2582     |
| AveragePolicyStd[0]  | 0.28579    |
| AveragePolicyStd[1]  | 0.33165    |
| AveragePolicyStd[2]  | 0.21156    |
| AveragePolicyStd[3]  | 0.27317    |
| AveragePolicyStd[4]  | 0.19477    |
| AveragePolicyStd[5]  | 0.25224    |
| AverageReturn        | 1266.2     |
| MinReturn            | 109.29     |
| MaxReturn            | 1443.1     |
| StdReturn            | 313.88     |
| AverageEpisodeLength | 915.68     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.85     |
| TotalNEpisodes       | 17121      |
| TotalNSamples        | 2.5928e+06 |
| ExplainedVariance    | 0.14038    |
-------------------------------------
[2018-12-22 10:59:57.721426 UTC] Saving snapshot
[2018-12-22 10:59:57.721695 UTC] Starting iteration 519
[2018-12-22 10:59:57.721853 UTC] Start collecting samples
[2018-12-22 11:00:00.683485 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:00.765948 UTC] Performing policy update
[2018-12-22 11:00:00.766808 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:00.857696 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:01.945211 UTC] Performing line search
[2018-12-22 11:00:02.074451 UTC] Updating baseline
[2018-12-22 11:00:03.457880 UTC] Computing logging information
-------------------------------------
| Iteration            | 519        |
| ExpectedImprovement  | 0.017326   |
| ActualImprovement    | 0.016845   |
| ImprovementRatio     | 0.97225    |
| MeanKL               | 0.0074346  |
| Entropy              | 0.2959     |
| Perplexity           | 1.3443     |
| AveragePolicyStd     | 0.25826    |
| AveragePolicyStd[0]  | 0.28623    |
| AveragePolicyStd[1]  | 0.33152    |
| AveragePolicyStd[2]  | 0.21283    |
| AveragePolicyStd[3]  | 0.27207    |
| AveragePolicyStd[4]  | 0.19469    |
| AveragePolicyStd[5]  | 0.25223    |
| AverageReturn        | 1280.9     |
| MinReturn            | 109.29     |
| MaxReturn            | 1443.1     |
| StdReturn            | 292.31     |
| AverageEpisodeLength | 926        |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.36     |
| TotalNEpisodes       | 17125      |
| TotalNSamples        | 2.5957e+06 |
| ExplainedVariance    | 0.19921    |
-------------------------------------
[2018-12-22 11:00:03.816770 UTC] Saving snapshot
[2018-12-22 11:00:03.817022 UTC] Starting iteration 520
[2018-12-22 11:00:03.817139 UTC] Start collecting samples
[2018-12-22 11:00:06.846808 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:06.929645 UTC] Performing policy update
[2018-12-22 11:00:06.930371 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:07.021674 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:08.092601 UTC] Performing line search
[2018-12-22 11:00:08.222436 UTC] Updating baseline
[2018-12-22 11:00:09.526139 UTC] Computing logging information
-------------------------------------
| Iteration            | 520        |
| ExpectedImprovement  | 0.017716   |
| ActualImprovement    | 0.017136   |
| ImprovementRatio     | 0.96726    |
| MeanKL               | 0.0073092  |
| Entropy              | 0.29947    |
| Perplexity           | 1.3491     |
| AveragePolicyStd     | 0.25842    |
| AveragePolicyStd[0]  | 0.28567    |
| AveragePolicyStd[1]  | 0.33196    |
| AveragePolicyStd[2]  | 0.21313    |
| AveragePolicyStd[3]  | 0.27219    |
| AveragePolicyStd[4]  | 0.1946     |
| AveragePolicyStd[5]  | 0.25295    |
| AverageReturn        | 1266.3     |
| MinReturn            | 109.29     |
| MaxReturn            | 1446.8     |
| StdReturn            | 317.38     |
| AverageEpisodeLength | 913.95     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.31     |
| TotalNEpisodes       | 17133      |
| TotalNSamples        | 2.6024e+06 |
| ExplainedVariance    | 0.16646    |
-------------------------------------
[2018-12-22 11:00:09.884409 UTC] Saving snapshot
[2018-12-22 11:00:09.892688 UTC] Starting iteration 521
[2018-12-22 11:00:09.892912 UTC] Start collecting samples
[2018-12-22 11:00:12.874227 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:12.955872 UTC] Performing policy update
[2018-12-22 11:00:12.956492 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:13.047359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:14.123919 UTC] Performing line search
[2018-12-22 11:00:14.251668 UTC] Updating baseline
[2018-12-22 11:00:15.630562 UTC] Computing logging information
-------------------------------------
| Iteration            | 521        |
| ExpectedImprovement  | 0.019204   |
| ActualImprovement    | 0.020152   |
| ImprovementRatio     | 1.0494     |
| MeanKL               | 0.0069582  |
| Entropy              | 0.29023    |
| Perplexity           | 1.3367     |
| AveragePolicyStd     | 0.25797    |
| AveragePolicyStd[0]  | 0.28492    |
| AveragePolicyStd[1]  | 0.33049    |
| AveragePolicyStd[2]  | 0.2129     |
| AveragePolicyStd[3]  | 0.27184    |
| AveragePolicyStd[4]  | 0.19436    |
| AveragePolicyStd[5]  | 0.25331    |
| AverageReturn        | 1263.6     |
| MinReturn            | 109.29     |
| MaxReturn            | 1446.8     |
| StdReturn            | 329.66     |
| AverageEpisodeLength | 911.64     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 229.3      |
| TotalNEpisodes       | 17137      |
| TotalNSamples        | 2.6056e+06 |
| ExplainedVariance    | 0.13414    |
-------------------------------------
[2018-12-22 11:00:15.996414 UTC] Saving snapshot
[2018-12-22 11:00:15.996703 UTC] Starting iteration 522
[2018-12-22 11:00:15.996833 UTC] Start collecting samples
[2018-12-22 11:00:18.984360 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:19.065015 UTC] Performing policy update
[2018-12-22 11:00:19.065867 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:19.156344 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:20.241841 UTC] Performing line search
[2018-12-22 11:00:20.370341 UTC] Updating baseline
[2018-12-22 11:00:21.747977 UTC] Computing logging information
-------------------------------------
| Iteration            | 522        |
| ExpectedImprovement  | 0.015077   |
| ActualImprovement    | 0.01428    |
| ImprovementRatio     | 0.9471     |
| MeanKL               | 0.0071996  |
| Entropy              | 0.29913    |
| Perplexity           | 1.3487     |
| AveragePolicyStd     | 0.25832    |
| AveragePolicyStd[0]  | 0.28575    |
| AveragePolicyStd[1]  | 0.33102    |
| AveragePolicyStd[2]  | 0.21368    |
| AveragePolicyStd[3]  | 0.27034    |
| AveragePolicyStd[4]  | 0.1947     |
| AveragePolicyStd[5]  | 0.25446    |
| AverageReturn        | 1262.5     |
| MinReturn            | 109.29     |
| MaxReturn            | 1446.8     |
| StdReturn            | 329.31     |
| AverageEpisodeLength | 911.64     |
| MinEpisodeLength     | 98         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 229.3      |
| TotalNEpisodes       | 17142      |
| TotalNSamples        | 2.6106e+06 |
| ExplainedVariance    | -0.0007161 |
-------------------------------------
[2018-12-22 11:00:22.107020 UTC] Saving snapshot
[2018-12-22 11:00:22.107258 UTC] Starting iteration 523
[2018-12-22 11:00:22.107390 UTC] Start collecting samples
[2018-12-22 11:00:25.120736 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:25.202991 UTC] Performing policy update
[2018-12-22 11:00:25.203969 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:25.294881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:26.376054 UTC] Performing line search
[2018-12-22 11:00:26.504869 UTC] Updating baseline
[2018-12-22 11:00:27.797664 UTC] Computing logging information
-------------------------------------
| Iteration            | 523        |
| ExpectedImprovement  | 0.018231   |
| ActualImprovement    | 0.017279   |
| ImprovementRatio     | 0.94777    |
| MeanKL               | 0.0071703  |
| Entropy              | 0.29562    |
| Perplexity           | 1.344      |
| AveragePolicyStd     | 0.25811    |
| AveragePolicyStd[0]  | 0.28603    |
| AveragePolicyStd[1]  | 0.32997    |
| AveragePolicyStd[2]  | 0.21319    |
| AveragePolicyStd[3]  | 0.26977    |
| AveragePolicyStd[4]  | 0.19552    |
| AveragePolicyStd[5]  | 0.25415    |
| AverageReturn        | 1264.4     |
| MinReturn            | 176.5      |
| MaxReturn            | 1446.8     |
| StdReturn            | 320.91     |
| AverageEpisodeLength | 913.52     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.31     |
| TotalNEpisodes       | 17150      |
| TotalNSamples        | 2.6179e+06 |
| ExplainedVariance    | 0.076458   |
-------------------------------------
[2018-12-22 11:00:28.159463 UTC] Saving snapshot
[2018-12-22 11:00:28.159726 UTC] Starting iteration 524
[2018-12-22 11:00:28.159847 UTC] Start collecting samples
[2018-12-22 11:00:31.095569 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:31.175766 UTC] Performing policy update
[2018-12-22 11:00:31.176673 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:31.269073 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:32.360257 UTC] Performing line search
[2018-12-22 11:00:32.489767 UTC] Updating baseline
[2018-12-22 11:00:33.874297 UTC] Computing logging information
-------------------------------------
| Iteration            | 524        |
| ExpectedImprovement  | 0.018145   |
| ActualImprovement    | 0.017593   |
| ImprovementRatio     | 0.96957    |
| MeanKL               | 0.0072608  |
| Entropy              | 0.28669    |
| Perplexity           | 1.332      |
| AveragePolicyStd     | 0.25771    |
| AveragePolicyStd[0]  | 0.28666    |
| AveragePolicyStd[1]  | 0.32874    |
| AveragePolicyStd[2]  | 0.21347    |
| AveragePolicyStd[3]  | 0.26872    |
| AveragePolicyStd[4]  | 0.19491    |
| AveragePolicyStd[5]  | 0.25374    |
| AverageReturn        | 1263.9     |
| MinReturn            | 176.5      |
| MaxReturn            | 1446.8     |
| StdReturn            | 320.68     |
| AverageEpisodeLength | 913.52     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.31     |
| TotalNEpisodes       | 17152      |
| TotalNSamples        | 2.6199e+06 |
| ExplainedVariance    | -0.16666   |
-------------------------------------
[2018-12-22 11:00:34.234785 UTC] Saving snapshot
[2018-12-22 11:00:34.235061 UTC] Starting iteration 525
[2018-12-22 11:00:34.235190 UTC] Start collecting samples
[2018-12-22 11:00:37.236726 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:37.317480 UTC] Performing policy update
[2018-12-22 11:00:37.318249 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:37.408317 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:38.484892 UTC] Performing line search
[2018-12-22 11:00:38.613501 UTC] Updating baseline
[2018-12-22 11:00:40.090696 UTC] Computing logging information
-------------------------------------
| Iteration            | 525        |
| ExpectedImprovement  | 0.018833   |
| ActualImprovement    | 0.017302   |
| ImprovementRatio     | 0.91868    |
| MeanKL               | 0.0072272  |
| Entropy              | 0.27394    |
| Perplexity           | 1.3151     |
| AveragePolicyStd     | 0.25719    |
| AveragePolicyStd[0]  | 0.28723    |
| AveragePolicyStd[1]  | 0.32817    |
| AveragePolicyStd[2]  | 0.21258    |
| AveragePolicyStd[3]  | 0.26745    |
| AveragePolicyStd[4]  | 0.19465    |
| AveragePolicyStd[5]  | 0.25304    |
| AverageReturn        | 1280       |
| MinReturn            | 176.5      |
| MaxReturn            | 1446.8     |
| StdReturn            | 304.82     |
| AverageEpisodeLength | 923.86     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.6      |
| TotalNEpisodes       | 17158      |
| TotalNSamples        | 2.6259e+06 |
| ExplainedVariance    | -0.0026155 |
-------------------------------------
[2018-12-22 11:00:40.445456 UTC] Saving snapshot
[2018-12-22 11:00:40.445722 UTC] Starting iteration 526
[2018-12-22 11:00:40.445880 UTC] Start collecting samples
[2018-12-22 11:00:43.422094 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:43.505154 UTC] Performing policy update
[2018-12-22 11:00:43.506223 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:43.596129 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:44.658821 UTC] Performing line search
[2018-12-22 11:00:44.787200 UTC] Updating baseline
[2018-12-22 11:00:45.995373 UTC] Computing logging information
-------------------------------------
| Iteration            | 526        |
| ExpectedImprovement  | 0.018018   |
| ActualImprovement    | 0.017092   |
| ImprovementRatio     | 0.94864    |
| MeanKL               | 0.0069596  |
| Entropy              | 0.25901    |
| Perplexity           | 1.2956     |
| AveragePolicyStd     | 0.25644    |
| AveragePolicyStd[0]  | 0.2862     |
| AveragePolicyStd[1]  | 0.32605    |
| AveragePolicyStd[2]  | 0.21224    |
| AveragePolicyStd[3]  | 0.26624    |
| AveragePolicyStd[4]  | 0.19493    |
| AveragePolicyStd[5]  | 0.25301    |
| AverageReturn        | 1270.5     |
| MinReturn            | 176.5      |
| MaxReturn            | 1446.8     |
| StdReturn            | 312.36     |
| AverageEpisodeLength | 917.78     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.64     |
| TotalNEpisodes       | 17166      |
| TotalNSamples        | 2.6333e+06 |
| ExplainedVariance    | 0.16311    |
-------------------------------------
[2018-12-22 11:00:46.351657 UTC] Saving snapshot
[2018-12-22 11:00:46.351956 UTC] Starting iteration 527
[2018-12-22 11:00:46.352075 UTC] Start collecting samples
[2018-12-22 11:00:49.314063 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:49.393588 UTC] Performing policy update
[2018-12-22 11:00:49.394410 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:49.485966 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:50.570146 UTC] Performing line search
[2018-12-22 11:00:50.699716 UTC] Updating baseline
[2018-12-22 11:00:51.904325 UTC] Computing logging information
-------------------------------------
| Iteration            | 527        |
| ExpectedImprovement  | 0.018982   |
| ActualImprovement    | 0.017746   |
| ImprovementRatio     | 0.93487    |
| MeanKL               | 0.0075743  |
| Entropy              | 0.24432    |
| Perplexity           | 1.2767     |
| AveragePolicyStd     | 0.25578    |
| AveragePolicyStd[0]  | 0.28642    |
| AveragePolicyStd[1]  | 0.3258     |
| AveragePolicyStd[2]  | 0.21211    |
| AveragePolicyStd[3]  | 0.26319    |
| AveragePolicyStd[4]  | 0.19532    |
| AveragePolicyStd[5]  | 0.25185    |
| AverageReturn        | 1269       |
| MinReturn            | 176.5      |
| MaxReturn            | 1446.8     |
| StdReturn            | 311.91     |
| AverageEpisodeLength | 917.78     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.64     |
| TotalNEpisodes       | 17169      |
| TotalNSamples        | 2.6363e+06 |
| ExplainedVariance    | 0.0061999  |
-------------------------------------
[2018-12-22 11:00:52.267613 UTC] Saving snapshot
[2018-12-22 11:00:52.267864 UTC] Starting iteration 528
[2018-12-22 11:00:52.267985 UTC] Start collecting samples
[2018-12-22 11:00:55.255576 UTC] Computing input variables for policy optimization
[2018-12-22 11:00:55.337026 UTC] Performing policy update
[2018-12-22 11:00:55.337665 UTC] Computing gradient in Euclidean space
[2018-12-22 11:00:55.429291 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:00:56.498492 UTC] Performing line search
[2018-12-22 11:00:56.627014 UTC] Updating baseline
[2018-12-22 11:00:58.081196 UTC] Computing logging information
-------------------------------------
| Iteration            | 528        |
| ExpectedImprovement  | 0.018131   |
| ActualImprovement    | 0.017314   |
| ImprovementRatio     | 0.95495    |
| MeanKL               | 0.007295   |
| Entropy              | 0.23006    |
| Perplexity           | 1.2587     |
| AveragePolicyStd     | 0.2552     |
| AveragePolicyStd[0]  | 0.28641    |
| AveragePolicyStd[1]  | 0.32509    |
| AveragePolicyStd[2]  | 0.21164    |
| AveragePolicyStd[3]  | 0.26221    |
| AveragePolicyStd[4]  | 0.19469    |
| AveragePolicyStd[5]  | 0.25114    |
| AverageReturn        | 1269       |
| MinReturn            | 176.5      |
| MaxReturn            | 1446.8     |
| StdReturn            | 310.71     |
| AverageEpisodeLength | 918.05     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.79     |
| TotalNEpisodes       | 17175      |
| TotalNSamples        | 2.6415e+06 |
| ExplainedVariance    | 0.090598   |
-------------------------------------
[2018-12-22 11:00:58.440662 UTC] Saving snapshot
[2018-12-22 11:00:58.440962 UTC] Starting iteration 529
[2018-12-22 11:00:58.441081 UTC] Start collecting samples
[2018-12-22 11:01:01.415830 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:01.500404 UTC] Performing policy update
[2018-12-22 11:01:01.501087 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:01.592994 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:02.666995 UTC] Performing line search
[2018-12-22 11:01:02.794366 UTC] Updating baseline
[2018-12-22 11:01:04.604779 UTC] Computing logging information
-------------------------------------
| Iteration            | 529        |
| ExpectedImprovement  | 0.017371   |
| ActualImprovement    | 0.016477   |
| ImprovementRatio     | 0.94853    |
| MeanKL               | 0.0070216  |
| Entropy              | 0.22779    |
| Perplexity           | 1.2558     |
| AveragePolicyStd     | 0.25499    |
| AveragePolicyStd[0]  | 0.28576    |
| AveragePolicyStd[1]  | 0.32307    |
| AveragePolicyStd[2]  | 0.21202    |
| AveragePolicyStd[3]  | 0.26278    |
| AveragePolicyStd[4]  | 0.19509    |
| AveragePolicyStd[5]  | 0.25119    |
| AverageReturn        | 1274.5     |
| MinReturn            | 176.5      |
| MaxReturn            | 1451.2     |
| StdReturn            | 307.68     |
| AverageEpisodeLength | 922.09     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.51     |
| TotalNEpisodes       | 17181      |
| TotalNSamples        | 2.6475e+06 |
| ExplainedVariance    | -0.0070492 |
-------------------------------------
[2018-12-22 11:01:04.966299 UTC] Saving snapshot
[2018-12-22 11:01:04.966584 UTC] Starting iteration 530
[2018-12-22 11:01:04.966704 UTC] Start collecting samples
[2018-12-22 11:01:07.926727 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:08.010435 UTC] Performing policy update
[2018-12-22 11:01:08.011280 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:08.101920 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:09.172119 UTC] Performing line search
[2018-12-22 11:01:09.301394 UTC] Updating baseline
[2018-12-22 11:01:10.507586 UTC] Computing logging information
-------------------------------------
| Iteration            | 530        |
| ExpectedImprovement  | 0.017039   |
| ActualImprovement    | 0.016126   |
| ImprovementRatio     | 0.94638    |
| MeanKL               | 0.0074435  |
| Entropy              | 0.22167    |
| Perplexity           | 1.2482     |
| AveragePolicyStd     | 0.25475    |
| AveragePolicyStd[0]  | 0.28576    |
| AveragePolicyStd[1]  | 0.32313    |
| AveragePolicyStd[2]  | 0.21128    |
| AveragePolicyStd[3]  | 0.26211    |
| AveragePolicyStd[4]  | 0.19513    |
| AveragePolicyStd[5]  | 0.25108    |
| AverageReturn        | 1276       |
| MinReturn            | 176.5      |
| MaxReturn            | 1451.2     |
| StdReturn            | 308.25     |
| AverageEpisodeLength | 922.09     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.51     |
| TotalNEpisodes       | 17185      |
| TotalNSamples        | 2.6515e+06 |
| ExplainedVariance    | 0.0002545  |
-------------------------------------
[2018-12-22 11:01:10.867932 UTC] Saving snapshot
[2018-12-22 11:01:10.876112 UTC] Starting iteration 531
[2018-12-22 11:01:10.876355 UTC] Start collecting samples
[2018-12-22 11:01:13.841875 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:13.926314 UTC] Performing policy update
[2018-12-22 11:01:13.927261 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:14.018830 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:15.091157 UTC] Performing line search
[2018-12-22 11:01:15.218222 UTC] Updating baseline
[2018-12-22 11:01:16.746594 UTC] Computing logging information
-------------------------------------
| Iteration            | 531        |
| ExpectedImprovement  | 0.019271   |
| ActualImprovement    | 0.01799    |
| ImprovementRatio     | 0.93354    |
| MeanKL               | 0.0070608  |
| Entropy              | 0.23043    |
| Perplexity           | 1.2591     |
| AveragePolicyStd     | 0.25516    |
| AveragePolicyStd[0]  | 0.28542    |
| AveragePolicyStd[1]  | 0.32459    |
| AveragePolicyStd[2]  | 0.21134    |
| AveragePolicyStd[3]  | 0.26249    |
| AveragePolicyStd[4]  | 0.19521    |
| AveragePolicyStd[5]  | 0.25189    |
| AverageReturn        | 1289.1     |
| MinReturn            | 176.5      |
| MaxReturn            | 1452.7     |
| StdReturn            | 301.54     |
| AverageEpisodeLength | 930.71     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.75     |
| TotalNEpisodes       | 17190      |
| TotalNSamples        | 2.6565e+06 |
| ExplainedVariance    | 0.0064351  |
-------------------------------------
[2018-12-22 11:01:17.109821 UTC] Saving snapshot
[2018-12-22 11:01:17.110076 UTC] Starting iteration 532
[2018-12-22 11:01:17.110198 UTC] Start collecting samples
[2018-12-22 11:01:20.066035 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:20.146340 UTC] Performing policy update
[2018-12-22 11:01:20.147078 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:20.236081 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:21.297641 UTC] Performing line search
[2018-12-22 11:01:21.425228 UTC] Updating baseline
[2018-12-22 11:01:22.712252 UTC] Computing logging information
-------------------------------------
| Iteration            | 532        |
| ExpectedImprovement  | 0.017823   |
| ActualImprovement    | 0.016688   |
| ImprovementRatio     | 0.93633    |
| MeanKL               | 0.0069418  |
| Entropy              | 0.22461    |
| Perplexity           | 1.2518     |
| AveragePolicyStd     | 0.2549     |
| AveragePolicyStd[0]  | 0.28443    |
| AveragePolicyStd[1]  | 0.32465    |
| AveragePolicyStd[2]  | 0.21107    |
| AveragePolicyStd[3]  | 0.26291    |
| AveragePolicyStd[4]  | 0.19539    |
| AveragePolicyStd[5]  | 0.25096    |
| AverageReturn        | 1290.5     |
| MinReturn            | 176.5      |
| MaxReturn            | 1456.6     |
| StdReturn            | 302.07     |
| AverageEpisodeLength | 930.71     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.75     |
| TotalNEpisodes       | 17194      |
| TotalNSamples        | 2.6605e+06 |
| ExplainedVariance    | 0.067642   |
-------------------------------------
[2018-12-22 11:01:23.069632 UTC] Saving snapshot
[2018-12-22 11:01:23.070027 UTC] Starting iteration 533
[2018-12-22 11:01:23.070158 UTC] Start collecting samples
[2018-12-22 11:01:26.051040 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:26.131012 UTC] Performing policy update
[2018-12-22 11:01:26.131815 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:26.222168 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:27.288904 UTC] Performing line search
[2018-12-22 11:01:27.416720 UTC] Updating baseline
[2018-12-22 11:01:28.619181 UTC] Computing logging information
-------------------------------------
| Iteration            | 533        |
| ExpectedImprovement  | 0.01703    |
| ActualImprovement    | 0.016131   |
| ImprovementRatio     | 0.94717    |
| MeanKL               | 0.0072307  |
| Entropy              | 0.20155    |
| Perplexity           | 1.2233     |
| AveragePolicyStd     | 0.25388    |
| AveragePolicyStd[0]  | 0.28241    |
| AveragePolicyStd[1]  | 0.32429    |
| AveragePolicyStd[2]  | 0.21105    |
| AveragePolicyStd[3]  | 0.26107    |
| AveragePolicyStd[4]  | 0.19516    |
| AveragePolicyStd[5]  | 0.24934    |
| AverageReturn        | 1290.8     |
| MinReturn            | 176.5      |
| MaxReturn            | 1456.6     |
| StdReturn            | 302.16     |
| AverageEpisodeLength | 930.71     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.75     |
| TotalNEpisodes       | 17200      |
| TotalNSamples        | 2.6665e+06 |
| ExplainedVariance    | -0.058959  |
-------------------------------------
[2018-12-22 11:01:28.976581 UTC] Saving snapshot
[2018-12-22 11:01:28.976853 UTC] Starting iteration 534
[2018-12-22 11:01:28.976974 UTC] Start collecting samples
[2018-12-22 11:01:31.966375 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:32.046115 UTC] Performing policy update
[2018-12-22 11:01:32.046883 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:32.136630 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:33.192857 UTC] Performing line search
[2018-12-22 11:01:33.321035 UTC] Updating baseline
[2018-12-22 11:01:35.126145 UTC] Computing logging information
-------------------------------------
| Iteration            | 534        |
| ExpectedImprovement  | 0.017766   |
| ActualImprovement    | 0.016649   |
| ImprovementRatio     | 0.93711    |
| MeanKL               | 0.0068672  |
| Entropy              | 0.1858     |
| Perplexity           | 1.2042     |
| AveragePolicyStd     | 0.25326    |
| AveragePolicyStd[0]  | 0.28145    |
| AveragePolicyStd[1]  | 0.32381    |
| AveragePolicyStd[2]  | 0.20948    |
| AveragePolicyStd[3]  | 0.26077    |
| AveragePolicyStd[4]  | 0.19467    |
| AveragePolicyStd[5]  | 0.2494     |
| AverageReturn        | 1303.2     |
| MinReturn            | 176.5      |
| MaxReturn            | 1456.6     |
| StdReturn            | 290.53     |
| AverageEpisodeLength | 938.59     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.29     |
| TotalNEpisodes       | 17206      |
| TotalNSamples        | 2.6725e+06 |
| ExplainedVariance    | -0.0019457 |
-------------------------------------
[2018-12-22 11:01:35.484955 UTC] Saving snapshot
[2018-12-22 11:01:35.485210 UTC] Starting iteration 535
[2018-12-22 11:01:35.485329 UTC] Start collecting samples
[2018-12-22 11:01:38.418859 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:38.497362 UTC] Performing policy update
[2018-12-22 11:01:38.498111 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:38.589963 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:39.658899 UTC] Performing line search
[2018-12-22 11:01:39.786894 UTC] Updating baseline
[2018-12-22 11:01:41.425496 UTC] Computing logging information
-------------------------------------
| Iteration            | 535        |
| ExpectedImprovement  | 0.016644   |
| ActualImprovement    | 0.015757   |
| ImprovementRatio     | 0.94671    |
| MeanKL               | 0.0073478  |
| Entropy              | 0.17541    |
| Perplexity           | 1.1917     |
| AveragePolicyStd     | 0.2528     |
| AveragePolicyStd[0]  | 0.28178    |
| AveragePolicyStd[1]  | 0.32229    |
| AveragePolicyStd[2]  | 0.20982    |
| AveragePolicyStd[3]  | 0.26053    |
| AveragePolicyStd[4]  | 0.19402    |
| AveragePolicyStd[5]  | 0.24834    |
| AverageReturn        | 1302.6     |
| MinReturn            | 176.5      |
| MaxReturn            | 1456.6     |
| StdReturn            | 290.32     |
| AverageEpisodeLength | 938.59     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.29     |
| TotalNEpisodes       | 17207      |
| TotalNSamples        | 2.6735e+06 |
| ExplainedVariance    | 0.0033858  |
-------------------------------------
[2018-12-22 11:01:41.788959 UTC] Saving snapshot
[2018-12-22 11:01:41.789203 UTC] Starting iteration 536
[2018-12-22 11:01:41.789325 UTC] Start collecting samples
[2018-12-22 11:01:44.867258 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:44.956003 UTC] Performing policy update
[2018-12-22 11:01:44.956695 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:45.054027 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:46.187196 UTC] Performing line search
[2018-12-22 11:01:46.320842 UTC] Updating baseline
[2018-12-22 11:01:47.743145 UTC] Computing logging information
-------------------------------------
| Iteration            | 536        |
| ExpectedImprovement  | 0.016878   |
| ActualImprovement    | 0.016099   |
| ImprovementRatio     | 0.95384    |
| MeanKL               | 0.0074039  |
| Entropy              | 0.16582    |
| Perplexity           | 1.1804     |
| AveragePolicyStd     | 0.25239    |
| AveragePolicyStd[0]  | 0.28187    |
| AveragePolicyStd[1]  | 0.32095    |
| AveragePolicyStd[2]  | 0.20917    |
| AveragePolicyStd[3]  | 0.26051    |
| AveragePolicyStd[4]  | 0.19351    |
| AveragePolicyStd[5]  | 0.24836    |
| AverageReturn        | 1304.6     |
| MinReturn            | 176.5      |
| MaxReturn            | 1456.6     |
| StdReturn            | 291.11     |
| AverageEpisodeLength | 938.59     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.29     |
| TotalNEpisodes       | 17216      |
| TotalNSamples        | 2.6825e+06 |
| ExplainedVariance    | -0.0091676 |
-------------------------------------
[2018-12-22 11:01:48.133262 UTC] Saving snapshot
[2018-12-22 11:01:48.133527 UTC] Starting iteration 537
[2018-12-22 11:01:48.133719 UTC] Start collecting samples
[2018-12-22 11:01:51.302675 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:51.384998 UTC] Performing policy update
[2018-12-22 11:01:51.385627 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:51.480226 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:52.601302 UTC] Performing line search
[2018-12-22 11:01:52.734728 UTC] Updating baseline
[2018-12-22 11:01:55.061041 UTC] Computing logging information
-------------------------------------
| Iteration            | 537        |
| ExpectedImprovement  | 0.020293   |
| ActualImprovement    | 0.018216   |
| ImprovementRatio     | 0.89764    |
| MeanKL               | 0.0069591  |
| Entropy              | 0.1648     |
| Perplexity           | 1.1792     |
| AveragePolicyStd     | 0.25226    |
| AveragePolicyStd[0]  | 0.28077    |
| AveragePolicyStd[1]  | 0.3198     |
| AveragePolicyStd[2]  | 0.20942    |
| AveragePolicyStd[3]  | 0.26118    |
| AveragePolicyStd[4]  | 0.19408    |
| AveragePolicyStd[5]  | 0.24831    |
| AverageReturn        | 1310.4     |
| MinReturn            | 176.5      |
| MaxReturn            | 1456.6     |
| StdReturn            | 275.91     |
| AverageEpisodeLength | 942.28     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.36     |
| TotalNEpisodes       | 17220      |
| TotalNSamples        | 2.6861e+06 |
| ExplainedVariance    | 0.080829   |
-------------------------------------
[2018-12-22 11:01:55.420249 UTC] Saving snapshot
[2018-12-22 11:01:55.420502 UTC] Starting iteration 538
[2018-12-22 11:01:55.420652 UTC] Start collecting samples
[2018-12-22 11:01:58.370499 UTC] Computing input variables for policy optimization
[2018-12-22 11:01:58.450457 UTC] Performing policy update
[2018-12-22 11:01:58.451244 UTC] Computing gradient in Euclidean space
[2018-12-22 11:01:58.542153 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:01:59.618071 UTC] Performing line search
[2018-12-22 11:01:59.746899 UTC] Updating baseline
[2018-12-22 11:02:01.118293 UTC] Computing logging information
-------------------------------------
| Iteration            | 538        |
| ExpectedImprovement  | 0.014823   |
| ActualImprovement    | 0.01439    |
| ImprovementRatio     | 0.97076    |
| MeanKL               | 0.0076773  |
| Entropy              | 0.15401    |
| Perplexity           | 1.1665     |
| AveragePolicyStd     | 0.25179    |
| AveragePolicyStd[0]  | 0.28004    |
| AveragePolicyStd[1]  | 0.31966    |
| AveragePolicyStd[2]  | 0.20859    |
| AveragePolicyStd[3]  | 0.26064    |
| AveragePolicyStd[4]  | 0.19465    |
| AveragePolicyStd[5]  | 0.24715    |
| AverageReturn        | 1328.9     |
| MinReturn            | 187.14     |
| MaxReturn            | 1456.6     |
| StdReturn            | 249.05     |
| AverageEpisodeLength | 953.96     |
| MinEpisodeLength     | 153        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.47     |
| TotalNEpisodes       | 17224      |
| TotalNSamples        | 2.6901e+06 |
| ExplainedVariance    | -0.0068178 |
-------------------------------------
[2018-12-22 11:02:01.478156 UTC] Saving snapshot
[2018-12-22 11:02:01.478402 UTC] Starting iteration 539
[2018-12-22 11:02:01.478522 UTC] Start collecting samples
[2018-12-22 11:02:04.501986 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:04.583505 UTC] Performing policy update
[2018-12-22 11:02:04.584251 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:04.674811 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:05.741273 UTC] Performing line search
[2018-12-22 11:02:05.869216 UTC] Updating baseline
[2018-12-22 11:02:07.624077 UTC] Computing logging information
-------------------------------------
| Iteration            | 539        |
| ExpectedImprovement  | 0.017291   |
| ActualImprovement    | 0.016593   |
| ImprovementRatio     | 0.95962    |
| MeanKL               | 0.0073394  |
| Entropy              | 0.15991    |
| Perplexity           | 1.1734     |
| AveragePolicyStd     | 0.25201    |
| AveragePolicyStd[0]  | 0.28103    |
| AveragePolicyStd[1]  | 0.31892    |
| AveragePolicyStd[2]  | 0.20894    |
| AveragePolicyStd[3]  | 0.26154    |
| AveragePolicyStd[4]  | 0.1948     |
| AveragePolicyStd[5]  | 0.24685    |
| AverageReturn        | 1349.4     |
| MinReturn            | 265.69     |
| MaxReturn            | 1456.6     |
| StdReturn            | 213.1      |
| AverageEpisodeLength | 966.97     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.41     |
| TotalNEpisodes       | 17232      |
| TotalNSamples        | 2.6981e+06 |
| ExplainedVariance    | 0.010219   |
-------------------------------------
[2018-12-22 11:02:07.989418 UTC] Saving snapshot
[2018-12-22 11:02:07.989726 UTC] Starting iteration 540
[2018-12-22 11:02:07.989893 UTC] Start collecting samples
[2018-12-22 11:02:10.924570 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:11.003794 UTC] Performing policy update
[2018-12-22 11:02:11.004393 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:11.095001 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:12.174415 UTC] Performing line search
[2018-12-22 11:02:12.302427 UTC] Updating baseline
[2018-12-22 11:02:13.679436 UTC] Computing logging information
-------------------------------------
| Iteration            | 540        |
| ExpectedImprovement  | 0.01855    |
| ActualImprovement    | 0.01713    |
| ImprovementRatio     | 0.92345    |
| MeanKL               | 0.0070239  |
| Entropy              | 0.15691    |
| Perplexity           | 1.1699     |
| AveragePolicyStd     | 0.2519     |
| AveragePolicyStd[0]  | 0.28127    |
| AveragePolicyStd[1]  | 0.31751    |
| AveragePolicyStd[2]  | 0.20895    |
| AveragePolicyStd[3]  | 0.2622     |
| AveragePolicyStd[4]  | 0.19367    |
| AveragePolicyStd[5]  | 0.2478     |
| AverageReturn        | 1341       |
| MinReturn            | 305.63     |
| MaxReturn            | 1456.6     |
| StdReturn            | 229.38     |
| AverageEpisodeLength | 960.44     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.87     |
| TotalNEpisodes       | 17235      |
| TotalNSamples        | 2.6996e+06 |
| ExplainedVariance    | 0.39134    |
-------------------------------------
[2018-12-22 11:02:14.040948 UTC] Saving snapshot
[2018-12-22 11:02:14.049156 UTC] Starting iteration 541
[2018-12-22 11:02:14.049377 UTC] Start collecting samples
[2018-12-22 11:02:17.042128 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:17.124174 UTC] Performing policy update
[2018-12-22 11:02:17.124779 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:17.216269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:18.287255 UTC] Performing line search
[2018-12-22 11:02:18.415308 UTC] Updating baseline
[2018-12-22 11:02:19.879995 UTC] Computing logging information
-------------------------------------
| Iteration            | 541        |
| ExpectedImprovement  | 0.016315   |
| ActualImprovement    | 0.015612   |
| ImprovementRatio     | 0.95692    |
| MeanKL               | 0.0070761  |
| Entropy              | 0.14506    |
| Perplexity           | 1.1561     |
| AveragePolicyStd     | 0.25141    |
| AveragePolicyStd[0]  | 0.28017    |
| AveragePolicyStd[1]  | 0.31734    |
| AveragePolicyStd[2]  | 0.20915    |
| AveragePolicyStd[3]  | 0.26161    |
| AveragePolicyStd[4]  | 0.19278    |
| AveragePolicyStd[5]  | 0.24743    |
| AverageReturn        | 1331.8     |
| MinReturn            | 140.59     |
| MaxReturn            | 1457.4     |
| StdReturn            | 259.35     |
| AverageEpisodeLength | 951.59     |
| MinEpisodeLength     | 115        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.93     |
| TotalNEpisodes       | 17243      |
| TotalNSamples        | 2.7068e+06 |
| ExplainedVariance    | 0.096566   |
-------------------------------------
[2018-12-22 11:02:20.241458 UTC] Saving snapshot
[2018-12-22 11:02:20.241721 UTC] Starting iteration 542
[2018-12-22 11:02:20.241872 UTC] Start collecting samples
[2018-12-22 11:02:23.240872 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:23.322021 UTC] Performing policy update
[2018-12-22 11:02:23.322612 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:23.413172 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:24.484159 UTC] Performing line search
[2018-12-22 11:02:24.612850 UTC] Updating baseline
[2018-12-22 11:02:26.079473 UTC] Computing logging information
-------------------------------------
| Iteration            | 542        |
| ExpectedImprovement  | 0.016888   |
| ActualImprovement    | 0.015735   |
| ImprovementRatio     | 0.93169    |
| MeanKL               | 0.0069975  |
| Entropy              | 0.1454     |
| Perplexity           | 1.1565     |
| AveragePolicyStd     | 0.2514     |
| AveragePolicyStd[0]  | 0.28031    |
| AveragePolicyStd[1]  | 0.31643    |
| AveragePolicyStd[2]  | 0.20938    |
| AveragePolicyStd[3]  | 0.26189    |
| AveragePolicyStd[4]  | 0.19254    |
| AveragePolicyStd[5]  | 0.24789    |
| AverageReturn        | 1315.2     |
| MinReturn            | 60.086     |
| MaxReturn            | 1466.3     |
| StdReturn            | 292.86     |
| AverageEpisodeLength | 938.07     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.63     |
| TotalNEpisodes       | 17252      |
| TotalNSamples        | 2.7137e+06 |
| ExplainedVariance    | 0.21542    |
-------------------------------------
[2018-12-22 11:02:26.440342 UTC] Saving snapshot
[2018-12-22 11:02:26.440602 UTC] Starting iteration 543
[2018-12-22 11:02:26.440723 UTC] Start collecting samples
[2018-12-22 11:02:29.389366 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:29.470865 UTC] Performing policy update
[2018-12-22 11:02:29.471775 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:29.562441 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:30.645631 UTC] Performing line search
[2018-12-22 11:02:30.773559 UTC] Updating baseline
[2018-12-22 11:02:32.076120 UTC] Computing logging information
-------------------------------------
| Iteration            | 543        |
| ExpectedImprovement  | 0.017561   |
| ActualImprovement    | 0.017097   |
| ImprovementRatio     | 0.97357    |
| MeanKL               | 0.0071446  |
| Entropy              | 0.14126    |
| Perplexity           | 1.1517     |
| AveragePolicyStd     | 0.25126    |
| AveragePolicyStd[0]  | 0.28105    |
| AveragePolicyStd[1]  | 0.31593    |
| AveragePolicyStd[2]  | 0.20896    |
| AveragePolicyStd[3]  | 0.26192    |
| AveragePolicyStd[4]  | 0.19221    |
| AveragePolicyStd[5]  | 0.24748    |
| AverageReturn        | 1303.3     |
| MinReturn            | 60.086     |
| MaxReturn            | 1466.3     |
| StdReturn            | 305.45     |
| AverageEpisodeLength | 928.69     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.54     |
| TotalNEpisodes       | 17256      |
| TotalNSamples        | 2.7168e+06 |
| ExplainedVariance    | 0.29054    |
-------------------------------------
[2018-12-22 11:02:32.447131 UTC] Saving snapshot
[2018-12-22 11:02:32.447410 UTC] Starting iteration 544
[2018-12-22 11:02:32.447566 UTC] Start collecting samples
[2018-12-22 11:02:35.666668 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:35.756006 UTC] Performing policy update
[2018-12-22 11:02:35.756697 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:35.853248 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:36.977403 UTC] Performing line search
[2018-12-22 11:02:37.112085 UTC] Updating baseline
[2018-12-22 11:02:38.616284 UTC] Computing logging information
------------------------------------
| Iteration            | 544       |
| ExpectedImprovement  | 0.017424  |
| ActualImprovement    | 0.017061  |
| ImprovementRatio     | 0.97921   |
| MeanKL               | 0.007572  |
| Entropy              | 0.13528   |
| Perplexity           | 1.1449    |
| AveragePolicyStd     | 0.251     |
| AveragePolicyStd[0]  | 0.28088   |
| AveragePolicyStd[1]  | 0.31478   |
| AveragePolicyStd[2]  | 0.20978   |
| AveragePolicyStd[3]  | 0.26134   |
| AveragePolicyStd[4]  | 0.19093   |
| AveragePolicyStd[5]  | 0.24829   |
| AverageReturn        | 1282.4    |
| MinReturn            | 60.086    |
| MaxReturn            | 1493.2    |
| StdReturn            | 346.65    |
| AverageEpisodeLength | 911.23    |
| MinEpisodeLength     | 63        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 238.3     |
| TotalNEpisodes       | 17263     |
| TotalNSamples        | 2.722e+06 |
| ExplainedVariance    | 0.19258   |
------------------------------------
[2018-12-22 11:02:38.972834 UTC] Saving snapshot
[2018-12-22 11:02:38.973106 UTC] Starting iteration 545
[2018-12-22 11:02:38.973226 UTC] Start collecting samples
[2018-12-22 11:02:41.914183 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:41.995154 UTC] Performing policy update
[2018-12-22 11:02:41.996087 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:42.084420 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:43.141161 UTC] Performing line search
[2018-12-22 11:02:43.267455 UTC] Updating baseline
[2018-12-22 11:02:44.975682 UTC] Computing logging information
------------------------------------
| Iteration            | 545       |
| ExpectedImprovement  | 0.019354  |
| ActualImprovement    | 0.017832  |
| ImprovementRatio     | 0.92136   |
| MeanKL               | 0.0069868 |
| Entropy              | 0.13439   |
| Perplexity           | 1.1438    |
| AveragePolicyStd     | 0.25096   |
| AveragePolicyStd[0]  | 0.28021   |
| AveragePolicyStd[1]  | 0.31492   |
| AveragePolicyStd[2]  | 0.20962   |
| AveragePolicyStd[3]  | 0.26155   |
| AveragePolicyStd[4]  | 0.19099   |
| AveragePolicyStd[5]  | 0.24848   |
| AverageReturn        | 1294.4    |
| MinReturn            | 60.086    |
| MaxReturn            | 1493.2    |
| StdReturn            | 340.26    |
| AverageEpisodeLength | 917.31    |
| MinEpisodeLength     | 63        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 233.88    |
| TotalNEpisodes       | 17267     |
| TotalNSamples        | 2.726e+06 |
| ExplainedVariance    | -0.13114  |
------------------------------------
[2018-12-22 11:02:45.333504 UTC] Saving snapshot
[2018-12-22 11:02:45.333788 UTC] Starting iteration 546
[2018-12-22 11:02:45.333931 UTC] Start collecting samples
[2018-12-22 11:02:48.294918 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:48.374661 UTC] Performing policy update
[2018-12-22 11:02:48.375324 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:48.466114 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:49.525808 UTC] Performing line search
[2018-12-22 11:02:49.651900 UTC] Updating baseline
[2018-12-22 11:02:51.359131 UTC] Computing logging information
-------------------------------------
| Iteration            | 546        |
| ExpectedImprovement  | 0.018387   |
| ActualImprovement    | 0.017619   |
| ImprovementRatio     | 0.95821    |
| MeanKL               | 0.0073504  |
| Entropy              | 0.11813    |
| Perplexity           | 1.1254     |
| AveragePolicyStd     | 0.2503     |
| AveragePolicyStd[0]  | 0.2797     |
| AveragePolicyStd[1]  | 0.31462    |
| AveragePolicyStd[2]  | 0.20906    |
| AveragePolicyStd[3]  | 0.25992    |
| AveragePolicyStd[4]  | 0.19039    |
| AveragePolicyStd[5]  | 0.24811    |
| AverageReturn        | 1308.3     |
| MinReturn            | 60.086     |
| MaxReturn            | 1493.2     |
| StdReturn            | 326.58     |
| AverageEpisodeLength | 923.64     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.15     |
| TotalNEpisodes       | 17274      |
| TotalNSamples        | 2.7329e+06 |
| ExplainedVariance    | 0.07097    |
-------------------------------------
[2018-12-22 11:02:51.711523 UTC] Saving snapshot
[2018-12-22 11:02:51.711790 UTC] Starting iteration 547
[2018-12-22 11:02:51.711929 UTC] Start collecting samples
[2018-12-22 11:02:54.628688 UTC] Computing input variables for policy optimization
[2018-12-22 11:02:54.706866 UTC] Performing policy update
[2018-12-22 11:02:54.707602 UTC] Computing gradient in Euclidean space
[2018-12-22 11:02:54.799604 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:02:55.867607 UTC] Performing line search
[2018-12-22 11:02:55.999447 UTC] Updating baseline
[2018-12-22 11:02:57.893317 UTC] Computing logging information
-------------------------------------
| Iteration            | 547        |
| ExpectedImprovement  | 0.016355   |
| ActualImprovement    | 0.015256   |
| ImprovementRatio     | 0.93278    |
| MeanKL               | 0.0069393  |
| Entropy              | 0.11634    |
| Perplexity           | 1.1234     |
| AveragePolicyStd     | 0.25022    |
| AveragePolicyStd[0]  | 0.27895    |
| AveragePolicyStd[1]  | 0.31435    |
| AveragePolicyStd[2]  | 0.20917    |
| AveragePolicyStd[3]  | 0.26019    |
| AveragePolicyStd[4]  | 0.18995    |
| AveragePolicyStd[5]  | 0.24872    |
| AverageReturn        | 1309.7     |
| MinReturn            | 60.086     |
| MaxReturn            | 1493.2     |
| StdReturn            | 326.97     |
| AverageEpisodeLength | 923.64     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.15     |
| TotalNEpisodes       | 17276      |
| TotalNSamples        | 2.7349e+06 |
| ExplainedVariance    | -0.064561  |
-------------------------------------
[2018-12-22 11:02:58.253584 UTC] Saving snapshot
[2018-12-22 11:02:58.253878 UTC] Starting iteration 548
[2018-12-22 11:02:58.254004 UTC] Start collecting samples
[2018-12-22 11:03:01.211362 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:01.291339 UTC] Performing policy update
[2018-12-22 11:03:01.291944 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:01.382095 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:02.445023 UTC] Performing line search
[2018-12-22 11:03:02.571300 UTC] Updating baseline
[2018-12-22 11:03:03.852140 UTC] Computing logging information
-------------------------------------
| Iteration            | 548        |
| ExpectedImprovement  | 0.020203   |
| ActualImprovement    | 0.018849   |
| ImprovementRatio     | 0.93299    |
| MeanKL               | 0.0071234  |
| Entropy              | 0.11024    |
| Perplexity           | 1.1165     |
| AveragePolicyStd     | 0.24989    |
| AveragePolicyStd[0]  | 0.27842    |
| AveragePolicyStd[1]  | 0.31226    |
| AveragePolicyStd[2]  | 0.20903    |
| AveragePolicyStd[3]  | 0.26057    |
| AveragePolicyStd[4]  | 0.19014    |
| AveragePolicyStd[5]  | 0.2489     |
| AverageReturn        | 1312.7     |
| MinReturn            | 60.086     |
| MaxReturn            | 1493.2     |
| StdReturn            | 327.95     |
| AverageEpisodeLength | 923.64     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.15     |
| TotalNEpisodes       | 17281      |
| TotalNSamples        | 2.7399e+06 |
| ExplainedVariance    | 0.18812    |
-------------------------------------
[2018-12-22 11:03:04.212415 UTC] Saving snapshot
[2018-12-22 11:03:04.212681 UTC] Starting iteration 549
[2018-12-22 11:03:04.212810 UTC] Start collecting samples
[2018-12-22 11:03:07.221113 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:07.302698 UTC] Performing policy update
[2018-12-22 11:03:07.303446 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:07.392586 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:08.468838 UTC] Performing line search
[2018-12-22 11:03:08.597372 UTC] Updating baseline
[2018-12-22 11:03:09.885231 UTC] Computing logging information
-------------------------------------
| Iteration            | 549        |
| ExpectedImprovement  | 0.017587   |
| ActualImprovement    | 0.018366   |
| ImprovementRatio     | 1.0443     |
| MeanKL               | 0.0069637  |
| Entropy              | 0.10752    |
| Perplexity           | 1.1135     |
| AveragePolicyStd     | 0.24976    |
| AveragePolicyStd[0]  | 0.27779    |
| AveragePolicyStd[1]  | 0.31233    |
| AveragePolicyStd[2]  | 0.20858    |
| AveragePolicyStd[3]  | 0.26069    |
| AveragePolicyStd[4]  | 0.19051    |
| AveragePolicyStd[5]  | 0.24866    |
| AverageReturn        | 1289.2     |
| MinReturn            | 60.086     |
| MaxReturn            | 1493.2     |
| StdReturn            | 367        |
| AverageEpisodeLength | 906.52     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.99     |
| TotalNEpisodes       | 17290      |
| TotalNSamples        | 2.7472e+06 |
| ExplainedVariance    | 0.14571    |
-------------------------------------
[2018-12-22 11:03:10.250914 UTC] Saving snapshot
[2018-12-22 11:03:10.251158 UTC] Starting iteration 550
[2018-12-22 11:03:10.251274 UTC] Start collecting samples
[2018-12-22 11:03:13.204711 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:13.284897 UTC] Performing policy update
[2018-12-22 11:03:13.285611 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:13.375825 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:14.466076 UTC] Performing line search
[2018-12-22 11:03:14.594857 UTC] Updating baseline
[2018-12-22 11:03:15.887587 UTC] Computing logging information
-------------------------------------
| Iteration            | 550        |
| ExpectedImprovement  | 0.022162   |
| ActualImprovement    | 0.020816   |
| ImprovementRatio     | 0.9393     |
| MeanKL               | 0.0068879  |
| Entropy              | 0.096318   |
| Perplexity           | 1.1011     |
| AveragePolicyStd     | 0.24933    |
| AveragePolicyStd[0]  | 0.27706    |
| AveragePolicyStd[1]  | 0.31234    |
| AveragePolicyStd[2]  | 0.20801    |
| AveragePolicyStd[3]  | 0.26106    |
| AveragePolicyStd[4]  | 0.18999    |
| AveragePolicyStd[5]  | 0.24755    |
| AverageReturn        | 1289.7     |
| MinReturn            | 60.086     |
| MaxReturn            | 1493.2     |
| StdReturn            | 367.22     |
| AverageEpisodeLength | 906.52     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.99     |
| TotalNEpisodes       | 17294      |
| TotalNSamples        | 2.7512e+06 |
| ExplainedVariance    | -0.033903  |
-------------------------------------
[2018-12-22 11:03:16.255162 UTC] Saving snapshot
[2018-12-22 11:03:16.263237 UTC] Starting iteration 551
[2018-12-22 11:03:16.263442 UTC] Start collecting samples
[2018-12-22 11:03:19.196798 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:19.275084 UTC] Performing policy update
[2018-12-22 11:03:19.275863 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:19.367455 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:20.438879 UTC] Performing line search
[2018-12-22 11:03:20.567459 UTC] Updating baseline
[2018-12-22 11:03:21.927985 UTC] Computing logging information
-------------------------------------
| Iteration            | 551        |
| ExpectedImprovement  | 0.017189   |
| ActualImprovement    | 0.016117   |
| ImprovementRatio     | 0.93762    |
| MeanKL               | 0.0070061  |
| Entropy              | 0.084805   |
| Perplexity           | 1.0885     |
| AveragePolicyStd     | 0.24882    |
| AveragePolicyStd[0]  | 0.27673    |
| AveragePolicyStd[1]  | 0.31067    |
| AveragePolicyStd[2]  | 0.2072     |
| AveragePolicyStd[3]  | 0.26099    |
| AveragePolicyStd[4]  | 0.19008    |
| AveragePolicyStd[5]  | 0.24723    |
| AverageReturn        | 1290.8     |
| MinReturn            | 60.086     |
| MaxReturn            | 1493.2     |
| StdReturn            | 367.63     |
| AverageEpisodeLength | 906.52     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.99     |
| TotalNEpisodes       | 17297      |
| TotalNSamples        | 2.7542e+06 |
| ExplainedVariance    | 0.016775   |
-------------------------------------
[2018-12-22 11:03:22.285217 UTC] Saving snapshot
[2018-12-22 11:03:22.285463 UTC] Starting iteration 552
[2018-12-22 11:03:22.285603 UTC] Start collecting samples
[2018-12-22 11:03:25.293441 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:25.374906 UTC] Performing policy update
[2018-12-22 11:03:25.375497 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:25.466076 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:26.537228 UTC] Performing line search
[2018-12-22 11:03:26.664762 UTC] Updating baseline
[2018-12-22 11:03:28.209157 UTC] Computing logging information
-------------------------------------
| Iteration            | 552        |
| ExpectedImprovement  | 0.016238   |
| ActualImprovement    | 0.015757   |
| ImprovementRatio     | 0.97039    |
| MeanKL               | 0.0075548  |
| Entropy              | 0.080042   |
| Perplexity           | 1.0833     |
| AveragePolicyStd     | 0.24856    |
| AveragePolicyStd[0]  | 0.27702    |
| AveragePolicyStd[1]  | 0.30879    |
| AveragePolicyStd[2]  | 0.20746    |
| AveragePolicyStd[3]  | 0.26093    |
| AveragePolicyStd[4]  | 0.18987    |
| AveragePolicyStd[5]  | 0.24732    |
| AverageReturn        | 1294.2     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 368.79     |
| AverageEpisodeLength | 906.29     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.91     |
| TotalNEpisodes       | 17306      |
| TotalNSamples        | 2.7632e+06 |
| ExplainedVariance    | 0.067561   |
-------------------------------------
[2018-12-22 11:03:28.567443 UTC] Saving snapshot
[2018-12-22 11:03:28.567720 UTC] Starting iteration 553
[2018-12-22 11:03:28.567854 UTC] Start collecting samples
[2018-12-22 11:03:31.511762 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:31.591419 UTC] Performing policy update
[2018-12-22 11:03:31.592260 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:31.682472 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:32.749192 UTC] Performing line search
[2018-12-22 11:03:32.877393 UTC] Updating baseline
[2018-12-22 11:03:34.243439 UTC] Computing logging information
-------------------------------------
| Iteration            | 553        |
| ExpectedImprovement  | 0.018437   |
| ActualImprovement    | 0.017426   |
| ImprovementRatio     | 0.94517    |
| MeanKL               | 0.0067663  |
| Entropy              | 0.07524    |
| Perplexity           | 1.0781     |
| AveragePolicyStd     | 0.24838    |
| AveragePolicyStd[0]  | 0.27767    |
| AveragePolicyStd[1]  | 0.30838    |
| AveragePolicyStd[2]  | 0.20727    |
| AveragePolicyStd[3]  | 0.26053    |
| AveragePolicyStd[4]  | 0.1897     |
| AveragePolicyStd[5]  | 0.2467     |
| AverageReturn        | 1282.2     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 387.14     |
| AverageEpisodeLength | 897.28     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 261.35     |
| TotalNEpisodes       | 17311      |
| TotalNSamples        | 2.7673e+06 |
| ExplainedVariance    | 0.10237    |
-------------------------------------
[2018-12-22 11:03:34.604197 UTC] Saving snapshot
[2018-12-22 11:03:34.604446 UTC] Starting iteration 554
[2018-12-22 11:03:34.604584 UTC] Start collecting samples
[2018-12-22 11:03:37.534230 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:37.612946 UTC] Performing policy update
[2018-12-22 11:03:37.613600 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:37.704088 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:38.771212 UTC] Performing line search
[2018-12-22 11:03:38.898965 UTC] Updating baseline
[2018-12-22 11:03:40.174446 UTC] Computing logging information
-------------------------------------
| Iteration            | 554        |
| ExpectedImprovement  | 0.015818   |
| ActualImprovement    | 0.014495   |
| ImprovementRatio     | 0.91638    |
| MeanKL               | 0.0073319  |
| Entropy              | 0.066972   |
| Perplexity           | 1.0693     |
| AveragePolicyStd     | 0.24805    |
| AveragePolicyStd[0]  | 0.27616    |
| AveragePolicyStd[1]  | 0.30855    |
| AveragePolicyStd[2]  | 0.2073     |
| AveragePolicyStd[3]  | 0.26129    |
| AveragePolicyStd[4]  | 0.18913    |
| AveragePolicyStd[5]  | 0.24587    |
| AverageReturn        | 1281.3     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 386.85     |
| AverageEpisodeLength | 897.28     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 261.35     |
| TotalNEpisodes       | 17314      |
| TotalNSamples        | 2.7703e+06 |
| ExplainedVariance    | 0.014878   |
-------------------------------------
[2018-12-22 11:03:40.535976 UTC] Saving snapshot
[2018-12-22 11:03:40.536249 UTC] Starting iteration 555
[2018-12-22 11:03:40.536368 UTC] Start collecting samples
[2018-12-22 11:03:43.507152 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:43.589091 UTC] Performing policy update
[2018-12-22 11:03:43.589871 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:43.680769 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:44.744879 UTC] Performing line search
[2018-12-22 11:03:44.872185 UTC] Updating baseline
[2018-12-22 11:03:46.151432 UTC] Computing logging information
-------------------------------------
| Iteration            | 555        |
| ExpectedImprovement  | 0.018269   |
| ActualImprovement    | 0.017467   |
| ImprovementRatio     | 0.95606    |
| MeanKL               | 0.0075339  |
| Entropy              | 0.063061   |
| Perplexity           | 1.0651     |
| AveragePolicyStd     | 0.2479     |
| AveragePolicyStd[0]  | 0.27527    |
| AveragePolicyStd[1]  | 0.30826    |
| AveragePolicyStd[2]  | 0.20719    |
| AveragePolicyStd[3]  | 0.26197    |
| AveragePolicyStd[4]  | 0.18859    |
| AveragePolicyStd[5]  | 0.24611    |
| AverageReturn        | 1287.8     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 383.22     |
| AverageEpisodeLength | 901.83     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 259.13     |
| TotalNEpisodes       | 17321      |
| TotalNSamples        | 2.7773e+06 |
| ExplainedVariance    | -0.005858  |
-------------------------------------
[2018-12-22 11:03:46.503570 UTC] Saving snapshot
[2018-12-22 11:03:46.503826 UTC] Starting iteration 556
[2018-12-22 11:03:46.503944 UTC] Start collecting samples
[2018-12-22 11:03:49.453313 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:49.534523 UTC] Performing policy update
[2018-12-22 11:03:49.535199 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:49.625350 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:50.592570 UTC] Performing line search
[2018-12-22 11:03:50.709087 UTC] Updating baseline
[2018-12-22 11:03:51.969244 UTC] Computing logging information
-------------------------------------
| Iteration            | 556        |
| ExpectedImprovement  | 0.017711   |
| ActualImprovement    | 0.016752   |
| ImprovementRatio     | 0.94584    |
| MeanKL               | 0.0070699  |
| Entropy              | 0.067961   |
| Perplexity           | 1.0703     |
| AveragePolicyStd     | 0.2481     |
| AveragePolicyStd[0]  | 0.27642    |
| AveragePolicyStd[1]  | 0.30747    |
| AveragePolicyStd[2]  | 0.20743    |
| AveragePolicyStd[3]  | 0.26251    |
| AveragePolicyStd[4]  | 0.1884     |
| AveragePolicyStd[5]  | 0.24639    |
| AverageReturn        | 1274.5     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 397.04     |
| AverageEpisodeLength | 893.75     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 268.38     |
| TotalNEpisodes       | 17327      |
| TotalNSamples        | 2.7824e+06 |
| ExplainedVariance    | 0.073234   |
-------------------------------------
[2018-12-22 11:03:52.321629 UTC] Saving snapshot
[2018-12-22 11:03:52.321939 UTC] Starting iteration 557
[2018-12-22 11:03:52.322060 UTC] Start collecting samples
[2018-12-22 11:03:55.258240 UTC] Computing input variables for policy optimization
[2018-12-22 11:03:55.336408 UTC] Performing policy update
[2018-12-22 11:03:55.338428 UTC] Computing gradient in Euclidean space
[2018-12-22 11:03:55.429076 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:03:56.500081 UTC] Performing line search
[2018-12-22 11:03:56.628931 UTC] Updating baseline
[2018-12-22 11:03:57.820751 UTC] Computing logging information
-------------------------------------
| Iteration            | 557        |
| ExpectedImprovement  | 0.01707    |
| ActualImprovement    | 0.015692   |
| ImprovementRatio     | 0.91925    |
| MeanKL               | 0.0075567  |
| Entropy              | 0.063258   |
| Perplexity           | 1.0653     |
| AveragePolicyStd     | 0.24785    |
| AveragePolicyStd[0]  | 0.27498    |
| AveragePolicyStd[1]  | 0.308      |
| AveragePolicyStd[2]  | 0.2079     |
| AveragePolicyStd[3]  | 0.26167    |
| AveragePolicyStd[4]  | 0.18893    |
| AveragePolicyStd[5]  | 0.24563    |
| AverageReturn        | 1273.7     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 396.73     |
| AverageEpisodeLength | 893.75     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 268.38     |
| TotalNEpisodes       | 17331      |
| TotalNSamples        | 2.7864e+06 |
| ExplainedVariance    | -0.0020065 |
-------------------------------------
[2018-12-22 11:03:58.183796 UTC] Saving snapshot
[2018-12-22 11:03:58.184040 UTC] Starting iteration 558
[2018-12-22 11:03:58.184157 UTC] Start collecting samples
[2018-12-22 11:04:01.154601 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:01.235742 UTC] Performing policy update
[2018-12-22 11:04:01.236432 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:01.327667 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:02.400729 UTC] Performing line search
[2018-12-22 11:04:02.528194 UTC] Updating baseline
[2018-12-22 11:04:04.496414 UTC] Computing logging information
-------------------------------------
| Iteration            | 558        |
| ExpectedImprovement  | 0.01679    |
| ActualImprovement    | 0.015909   |
| ImprovementRatio     | 0.94751    |
| MeanKL               | 0.007303   |
| Entropy              | 0.065636   |
| Perplexity           | 1.0678     |
| AveragePolicyStd     | 0.24797    |
| AveragePolicyStd[0]  | 0.27542    |
| AveragePolicyStd[1]  | 0.30711    |
| AveragePolicyStd[2]  | 0.2075     |
| AveragePolicyStd[3]  | 0.26206    |
| AveragePolicyStd[4]  | 0.1884     |
| AveragePolicyStd[5]  | 0.24734    |
| AverageReturn        | 1293.2     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 375.66     |
| AverageEpisodeLength | 907.16     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.96     |
| TotalNEpisodes       | 17337      |
| TotalNSamples        | 2.7924e+06 |
| ExplainedVariance    | 0.111      |
-------------------------------------
[2018-12-22 11:04:04.857060 UTC] Saving snapshot
[2018-12-22 11:04:04.857323 UTC] Starting iteration 559
[2018-12-22 11:04:04.857443 UTC] Start collecting samples
[2018-12-22 11:04:07.818225 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:07.899924 UTC] Performing policy update
[2018-12-22 11:04:07.900780 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:07.991698 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:09.058527 UTC] Performing line search
[2018-12-22 11:04:09.186865 UTC] Updating baseline
[2018-12-22 11:04:10.556256 UTC] Computing logging information
-------------------------------------
| Iteration            | 559        |
| ExpectedImprovement  | 0.017161   |
| ActualImprovement    | 0.016174   |
| ImprovementRatio     | 0.94247    |
| MeanKL               | 0.0068007  |
| Entropy              | 0.072554   |
| Perplexity           | 1.0753     |
| AveragePolicyStd     | 0.24815    |
| AveragePolicyStd[0]  | 0.27531    |
| AveragePolicyStd[1]  | 0.30598    |
| AveragePolicyStd[2]  | 0.20837    |
| AveragePolicyStd[3]  | 0.26212    |
| AveragePolicyStd[4]  | 0.18917    |
| AveragePolicyStd[5]  | 0.24798    |
| AverageReturn        | 1296.9     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 366.41     |
| AverageEpisodeLength | 910.06     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.4      |
| TotalNEpisodes       | 17344      |
| TotalNSamples        | 2.7981e+06 |
| ExplainedVariance    | 0.25032    |
-------------------------------------
[2018-12-22 11:04:10.916602 UTC] Saving snapshot
[2018-12-22 11:04:10.916851 UTC] Starting iteration 560
[2018-12-22 11:04:10.916982 UTC] Start collecting samples
[2018-12-22 11:04:13.882495 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:13.965019 UTC] Performing policy update
[2018-12-22 11:04:13.965662 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:14.055477 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:15.120235 UTC] Performing line search
[2018-12-22 11:04:15.246129 UTC] Updating baseline
[2018-12-22 11:04:17.132181 UTC] Computing logging information
-------------------------------------
| Iteration            | 560        |
| ExpectedImprovement  | 0.016316   |
| ActualImprovement    | 0.015439   |
| ImprovementRatio     | 0.94625    |
| MeanKL               | 0.0069574  |
| Entropy              | 0.063943   |
| Perplexity           | 1.066      |
| AveragePolicyStd     | 0.24792    |
| AveragePolicyStd[0]  | 0.27574    |
| AveragePolicyStd[1]  | 0.30649    |
| AveragePolicyStd[2]  | 0.20809    |
| AveragePolicyStd[3]  | 0.2614     |
| AveragePolicyStd[4]  | 0.18737    |
| AveragePolicyStd[5]  | 0.24841    |
| AverageReturn        | 1296.9     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 366.43     |
| AverageEpisodeLength | 910.06     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.4      |
| TotalNEpisodes       | 17349      |
| TotalNSamples        | 2.8031e+06 |
| ExplainedVariance    | -0.033835  |
-------------------------------------
[2018-12-22 11:04:17.491873 UTC] Saving snapshot
[2018-12-22 11:04:17.499882 UTC] Starting iteration 561
[2018-12-22 11:04:17.500087 UTC] Start collecting samples
[2018-12-22 11:04:20.437756 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:20.515741 UTC] Performing policy update
[2018-12-22 11:04:20.516441 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:20.608225 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:21.686777 UTC] Performing line search
[2018-12-22 11:04:21.814980 UTC] Updating baseline
[2018-12-22 11:04:23.097291 UTC] Computing logging information
-------------------------------------
| Iteration            | 561        |
| ExpectedImprovement  | 0.02036    |
| ActualImprovement    | 0.018118   |
| ImprovementRatio     | 0.8899     |
| MeanKL               | 0.0069609  |
| Entropy              | 0.06739    |
| Perplexity           | 1.0697     |
| AveragePolicyStd     | 0.24807    |
| AveragePolicyStd[0]  | 0.27605    |
| AveragePolicyStd[1]  | 0.30634    |
| AveragePolicyStd[2]  | 0.20777    |
| AveragePolicyStd[3]  | 0.2623     |
| AveragePolicyStd[4]  | 0.18747    |
| AveragePolicyStd[5]  | 0.24851    |
| AverageReturn        | 1292.4     |
| MinReturn            | 60.086     |
| MaxReturn            | 1495.2     |
| StdReturn            | 373.79     |
| AverageEpisodeLength | 907.46     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 251.18     |
| TotalNEpisodes       | 17351      |
| TotalNSamples        | 2.8044e+06 |
| ExplainedVariance    | 0.35349    |
-------------------------------------
[2018-12-22 11:04:23.460964 UTC] Saving snapshot
[2018-12-22 11:04:23.461242 UTC] Starting iteration 562
[2018-12-22 11:04:23.461373 UTC] Start collecting samples
[2018-12-22 11:04:26.484571 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:26.567646 UTC] Performing policy update
[2018-12-22 11:04:26.568282 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:26.658271 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:27.719351 UTC] Performing line search
[2018-12-22 11:04:27.850887 UTC] Updating baseline
[2018-12-22 11:04:29.016098 UTC] Computing logging information
-------------------------------------
| Iteration            | 562        |
| ExpectedImprovement  | 0.017971   |
| ActualImprovement    | 0.01702    |
| ImprovementRatio     | 0.9471     |
| MeanKL               | 0.0071227  |
| Entropy              | 0.063608   |
| Perplexity           | 1.0657     |
| AveragePolicyStd     | 0.2479     |
| AveragePolicyStd[0]  | 0.27625    |
| AveragePolicyStd[1]  | 0.30556    |
| AveragePolicyStd[2]  | 0.2072     |
| AveragePolicyStd[3]  | 0.26153    |
| AveragePolicyStd[4]  | 0.18767    |
| AveragePolicyStd[5]  | 0.24917    |
| AverageReturn        | 1330       |
| MinReturn            | 112.73     |
| MaxReturn            | 1495.2     |
| StdReturn            | 320.58     |
| AverageEpisodeLength | 935.16     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.86     |
| TotalNEpisodes       | 17360      |
| TotalNSamples        | 2.8134e+06 |
| ExplainedVariance    | -0.0088647 |
-------------------------------------
[2018-12-22 11:04:29.375371 UTC] Saving snapshot
[2018-12-22 11:04:29.375693 UTC] Starting iteration 563
[2018-12-22 11:04:29.375830 UTC] Start collecting samples
[2018-12-22 11:04:32.367174 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:32.446007 UTC] Performing policy update
[2018-12-22 11:04:32.446650 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:32.536812 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:33.599061 UTC] Performing line search
[2018-12-22 11:04:33.726528 UTC] Updating baseline
[2018-12-22 11:04:35.275444 UTC] Computing logging information
-------------------------------------
| Iteration            | 563        |
| ExpectedImprovement  | 0.016189   |
| ActualImprovement    | 0.015403   |
| ImprovementRatio     | 0.95141    |
| MeanKL               | 0.007327   |
| Entropy              | 0.064398   |
| Perplexity           | 1.0665     |
| AveragePolicyStd     | 0.24793    |
| AveragePolicyStd[0]  | 0.27611    |
| AveragePolicyStd[1]  | 0.30501    |
| AveragePolicyStd[2]  | 0.20731    |
| AveragePolicyStd[3]  | 0.26224    |
| AveragePolicyStd[4]  | 0.18735    |
| AveragePolicyStd[5]  | 0.24956    |
| AverageReturn        | 1324.5     |
| MinReturn            | 112.73     |
| MaxReturn            | 1495.2     |
| StdReturn            | 314.51     |
| AverageEpisodeLength | 933.33     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.6      |
| TotalNEpisodes       | 17366      |
| TotalNSamples        | 2.8183e+06 |
| ExplainedVariance    | 0.22191    |
-------------------------------------
[2018-12-22 11:04:35.636656 UTC] Saving snapshot
[2018-12-22 11:04:35.636979 UTC] Starting iteration 564
[2018-12-22 11:04:35.637101 UTC] Start collecting samples
[2018-12-22 11:04:38.541346 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:38.618842 UTC] Performing policy update
[2018-12-22 11:04:38.619511 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:38.711109 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:39.768693 UTC] Performing line search
[2018-12-22 11:04:39.897457 UTC] Updating baseline
[2018-12-22 11:04:41.362647 UTC] Computing logging information
-------------------------------------
| Iteration            | 564        |
| ExpectedImprovement  | 0.018131   |
| ActualImprovement    | 0.018764   |
| ImprovementRatio     | 1.0349     |
| MeanKL               | 0.007043   |
| Entropy              | 0.069171   |
| Perplexity           | 1.0716     |
| AveragePolicyStd     | 0.2481     |
| AveragePolicyStd[0]  | 0.27569    |
| AveragePolicyStd[1]  | 0.30478    |
| AveragePolicyStd[2]  | 0.20721    |
| AveragePolicyStd[3]  | 0.26292    |
| AveragePolicyStd[4]  | 0.18772    |
| AveragePolicyStd[5]  | 0.25031    |
| AverageReturn        | 1324.3     |
| MinReturn            | 112.73     |
| MaxReturn            | 1495.2     |
| StdReturn            | 314.46     |
| AverageEpisodeLength | 933.33     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.6      |
| TotalNEpisodes       | 17367      |
| TotalNSamples        | 2.8193e+06 |
| ExplainedVariance    | -0.095791  |
-------------------------------------
[2018-12-22 11:04:41.719251 UTC] Saving snapshot
[2018-12-22 11:04:41.719504 UTC] Starting iteration 565
[2018-12-22 11:04:41.719648 UTC] Start collecting samples
[2018-12-22 11:04:44.686907 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:44.768151 UTC] Performing policy update
[2018-12-22 11:04:44.768880 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:44.859503 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:45.933273 UTC] Performing line search
[2018-12-22 11:04:46.061482 UTC] Updating baseline
[2018-12-22 11:04:47.514719 UTC] Computing logging information
-------------------------------------
| Iteration            | 565        |
| ExpectedImprovement  | 0.016203   |
| ActualImprovement    | 0.015186   |
| ImprovementRatio     | 0.93723    |
| MeanKL               | 0.0073978  |
| Entropy              | 0.061836   |
| Perplexity           | 1.0638     |
| AveragePolicyStd     | 0.24784    |
| AveragePolicyStd[0]  | 0.27668    |
| AveragePolicyStd[1]  | 0.30464    |
| AveragePolicyStd[2]  | 0.20597    |
| AveragePolicyStd[3]  | 0.26241    |
| AveragePolicyStd[4]  | 0.1879     |
| AveragePolicyStd[5]  | 0.24945    |
| AverageReturn        | 1324.6     |
| MinReturn            | 112.73     |
| MaxReturn            | 1495.2     |
| StdReturn            | 314.55     |
| AverageEpisodeLength | 933.33     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.6      |
| TotalNEpisodes       | 17373      |
| TotalNSamples        | 2.8253e+06 |
| ExplainedVariance    | 0.054071   |
-------------------------------------
[2018-12-22 11:04:47.876064 UTC] Saving snapshot
[2018-12-22 11:04:47.876340 UTC] Starting iteration 566
[2018-12-22 11:04:47.876497 UTC] Start collecting samples
[2018-12-22 11:04:50.846770 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:50.927632 UTC] Performing policy update
[2018-12-22 11:04:50.928309 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:51.020396 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:52.082211 UTC] Performing line search
[2018-12-22 11:04:52.211473 UTC] Updating baseline
[2018-12-22 11:04:53.577096 UTC] Computing logging information
-------------------------------------
| Iteration            | 566        |
| ExpectedImprovement  | 0.019691   |
| ActualImprovement    | 0.018532   |
| ImprovementRatio     | 0.94116    |
| MeanKL               | 0.0069184  |
| Entropy              | 0.062546   |
| Perplexity           | 1.0645     |
| AveragePolicyStd     | 0.24785    |
| AveragePolicyStd[0]  | 0.27641    |
| AveragePolicyStd[1]  | 0.30379    |
| AveragePolicyStd[2]  | 0.2066     |
| AveragePolicyStd[3]  | 0.26308    |
| AveragePolicyStd[4]  | 0.18741    |
| AveragePolicyStd[5]  | 0.2498     |
| AverageReturn        | 1323.5     |
| MinReturn            | 112.73     |
| MaxReturn            | 1495.2     |
| StdReturn            | 313.94     |
| AverageEpisodeLength | 934.48     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.64     |
| TotalNEpisodes       | 17380      |
| TotalNSamples        | 2.8323e+06 |
| ExplainedVariance    | -0.014973  |
-------------------------------------
[2018-12-22 11:04:53.949049 UTC] Saving snapshot
[2018-12-22 11:04:53.949334 UTC] Starting iteration 567
[2018-12-22 11:04:53.949456 UTC] Start collecting samples
[2018-12-22 11:04:56.904374 UTC] Computing input variables for policy optimization
[2018-12-22 11:04:56.982816 UTC] Performing policy update
[2018-12-22 11:04:56.983452 UTC] Computing gradient in Euclidean space
[2018-12-22 11:04:57.073998 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:04:58.148124 UTC] Performing line search
[2018-12-22 11:04:58.277319 UTC] Updating baseline
[2018-12-22 11:04:59.917214 UTC] Computing logging information
-------------------------------------
| Iteration            | 567        |
| ExpectedImprovement  | 0.018049   |
| ActualImprovement    | 0.017227   |
| ImprovementRatio     | 0.95446    |
| MeanKL               | 0.0075146  |
| Entropy              | 0.058155   |
| Perplexity           | 1.0599     |
| AveragePolicyStd     | 0.24769    |
| AveragePolicyStd[0]  | 0.27706    |
| AveragePolicyStd[1]  | 0.30331    |
| AveragePolicyStd[2]  | 0.20667    |
| AveragePolicyStd[3]  | 0.26254    |
| AveragePolicyStd[4]  | 0.18676    |
| AveragePolicyStd[5]  | 0.24982    |
| AverageReturn        | 1322.1     |
| MinReturn            | 53.971     |
| MaxReturn            | 1495.2     |
| StdReturn            | 317.47     |
| AverageEpisodeLength | 933.44     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.71     |
| TotalNEpisodes       | 17384      |
| TotalNSamples        | 2.8354e+06 |
| ExplainedVariance    | 0.098492   |
-------------------------------------
[2018-12-22 11:05:00.277311 UTC] Saving snapshot
[2018-12-22 11:05:00.277607 UTC] Starting iteration 568
[2018-12-22 11:05:00.277755 UTC] Start collecting samples
[2018-12-22 11:05:03.256621 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:03.337617 UTC] Performing policy update
[2018-12-22 11:05:03.338498 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:03.428490 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:04.499402 UTC] Performing line search
[2018-12-22 11:05:04.627097 UTC] Updating baseline
[2018-12-22 11:05:05.824610 UTC] Computing logging information
-------------------------------------
| Iteration            | 568        |
| ExpectedImprovement  | 0.019741   |
| ActualImprovement    | 0.01811    |
| ImprovementRatio     | 0.91741    |
| MeanKL               | 0.0072267  |
| Entropy              | 0.055313   |
| Perplexity           | 1.0569     |
| AveragePolicyStd     | 0.24754    |
| AveragePolicyStd[0]  | 0.27638    |
| AveragePolicyStd[1]  | 0.30312    |
| AveragePolicyStd[2]  | 0.20736    |
| AveragePolicyStd[3]  | 0.26137    |
| AveragePolicyStd[4]  | 0.18656    |
| AveragePolicyStd[5]  | 0.25042    |
| AverageReturn        | 1335.7     |
| MinReturn            | 53.971     |
| MaxReturn            | 1495.2     |
| StdReturn            | 294.48     |
| AverageEpisodeLength | 942.15     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.91     |
| TotalNEpisodes       | 17388      |
| TotalNSamples        | 2.8394e+06 |
| ExplainedVariance    | -0.034481  |
-------------------------------------
[2018-12-22 11:05:06.185957 UTC] Saving snapshot
[2018-12-22 11:05:06.186209 UTC] Starting iteration 569
[2018-12-22 11:05:06.186328 UTC] Start collecting samples
[2018-12-22 11:05:09.210306 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:09.295210 UTC] Performing policy update
[2018-12-22 11:05:09.295841 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:09.385949 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:10.456384 UTC] Performing line search
[2018-12-22 11:05:10.583774 UTC] Updating baseline
[2018-12-22 11:05:12.303046 UTC] Computing logging information
-------------------------------------
| Iteration            | 569        |
| ExpectedImprovement  | 0.019678   |
| ActualImprovement    | 0.018319   |
| ImprovementRatio     | 0.93095    |
| MeanKL               | 0.0068908  |
| Entropy              | 0.054426   |
| Perplexity           | 1.0559     |
| AveragePolicyStd     | 0.24752    |
| AveragePolicyStd[0]  | 0.27706    |
| AveragePolicyStd[1]  | 0.30311    |
| AveragePolicyStd[2]  | 0.20638    |
| AveragePolicyStd[3]  | 0.26107    |
| AveragePolicyStd[4]  | 0.18698    |
| AveragePolicyStd[5]  | 0.2505     |
| AverageReturn        | 1320.3     |
| MinReturn            | 53.971     |
| MaxReturn            | 1495.2     |
| StdReturn            | 318.48     |
| AverageEpisodeLength | 932.88     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.88     |
| TotalNEpisodes       | 17398      |
| TotalNSamples        | 2.8485e+06 |
| ExplainedVariance    | 0.035242   |
-------------------------------------
[2018-12-22 11:05:12.665038 UTC] Saving snapshot
[2018-12-22 11:05:12.665278 UTC] Starting iteration 570
[2018-12-22 11:05:12.665399 UTC] Start collecting samples
[2018-12-22 11:05:15.626867 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:15.708233 UTC] Performing policy update
[2018-12-22 11:05:15.708995 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:15.804665 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:16.882296 UTC] Performing line search
[2018-12-22 11:05:17.009789 UTC] Updating baseline
[2018-12-22 11:05:18.207578 UTC] Computing logging information
-------------------------------------
| Iteration            | 570        |
| ExpectedImprovement  | 0.018106   |
| ActualImprovement    | 0.016838   |
| ImprovementRatio     | 0.92996    |
| MeanKL               | 0.0070581  |
| Entropy              | 0.044754   |
| Perplexity           | 1.0458     |
| AveragePolicyStd     | 0.24712    |
| AveragePolicyStd[0]  | 0.27694    |
| AveragePolicyStd[1]  | 0.3027     |
| AveragePolicyStd[2]  | 0.20575    |
| AveragePolicyStd[3]  | 0.26049    |
| AveragePolicyStd[4]  | 0.18703    |
| AveragePolicyStd[5]  | 0.24977    |
| AverageReturn        | 1311.6     |
| MinReturn            | 53.971     |
| MaxReturn            | 1495.2     |
| StdReturn            | 326.96     |
| AverageEpisodeLength | 927.17     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.5      |
| TotalNEpisodes       | 17402      |
| TotalNSamples        | 2.8519e+06 |
| ExplainedVariance    | 0.15471    |
-------------------------------------
[2018-12-22 11:05:18.573798 UTC] Saving snapshot
[2018-12-22 11:05:18.582250 UTC] Starting iteration 571
[2018-12-22 11:05:18.582508 UTC] Start collecting samples
[2018-12-22 11:05:21.567369 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:21.648080 UTC] Performing policy update
[2018-12-22 11:05:21.648791 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:21.739625 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:22.807385 UTC] Performing line search
[2018-12-22 11:05:22.937920 UTC] Updating baseline
[2018-12-22 11:05:24.212507 UTC] Computing logging information
-------------------------------------
| Iteration            | 571        |
| ExpectedImprovement  | 0.015373   |
| ActualImprovement    | 0.014732   |
| ImprovementRatio     | 0.95829    |
| MeanKL               | 0.007206   |
| Entropy              | 0.035622   |
| Perplexity           | 1.0363     |
| AveragePolicyStd     | 0.24672    |
| AveragePolicyStd[0]  | 0.27607    |
| AveragePolicyStd[1]  | 0.30257    |
| AveragePolicyStd[2]  | 0.20643    |
| AveragePolicyStd[3]  | 0.2604     |
| AveragePolicyStd[4]  | 0.18664    |
| AveragePolicyStd[5]  | 0.24817    |
| AverageReturn        | 1309.1     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 327.86     |
| AverageEpisodeLength | 924.33     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222        |
| TotalNEpisodes       | 17408      |
| TotalNSamples        | 2.8567e+06 |
| ExplainedVariance    | 0.18327    |
-------------------------------------
[2018-12-22 11:05:24.575368 UTC] Saving snapshot
[2018-12-22 11:05:24.575662 UTC] Starting iteration 572
[2018-12-22 11:05:24.575802 UTC] Start collecting samples
[2018-12-22 11:05:27.630772 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:27.712900 UTC] Performing policy update
[2018-12-22 11:05:27.713614 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:27.808487 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:28.890228 UTC] Performing line search
[2018-12-22 11:05:29.018677 UTC] Updating baseline
[2018-12-22 11:05:31.015827 UTC] Computing logging information
-------------------------------------
| Iteration            | 572        |
| ExpectedImprovement  | 0.016085   |
| ActualImprovement    | 0.015074   |
| ImprovementRatio     | 0.93718    |
| MeanKL               | 0.0071204  |
| Entropy              | 0.039289   |
| Perplexity           | 1.0401     |
| AveragePolicyStd     | 0.24686    |
| AveragePolicyStd[0]  | 0.27581    |
| AveragePolicyStd[1]  | 0.30265    |
| AveragePolicyStd[2]  | 0.20711    |
| AveragePolicyStd[3]  | 0.26025    |
| AveragePolicyStd[4]  | 0.18624    |
| AveragePolicyStd[5]  | 0.24912    |
| AverageReturn        | 1303.1     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 337.12     |
| AverageEpisodeLength | 918.36     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.84     |
| TotalNEpisodes       | 17416      |
| TotalNSamples        | 2.8641e+06 |
| ExplainedVariance    | 0.13634    |
-------------------------------------
[2018-12-22 11:05:31.377450 UTC] Saving snapshot
[2018-12-22 11:05:31.377713 UTC] Starting iteration 573
[2018-12-22 11:05:31.377875 UTC] Start collecting samples
[2018-12-22 11:05:34.317563 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:34.396689 UTC] Performing policy update
[2018-12-22 11:05:34.397659 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:34.488325 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:35.553341 UTC] Performing line search
[2018-12-22 11:05:35.680667 UTC] Updating baseline
[2018-12-22 11:05:36.962352 UTC] Computing logging information
-------------------------------------
| Iteration            | 573        |
| ExpectedImprovement  | 0.017501   |
| ActualImprovement    | 0.016307   |
| ImprovementRatio     | 0.93177    |
| MeanKL               | 0.0073293  |
| Entropy              | 0.032009   |
| Perplexity           | 1.0325     |
| AveragePolicyStd     | 0.24648    |
| AveragePolicyStd[0]  | 0.27478    |
| AveragePolicyStd[1]  | 0.30172    |
| AveragePolicyStd[2]  | 0.20769    |
| AveragePolicyStd[3]  | 0.25931    |
| AveragePolicyStd[4]  | 0.18646    |
| AveragePolicyStd[5]  | 0.24892    |
| AverageReturn        | 1302.1     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 336.76     |
| AverageEpisodeLength | 918.36     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.84     |
| TotalNEpisodes       | 17419      |
| TotalNSamples        | 2.8671e+06 |
| ExplainedVariance    | -0.014817  |
-------------------------------------
[2018-12-22 11:05:37.324022 UTC] Saving snapshot
[2018-12-22 11:05:37.324298 UTC] Starting iteration 574
[2018-12-22 11:05:37.324421 UTC] Start collecting samples
[2018-12-22 11:05:40.270282 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:40.349425 UTC] Performing policy update
[2018-12-22 11:05:40.350307 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:40.442260 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:41.506973 UTC] Performing line search
[2018-12-22 11:05:41.635305 UTC] Updating baseline
[2018-12-22 11:05:43.915959 UTC] Computing logging information
--------------------------------------
| Iteration            | 574         |
| ExpectedImprovement  | 0.016165    |
| ActualImprovement    | 0.015258    |
| ImprovementRatio     | 0.94388     |
| MeanKL               | 0.0075108   |
| Entropy              | 0.011701    |
| Perplexity           | 1.0118      |
| AveragePolicyStd     | 0.24569     |
| AveragePolicyStd[0]  | 0.2745      |
| AveragePolicyStd[1]  | 0.30166     |
| AveragePolicyStd[2]  | 0.20641     |
| AveragePolicyStd[3]  | 0.25713     |
| AveragePolicyStd[4]  | 0.18589     |
| AveragePolicyStd[5]  | 0.24858     |
| AverageReturn        | 1303        |
| MinReturn            | 53.971      |
| MaxReturn            | 1489.3      |
| StdReturn            | 337.09      |
| AverageEpisodeLength | 918.36      |
| MinEpisodeLength     | 55          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 227.84      |
| TotalNEpisodes       | 17423       |
| TotalNSamples        | 2.8711e+06  |
| ExplainedVariance    | -0.00057698 |
--------------------------------------
[2018-12-22 11:05:44.276959 UTC] Saving snapshot
[2018-12-22 11:05:44.277208 UTC] Starting iteration 575
[2018-12-22 11:05:44.277340 UTC] Start collecting samples
[2018-12-22 11:05:47.244944 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:47.323323 UTC] Performing policy update
[2018-12-22 11:05:47.323955 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:47.414640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:48.490485 UTC] Performing line search
[2018-12-22 11:05:48.618744 UTC] Updating baseline
[2018-12-22 11:05:50.080972 UTC] Computing logging information
-------------------------------------
| Iteration            | 575        |
| ExpectedImprovement  | 0.01776    |
| ActualImprovement    | 0.016652   |
| ImprovementRatio     | 0.93764    |
| MeanKL               | 0.0074417  |
| Entropy              | 0.013117   |
| Perplexity           | 1.0132     |
| AveragePolicyStd     | 0.24574    |
| AveragePolicyStd[0]  | 0.2734     |
| AveragePolicyStd[1]  | 0.30252    |
| AveragePolicyStd[2]  | 0.20616    |
| AveragePolicyStd[3]  | 0.25729    |
| AveragePolicyStd[4]  | 0.1865     |
| AveragePolicyStd[5]  | 0.24855    |
| AverageReturn        | 1316.1     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 319.6      |
| AverageEpisodeLength | 926.44     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.95     |
| TotalNEpisodes       | 17429      |
| TotalNSamples        | 2.8771e+06 |
| ExplainedVariance    | 0.00040545 |
-------------------------------------
[2018-12-22 11:05:50.442752 UTC] Saving snapshot
[2018-12-22 11:05:50.442992 UTC] Starting iteration 576
[2018-12-22 11:05:50.443107 UTC] Start collecting samples
[2018-12-22 11:05:53.443196 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:53.524765 UTC] Performing policy update
[2018-12-22 11:05:53.525348 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:53.614955 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:05:54.674913 UTC] Performing line search
[2018-12-22 11:05:54.802167 UTC] Updating baseline
[2018-12-22 11:05:55.993027 UTC] Computing logging information
-------------------------------------
| Iteration            | 576        |
| ExpectedImprovement  | 0.019353   |
| ActualImprovement    | 0.018205   |
| ImprovementRatio     | 0.94069    |
| MeanKL               | 0.0071183  |
| Entropy              | 0.016809   |
| Perplexity           | 1.017      |
| AveragePolicyStd     | 0.24586    |
| AveragePolicyStd[0]  | 0.27346    |
| AveragePolicyStd[1]  | 0.30341    |
| AveragePolicyStd[2]  | 0.20686    |
| AveragePolicyStd[3]  | 0.25525    |
| AveragePolicyStd[4]  | 0.18684    |
| AveragePolicyStd[5]  | 0.24938    |
| AverageReturn        | 1291.2     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 353.03     |
| AverageEpisodeLength | 907.73     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 238.29     |
| TotalNEpisodes       | 17438      |
| TotalNSamples        | 2.8841e+06 |
| ExplainedVariance    | 0.22217    |
-------------------------------------
[2018-12-22 11:05:56.360868 UTC] Saving snapshot
[2018-12-22 11:05:56.361118 UTC] Starting iteration 577
[2018-12-22 11:05:56.361238 UTC] Start collecting samples
[2018-12-22 11:05:59.296773 UTC] Computing input variables for policy optimization
[2018-12-22 11:05:59.375786 UTC] Performing policy update
[2018-12-22 11:05:59.376483 UTC] Computing gradient in Euclidean space
[2018-12-22 11:05:59.466992 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:00.543025 UTC] Performing line search
[2018-12-22 11:06:00.673229 UTC] Updating baseline
[2018-12-22 11:06:01.958160 UTC] Computing logging information
-------------------------------------
| Iteration            | 577        |
| ExpectedImprovement  | 0.015775   |
| ActualImprovement    | 0.014967   |
| ImprovementRatio     | 0.94877    |
| MeanKL               | 0.0074452  |
| Entropy              | 0.0085322  |
| Perplexity           | 1.0086     |
| AveragePolicyStd     | 0.24554    |
| AveragePolicyStd[0]  | 0.27301    |
| AveragePolicyStd[1]  | 0.30385    |
| AveragePolicyStd[2]  | 0.20677    |
| AveragePolicyStd[3]  | 0.25377    |
| AveragePolicyStd[4]  | 0.18655    |
| AveragePolicyStd[5]  | 0.2493     |
| AverageReturn        | 1303.9     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 337.56     |
| AverageEpisodeLength | 915.83     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.25     |
| TotalNEpisodes       | 17441      |
| TotalNSamples        | 2.8866e+06 |
| ExplainedVariance    | 0.17178    |
-------------------------------------
[2018-12-22 11:06:02.322093 UTC] Saving snapshot
[2018-12-22 11:06:02.322352 UTC] Starting iteration 578
[2018-12-22 11:06:02.322478 UTC] Start collecting samples
[2018-12-22 11:06:05.294799 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:05.373194 UTC] Performing policy update
[2018-12-22 11:06:05.373815 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:05.464408 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:06.536884 UTC] Performing line search
[2018-12-22 11:06:06.665451 UTC] Updating baseline
[2018-12-22 11:06:08.129068 UTC] Computing logging information
-------------------------------------
| Iteration            | 578        |
| ExpectedImprovement  | 0.017738   |
| ActualImprovement    | 0.016303   |
| ImprovementRatio     | 0.91909    |
| MeanKL               | 0.0070564  |
| Entropy              | -0.0043935 |
| Perplexity           | 0.99562    |
| AveragePolicyStd     | 0.24504    |
| AveragePolicyStd[0]  | 0.27327    |
| AveragePolicyStd[1]  | 0.3035     |
| AveragePolicyStd[2]  | 0.2067     |
| AveragePolicyStd[3]  | 0.25341    |
| AveragePolicyStd[4]  | 0.18583    |
| AveragePolicyStd[5]  | 0.24754    |
| AverageReturn        | 1300.5     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 337.57     |
| AverageEpisodeLength | 913.79     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.4      |
| TotalNEpisodes       | 17445      |
| TotalNSamples        | 2.8904e+06 |
| ExplainedVariance    | 0.17915    |
-------------------------------------
[2018-12-22 11:06:08.491433 UTC] Saving snapshot
[2018-12-22 11:06:08.491727 UTC] Starting iteration 579
[2018-12-22 11:06:08.491851 UTC] Start collecting samples
[2018-12-22 11:06:11.496916 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:11.578180 UTC] Performing policy update
[2018-12-22 11:06:11.578948 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:11.669552 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:12.749130 UTC] Performing line search
[2018-12-22 11:06:12.875330 UTC] Updating baseline
[2018-12-22 11:06:14.155604 UTC] Computing logging information
-------------------------------------
| Iteration            | 579        |
| ExpectedImprovement  | 0.017361   |
| ActualImprovement    | 0.016391   |
| ImprovementRatio     | 0.94411    |
| MeanKL               | 0.0076261  |
| Entropy              | -0.018557  |
| Perplexity           | 0.98161    |
| AveragePolicyStd     | 0.24445    |
| AveragePolicyStd[0]  | 0.27299    |
| AveragePolicyStd[1]  | 0.30301    |
| AveragePolicyStd[2]  | 0.2061     |
| AveragePolicyStd[3]  | 0.25048    |
| AveragePolicyStd[4]  | 0.18575    |
| AveragePolicyStd[5]  | 0.24839    |
| AverageReturn        | 1310.5     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 326.28     |
| AverageEpisodeLength | 919.04     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.64     |
| TotalNEpisodes       | 17453      |
| TotalNSamples        | 2.8983e+06 |
| ExplainedVariance    | 0.094762   |
-------------------------------------
[2018-12-22 11:06:14.518880 UTC] Saving snapshot
[2018-12-22 11:06:14.519147 UTC] Starting iteration 580
[2018-12-22 11:06:14.519264 UTC] Start collecting samples
[2018-12-22 11:06:17.469825 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:17.549350 UTC] Performing policy update
[2018-12-22 11:06:17.550243 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:17.640458 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:18.712781 UTC] Performing line search
[2018-12-22 11:06:18.841402 UTC] Updating baseline
[2018-12-22 11:06:20.192436 UTC] Computing logging information
-------------------------------------
| Iteration            | 580        |
| ExpectedImprovement  | 0.016886   |
| ActualImprovement    | 0.016397   |
| ImprovementRatio     | 0.97105    |
| MeanKL               | 0.0076225  |
| Entropy              | -0.023841  |
| Perplexity           | 0.97644    |
| AveragePolicyStd     | 0.2443     |
| AveragePolicyStd[0]  | 0.27368    |
| AveragePolicyStd[1]  | 0.30289    |
| AveragePolicyStd[2]  | 0.20659    |
| AveragePolicyStd[3]  | 0.24956    |
| AveragePolicyStd[4]  | 0.18434    |
| AveragePolicyStd[5]  | 0.24876    |
| AverageReturn        | 1312.3     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 326.97     |
| AverageEpisodeLength | 919.04     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.64     |
| TotalNEpisodes       | 17456      |
| TotalNSamples        | 2.9013e+06 |
| ExplainedVariance    | -0.0011521 |
-------------------------------------
[2018-12-22 11:06:20.555399 UTC] Saving snapshot
[2018-12-22 11:06:20.563430 UTC] Starting iteration 581
[2018-12-22 11:06:20.563639 UTC] Start collecting samples
[2018-12-22 11:06:23.522541 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:23.602220 UTC] Performing policy update
[2018-12-22 11:06:23.602872 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:23.694545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:24.766153 UTC] Performing line search
[2018-12-22 11:06:24.897275 UTC] Updating baseline
[2018-12-22 11:06:26.246461 UTC] Computing logging information
-------------------------------------
| Iteration            | 581        |
| ExpectedImprovement  | 0.018343   |
| ActualImprovement    | 0.017071   |
| ImprovementRatio     | 0.93064    |
| MeanKL               | 0.0076038  |
| Entropy              | -0.023381  |
| Perplexity           | 0.97689    |
| AveragePolicyStd     | 0.24432    |
| AveragePolicyStd[0]  | 0.27423    |
| AveragePolicyStd[1]  | 0.30217    |
| AveragePolicyStd[2]  | 0.20681    |
| AveragePolicyStd[3]  | 0.24983    |
| AveragePolicyStd[4]  | 0.18401    |
| AveragePolicyStd[5]  | 0.24889    |
| AverageReturn        | 1313.7     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 327.54     |
| AverageEpisodeLength | 919.04     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.64     |
| TotalNEpisodes       | 17459      |
| TotalNSamples        | 2.9043e+06 |
| ExplainedVariance    | 0.0039732  |
-------------------------------------
[2018-12-22 11:06:26.608959 UTC] Saving snapshot
[2018-12-22 11:06:26.609219 UTC] Starting iteration 582
[2018-12-22 11:06:26.609336 UTC] Start collecting samples
[2018-12-22 11:06:29.635698 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:29.719013 UTC] Performing policy update
[2018-12-22 11:06:29.719817 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:29.814834 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:30.889028 UTC] Performing line search
[2018-12-22 11:06:31.017500 UTC] Updating baseline
[2018-12-22 11:06:32.389143 UTC] Computing logging information
-------------------------------------
| Iteration            | 582        |
| ExpectedImprovement  | 0.020227   |
| ActualImprovement    | 0.019341   |
| ImprovementRatio     | 0.95617    |
| MeanKL               | 0.0071687  |
| Entropy              | -0.026182  |
| Perplexity           | 0.97416    |
| AveragePolicyStd     | 0.24422    |
| AveragePolicyStd[0]  | 0.2743     |
| AveragePolicyStd[1]  | 0.30179    |
| AveragePolicyStd[2]  | 0.20618    |
| AveragePolicyStd[3]  | 0.24984    |
| AveragePolicyStd[4]  | 0.18397    |
| AveragePolicyStd[5]  | 0.24923    |
| AverageReturn        | 1320.3     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 327.36     |
| AverageEpisodeLength | 922.05     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.03     |
| TotalNEpisodes       | 17467      |
| TotalNSamples        | 2.9115e+06 |
| ExplainedVariance    | 0.090516   |
-------------------------------------
[2018-12-22 11:06:32.756993 UTC] Saving snapshot
[2018-12-22 11:06:32.757243 UTC] Starting iteration 583
[2018-12-22 11:06:32.757398 UTC] Start collecting samples
[2018-12-22 11:06:35.737108 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:35.819691 UTC] Performing policy update
[2018-12-22 11:06:35.820582 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:35.911311 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:36.881698 UTC] Performing line search
[2018-12-22 11:06:36.999836 UTC] Updating baseline
[2018-12-22 11:06:38.562819 UTC] Computing logging information
-------------------------------------
| Iteration            | 583        |
| ExpectedImprovement  | 0.015719   |
| ActualImprovement    | 0.014638   |
| ImprovementRatio     | 0.93126    |
| MeanKL               | 0.0075707  |
| Entropy              | -0.020337  |
| Perplexity           | 0.97987    |
| AveragePolicyStd     | 0.24452    |
| AveragePolicyStd[0]  | 0.27491    |
| AveragePolicyStd[1]  | 0.30209    |
| AveragePolicyStd[2]  | 0.2059     |
| AveragePolicyStd[3]  | 0.25086    |
| AveragePolicyStd[4]  | 0.18348    |
| AveragePolicyStd[5]  | 0.24989    |
| AverageReturn        | 1320.9     |
| MinReturn            | 53.971     |
| MaxReturn            | 1489.3     |
| StdReturn            | 327.57     |
| AverageEpisodeLength | 922.05     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.03     |
| TotalNEpisodes       | 17473      |
| TotalNSamples        | 2.9175e+06 |
| ExplainedVariance    | -0.018016  |
-------------------------------------
[2018-12-22 11:06:38.922134 UTC] Saving snapshot
[2018-12-22 11:06:38.922391 UTC] Starting iteration 584
[2018-12-22 11:06:38.922515 UTC] Start collecting samples
[2018-12-22 11:06:41.872666 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:41.951873 UTC] Performing policy update
[2018-12-22 11:06:41.953937 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:42.044173 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:43.103764 UTC] Performing line search
[2018-12-22 11:06:43.231578 UTC] Updating baseline
[2018-12-22 11:06:44.604707 UTC] Computing logging information
-------------------------------------
| Iteration            | 584        |
| ExpectedImprovement  | 0.016753   |
| ActualImprovement    | 0.01558    |
| ImprovementRatio     | 0.92997    |
| MeanKL               | 0.00754    |
| Entropy              | -0.02906   |
| Perplexity           | 0.97136    |
| AveragePolicyStd     | 0.24419    |
| AveragePolicyStd[0]  | 0.27313    |
| AveragePolicyStd[1]  | 0.30211    |
| AveragePolicyStd[2]  | 0.20477    |
| AveragePolicyStd[3]  | 0.25175    |
| AveragePolicyStd[4]  | 0.18318    |
| AveragePolicyStd[5]  | 0.25021    |
| AverageReturn        | 1322.7     |
| MinReturn            | 53.971     |
| MaxReturn            | 1494.3     |
| StdReturn            | 328.13     |
| AverageEpisodeLength | 922.05     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.03     |
| TotalNEpisodes       | 17476      |
| TotalNSamples        | 2.9205e+06 |
| ExplainedVariance    | 0.14181    |
-------------------------------------
[2018-12-22 11:06:44.967870 UTC] Saving snapshot
[2018-12-22 11:06:44.968116 UTC] Starting iteration 585
[2018-12-22 11:06:44.968232 UTC] Start collecting samples
[2018-12-22 11:06:47.952207 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:48.033358 UTC] Performing policy update
[2018-12-22 11:06:48.033979 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:48.123790 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:49.191022 UTC] Performing line search
[2018-12-22 11:06:49.319044 UTC] Updating baseline
[2018-12-22 11:06:51.128846 UTC] Computing logging information
-------------------------------------
| Iteration            | 585        |
| ExpectedImprovement  | 0.018748   |
| ActualImprovement    | 0.018732   |
| ImprovementRatio     | 0.99919    |
| MeanKL               | 0.0072993  |
| Entropy              | -0.023947  |
| Perplexity           | 0.97634    |
| AveragePolicyStd     | 0.24437    |
| AveragePolicyStd[0]  | 0.27325    |
| AveragePolicyStd[1]  | 0.3014     |
| AveragePolicyStd[2]  | 0.20512    |
| AveragePolicyStd[3]  | 0.25204    |
| AveragePolicyStd[4]  | 0.18327    |
| AveragePolicyStd[5]  | 0.25113    |
| AverageReturn        | 1322.9     |
| MinReturn            | 53.971     |
| MaxReturn            | 1504.9     |
| StdReturn            | 328.79     |
| AverageEpisodeLength | 920.85     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.93     |
| TotalNEpisodes       | 17482      |
| TotalNSamples        | 2.9264e+06 |
| ExplainedVariance    | -0.049296  |
-------------------------------------
[2018-12-22 11:06:51.489553 UTC] Saving snapshot
[2018-12-22 11:06:51.489839 UTC] Starting iteration 586
[2018-12-22 11:06:51.489969 UTC] Start collecting samples
[2018-12-22 11:06:54.481202 UTC] Computing input variables for policy optimization
[2018-12-22 11:06:54.563019 UTC] Performing policy update
[2018-12-22 11:06:54.563681 UTC] Computing gradient in Euclidean space
[2018-12-22 11:06:54.655962 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:06:55.770085 UTC] Performing line search
[2018-12-22 11:06:55.903421 UTC] Updating baseline
[2018-12-22 11:06:57.790141 UTC] Computing logging information
-------------------------------------
| Iteration            | 586        |
| ExpectedImprovement  | 0.015626   |
| ActualImprovement    | 0.01454    |
| ImprovementRatio     | 0.93055    |
| MeanKL               | 0.0072243  |
| Entropy              | -0.023967  |
| Perplexity           | 0.97632    |
| AveragePolicyStd     | 0.24446    |
| AveragePolicyStd[0]  | 0.27328    |
| AveragePolicyStd[1]  | 0.30248    |
| AveragePolicyStd[2]  | 0.20418    |
| AveragePolicyStd[3]  | 0.25259    |
| AveragePolicyStd[4]  | 0.18288    |
| AveragePolicyStd[5]  | 0.25134    |
| AverageReturn        | 1338.8     |
| MinReturn            | 92.116     |
| MaxReturn            | 1504.9     |
| StdReturn            | 303.93     |
| AverageEpisodeLength | 930.3      |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.19     |
| TotalNEpisodes       | 17489      |
| TotalNSamples        | 2.9334e+06 |
| ExplainedVariance    | 6.9449e-05 |
-------------------------------------
[2018-12-22 11:06:58.190185 UTC] Saving snapshot
[2018-12-22 11:06:58.190463 UTC] Starting iteration 587
[2018-12-22 11:06:58.190654 UTC] Start collecting samples
[2018-12-22 11:07:01.374718 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:01.458968 UTC] Performing policy update
[2018-12-22 11:07:01.459707 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:01.553357 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:02.664495 UTC] Performing line search
[2018-12-22 11:07:02.797384 UTC] Updating baseline
[2018-12-22 11:07:04.060172 UTC] Computing logging information
-------------------------------------
| Iteration            | 587        |
| ExpectedImprovement  | 0.022231   |
| ActualImprovement    | 0.019799   |
| ImprovementRatio     | 0.8906     |
| MeanKL               | 0.0071801  |
| Entropy              | -0.030898  |
| Perplexity           | 0.96957    |
| AveragePolicyStd     | 0.24422    |
| AveragePolicyStd[0]  | 0.27229    |
| AveragePolicyStd[1]  | 0.30279    |
| AveragePolicyStd[2]  | 0.20409    |
| AveragePolicyStd[3]  | 0.25258    |
| AveragePolicyStd[4]  | 0.18201    |
| AveragePolicyStd[5]  | 0.25158    |
| AverageReturn        | 1326.7     |
| MinReturn            | 92.116     |
| MaxReturn            | 1504.9     |
| StdReturn            | 325.18     |
| AverageEpisodeLength | 921.59     |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.13     |
| TotalNEpisodes       | 17493      |
| TotalNSamples        | 2.9366e+06 |
| ExplainedVariance    | 0.13516    |
-------------------------------------
[2018-12-22 11:07:04.443327 UTC] Saving snapshot
[2018-12-22 11:07:04.443665 UTC] Starting iteration 588
[2018-12-22 11:07:04.443808 UTC] Start collecting samples
[2018-12-22 11:07:07.452098 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:07.532041 UTC] Performing policy update
[2018-12-22 11:07:07.532782 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:07.622893 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:08.693203 UTC] Performing line search
[2018-12-22 11:07:08.821792 UTC] Updating baseline
[2018-12-22 11:07:10.383649 UTC] Computing logging information
-------------------------------------
| Iteration            | 588        |
| ExpectedImprovement  | 0.015744   |
| ActualImprovement    | 0.014993   |
| ImprovementRatio     | 0.9523     |
| MeanKL               | 0.0070492  |
| Entropy              | -0.043163  |
| Perplexity           | 0.95776    |
| AveragePolicyStd     | 0.2437     |
| AveragePolicyStd[0]  | 0.27208    |
| AveragePolicyStd[1]  | 0.30182    |
| AveragePolicyStd[2]  | 0.20339    |
| AveragePolicyStd[3]  | 0.25225    |
| AveragePolicyStd[4]  | 0.18205    |
| AveragePolicyStd[5]  | 0.25063    |
| AverageReturn        | 1341.6     |
| MinReturn            | 152.99     |
| MaxReturn            | 1504.9     |
| StdReturn            | 301.21     |
| AverageEpisodeLength | 930.86     |
| MinEpisodeLength     | 125        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.89     |
| TotalNEpisodes       | 17498      |
| TotalNSamples        | 2.9416e+06 |
| ExplainedVariance    | -0.043703  |
-------------------------------------
[2018-12-22 11:07:10.748408 UTC] Saving snapshot
[2018-12-22 11:07:10.748693 UTC] Starting iteration 589
[2018-12-22 11:07:10.748809 UTC] Start collecting samples
[2018-12-22 11:07:13.778943 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:13.861787 UTC] Performing policy update
[2018-12-22 11:07:13.862449 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:13.953074 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:15.019504 UTC] Performing line search
[2018-12-22 11:07:15.147007 UTC] Updating baseline
[2018-12-22 11:07:16.528170 UTC] Computing logging information
-------------------------------------
| Iteration            | 589        |
| ExpectedImprovement  | 0.017074   |
| ActualImprovement    | 0.01609    |
| ImprovementRatio     | 0.94236    |
| MeanKL               | 0.0069878  |
| Entropy              | -0.058077  |
| Perplexity           | 0.94358    |
| AveragePolicyStd     | 0.24312    |
| AveragePolicyStd[0]  | 0.27206    |
| AveragePolicyStd[1]  | 0.30113    |
| AveragePolicyStd[2]  | 0.20369    |
| AveragePolicyStd[3]  | 0.2497     |
| AveragePolicyStd[4]  | 0.18087    |
| AveragePolicyStd[5]  | 0.25129    |
| AverageReturn        | 1338.7     |
| MinReturn            | 160.28     |
| MaxReturn            | 1506.7     |
| StdReturn            | 312.09     |
| AverageEpisodeLength | 928.75     |
| MinEpisodeLength     | 129        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.87     |
| TotalNEpisodes       | 17507      |
| TotalNSamples        | 2.9486e+06 |
| ExplainedVariance    | 0.14646    |
-------------------------------------
[2018-12-22 11:07:16.890120 UTC] Saving snapshot
[2018-12-22 11:07:16.890371 UTC] Starting iteration 590
[2018-12-22 11:07:16.890492 UTC] Start collecting samples
[2018-12-22 11:07:19.835812 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:19.915684 UTC] Performing policy update
[2018-12-22 11:07:19.916262 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:20.009316 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:21.079590 UTC] Performing line search
[2018-12-22 11:07:21.206446 UTC] Updating baseline
[2018-12-22 11:07:22.495389 UTC] Computing logging information
-------------------------------------
| Iteration            | 590        |
| ExpectedImprovement  | 0.019097   |
| ActualImprovement    | 0.01695    |
| ImprovementRatio     | 0.88756    |
| MeanKL               | 0.0069774  |
| Entropy              | -0.054437  |
| Perplexity           | 0.94702    |
| AveragePolicyStd     | 0.2433     |
| AveragePolicyStd[0]  | 0.27273    |
| AveragePolicyStd[1]  | 0.30165    |
| AveragePolicyStd[2]  | 0.20361    |
| AveragePolicyStd[3]  | 0.24917    |
| AveragePolicyStd[4]  | 0.18095    |
| AveragePolicyStd[5]  | 0.25168    |
| AverageReturn        | 1338.1     |
| MinReturn            | 160.28     |
| MaxReturn            | 1506.7     |
| StdReturn            | 311.88     |
| AverageEpisodeLength | 928.75     |
| MinEpisodeLength     | 129        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.87     |
| TotalNEpisodes       | 17510      |
| TotalNSamples        | 2.9516e+06 |
| ExplainedVariance    | -0.0076572 |
-------------------------------------
[2018-12-22 11:07:22.862093 UTC] Saving snapshot
[2018-12-22 11:07:22.870246 UTC] Starting iteration 591
[2018-12-22 11:07:22.870440 UTC] Start collecting samples
[2018-12-22 11:07:25.832324 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:25.911891 UTC] Performing policy update
[2018-12-22 11:07:25.912673 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:26.007102 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:27.080063 UTC] Performing line search
[2018-12-22 11:07:27.208385 UTC] Updating baseline
[2018-12-22 11:07:28.572094 UTC] Computing logging information
-------------------------------------
| Iteration            | 591        |
| ExpectedImprovement  | 0.019111   |
| ActualImprovement    | 0.017633   |
| ImprovementRatio     | 0.92265    |
| MeanKL               | 0.0072404  |
| Entropy              | -0.064559  |
| Perplexity           | 0.93748    |
| AveragePolicyStd     | 0.2429     |
| AveragePolicyStd[0]  | 0.27217    |
| AveragePolicyStd[1]  | 0.30103    |
| AveragePolicyStd[2]  | 0.20333    |
| AveragePolicyStd[3]  | 0.2485     |
| AveragePolicyStd[4]  | 0.1803     |
| AveragePolicyStd[5]  | 0.2521     |
| AverageReturn        | 1338.3     |
| MinReturn            | 160.28     |
| MaxReturn            | 1506.7     |
| StdReturn            | 311.9      |
| AverageEpisodeLength | 928.75     |
| MinEpisodeLength     | 129        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.87     |
| TotalNEpisodes       | 17514      |
| TotalNSamples        | 2.9556e+06 |
| ExplainedVariance    | -0.011576  |
-------------------------------------
[2018-12-22 11:07:28.936361 UTC] Saving snapshot
[2018-12-22 11:07:28.936612 UTC] Starting iteration 592
[2018-12-22 11:07:28.936729 UTC] Start collecting samples
[2018-12-22 11:07:31.997576 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:32.080129 UTC] Performing policy update
[2018-12-22 11:07:32.080702 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:32.170601 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:33.229061 UTC] Performing line search
[2018-12-22 11:07:33.357789 UTC] Updating baseline
[2018-12-22 11:07:34.729492 UTC] Computing logging information
-------------------------------------
| Iteration            | 592        |
| ExpectedImprovement  | 0.016939   |
| ActualImprovement    | 0.016116   |
| ImprovementRatio     | 0.95141    |
| MeanKL               | 0.0078202  |
| Entropy              | -0.075883  |
| Perplexity           | 0.92692    |
| AveragePolicyStd     | 0.24236    |
| AveragePolicyStd[0]  | 0.27053    |
| AveragePolicyStd[1]  | 0.29955    |
| AveragePolicyStd[2]  | 0.20294    |
| AveragePolicyStd[3]  | 0.24866    |
| AveragePolicyStd[4]  | 0.1807     |
| AveragePolicyStd[5]  | 0.25178    |
| AverageReturn        | 1324.3     |
| MinReturn            | 70.069     |
| MaxReturn            | 1506.7     |
| StdReturn            | 350.38     |
| AverageEpisodeLength | 916.36     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.63     |
| TotalNEpisodes       | 17525      |
| TotalNSamples        | 2.9647e+06 |
| ExplainedVariance    | 0.084184   |
-------------------------------------
[2018-12-22 11:07:35.093986 UTC] Saving snapshot
[2018-12-22 11:07:35.094232 UTC] Starting iteration 593
[2018-12-22 11:07:35.094349 UTC] Start collecting samples
[2018-12-22 11:07:38.062978 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:38.141865 UTC] Performing policy update
[2018-12-22 11:07:38.142848 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:38.234983 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:39.318997 UTC] Performing line search
[2018-12-22 11:07:39.452886 UTC] Updating baseline
[2018-12-22 11:07:41.084730 UTC] Computing logging information
-------------------------------------
| Iteration            | 593        |
| ExpectedImprovement  | 0.019228   |
| ActualImprovement    | 0.018349   |
| ImprovementRatio     | 0.95433    |
| MeanKL               | 0.0072781  |
| Entropy              | -0.082768  |
| Perplexity           | 0.92056    |
| AveragePolicyStd     | 0.24201    |
| AveragePolicyStd[0]  | 0.27015    |
| AveragePolicyStd[1]  | 0.29872    |
| AveragePolicyStd[2]  | 0.20306    |
| AveragePolicyStd[3]  | 0.24738    |
| AveragePolicyStd[4]  | 0.18121    |
| AveragePolicyStd[5]  | 0.25152    |
| AverageReturn        | 1315       |
| MinReturn            | 70.069     |
| MaxReturn            | 1506.7     |
| StdReturn            | 360.88     |
| AverageEpisodeLength | 909.63     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.71     |
| TotalNEpisodes       | 17528      |
| TotalNSamples        | 2.9671e+06 |
| ExplainedVariance    | 0.18617    |
-------------------------------------
[2018-12-22 11:07:41.477627 UTC] Saving snapshot
[2018-12-22 11:07:41.477918 UTC] Starting iteration 594
[2018-12-22 11:07:41.478046 UTC] Start collecting samples
[2018-12-22 11:07:44.659951 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:44.739557 UTC] Performing policy update
[2018-12-22 11:07:44.740185 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:44.831746 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:45.907334 UTC] Performing line search
[2018-12-22 11:07:46.041337 UTC] Updating baseline
[2018-12-22 11:07:47.384031 UTC] Computing logging information
-------------------------------------
| Iteration            | 594        |
| ExpectedImprovement  | 0.015961   |
| ActualImprovement    | 0.015137   |
| ImprovementRatio     | 0.94839    |
| MeanKL               | 0.0072968  |
| Entropy              | -0.097833  |
| Perplexity           | 0.9068     |
| AveragePolicyStd     | 0.24139    |
| AveragePolicyStd[0]  | 0.26867    |
| AveragePolicyStd[1]  | 0.29864    |
| AveragePolicyStd[2]  | 0.20316    |
| AveragePolicyStd[3]  | 0.24599    |
| AveragePolicyStd[4]  | 0.18075    |
| AveragePolicyStd[5]  | 0.25113    |
| AverageReturn        | 1315.3     |
| MinReturn            | 70.069     |
| MaxReturn            | 1514.3     |
| StdReturn            | 361.02     |
| AverageEpisodeLength | 909.63     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.71     |
| TotalNEpisodes       | 17532      |
| TotalNSamples        | 2.9711e+06 |
| ExplainedVariance    | 0.037136   |
-------------------------------------
[2018-12-22 11:07:47.745381 UTC] Saving snapshot
[2018-12-22 11:07:47.745649 UTC] Starting iteration 595
[2018-12-22 11:07:47.745796 UTC] Start collecting samples
[2018-12-22 11:07:50.721079 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:50.802669 UTC] Performing policy update
[2018-12-22 11:07:50.803313 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:50.893576 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:51.958000 UTC] Performing line search
[2018-12-22 11:07:52.083830 UTC] Updating baseline
[2018-12-22 11:07:53.371890 UTC] Computing logging information
-------------------------------------
| Iteration            | 595        |
| ExpectedImprovement  | 0.016311   |
| ActualImprovement    | 0.015661   |
| ImprovementRatio     | 0.96014    |
| MeanKL               | 0.0075928  |
| Entropy              | -0.097391  |
| Perplexity           | 0.9072     |
| AveragePolicyStd     | 0.24144    |
| AveragePolicyStd[0]  | 0.26901    |
| AveragePolicyStd[1]  | 0.29817    |
| AveragePolicyStd[2]  | 0.20304    |
| AveragePolicyStd[3]  | 0.24644    |
| AveragePolicyStd[4]  | 0.18018    |
| AveragePolicyStd[5]  | 0.25179    |
| AverageReturn        | 1342.8     |
| MinReturn            | 70.069     |
| MaxReturn            | 1517.1     |
| StdReturn            | 328.17     |
| AverageEpisodeLength | 926.5      |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.56     |
| TotalNEpisodes       | 17538      |
| TotalNSamples        | 2.9768e+06 |
| ExplainedVariance    | 0.24948    |
-------------------------------------
[2018-12-22 11:07:53.741451 UTC] Saving snapshot
[2018-12-22 11:07:53.741755 UTC] Starting iteration 596
[2018-12-22 11:07:53.741915 UTC] Start collecting samples
[2018-12-22 11:07:56.715458 UTC] Computing input variables for policy optimization
[2018-12-22 11:07:56.795518 UTC] Performing policy update
[2018-12-22 11:07:56.796325 UTC] Computing gradient in Euclidean space
[2018-12-22 11:07:56.886720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:07:57.943741 UTC] Performing line search
[2018-12-22 11:07:58.072029 UTC] Updating baseline
[2018-12-22 11:07:59.426477 UTC] Computing logging information
-------------------------------------
| Iteration            | 596        |
| ExpectedImprovement  | 0.016547   |
| ActualImprovement    | 0.015537   |
| ImprovementRatio     | 0.93901    |
| MeanKL               | 0.0078901  |
| Entropy              | -0.10759   |
| Perplexity           | 0.89799    |
| AveragePolicyStd     | 0.24105    |
| AveragePolicyStd[0]  | 0.26813    |
| AveragePolicyStd[1]  | 0.2977     |
| AveragePolicyStd[2]  | 0.20166    |
| AveragePolicyStd[3]  | 0.24599    |
| AveragePolicyStd[4]  | 0.18022    |
| AveragePolicyStd[5]  | 0.25256    |
| AverageReturn        | 1342.2     |
| MinReturn            | 70.069     |
| MaxReturn            | 1520.1     |
| StdReturn            | 336.29     |
| AverageEpisodeLength | 924.43     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.47     |
| TotalNEpisodes       | 17544      |
| TotalNSamples        | 2.9821e+06 |
| ExplainedVariance    | 0.14422    |
-------------------------------------
[2018-12-22 11:07:59.793082 UTC] Saving snapshot
[2018-12-22 11:07:59.793340 UTC] Starting iteration 597
[2018-12-22 11:07:59.793472 UTC] Start collecting samples
[2018-12-22 11:08:02.752256 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:02.830913 UTC] Performing policy update
[2018-12-22 11:08:02.831704 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:02.922422 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:03.987617 UTC] Performing line search
[2018-12-22 11:08:04.114639 UTC] Updating baseline
[2018-12-22 11:08:05.451928 UTC] Computing logging information
-------------------------------------
| Iteration            | 597        |
| ExpectedImprovement  | 0.019997   |
| ActualImprovement    | 0.018824   |
| ImprovementRatio     | 0.94135    |
| MeanKL               | 0.0071363  |
| Entropy              | -0.11294   |
| Perplexity           | 0.8932     |
| AveragePolicyStd     | 0.24081    |
| AveragePolicyStd[0]  | 0.26772    |
| AveragePolicyStd[1]  | 0.29703    |
| AveragePolicyStd[2]  | 0.20163    |
| AveragePolicyStd[3]  | 0.24574    |
| AveragePolicyStd[4]  | 0.18007    |
| AveragePolicyStd[5]  | 0.25269    |
| AverageReturn        | 1347.5     |
| MinReturn            | 70.069     |
| MaxReturn            | 1524.1     |
| StdReturn            | 336.62     |
| AverageEpisodeLength | 926.47     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.22     |
| TotalNEpisodes       | 17548      |
| TotalNSamples        | 2.9861e+06 |
| ExplainedVariance    | -0.064171  |
-------------------------------------
[2018-12-22 11:08:05.811308 UTC] Saving snapshot
[2018-12-22 11:08:05.811570 UTC] Starting iteration 598
[2018-12-22 11:08:05.811691 UTC] Start collecting samples
[2018-12-22 11:08:08.772453 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:08.851255 UTC] Performing policy update
[2018-12-22 11:08:08.852001 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:08.941783 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:10.012137 UTC] Performing line search
[2018-12-22 11:08:10.139363 UTC] Updating baseline
[2018-12-22 11:08:11.502251 UTC] Computing logging information
-------------------------------------
| Iteration            | 598        |
| ExpectedImprovement  | 0.015581   |
| ActualImprovement    | 0.01481    |
| ImprovementRatio     | 0.95049    |
| MeanKL               | 0.0080473  |
| Entropy              | -0.11966   |
| Perplexity           | 0.88722    |
| AveragePolicyStd     | 0.2405     |
| AveragePolicyStd[0]  | 0.26737    |
| AveragePolicyStd[1]  | 0.29622    |
| AveragePolicyStd[2]  | 0.20184    |
| AveragePolicyStd[3]  | 0.24472    |
| AveragePolicyStd[4]  | 0.18012    |
| AveragePolicyStd[5]  | 0.25274    |
| AverageReturn        | 1350.5     |
| MinReturn            | 70.069     |
| MaxReturn            | 1524.1     |
| StdReturn            | 336.35     |
| AverageEpisodeLength | 928        |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.2      |
| TotalNEpisodes       | 17552      |
| TotalNSamples        | 2.9901e+06 |
| ExplainedVariance    | -0.0014973 |
-------------------------------------
[2018-12-22 11:08:11.873013 UTC] Saving snapshot
[2018-12-22 11:08:11.873265 UTC] Starting iteration 599
[2018-12-22 11:08:11.873406 UTC] Start collecting samples
[2018-12-22 11:08:14.865637 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:14.947757 UTC] Performing policy update
[2018-12-22 11:08:14.948597 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:15.039695 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:16.114793 UTC] Performing line search
[2018-12-22 11:08:16.242351 UTC] Updating baseline
[2018-12-22 11:08:18.282326 UTC] Computing logging information
--------------------------------------
| Iteration            | 599         |
| ExpectedImprovement  | 0.016393    |
| ActualImprovement    | 0.015406    |
| ImprovementRatio     | 0.9398      |
| MeanKL               | 0.0074556   |
| Entropy              | -0.1085     |
| Perplexity           | 0.89718     |
| AveragePolicyStd     | 0.24094     |
| AveragePolicyStd[0]  | 0.26839     |
| AveragePolicyStd[1]  | 0.29652     |
| AveragePolicyStd[2]  | 0.20273     |
| AveragePolicyStd[3]  | 0.24512     |
| AveragePolicyStd[4]  | 0.18024     |
| AveragePolicyStd[5]  | 0.25263     |
| AverageReturn        | 1351.3      |
| MinReturn            | 70.069      |
| MaxReturn            | 1524.1      |
| StdReturn            | 336.7       |
| AverageEpisodeLength | 928         |
| MinEpisodeLength     | 67          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 223.2       |
| TotalNEpisodes       | 17559       |
| TotalNSamples        | 2.9971e+06  |
| ExplainedVariance    | -7.4037e-05 |
--------------------------------------
[2018-12-22 11:08:18.641800 UTC] Saving snapshot
[2018-12-22 11:08:18.642048 UTC] Starting iteration 600
[2018-12-22 11:08:18.642164 UTC] Start collecting samples
[2018-12-22 11:08:21.622954 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:21.702774 UTC] Performing policy update
[2018-12-22 11:08:21.703355 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:21.798036 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:22.868766 UTC] Performing line search
[2018-12-22 11:08:22.997674 UTC] Updating baseline
[2018-12-22 11:08:24.889604 UTC] Computing logging information
-------------------------------------
| Iteration            | 600        |
| ExpectedImprovement  | 0.015722   |
| ActualImprovement    | 0.014737   |
| ImprovementRatio     | 0.93735    |
| MeanKL               | 0.0071093  |
| Entropy              | -0.10834   |
| Perplexity           | 0.89732    |
| AveragePolicyStd     | 0.24095    |
| AveragePolicyStd[0]  | 0.26861    |
| AveragePolicyStd[1]  | 0.29743    |
| AveragePolicyStd[2]  | 0.20293    |
| AveragePolicyStd[3]  | 0.24488    |
| AveragePolicyStd[4]  | 0.18045    |
| AveragePolicyStd[5]  | 0.25138    |
| AverageReturn        | 1348.4     |
| MinReturn            | 70.069     |
| MaxReturn            | 1524.1     |
| StdReturn            | 339.29     |
| AverageEpisodeLength | 924.77     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.46     |
| TotalNEpisodes       | 17564      |
| TotalNSamples        | 3.0018e+06 |
| ExplainedVariance    | 0.11372    |
-------------------------------------
[2018-12-22 11:08:25.257600 UTC] Saving snapshot
[2018-12-22 11:08:25.266004 UTC] Starting iteration 601
[2018-12-22 11:08:25.266219 UTC] Start collecting samples
[2018-12-22 11:08:28.241995 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:28.322267 UTC] Performing policy update
[2018-12-22 11:08:28.322965 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:28.412762 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:29.479357 UTC] Performing line search
[2018-12-22 11:08:29.606822 UTC] Updating baseline
[2018-12-22 11:08:30.986145 UTC] Computing logging information
-------------------------------------
| Iteration            | 601        |
| ExpectedImprovement  | 0.017905   |
| ActualImprovement    | 0.016365   |
| ImprovementRatio     | 0.914      |
| MeanKL               | 0.0072522  |
| Entropy              | -0.10226   |
| Perplexity           | 0.9028     |
| AveragePolicyStd     | 0.24118    |
| AveragePolicyStd[0]  | 0.268      |
| AveragePolicyStd[1]  | 0.29778    |
| AveragePolicyStd[2]  | 0.20299    |
| AveragePolicyStd[3]  | 0.24545    |
| AveragePolicyStd[4]  | 0.1808     |
| AveragePolicyStd[5]  | 0.25207    |
| AverageReturn        | 1361       |
| MinReturn            | 70.069     |
| MaxReturn            | 1526.1     |
| StdReturn            | 324.71     |
| AverageEpisodeLength | 932.1      |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.62     |
| TotalNEpisodes       | 17568      |
| TotalNSamples        | 3.0058e+06 |
| ExplainedVariance    | 0.0091853  |
-------------------------------------
[2018-12-22 11:08:31.351496 UTC] Saving snapshot
[2018-12-22 11:08:31.351765 UTC] Starting iteration 602
[2018-12-22 11:08:31.351897 UTC] Start collecting samples
[2018-12-22 11:08:34.368714 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:34.450479 UTC] Performing policy update
[2018-12-22 11:08:34.451392 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:34.542422 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:35.612853 UTC] Performing line search
[2018-12-22 11:08:35.742853 UTC] Updating baseline
[2018-12-22 11:08:37.124564 UTC] Computing logging information
-------------------------------------
| Iteration            | 602        |
| ExpectedImprovement  | 0.016492   |
| ActualImprovement    | 0.015724   |
| ImprovementRatio     | 0.95344    |
| MeanKL               | 0.0072395  |
| Entropy              | -0.11939   |
| Perplexity           | 0.88746    |
| AveragePolicyStd     | 0.24057    |
| AveragePolicyStd[0]  | 0.26804    |
| AveragePolicyStd[1]  | 0.29755    |
| AveragePolicyStd[2]  | 0.201      |
| AveragePolicyStd[3]  | 0.24457    |
| AveragePolicyStd[4]  | 0.18039    |
| AveragePolicyStd[5]  | 0.25186    |
| AverageReturn        | 1362.9     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 325.38     |
| AverageEpisodeLength | 932.1      |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.62     |
| TotalNEpisodes       | 17575      |
| TotalNSamples        | 3.0128e+06 |
| ExplainedVariance    | 0.064225   |
-------------------------------------
[2018-12-22 11:08:37.494946 UTC] Saving snapshot
[2018-12-22 11:08:37.495189 UTC] Starting iteration 603
[2018-12-22 11:08:37.495321 UTC] Start collecting samples
[2018-12-22 11:08:40.452852 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:40.533171 UTC] Performing policy update
[2018-12-22 11:08:40.533814 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:40.623062 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:41.690653 UTC] Performing line search
[2018-12-22 11:08:41.824469 UTC] Updating baseline
[2018-12-22 11:08:43.290822 UTC] Computing logging information
-------------------------------------
| Iteration            | 603        |
| ExpectedImprovement  | 0.018232   |
| ActualImprovement    | 0.017478   |
| ImprovementRatio     | 0.95866    |
| MeanKL               | 0.0070121  |
| Entropy              | -0.1203    |
| Perplexity           | 0.88665    |
| AveragePolicyStd     | 0.24058    |
| AveragePolicyStd[0]  | 0.26841    |
| AveragePolicyStd[1]  | 0.29867    |
| AveragePolicyStd[2]  | 0.20133    |
| AveragePolicyStd[3]  | 0.24421    |
| AveragePolicyStd[4]  | 0.17991    |
| AveragePolicyStd[5]  | 0.25097    |
| AverageReturn        | 1359.5     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 328.62     |
| AverageEpisodeLength | 929.57     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.7      |
| TotalNEpisodes       | 17580      |
| TotalNSamples        | 3.0174e+06 |
| ExplainedVariance    | -0.017868  |
-------------------------------------
[2018-12-22 11:08:43.658070 UTC] Saving snapshot
[2018-12-22 11:08:43.658347 UTC] Starting iteration 604
[2018-12-22 11:08:43.658470 UTC] Start collecting samples
[2018-12-22 11:08:46.636560 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:46.716837 UTC] Performing policy update
[2018-12-22 11:08:46.717796 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:46.808473 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:47.883863 UTC] Performing line search
[2018-12-22 11:08:48.017264 UTC] Updating baseline
[2018-12-22 11:08:49.306342 UTC] Computing logging information
-------------------------------------
| Iteration            | 604        |
| ExpectedImprovement  | 0.017626   |
| ActualImprovement    | 0.01697    |
| ImprovementRatio     | 0.96277    |
| MeanKL               | 0.0077737  |
| Entropy              | -0.12799   |
| Perplexity           | 0.87986    |
| AveragePolicyStd     | 0.24025    |
| AveragePolicyStd[0]  | 0.26668    |
| AveragePolicyStd[1]  | 0.29823    |
| AveragePolicyStd[2]  | 0.20176    |
| AveragePolicyStd[3]  | 0.24477    |
| AveragePolicyStd[4]  | 0.1794     |
| AveragePolicyStd[5]  | 0.25063    |
| AverageReturn        | 1347.8     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 345.54     |
| AverageEpisodeLength | 921.82     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.62     |
| TotalNEpisodes       | 17585      |
| TotalNSamples        | 3.0216e+06 |
| ExplainedVariance    | 0.11432    |
-------------------------------------
[2018-12-22 11:08:49.672367 UTC] Saving snapshot
[2018-12-22 11:08:49.672633 UTC] Starting iteration 605
[2018-12-22 11:08:49.672752 UTC] Start collecting samples
[2018-12-22 11:08:52.656028 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:52.738457 UTC] Performing policy update
[2018-12-22 11:08:52.739260 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:52.830276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:08:53.907095 UTC] Performing line search
[2018-12-22 11:08:54.039689 UTC] Updating baseline
[2018-12-22 11:08:55.514142 UTC] Computing logging information
-------------------------------------
| Iteration            | 605        |
| ExpectedImprovement  | 0.016309   |
| ActualImprovement    | 0.015753   |
| ImprovementRatio     | 0.96591    |
| MeanKL               | 0.0074471  |
| Entropy              | -0.12739   |
| Perplexity           | 0.88039    |
| AveragePolicyStd     | 0.24021    |
| AveragePolicyStd[0]  | 0.2672     |
| AveragePolicyStd[1]  | 0.29708    |
| AveragePolicyStd[2]  | 0.20193    |
| AveragePolicyStd[3]  | 0.24459    |
| AveragePolicyStd[4]  | 0.17987    |
| AveragePolicyStd[5]  | 0.25058    |
| AverageReturn        | 1363.4     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 325.88     |
| AverageEpisodeLength | 930.53     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.34     |
| TotalNEpisodes       | 17591      |
| TotalNSamples        | 3.0276e+06 |
| ExplainedVariance    | -0.0073385 |
-------------------------------------
[2018-12-22 11:08:55.884761 UTC] Saving snapshot
[2018-12-22 11:08:55.885006 UTC] Starting iteration 606
[2018-12-22 11:08:55.885124 UTC] Start collecting samples
[2018-12-22 11:08:58.845638 UTC] Computing input variables for policy optimization
[2018-12-22 11:08:58.925439 UTC] Performing policy update
[2018-12-22 11:08:58.926123 UTC] Computing gradient in Euclidean space
[2018-12-22 11:08:59.014967 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:00.096214 UTC] Performing line search
[2018-12-22 11:09:00.225782 UTC] Updating baseline
[2018-12-22 11:09:02.072513 UTC] Computing logging information
-------------------------------------
| Iteration            | 606        |
| ExpectedImprovement  | 0.018828   |
| ActualImprovement    | 0.017533   |
| ImprovementRatio     | 0.9312     |
| MeanKL               | 0.0070426  |
| Entropy              | -0.12072   |
| Perplexity           | 0.88629    |
| AveragePolicyStd     | 0.24044    |
| AveragePolicyStd[0]  | 0.2673     |
| AveragePolicyStd[1]  | 0.29689    |
| AveragePolicyStd[2]  | 0.2022     |
| AveragePolicyStd[3]  | 0.24457    |
| AveragePolicyStd[4]  | 0.18034    |
| AveragePolicyStd[5]  | 0.25135    |
| AverageReturn        | 1364.5     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 326.24     |
| AverageEpisodeLength | 930.53     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.34     |
| TotalNEpisodes       | 17596      |
| TotalNSamples        | 3.0326e+06 |
| ExplainedVariance    | 0.0044236  |
-------------------------------------
[2018-12-22 11:09:02.443439 UTC] Saving snapshot
[2018-12-22 11:09:02.443707 UTC] Starting iteration 607
[2018-12-22 11:09:02.443826 UTC] Start collecting samples
[2018-12-22 11:09:05.433847 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:05.513264 UTC] Performing policy update
[2018-12-22 11:09:05.513969 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:05.603966 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:06.681313 UTC] Performing line search
[2018-12-22 11:09:06.810453 UTC] Updating baseline
[2018-12-22 11:09:08.280797 UTC] Computing logging information
-------------------------------------
| Iteration            | 607        |
| ExpectedImprovement  | 0.02088    |
| ActualImprovement    | 0.018523   |
| ImprovementRatio     | 0.88714    |
| MeanKL               | 0.0069     |
| Entropy              | -0.13612   |
| Perplexity           | 0.87274    |
| AveragePolicyStd     | 0.23984    |
| AveragePolicyStd[0]  | 0.26762    |
| AveragePolicyStd[1]  | 0.2961     |
| AveragePolicyStd[2]  | 0.20179    |
| AveragePolicyStd[3]  | 0.24237    |
| AveragePolicyStd[4]  | 0.1799     |
| AveragePolicyStd[5]  | 0.25124    |
| AverageReturn        | 1367.4     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 314.11     |
| AverageEpisodeLength | 933.29     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.28     |
| TotalNEpisodes       | 17601      |
| TotalNSamples        | 3.0371e+06 |
| ExplainedVariance    | 0.17119    |
-------------------------------------
[2018-12-22 11:09:08.644808 UTC] Saving snapshot
[2018-12-22 11:09:08.645048 UTC] Starting iteration 608
[2018-12-22 11:09:08.645164 UTC] Start collecting samples
[2018-12-22 11:09:11.643627 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:11.724719 UTC] Performing policy update
[2018-12-22 11:09:11.725363 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:11.819856 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:12.899105 UTC] Performing line search
[2018-12-22 11:09:13.027217 UTC] Updating baseline
[2018-12-22 11:09:14.767199 UTC] Computing logging information
-------------------------------------
| Iteration            | 608        |
| ExpectedImprovement  | 0.016362   |
| ActualImprovement    | 0.015821   |
| ImprovementRatio     | 0.96696    |
| MeanKL               | 0.0075239  |
| Entropy              | -0.14057   |
| Perplexity           | 0.86886    |
| AveragePolicyStd     | 0.23962    |
| AveragePolicyStd[0]  | 0.26647    |
| AveragePolicyStd[1]  | 0.29543    |
| AveragePolicyStd[2]  | 0.20189    |
| AveragePolicyStd[3]  | 0.24158    |
| AveragePolicyStd[4]  | 0.17996    |
| AveragePolicyStd[5]  | 0.25239    |
| AverageReturn        | 1388.4     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 287.44     |
| AverageEpisodeLength | 945.02     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.27     |
| TotalNEpisodes       | 17607      |
| TotalNSamples        | 3.0431e+06 |
| ExplainedVariance    | -0.0055266 |
-------------------------------------
[2018-12-22 11:09:15.135169 UTC] Saving snapshot
[2018-12-22 11:09:15.135457 UTC] Starting iteration 609
[2018-12-22 11:09:15.135592 UTC] Start collecting samples
[2018-12-22 11:09:18.106816 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:18.186807 UTC] Performing policy update
[2018-12-22 11:09:18.187591 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:18.279406 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:19.342504 UTC] Performing line search
[2018-12-22 11:09:19.471107 UTC] Updating baseline
[2018-12-22 11:09:20.931872 UTC] Computing logging information
-------------------------------------
| Iteration            | 609        |
| ExpectedImprovement  | 0.016678   |
| ActualImprovement    | 0.016468   |
| ImprovementRatio     | 0.98742    |
| MeanKL               | 0.0070888  |
| Entropy              | -0.14272   |
| Perplexity           | 0.86699    |
| AveragePolicyStd     | 0.23947    |
| AveragePolicyStd[0]  | 0.26609    |
| AveragePolicyStd[1]  | 0.29534    |
| AveragePolicyStd[2]  | 0.20273    |
| AveragePolicyStd[3]  | 0.24137    |
| AveragePolicyStd[4]  | 0.18022    |
| AveragePolicyStd[5]  | 0.2511     |
| AverageReturn        | 1375.9     |
| MinReturn            | 70.069     |
| MaxReturn            | 1529       |
| StdReturn            | 312.13     |
| AverageEpisodeLength | 936.44     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.5      |
| TotalNEpisodes       | 17612      |
| TotalNSamples        | 3.0472e+06 |
| ExplainedVariance    | 0.10324    |
-------------------------------------
[2018-12-22 11:09:21.299939 UTC] Saving snapshot
[2018-12-22 11:09:21.300200 UTC] Starting iteration 610
[2018-12-22 11:09:21.300317 UTC] Start collecting samples
[2018-12-22 11:09:24.294337 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:24.374842 UTC] Performing policy update
[2018-12-22 11:09:24.375501 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:24.466473 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:25.540672 UTC] Performing line search
[2018-12-22 11:09:25.669218 UTC] Updating baseline
[2018-12-22 11:09:27.139540 UTC] Computing logging information
-------------------------------------
| Iteration            | 610        |
| ExpectedImprovement  | 0.017191   |
| ActualImprovement    | 0.016201   |
| ImprovementRatio     | 0.9424     |
| MeanKL               | 0.0071814  |
| Entropy              | -0.13775   |
| Perplexity           | 0.87132    |
| AveragePolicyStd     | 0.23975    |
| AveragePolicyStd[0]  | 0.26566    |
| AveragePolicyStd[1]  | 0.29728    |
| AveragePolicyStd[2]  | 0.2023     |
| AveragePolicyStd[3]  | 0.24147    |
| AveragePolicyStd[4]  | 0.18012    |
| AveragePolicyStd[5]  | 0.25168    |
| AverageReturn        | 1392.1     |
| MinReturn            | 70.069     |
| MaxReturn            | 1537.3     |
| StdReturn            | 285.91     |
| AverageEpisodeLength | 945.47     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.27     |
| TotalNEpisodes       | 17617      |
| TotalNSamples        | 3.0522e+06 |
| ExplainedVariance    | 0.003592   |
-------------------------------------
[2018-12-22 11:09:27.508005 UTC] Saving snapshot
[2018-12-22 11:09:27.516496 UTC] Starting iteration 611
[2018-12-22 11:09:27.516717 UTC] Start collecting samples
[2018-12-22 11:09:30.492246 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:30.571745 UTC] Performing policy update
[2018-12-22 11:09:30.572387 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:30.664653 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:31.735582 UTC] Performing line search
[2018-12-22 11:09:31.863601 UTC] Updating baseline
[2018-12-22 11:09:33.339023 UTC] Computing logging information
-------------------------------------
| Iteration            | 611        |
| ExpectedImprovement  | 0.017687   |
| ActualImprovement    | 0.016583   |
| ImprovementRatio     | 0.93758    |
| MeanKL               | 0.0071338  |
| Entropy              | -0.13041   |
| Perplexity           | 0.87773    |
| AveragePolicyStd     | 0.24005    |
| AveragePolicyStd[0]  | 0.26698    |
| AveragePolicyStd[1]  | 0.29776    |
| AveragePolicyStd[2]  | 0.20303    |
| AveragePolicyStd[3]  | 0.24089    |
| AveragePolicyStd[4]  | 0.18019    |
| AveragePolicyStd[5]  | 0.25147    |
| AverageReturn        | 1392.5     |
| MinReturn            | 70.069     |
| MaxReturn            | 1537.3     |
| StdReturn            | 286.05     |
| AverageEpisodeLength | 945.47     |
| MinEpisodeLength     | 67         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.27     |
| TotalNEpisodes       | 17621      |
| TotalNSamples        | 3.0562e+06 |
| ExplainedVariance    | -0.0081774 |
-------------------------------------
[2018-12-22 11:09:33.708159 UTC] Saving snapshot
[2018-12-22 11:09:33.708461 UTC] Starting iteration 612
[2018-12-22 11:09:33.708598 UTC] Start collecting samples
[2018-12-22 11:09:36.692851 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:36.772813 UTC] Performing policy update
[2018-12-22 11:09:36.773607 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:36.864294 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:37.941280 UTC] Performing line search
[2018-12-22 11:09:38.070494 UTC] Updating baseline
[2018-12-22 11:09:39.446022 UTC] Computing logging information
-------------------------------------
| Iteration            | 612        |
| ExpectedImprovement  | 0.016007   |
| ActualImprovement    | 0.015342   |
| ImprovementRatio     | 0.95843    |
| MeanKL               | 0.0074474  |
| Entropy              | -0.13657   |
| Perplexity           | 0.87235    |
| AveragePolicyStd     | 0.23981    |
| AveragePolicyStd[0]  | 0.2669     |
| AveragePolicyStd[1]  | 0.29759    |
| AveragePolicyStd[2]  | 0.20337    |
| AveragePolicyStd[3]  | 0.24066    |
| AveragePolicyStd[4]  | 0.17971    |
| AveragePolicyStd[5]  | 0.25062    |
| AverageReturn        | 1407.2     |
| MinReturn            | 165.79     |
| MaxReturn            | 1537.3     |
| StdReturn            | 253.67     |
| AverageEpisodeLength | 954.8      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.94     |
| TotalNEpisodes       | 17626      |
| TotalNSamples        | 3.0612e+06 |
| ExplainedVariance    | -0.01406   |
-------------------------------------
[2018-12-22 11:09:39.816551 UTC] Saving snapshot
[2018-12-22 11:09:39.816797 UTC] Starting iteration 613
[2018-12-22 11:09:39.816916 UTC] Start collecting samples
[2018-12-22 11:09:42.806291 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:42.885713 UTC] Performing policy update
[2018-12-22 11:09:42.886464 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:42.977122 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:44.051064 UTC] Performing line search
[2018-12-22 11:09:44.178737 UTC] Updating baseline
[2018-12-22 11:09:46.192515 UTC] Computing logging information
-------------------------------------
| Iteration            | 613        |
| ExpectedImprovement  | 0.020054   |
| ActualImprovement    | 0.018616   |
| ImprovementRatio     | 0.92826    |
| MeanKL               | 0.0071393  |
| Entropy              | -0.12483   |
| Perplexity           | 0.88265    |
| AveragePolicyStd     | 0.24024    |
| AveragePolicyStd[0]  | 0.26813    |
| AveragePolicyStd[1]  | 0.29735    |
| AveragePolicyStd[2]  | 0.20441    |
| AveragePolicyStd[3]  | 0.24108    |
| AveragePolicyStd[4]  | 0.18002    |
| AveragePolicyStd[5]  | 0.25048    |
| AverageReturn        | 1404.5     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 270.53     |
| AverageEpisodeLength | 952.24     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.4      |
| TotalNEpisodes       | 17632      |
| TotalNSamples        | 3.0663e+06 |
| ExplainedVariance    | 0.042391   |
-------------------------------------
[2018-12-22 11:09:46.557935 UTC] Saving snapshot
[2018-12-22 11:09:46.558185 UTC] Starting iteration 614
[2018-12-22 11:09:46.558317 UTC] Start collecting samples
[2018-12-22 11:09:49.499135 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:49.579766 UTC] Performing policy update
[2018-12-22 11:09:49.580360 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:49.673662 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:50.751131 UTC] Performing line search
[2018-12-22 11:09:50.879985 UTC] Updating baseline
[2018-12-22 11:09:52.440738 UTC] Computing logging information
-------------------------------------
| Iteration            | 614        |
| ExpectedImprovement  | 0.017108   |
| ActualImprovement    | 0.016726   |
| ImprovementRatio     | 0.97766    |
| MeanKL               | 0.0071133  |
| Entropy              | -0.12934   |
| Perplexity           | 0.87867    |
| AveragePolicyStd     | 0.24002    |
| AveragePolicyStd[0]  | 0.2683     |
| AveragePolicyStd[1]  | 0.29701    |
| AveragePolicyStd[2]  | 0.20409    |
| AveragePolicyStd[3]  | 0.23983    |
| AveragePolicyStd[4]  | 0.18084    |
| AveragePolicyStd[5]  | 0.25005    |
| AverageReturn        | 1409.7     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 269.91     |
| AverageEpisodeLength | 954.98     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174        |
| TotalNEpisodes       | 17636      |
| TotalNSamples        | 3.0703e+06 |
| ExplainedVariance    | -0.042796  |
-------------------------------------
[2018-12-22 11:09:52.810097 UTC] Saving snapshot
[2018-12-22 11:09:52.810359 UTC] Starting iteration 615
[2018-12-22 11:09:52.810476 UTC] Start collecting samples
[2018-12-22 11:09:55.835309 UTC] Computing input variables for policy optimization
[2018-12-22 11:09:55.923761 UTC] Performing policy update
[2018-12-22 11:09:55.924463 UTC] Computing gradient in Euclidean space
[2018-12-22 11:09:56.016325 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:09:57.084837 UTC] Performing line search
[2018-12-22 11:09:57.212702 UTC] Updating baseline
[2018-12-22 11:09:58.483755 UTC] Computing logging information
-------------------------------------
| Iteration            | 615        |
| ExpectedImprovement  | 0.017148   |
| ActualImprovement    | 0.016384   |
| ImprovementRatio     | 0.9554     |
| MeanKL               | 0.0070707  |
| Entropy              | -0.13132   |
| Perplexity           | 0.87694    |
| AveragePolicyStd     | 0.23991    |
| AveragePolicyStd[0]  | 0.26771    |
| AveragePolicyStd[1]  | 0.29614    |
| AveragePolicyStd[2]  | 0.20467    |
| AveragePolicyStd[3]  | 0.23985    |
| AveragePolicyStd[4]  | 0.18045    |
| AveragePolicyStd[5]  | 0.25064    |
| AverageReturn        | 1399.5     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 287.01     |
| AverageEpisodeLength | 948.33     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.7      |
| TotalNEpisodes       | 17644      |
| TotalNSamples        | 3.0769e+06 |
| ExplainedVariance    | 0.16622    |
-------------------------------------
[2018-12-22 11:09:58.844614 UTC] Saving snapshot
[2018-12-22 11:09:58.844880 UTC] Starting iteration 616
[2018-12-22 11:09:58.845003 UTC] Start collecting samples
[2018-12-22 11:10:01.870822 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:01.954262 UTC] Performing policy update
[2018-12-22 11:10:01.954942 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:02.047652 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:03.123974 UTC] Performing line search
[2018-12-22 11:10:03.251840 UTC] Updating baseline
[2018-12-22 11:10:04.644504 UTC] Computing logging information
-------------------------------------
| Iteration            | 616        |
| ExpectedImprovement  | 0.017729   |
| ActualImprovement    | 0.017222   |
| ImprovementRatio     | 0.97141    |
| MeanKL               | 0.0073808  |
| Entropy              | -0.13397   |
| Perplexity           | 0.87462    |
| AveragePolicyStd     | 0.23978    |
| AveragePolicyStd[0]  | 0.26659    |
| AveragePolicyStd[1]  | 0.29603    |
| AveragePolicyStd[2]  | 0.20446    |
| AveragePolicyStd[3]  | 0.24042    |
| AveragePolicyStd[4]  | 0.18053    |
| AveragePolicyStd[5]  | 0.25066    |
| AverageReturn        | 1394.9     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 290.53     |
| AverageEpisodeLength | 945        |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.72     |
| TotalNEpisodes       | 17650      |
| TotalNSamples        | 3.0826e+06 |
| ExplainedVariance    | 0.12388    |
-------------------------------------
[2018-12-22 11:10:05.015129 UTC] Saving snapshot
[2018-12-22 11:10:05.015400 UTC] Starting iteration 617
[2018-12-22 11:10:05.015540 UTC] Start collecting samples
[2018-12-22 11:10:07.967857 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:08.049253 UTC] Performing policy update
[2018-12-22 11:10:08.050082 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:08.140806 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:09.201964 UTC] Performing line search
[2018-12-22 11:10:09.330091 UTC] Updating baseline
[2018-12-22 11:10:10.880871 UTC] Computing logging information
-------------------------------------
| Iteration            | 617        |
| ExpectedImprovement  | 0.016545   |
| ActualImprovement    | 0.016042   |
| ImprovementRatio     | 0.96959    |
| MeanKL               | 0.0071621  |
| Entropy              | -0.13383   |
| Perplexity           | 0.87474    |
| AveragePolicyStd     | 0.23982    |
| AveragePolicyStd[0]  | 0.26713    |
| AveragePolicyStd[1]  | 0.29652    |
| AveragePolicyStd[2]  | 0.20468    |
| AveragePolicyStd[3]  | 0.24034    |
| AveragePolicyStd[4]  | 0.1802     |
| AveragePolicyStd[5]  | 0.25005    |
| AverageReturn        | 1381.6     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 317.36     |
| AverageEpisodeLength | 935.95     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.8      |
| TotalNEpisodes       | 17654      |
| TotalNSamples        | 3.0857e+06 |
| ExplainedVariance    | 0.10252    |
-------------------------------------
[2018-12-22 11:10:11.241717 UTC] Saving snapshot
[2018-12-22 11:10:11.242013 UTC] Starting iteration 618
[2018-12-22 11:10:11.242132 UTC] Start collecting samples
[2018-12-22 11:10:14.250600 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:14.331327 UTC] Performing policy update
[2018-12-22 11:10:14.331918 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:14.422201 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:15.496656 UTC] Performing line search
[2018-12-22 11:10:15.625692 UTC] Updating baseline
[2018-12-22 11:10:17.358401 UTC] Computing logging information
-------------------------------------
| Iteration            | 618        |
| ExpectedImprovement  | 0.017888   |
| ActualImprovement    | 0.016773   |
| ImprovementRatio     | 0.93765    |
| MeanKL               | 0.0074844  |
| Entropy              | -0.14101   |
| Perplexity           | 0.86848    |
| AveragePolicyStd     | 0.23957    |
| AveragePolicyStd[0]  | 0.26695    |
| AveragePolicyStd[1]  | 0.29626    |
| AveragePolicyStd[2]  | 0.20414    |
| AveragePolicyStd[3]  | 0.24011    |
| AveragePolicyStd[4]  | 0.17965    |
| AveragePolicyStd[5]  | 0.25031    |
| AverageReturn        | 1381.9     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 317.53     |
| AverageEpisodeLength | 935.95     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.8      |
| TotalNEpisodes       | 17661      |
| TotalNSamples        | 3.0927e+06 |
| ExplainedVariance    | -0.0083096 |
-------------------------------------
[2018-12-22 11:10:17.727095 UTC] Saving snapshot
[2018-12-22 11:10:17.727373 UTC] Starting iteration 619
[2018-12-22 11:10:17.727490 UTC] Start collecting samples
[2018-12-22 11:10:20.696958 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:20.775607 UTC] Performing policy update
[2018-12-22 11:10:20.776223 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:20.867151 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:21.934358 UTC] Performing line search
[2018-12-22 11:10:22.062397 UTC] Updating baseline
[2018-12-22 11:10:23.734328 UTC] Computing logging information
-------------------------------------
| Iteration            | 619        |
| ExpectedImprovement  | 0.016878   |
| ActualImprovement    | 0.016085   |
| ImprovementRatio     | 0.95302    |
| MeanKL               | 0.0069806  |
| Entropy              | -0.14285   |
| Perplexity           | 0.86688    |
| AveragePolicyStd     | 0.23945    |
| AveragePolicyStd[0]  | 0.26686    |
| AveragePolicyStd[1]  | 0.2959     |
| AveragePolicyStd[2]  | 0.2033     |
| AveragePolicyStd[3]  | 0.24119    |
| AveragePolicyStd[4]  | 0.18049    |
| AveragePolicyStd[5]  | 0.24897    |
| AverageReturn        | 1387.8     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 315.11     |
| AverageEpisodeLength | 939.18     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.23     |
| TotalNEpisodes       | 17666      |
| TotalNSamples        | 3.0977e+06 |
| ExplainedVariance    | -0.013054  |
-------------------------------------
[2018-12-22 11:10:24.103467 UTC] Saving snapshot
[2018-12-22 11:10:24.103735 UTC] Starting iteration 620
[2018-12-22 11:10:24.103866 UTC] Start collecting samples
[2018-12-22 11:10:27.049935 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:27.127275 UTC] Performing policy update
[2018-12-22 11:10:27.127938 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:27.218104 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:28.289186 UTC] Performing line search
[2018-12-22 11:10:28.417122 UTC] Updating baseline
[2018-12-22 11:10:29.687122 UTC] Computing logging information
-------------------------------------
| Iteration            | 620        |
| ExpectedImprovement  | 0.015812   |
| ActualImprovement    | 0.015065   |
| ImprovementRatio     | 0.95273    |
| MeanKL               | 0.0074306  |
| Entropy              | -0.13837   |
| Perplexity           | 0.87078    |
| AveragePolicyStd     | 0.23963    |
| AveragePolicyStd[0]  | 0.26587    |
| AveragePolicyStd[1]  | 0.29616    |
| AveragePolicyStd[2]  | 0.20349    |
| AveragePolicyStd[3]  | 0.24249    |
| AveragePolicyStd[4]  | 0.18039    |
| AveragePolicyStd[5]  | 0.24935    |
| AverageReturn        | 1387.8     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 315        |
| AverageEpisodeLength | 939.18     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.23     |
| TotalNEpisodes       | 17669      |
| TotalNSamples        | 3.1007e+06 |
| ExplainedVariance    | -0.024032  |
-------------------------------------
[2018-12-22 11:10:30.063631 UTC] Saving snapshot
[2018-12-22 11:10:30.071714 UTC] Starting iteration 621
[2018-12-22 11:10:30.071913 UTC] Start collecting samples
[2018-12-22 11:10:33.081762 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:33.163514 UTC] Performing policy update
[2018-12-22 11:10:33.164276 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:33.255447 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:34.325303 UTC] Performing line search
[2018-12-22 11:10:34.453061 UTC] Updating baseline
[2018-12-22 11:10:35.815971 UTC] Computing logging information
-------------------------------------
| Iteration            | 621        |
| ExpectedImprovement  | 0.018123   |
| ActualImprovement    | 0.017586   |
| ImprovementRatio     | 0.97036    |
| MeanKL               | 0.0072851  |
| Entropy              | -0.12865   |
| Perplexity           | 0.87928    |
| AveragePolicyStd     | 0.24001    |
| AveragePolicyStd[0]  | 0.26586    |
| AveragePolicyStd[1]  | 0.29679    |
| AveragePolicyStd[2]  | 0.20393    |
| AveragePolicyStd[3]  | 0.24292    |
| AveragePolicyStd[4]  | 0.18074    |
| AveragePolicyStd[5]  | 0.2498     |
| AverageReturn        | 1390       |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 313.52     |
| AverageEpisodeLength | 939.91     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.25     |
| TotalNEpisodes       | 17678      |
| TotalNSamples        | 3.1094e+06 |
| ExplainedVariance    | 0.048995   |
-------------------------------------
[2018-12-22 11:10:36.184668 UTC] Saving snapshot
[2018-12-22 11:10:36.185133 UTC] Starting iteration 622
[2018-12-22 11:10:36.185323 UTC] Start collecting samples
[2018-12-22 11:10:39.125259 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:39.204470 UTC] Performing policy update
[2018-12-22 11:10:39.205111 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:39.295628 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:40.378231 UTC] Performing line search
[2018-12-22 11:10:40.506866 UTC] Updating baseline
[2018-12-22 11:10:41.960728 UTC] Computing logging information
-------------------------------------
| Iteration            | 622        |
| ExpectedImprovement  | 0.021851   |
| ActualImprovement    | 0.019916   |
| ImprovementRatio     | 0.91143    |
| MeanKL               | 0.0068094  |
| Entropy              | -0.12474   |
| Perplexity           | 0.88273    |
| AveragePolicyStd     | 0.24014    |
| AveragePolicyStd[0]  | 0.26577    |
| AveragePolicyStd[1]  | 0.29627    |
| AveragePolicyStd[2]  | 0.20392    |
| AveragePolicyStd[3]  | 0.24394    |
| AveragePolicyStd[4]  | 0.18087    |
| AveragePolicyStd[5]  | 0.25009    |
| AverageReturn        | 1389.9     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 313.59     |
| AverageEpisodeLength | 939.91     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.25     |
| TotalNEpisodes       | 17682      |
| TotalNSamples        | 3.1134e+06 |
| ExplainedVariance    | -0.017911  |
-------------------------------------
[2018-12-22 11:10:42.327845 UTC] Saving snapshot
[2018-12-22 11:10:42.328087 UTC] Starting iteration 623
[2018-12-22 11:10:42.328203 UTC] Start collecting samples
[2018-12-22 11:10:45.247295 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:45.325315 UTC] Performing policy update
[2018-12-22 11:10:45.325969 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:45.415546 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:46.491708 UTC] Performing line search
[2018-12-22 11:10:46.619914 UTC] Updating baseline
[2018-12-22 11:10:47.999234 UTC] Computing logging information
-------------------------------------
| Iteration            | 623        |
| ExpectedImprovement  | 0.016155   |
| ActualImprovement    | 0.015542   |
| ImprovementRatio     | 0.9621     |
| MeanKL               | 0.0072759  |
| Entropy              | -0.12978   |
| Perplexity           | 0.87829    |
| AveragePolicyStd     | 0.23992    |
| AveragePolicyStd[0]  | 0.26522    |
| AveragePolicyStd[1]  | 0.29647    |
| AveragePolicyStd[2]  | 0.2038     |
| AveragePolicyStd[3]  | 0.24334    |
| AveragePolicyStd[4]  | 0.18126    |
| AveragePolicyStd[5]  | 0.24941    |
| AverageReturn        | 1401.2     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 293        |
| AverageEpisodeLength | 947.51     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.16     |
| TotalNEpisodes       | 17684      |
| TotalNSamples        | 3.1154e+06 |
| ExplainedVariance    | 0.34078    |
-------------------------------------
[2018-12-22 11:10:48.368743 UTC] Saving snapshot
[2018-12-22 11:10:48.368984 UTC] Starting iteration 624
[2018-12-22 11:10:48.369101 UTC] Start collecting samples
[2018-12-22 11:10:51.371773 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:51.454193 UTC] Performing policy update
[2018-12-22 11:10:51.454787 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:51.547035 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:52.631052 UTC] Performing line search
[2018-12-22 11:10:52.759305 UTC] Updating baseline
[2018-12-22 11:10:54.501209 UTC] Computing logging information
-------------------------------------
| Iteration            | 624        |
| ExpectedImprovement  | 0.017831   |
| ActualImprovement    | 0.017208   |
| ImprovementRatio     | 0.96502    |
| MeanKL               | 0.0070648  |
| Entropy              | -0.13247   |
| Perplexity           | 0.87593    |
| AveragePolicyStd     | 0.23987    |
| AveragePolicyStd[0]  | 0.26447    |
| AveragePolicyStd[1]  | 0.29701    |
| AveragePolicyStd[2]  | 0.20346    |
| AveragePolicyStd[3]  | 0.24403    |
| AveragePolicyStd[4]  | 0.18051    |
| AveragePolicyStd[5]  | 0.24973    |
| AverageReturn        | 1403.2     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 293.61     |
| AverageEpisodeLength | 947.51     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.16     |
| TotalNEpisodes       | 17692      |
| TotalNSamples        | 3.1234e+06 |
| ExplainedVariance    | 0.059268   |
-------------------------------------
[2018-12-22 11:10:54.869788 UTC] Saving snapshot
[2018-12-22 11:10:54.870041 UTC] Starting iteration 625
[2018-12-22 11:10:54.870179 UTC] Start collecting samples
[2018-12-22 11:10:57.838728 UTC] Computing input variables for policy optimization
[2018-12-22 11:10:57.919914 UTC] Performing policy update
[2018-12-22 11:10:57.920505 UTC] Computing gradient in Euclidean space
[2018-12-22 11:10:58.015979 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:10:59.082083 UTC] Performing line search
[2018-12-22 11:10:59.211475 UTC] Updating baseline
[2018-12-22 11:11:00.495485 UTC] Computing logging information
-------------------------------------
| Iteration            | 625        |
| ExpectedImprovement  | 0.018678   |
| ActualImprovement    | 0.017225   |
| ImprovementRatio     | 0.92217    |
| MeanKL               | 0.007211   |
| Entropy              | -0.13493   |
| Perplexity           | 0.87378    |
| AveragePolicyStd     | 0.23978    |
| AveragePolicyStd[0]  | 0.26461    |
| AveragePolicyStd[1]  | 0.2969     |
| AveragePolicyStd[2]  | 0.20349    |
| AveragePolicyStd[3]  | 0.24413    |
| AveragePolicyStd[4]  | 0.18027    |
| AveragePolicyStd[5]  | 0.24927    |
| AverageReturn        | 1412.1     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 283.03     |
| AverageEpisodeLength | 952.92     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.77     |
| TotalNEpisodes       | 17697      |
| TotalNSamples        | 3.1284e+06 |
| ExplainedVariance    | -0.026073  |
-------------------------------------
[2018-12-22 11:11:00.864987 UTC] Saving snapshot
[2018-12-22 11:11:00.865258 UTC] Starting iteration 626
[2018-12-22 11:11:00.865389 UTC] Start collecting samples
[2018-12-22 11:11:03.827852 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:03.906793 UTC] Performing policy update
[2018-12-22 11:11:03.907684 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:04.001714 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:05.075603 UTC] Performing line search
[2018-12-22 11:11:05.205547 UTC] Updating baseline
[2018-12-22 11:11:06.761546 UTC] Computing logging information
-------------------------------------
| Iteration            | 626        |
| ExpectedImprovement  | 0.017338   |
| ActualImprovement    | 0.016369   |
| ImprovementRatio     | 0.9441     |
| MeanKL               | 0.0073261  |
| Entropy              | -0.13609   |
| Perplexity           | 0.87276    |
| AveragePolicyStd     | 0.2397     |
| AveragePolicyStd[0]  | 0.2646     |
| AveragePolicyStd[1]  | 0.29637    |
| AveragePolicyStd[2]  | 0.20402    |
| AveragePolicyStd[3]  | 0.24458    |
| AveragePolicyStd[4]  | 0.18028    |
| AveragePolicyStd[5]  | 0.24832    |
| AverageReturn        | 1399.2     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 298.51     |
| AverageEpisodeLength | 943.64     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.68     |
| TotalNEpisodes       | 17702      |
| TotalNSamples        | 3.1324e+06 |
| ExplainedVariance    | 0.15768    |
-------------------------------------
[2018-12-22 11:11:07.133500 UTC] Saving snapshot
[2018-12-22 11:11:07.133781 UTC] Starting iteration 627
[2018-12-22 11:11:07.133907 UTC] Start collecting samples
[2018-12-22 11:11:10.110976 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:10.190919 UTC] Performing policy update
[2018-12-22 11:11:10.191712 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:10.282568 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:11.357018 UTC] Performing line search
[2018-12-22 11:11:11.485799 UTC] Updating baseline
[2018-12-22 11:11:13.442880 UTC] Computing logging information
-------------------------------------
| Iteration            | 627        |
| ExpectedImprovement  | 0.017228   |
| ActualImprovement    | 0.016079   |
| ImprovementRatio     | 0.9333     |
| MeanKL               | 0.0072496  |
| Entropy              | -0.12838   |
| Perplexity           | 0.87952    |
| AveragePolicyStd     | 0.23998    |
| AveragePolicyStd[0]  | 0.26508    |
| AveragePolicyStd[1]  | 0.29701    |
| AveragePolicyStd[2]  | 0.20472    |
| AveragePolicyStd[3]  | 0.24406    |
| AveragePolicyStd[4]  | 0.18077    |
| AveragePolicyStd[5]  | 0.24826    |
| AverageReturn        | 1392       |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 305.48     |
| AverageEpisodeLength | 938.82     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.18     |
| TotalNEpisodes       | 17706      |
| TotalNSamples        | 3.1359e+06 |
| ExplainedVariance    | 0.1303     |
-------------------------------------
[2018-12-22 11:11:13.813092 UTC] Saving snapshot
[2018-12-22 11:11:13.813340 UTC] Starting iteration 628
[2018-12-22 11:11:13.813460 UTC] Start collecting samples
[2018-12-22 11:11:16.809068 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:16.889199 UTC] Performing policy update
[2018-12-22 11:11:16.889949 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:16.979542 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:18.055484 UTC] Performing line search
[2018-12-22 11:11:18.184218 UTC] Updating baseline
[2018-12-22 11:11:19.668578 UTC] Computing logging information
-------------------------------------
| Iteration            | 628        |
| ExpectedImprovement  | 0.01644    |
| ActualImprovement    | 0.015318   |
| ImprovementRatio     | 0.93174    |
| MeanKL               | 0.007589   |
| Entropy              | -0.13668   |
| Perplexity           | 0.87225    |
| AveragePolicyStd     | 0.23969    |
| AveragePolicyStd[0]  | 0.26456    |
| AveragePolicyStd[1]  | 0.29807    |
| AveragePolicyStd[2]  | 0.20424    |
| AveragePolicyStd[3]  | 0.24339    |
| AveragePolicyStd[4]  | 0.18061    |
| AveragePolicyStd[5]  | 0.24728    |
| AverageReturn        | 1407.1     |
| MinReturn            | 73.7       |
| MaxReturn            | 1537.3     |
| StdReturn            | 280.05     |
| AverageEpisodeLength | 947.4      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.27     |
| TotalNEpisodes       | 17712      |
| TotalNSamples        | 3.1419e+06 |
| ExplainedVariance    | -0.0056026 |
-------------------------------------
[2018-12-22 11:11:20.040032 UTC] Saving snapshot
[2018-12-22 11:11:20.040276 UTC] Starting iteration 629
[2018-12-22 11:11:20.040414 UTC] Start collecting samples
[2018-12-22 11:11:23.013048 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:23.092029 UTC] Performing policy update
[2018-12-22 11:11:23.092630 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:23.183678 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:24.258716 UTC] Performing line search
[2018-12-22 11:11:24.386687 UTC] Updating baseline
[2018-12-22 11:11:25.731765 UTC] Computing logging information
-------------------------------------
| Iteration            | 629        |
| ExpectedImprovement  | 0.016399   |
| ActualImprovement    | 0.015449   |
| ImprovementRatio     | 0.94207    |
| MeanKL               | 0.0072746  |
| Entropy              | -0.14656   |
| Perplexity           | 0.86367    |
| AveragePolicyStd     | 0.23932    |
| AveragePolicyStd[0]  | 0.26402    |
| AveragePolicyStd[1]  | 0.29721    |
| AveragePolicyStd[2]  | 0.20416    |
| AveragePolicyStd[3]  | 0.24394    |
| AveragePolicyStd[4]  | 0.17951    |
| AveragePolicyStd[5]  | 0.24712    |
| AverageReturn        | 1406.8     |
| MinReturn            | 73.7       |
| MaxReturn            | 1535.7     |
| StdReturn            | 280.02     |
| AverageEpisodeLength | 947.4      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.27     |
| TotalNEpisodes       | 17716      |
| TotalNSamples        | 3.1459e+06 |
| ExplainedVariance    | 0.0014981  |
-------------------------------------
[2018-12-22 11:11:26.102014 UTC] Saving snapshot
[2018-12-22 11:11:26.102256 UTC] Starting iteration 630
[2018-12-22 11:11:26.102386 UTC] Start collecting samples
[2018-12-22 11:11:29.067217 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:29.146582 UTC] Performing policy update
[2018-12-22 11:11:29.147149 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:29.236729 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:30.305859 UTC] Performing line search
[2018-12-22 11:11:30.434582 UTC] Updating baseline
[2018-12-22 11:11:31.813617 UTC] Computing logging information
-------------------------------------
| Iteration            | 630        |
| ExpectedImprovement  | 0.016616   |
| ActualImprovement    | 0.015365   |
| ImprovementRatio     | 0.92469    |
| MeanKL               | 0.0070936  |
| Entropy              | -0.15268   |
| Perplexity           | 0.8584     |
| AveragePolicyStd     | 0.2391     |
| AveragePolicyStd[0]  | 0.26529    |
| AveragePolicyStd[1]  | 0.29653    |
| AveragePolicyStd[2]  | 0.20397    |
| AveragePolicyStd[3]  | 0.24246    |
| AveragePolicyStd[4]  | 0.17911    |
| AveragePolicyStd[5]  | 0.24725    |
| AverageReturn        | 1406.3     |
| MinReturn            | 73.7       |
| MaxReturn            | 1535.7     |
| StdReturn            | 279.92     |
| AverageEpisodeLength | 947.19     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.22     |
| TotalNEpisodes       | 17721      |
| TotalNSamples        | 3.1509e+06 |
| ExplainedVariance    | 0.1161     |
-------------------------------------
[2018-12-22 11:11:32.181921 UTC] Saving snapshot
[2018-12-22 11:11:32.190124 UTC] Starting iteration 631
[2018-12-22 11:11:32.190362 UTC] Start collecting samples
[2018-12-22 11:11:35.142529 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:35.223134 UTC] Performing policy update
[2018-12-22 11:11:35.223738 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:35.313981 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:36.376977 UTC] Performing line search
[2018-12-22 11:11:36.504034 UTC] Updating baseline
[2018-12-22 11:11:37.855171 UTC] Computing logging information
-------------------------------------
| Iteration            | 631        |
| ExpectedImprovement  | 0.015541   |
| ActualImprovement    | 0.014975   |
| ImprovementRatio     | 0.9636     |
| MeanKL               | 0.0074043  |
| Entropy              | -0.15762   |
| Perplexity           | 0.85418    |
| AveragePolicyStd     | 0.23887    |
| AveragePolicyStd[0]  | 0.26481    |
| AveragePolicyStd[1]  | 0.29591    |
| AveragePolicyStd[2]  | 0.20357    |
| AveragePolicyStd[3]  | 0.2427     |
| AveragePolicyStd[4]  | 0.17949    |
| AveragePolicyStd[5]  | 0.24672    |
| AverageReturn        | 1406.4     |
| MinReturn            | 73.7       |
| MaxReturn            | 1555.3     |
| StdReturn            | 280.01     |
| AverageEpisodeLength | 947.19     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.22     |
| TotalNEpisodes       | 17727      |
| TotalNSamples        | 3.1569e+06 |
| ExplainedVariance    | 0.0042432  |
-------------------------------------
[2018-12-22 11:11:38.226540 UTC] Saving snapshot
[2018-12-22 11:11:38.226802 UTC] Starting iteration 632
[2018-12-22 11:11:38.226925 UTC] Start collecting samples
[2018-12-22 11:11:41.153413 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:41.230911 UTC] Performing policy update
[2018-12-22 11:11:41.231477 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:41.321350 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:42.381688 UTC] Performing line search
[2018-12-22 11:11:42.508988 UTC] Updating baseline
[2018-12-22 11:11:44.136042 UTC] Computing logging information
-------------------------------------
| Iteration            | 632        |
| ExpectedImprovement  | 0.016582   |
| ActualImprovement    | 0.015536   |
| ImprovementRatio     | 0.93687    |
| MeanKL               | 0.0075898  |
| Entropy              | -0.15468   |
| Perplexity           | 0.85669    |
| AveragePolicyStd     | 0.239      |
| AveragePolicyStd[0]  | 0.26557    |
| AveragePolicyStd[1]  | 0.29562    |
| AveragePolicyStd[2]  | 0.20285    |
| AveragePolicyStd[3]  | 0.24298    |
| AveragePolicyStd[4]  | 0.17963    |
| AveragePolicyStd[5]  | 0.24737    |
| AverageReturn        | 1406       |
| MinReturn            | 73.7       |
| MaxReturn            | 1555.3     |
| StdReturn            | 279.92     |
| AverageEpisodeLength | 947.19     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.22     |
| TotalNEpisodes       | 17731      |
| TotalNSamples        | 3.1609e+06 |
| ExplainedVariance    | 0.010113   |
-------------------------------------
[2018-12-22 11:11:44.502286 UTC] Saving snapshot
[2018-12-22 11:11:44.502549 UTC] Starting iteration 633
[2018-12-22 11:11:44.502676 UTC] Start collecting samples
[2018-12-22 11:11:47.468495 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:47.547812 UTC] Performing policy update
[2018-12-22 11:11:47.548478 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:47.637863 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:48.705420 UTC] Performing line search
[2018-12-22 11:11:48.833030 UTC] Updating baseline
[2018-12-22 11:11:50.107385 UTC] Computing logging information
-------------------------------------
| Iteration            | 633        |
| ExpectedImprovement  | 0.019231   |
| ActualImprovement    | 0.017783   |
| ImprovementRatio     | 0.92471    |
| MeanKL               | 0.0073105  |
| Entropy              | -0.16256   |
| Perplexity           | 0.84996    |
| AveragePolicyStd     | 0.23877    |
| AveragePolicyStd[0]  | 0.26515    |
| AveragePolicyStd[1]  | 0.29612    |
| AveragePolicyStd[2]  | 0.20204    |
| AveragePolicyStd[3]  | 0.24329    |
| AveragePolicyStd[4]  | 0.17873    |
| AveragePolicyStd[5]  | 0.24732    |
| AverageReturn        | 1419.9     |
| MinReturn            | 109.28     |
| MaxReturn            | 1555.3     |
| StdReturn            | 245.9      |
| AverageEpisodeLength | 956.48     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.3      |
| TotalNEpisodes       | 17737      |
| TotalNSamples        | 3.1669e+06 |
| ExplainedVariance    | -0.0081786 |
-------------------------------------
[2018-12-22 11:11:50.475353 UTC] Saving snapshot
[2018-12-22 11:11:50.475613 UTC] Starting iteration 634
[2018-12-22 11:11:50.475740 UTC] Start collecting samples
[2018-12-22 11:11:53.435690 UTC] Computing input variables for policy optimization
[2018-12-22 11:11:53.515015 UTC] Performing policy update
[2018-12-22 11:11:53.515710 UTC] Computing gradient in Euclidean space
[2018-12-22 11:11:53.605918 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:11:54.674744 UTC] Performing line search
[2018-12-22 11:11:54.803545 UTC] Updating baseline
[2018-12-22 11:11:56.785458 UTC] Computing logging information
-------------------------------------
| Iteration            | 634        |
| ExpectedImprovement  | 0.01577    |
| ActualImprovement    | 0.01522    |
| ImprovementRatio     | 0.96513    |
| MeanKL               | 0.0074138  |
| Entropy              | -0.16731   |
| Perplexity           | 0.84593    |
| AveragePolicyStd     | 0.23857    |
| AveragePolicyStd[0]  | 0.2646     |
| AveragePolicyStd[1]  | 0.29586    |
| AveragePolicyStd[2]  | 0.20196    |
| AveragePolicyStd[3]  | 0.24373    |
| AveragePolicyStd[4]  | 0.17869    |
| AveragePolicyStd[5]  | 0.24659    |
| AverageReturn        | 1441       |
| MinReturn            | 109.28     |
| MaxReturn            | 1555.3     |
| StdReturn            | 198.94     |
| AverageEpisodeLength | 970.16     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.3      |
| TotalNEpisodes       | 17743      |
| TotalNSamples        | 3.1729e+06 |
| ExplainedVariance    | 0.0011588  |
-------------------------------------
[2018-12-22 11:11:57.154350 UTC] Saving snapshot
[2018-12-22 11:11:57.154639 UTC] Starting iteration 635
[2018-12-22 11:11:57.154814 UTC] Start collecting samples
[2018-12-22 11:12:00.121563 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:00.200747 UTC] Performing policy update
[2018-12-22 11:12:00.201497 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:00.293047 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:01.361382 UTC] Performing line search
[2018-12-22 11:12:01.489326 UTC] Updating baseline
[2018-12-22 11:12:02.691797 UTC] Computing logging information
-------------------------------------
| Iteration            | 635        |
| ExpectedImprovement  | 0.015366   |
| ActualImprovement    | 0.014396   |
| ImprovementRatio     | 0.93684    |
| MeanKL               | 0.0075786  |
| Entropy              | -0.16754   |
| Perplexity           | 0.84574    |
| AveragePolicyStd     | 0.23854    |
| AveragePolicyStd[0]  | 0.26392    |
| AveragePolicyStd[1]  | 0.296      |
| AveragePolicyStd[2]  | 0.20155    |
| AveragePolicyStd[3]  | 0.24451    |
| AveragePolicyStd[4]  | 0.17922    |
| AveragePolicyStd[5]  | 0.24602    |
| AverageReturn        | 1446.1     |
| MinReturn            | 109.28     |
| MaxReturn            | 1555.3     |
| StdReturn            | 192.58     |
| AverageEpisodeLength | 973.49     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.6      |
| TotalNEpisodes       | 17747      |
| TotalNSamples        | 3.1769e+06 |
| ExplainedVariance    | -0.02544   |
-------------------------------------
[2018-12-22 11:12:03.061600 UTC] Saving snapshot
[2018-12-22 11:12:03.061879 UTC] Starting iteration 636
[2018-12-22 11:12:03.062006 UTC] Start collecting samples
[2018-12-22 11:12:06.070383 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:06.153962 UTC] Performing policy update
[2018-12-22 11:12:06.154593 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:06.248404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:07.356790 UTC] Performing line search
[2018-12-22 11:12:07.489682 UTC] Updating baseline
[2018-12-22 11:12:09.471562 UTC] Computing logging information
-------------------------------------
| Iteration            | 636        |
| ExpectedImprovement  | 0.020987   |
| ActualImprovement    | 0.018684   |
| ImprovementRatio     | 0.89029    |
| MeanKL               | 0.0070708  |
| Entropy              | -0.16631   |
| Perplexity           | 0.84679    |
| AveragePolicyStd     | 0.23859    |
| AveragePolicyStd[0]  | 0.26452    |
| AveragePolicyStd[1]  | 0.29611    |
| AveragePolicyStd[2]  | 0.20125    |
| AveragePolicyStd[3]  | 0.24442    |
| AveragePolicyStd[4]  | 0.17956    |
| AveragePolicyStd[5]  | 0.24568    |
| AverageReturn        | 1446.6     |
| MinReturn            | 109.28     |
| MaxReturn            | 1555.3     |
| StdReturn            | 192.79     |
| AverageEpisodeLength | 973.49     |
| MinEpisodeLength     | 95         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.6      |
| TotalNEpisodes       | 17752      |
| TotalNSamples        | 3.1819e+06 |
| ExplainedVariance    | 0.0091852  |
-------------------------------------
[2018-12-22 11:12:09.868143 UTC] Saving snapshot
[2018-12-22 11:12:09.868409 UTC] Starting iteration 637
[2018-12-22 11:12:09.868592 UTC] Start collecting samples
[2018-12-22 11:12:13.073099 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:13.158377 UTC] Performing policy update
[2018-12-22 11:12:13.158987 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:13.254064 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:14.357041 UTC] Performing line search
[2018-12-22 11:12:14.491144 UTC] Updating baseline
[2018-12-22 11:12:15.743145 UTC] Computing logging information
-------------------------------------
| Iteration            | 637        |
| ExpectedImprovement  | 0.01783    |
| ActualImprovement    | 0.016964   |
| ImprovementRatio     | 0.95144    |
| MeanKL               | 0.0076394  |
| Entropy              | -0.18059   |
| Perplexity           | 0.83478    |
| AveragePolicyStd     | 0.23803    |
| AveragePolicyStd[0]  | 0.26353    |
| AveragePolicyStd[1]  | 0.29613    |
| AveragePolicyStd[2]  | 0.20153    |
| AveragePolicyStd[3]  | 0.2432     |
| AveragePolicyStd[4]  | 0.17888    |
| AveragePolicyStd[5]  | 0.24489    |
| AverageReturn        | 1458.5     |
| MinReturn            | 717.61     |
| MaxReturn            | 1555.3     |
| StdReturn            | 140.2      |
| AverageEpisodeLength | 980.76     |
| MinEpisodeLength     | 518        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 86.539     |
| TotalNEpisodes       | 17759      |
| TotalNSamples        | 3.1887e+06 |
| ExplainedVariance    | 0.050024   |
-------------------------------------
[2018-12-22 11:12:16.129772 UTC] Saving snapshot
[2018-12-22 11:12:16.130025 UTC] Starting iteration 638
[2018-12-22 11:12:16.130143 UTC] Start collecting samples
[2018-12-22 11:12:19.098873 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:19.176431 UTC] Performing policy update
[2018-12-22 11:12:19.177090 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:19.267169 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:20.331022 UTC] Performing line search
[2018-12-22 11:12:20.458488 UTC] Updating baseline
[2018-12-22 11:12:22.044006 UTC] Computing logging information
-------------------------------------
| Iteration            | 638        |
| ExpectedImprovement  | 0.018651   |
| ActualImprovement    | 0.017332   |
| ImprovementRatio     | 0.92927    |
| MeanKL               | 0.0074084  |
| Entropy              | -0.18106   |
| Perplexity           | 0.83438    |
| AveragePolicyStd     | 0.23808    |
| AveragePolicyStd[0]  | 0.26327    |
| AveragePolicyStd[1]  | 0.29737    |
| AveragePolicyStd[2]  | 0.20039    |
| AveragePolicyStd[3]  | 0.24371    |
| AveragePolicyStd[4]  | 0.17894    |
| AveragePolicyStd[5]  | 0.24478    |
| AverageReturn        | 1453.4     |
| MinReturn            | 717.61     |
| MaxReturn            | 1555.3     |
| StdReturn            | 145.65     |
| AverageEpisodeLength | 978.11     |
| MinEpisodeLength     | 518        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 89.901     |
| TotalNEpisodes       | 17763      |
| TotalNSamples        | 3.1925e+06 |
| ExplainedVariance    | 0.099318   |
-------------------------------------
[2018-12-22 11:12:22.412404 UTC] Saving snapshot
[2018-12-22 11:12:22.412699 UTC] Starting iteration 639
[2018-12-22 11:12:22.412831 UTC] Start collecting samples
[2018-12-22 11:12:25.334469 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:25.412950 UTC] Performing policy update
[2018-12-22 11:12:25.413952 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:25.504125 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:26.586874 UTC] Performing line search
[2018-12-22 11:12:26.715397 UTC] Updating baseline
[2018-12-22 11:12:28.133873 UTC] Computing logging information
-------------------------------------
| Iteration            | 639        |
| ExpectedImprovement  | 0.018071   |
| ActualImprovement    | 0.016888   |
| ImprovementRatio     | 0.93451    |
| MeanKL               | 0.007054   |
| Entropy              | -0.18012   |
| Perplexity           | 0.83517    |
| AveragePolicyStd     | 0.23812    |
| AveragePolicyStd[0]  | 0.26237    |
| AveragePolicyStd[1]  | 0.29758    |
| AveragePolicyStd[2]  | 0.20011    |
| AveragePolicyStd[3]  | 0.24401    |
| AveragePolicyStd[4]  | 0.17896    |
| AveragePolicyStd[5]  | 0.2457     |
| AverageReturn        | 1453.5     |
| MinReturn            | 717.61     |
| MaxReturn            | 1555.3     |
| StdReturn            | 145.67     |
| AverageEpisodeLength | 978.11     |
| MinEpisodeLength     | 518        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 89.901     |
| TotalNEpisodes       | 17766      |
| TotalNSamples        | 3.1955e+06 |
| ExplainedVariance    | 0.044159   |
-------------------------------------
[2018-12-22 11:12:28.505209 UTC] Saving snapshot
[2018-12-22 11:12:28.505489 UTC] Starting iteration 640
[2018-12-22 11:12:28.505628 UTC] Start collecting samples
[2018-12-22 11:12:31.523047 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:31.605264 UTC] Performing policy update
[2018-12-22 11:12:31.606115 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:31.695411 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:32.754865 UTC] Performing line search
[2018-12-22 11:12:32.886426 UTC] Updating baseline
[2018-12-22 11:12:34.160998 UTC] Computing logging information
-------------------------------------
| Iteration            | 640        |
| ExpectedImprovement  | 0.017897   |
| ActualImprovement    | 0.017169   |
| ImprovementRatio     | 0.95932    |
| MeanKL               | 0.0075798  |
| Entropy              | -0.19128   |
| Perplexity           | 0.8259     |
| AveragePolicyStd     | 0.23777    |
| AveragePolicyStd[0]  | 0.26302    |
| AveragePolicyStd[1]  | 0.29773    |
| AveragePolicyStd[2]  | 0.19929    |
| AveragePolicyStd[3]  | 0.24271    |
| AveragePolicyStd[4]  | 0.17805    |
| AveragePolicyStd[5]  | 0.24579    |
| AverageReturn        | 1445.3     |
| MinReturn            | 717.61     |
| MaxReturn            | 1555.3     |
| StdReturn            | 166.05     |
| AverageEpisodeLength | 972.52     |
| MinEpisodeLength     | 518        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 103.06     |
| TotalNEpisodes       | 17776      |
| TotalNSamples        | 3.2046e+06 |
| ExplainedVariance    | 0.1505     |
-------------------------------------
[2018-12-22 11:12:34.527003 UTC] Saving snapshot
[2018-12-22 11:12:34.535077 UTC] Starting iteration 641
[2018-12-22 11:12:34.535279 UTC] Start collecting samples
[2018-12-22 11:12:37.487330 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:37.565448 UTC] Performing policy update
[2018-12-22 11:12:37.566150 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:37.655415 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:38.621195 UTC] Performing line search
[2018-12-22 11:12:38.737127 UTC] Updating baseline
[2018-12-22 11:12:40.034796 UTC] Computing logging information
-------------------------------------
| Iteration            | 641        |
| ExpectedImprovement  | 0.01825    |
| ActualImprovement    | 0.016893   |
| ImprovementRatio     | 0.92564    |
| MeanKL               | 0.0072298  |
| Entropy              | -0.18308   |
| Perplexity           | 0.8327     |
| AveragePolicyStd     | 0.23815    |
| AveragePolicyStd[0]  | 0.26349    |
| AveragePolicyStd[1]  | 0.29904    |
| AveragePolicyStd[2]  | 0.19952    |
| AveragePolicyStd[3]  | 0.24295    |
| AveragePolicyStd[4]  | 0.17783    |
| AveragePolicyStd[5]  | 0.24606    |
| AverageReturn        | 1422.4     |
| MinReturn            | 250.26     |
| MaxReturn            | 1555.3     |
| StdReturn            | 230.91     |
| AverageEpisodeLength | 957.15     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.19     |
| TotalNEpisodes       | 17781      |
| TotalNSamples        | 3.2081e+06 |
| ExplainedVariance    | 0.28749    |
-------------------------------------
[2018-12-22 11:12:40.400197 UTC] Saving snapshot
[2018-12-22 11:12:40.400496 UTC] Starting iteration 642
[2018-12-22 11:12:40.400641 UTC] Start collecting samples
[2018-12-22 11:12:43.330081 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:43.409524 UTC] Performing policy update
[2018-12-22 11:12:43.411262 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:43.503549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:44.577445 UTC] Performing line search
[2018-12-22 11:12:44.706724 UTC] Updating baseline
[2018-12-22 11:12:46.267411 UTC] Computing logging information
-------------------------------------
| Iteration            | 642        |
| ExpectedImprovement  | 0.017456   |
| ActualImprovement    | 0.016259   |
| ImprovementRatio     | 0.9314     |
| MeanKL               | 0.0071772  |
| Entropy              | -0.18411   |
| Perplexity           | 0.83184    |
| AveragePolicyStd     | 0.23804    |
| AveragePolicyStd[0]  | 0.26291    |
| AveragePolicyStd[1]  | 0.29735    |
| AveragePolicyStd[2]  | 0.19988    |
| AveragePolicyStd[3]  | 0.24338    |
| AveragePolicyStd[4]  | 0.17784    |
| AveragePolicyStd[5]  | 0.24685    |
| AverageReturn        | 1409.6     |
| MinReturn            | 128.5      |
| MaxReturn            | 1555.3     |
| StdReturn            | 264.43     |
| AverageEpisodeLength | 948.31     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.17     |
| TotalNEpisodes       | 17785      |
| TotalNSamples        | 3.2112e+06 |
| ExplainedVariance    | 0.26163    |
-------------------------------------
[2018-12-22 11:12:46.670343 UTC] Saving snapshot
[2018-12-22 11:12:46.670822 UTC] Starting iteration 643
[2018-12-22 11:12:46.671051 UTC] Start collecting samples
[2018-12-22 11:12:49.916362 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:50.006740 UTC] Performing policy update
[2018-12-22 11:12:50.007315 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:50.101278 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:51.180965 UTC] Performing line search
[2018-12-22 11:12:51.308551 UTC] Updating baseline
[2018-12-22 11:12:52.494266 UTC] Computing logging information
-------------------------------------
| Iteration            | 643        |
| ExpectedImprovement  | 0.018553   |
| ActualImprovement    | 0.01753    |
| ImprovementRatio     | 0.94484    |
| MeanKL               | 0.0068053  |
| Entropy              | -0.18465   |
| Perplexity           | 0.83139    |
| AveragePolicyStd     | 0.23803    |
| AveragePolicyStd[0]  | 0.26284    |
| AveragePolicyStd[1]  | 0.29766    |
| AveragePolicyStd[2]  | 0.19991    |
| AveragePolicyStd[3]  | 0.24319    |
| AveragePolicyStd[4]  | 0.17771    |
| AveragePolicyStd[5]  | 0.24688    |
| AverageReturn        | 1407.1     |
| MinReturn            | 128.5      |
| MaxReturn            | 1555.3     |
| StdReturn            | 263.96     |
| AverageEpisodeLength | 947.7      |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.09     |
| TotalNEpisodes       | 17792      |
| TotalNSamples        | 3.2181e+06 |
| ExplainedVariance    | 0.012117   |
-------------------------------------
[2018-12-22 11:12:52.858928 UTC] Saving snapshot
[2018-12-22 11:12:52.859173 UTC] Starting iteration 644
[2018-12-22 11:12:52.859292 UTC] Start collecting samples
[2018-12-22 11:12:55.820310 UTC] Computing input variables for policy optimization
[2018-12-22 11:12:55.900390 UTC] Performing policy update
[2018-12-22 11:12:55.901048 UTC] Computing gradient in Euclidean space
[2018-12-22 11:12:55.995152 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:12:57.059598 UTC] Performing line search
[2018-12-22 11:12:57.186968 UTC] Updating baseline
[2018-12-22 11:12:58.917211 UTC] Computing logging information
-------------------------------------
| Iteration            | 644        |
| ExpectedImprovement  | 0.021309   |
| ActualImprovement    | 0.019342   |
| ImprovementRatio     | 0.90767    |
| MeanKL               | 0.0069122  |
| Entropy              | -0.18711   |
| Perplexity           | 0.82935    |
| AveragePolicyStd     | 0.23791    |
| AveragePolicyStd[0]  | 0.26317    |
| AveragePolicyStd[1]  | 0.29716    |
| AveragePolicyStd[2]  | 0.20005    |
| AveragePolicyStd[3]  | 0.24297    |
| AveragePolicyStd[4]  | 0.1778     |
| AveragePolicyStd[5]  | 0.2463     |
| AverageReturn        | 1384.6     |
| MinReturn            | 128.5      |
| MaxReturn            | 1555.3     |
| StdReturn            | 301.52     |
| AverageEpisodeLength | 933.04     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.9      |
| TotalNEpisodes       | 17797      |
| TotalNSamples        | 3.2217e+06 |
| ExplainedVariance    | 0.28009    |
-------------------------------------
[2018-12-22 11:12:59.283587 UTC] Saving snapshot
[2018-12-22 11:12:59.283867 UTC] Starting iteration 645
[2018-12-22 11:12:59.283987 UTC] Start collecting samples
[2018-12-22 11:13:02.256234 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:02.335720 UTC] Performing policy update
[2018-12-22 11:13:02.336410 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:02.425705 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:03.493043 UTC] Performing line search
[2018-12-22 11:13:03.622341 UTC] Updating baseline
[2018-12-22 11:13:05.079926 UTC] Computing logging information
------------------------------------
| Iteration            | 645       |
| ExpectedImprovement  | 0.016583  |
| ActualImprovement    | 0.015824  |
| ImprovementRatio     | 0.95421   |
| MeanKL               | 0.0072812 |
| Entropy              | -0.18944  |
| Perplexity           | 0.82742   |
| AveragePolicyStd     | 0.23776   |
| AveragePolicyStd[0]  | 0.26316   |
| AveragePolicyStd[1]  | 0.29581   |
| AveragePolicyStd[2]  | 0.20032   |
| AveragePolicyStd[3]  | 0.24331   |
| AveragePolicyStd[4]  | 0.17789   |
| AveragePolicyStd[5]  | 0.24604   |
| AverageReturn        | 1387.4    |
| MinReturn            | 128.5     |
| MaxReturn            | 1555.3    |
| StdReturn            | 302.93    |
| AverageEpisodeLength | 935.61    |
| MinEpisodeLength     | 101       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 195.25    |
| TotalNEpisodes       | 17802     |
| TotalNSamples        | 3.226e+06 |
| ExplainedVariance    | 0.16859   |
------------------------------------
[2018-12-22 11:13:05.449127 UTC] Saving snapshot
[2018-12-22 11:13:05.449395 UTC] Starting iteration 646
[2018-12-22 11:13:05.449545 UTC] Start collecting samples
[2018-12-22 11:13:08.442701 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:08.523323 UTC] Performing policy update
[2018-12-22 11:13:08.523971 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:08.613276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:09.674741 UTC] Performing line search
[2018-12-22 11:13:09.807065 UTC] Updating baseline
[2018-12-22 11:13:11.004809 UTC] Computing logging information
-------------------------------------
| Iteration            | 646        |
| ExpectedImprovement  | 0.017285   |
| ActualImprovement    | 0.01722    |
| ImprovementRatio     | 0.99627    |
| MeanKL               | 0.0073718  |
| Entropy              | -0.19211   |
| Perplexity           | 0.82522    |
| AveragePolicyStd     | 0.23765    |
| AveragePolicyStd[0]  | 0.26346    |
| AveragePolicyStd[1]  | 0.2951     |
| AveragePolicyStd[2]  | 0.19996    |
| AveragePolicyStd[3]  | 0.24273    |
| AveragePolicyStd[4]  | 0.1779     |
| AveragePolicyStd[5]  | 0.24672    |
| AverageReturn        | 1391.1     |
| MinReturn            | 128.5      |
| MaxReturn            | 1555.3     |
| StdReturn            | 296.4      |
| AverageEpisodeLength | 938.53     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.12     |
| TotalNEpisodes       | 17810      |
| TotalNSamples        | 3.2338e+06 |
| ExplainedVariance    | 0.084061   |
-------------------------------------
[2018-12-22 11:13:11.372742 UTC] Saving snapshot
[2018-12-22 11:13:11.373032 UTC] Starting iteration 647
[2018-12-22 11:13:11.373150 UTC] Start collecting samples
[2018-12-22 11:13:14.300376 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:14.378943 UTC] Performing policy update
[2018-12-22 11:13:14.379617 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:14.470356 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:15.538521 UTC] Performing line search
[2018-12-22 11:13:15.667811 UTC] Updating baseline
[2018-12-22 11:13:17.029602 UTC] Computing logging information
-------------------------------------
| Iteration            | 647        |
| ExpectedImprovement  | 0.020674   |
| ActualImprovement    | 0.018374   |
| ImprovementRatio     | 0.88874    |
| MeanKL               | 0.0067888  |
| Entropy              | -0.19638   |
| Perplexity           | 0.8217     |
| AveragePolicyStd     | 0.23741    |
| AveragePolicyStd[0]  | 0.26267    |
| AveragePolicyStd[1]  | 0.29412    |
| AveragePolicyStd[2]  | 0.20013    |
| AveragePolicyStd[3]  | 0.24346    |
| AveragePolicyStd[4]  | 0.1782     |
| AveragePolicyStd[5]  | 0.24587    |
| AverageReturn        | 1376.7     |
| MinReturn            | 128.5      |
| MaxReturn            | 1555.3     |
| StdReturn            | 320.41     |
| AverageEpisodeLength | 929.76     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.52     |
| TotalNEpisodes       | 17814      |
| TotalNSamples        | 3.2369e+06 |
| ExplainedVariance    | 0.13299    |
-------------------------------------
[2018-12-22 11:13:17.396440 UTC] Saving snapshot
[2018-12-22 11:13:17.396697 UTC] Starting iteration 648
[2018-12-22 11:13:17.396812 UTC] Start collecting samples
[2018-12-22 11:13:20.344272 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:20.423429 UTC] Performing policy update
[2018-12-22 11:13:20.424079 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:20.514151 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:21.585796 UTC] Performing line search
[2018-12-22 11:13:21.714743 UTC] Updating baseline
[2018-12-22 11:13:23.070921 UTC] Computing logging information
-------------------------------------
| Iteration            | 648        |
| ExpectedImprovement  | 0.016699   |
| ActualImprovement    | 0.015803   |
| ImprovementRatio     | 0.94632    |
| MeanKL               | 0.0075259  |
| Entropy              | -0.19879   |
| Perplexity           | 0.81972    |
| AveragePolicyStd     | 0.23738    |
| AveragePolicyStd[0]  | 0.26316    |
| AveragePolicyStd[1]  | 0.2951     |
| AveragePolicyStd[2]  | 0.2        |
| AveragePolicyStd[3]  | 0.24265    |
| AveragePolicyStd[4]  | 0.17774    |
| AveragePolicyStd[5]  | 0.24561    |
| AverageReturn        | 1377.6     |
| MinReturn            | 128.5      |
| MaxReturn            | 1555.3     |
| StdReturn            | 320.61     |
| AverageEpisodeLength | 929.97     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.58     |
| TotalNEpisodes       | 17819      |
| TotalNSamples        | 3.2419e+06 |
| ExplainedVariance    | 0.013843   |
-------------------------------------
[2018-12-22 11:13:23.444186 UTC] Saving snapshot
[2018-12-22 11:13:23.444444 UTC] Starting iteration 649
[2018-12-22 11:13:23.444579 UTC] Start collecting samples
[2018-12-22 11:13:26.385263 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:26.465879 UTC] Performing policy update
[2018-12-22 11:13:26.466632 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:26.557475 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:27.620643 UTC] Performing line search
[2018-12-22 11:13:27.747998 UTC] Updating baseline
[2018-12-22 11:13:29.547272 UTC] Computing logging information
-------------------------------------
| Iteration            | 649        |
| ExpectedImprovement  | 0.016441   |
| ActualImprovement    | 0.015085   |
| ImprovementRatio     | 0.91752    |
| MeanKL               | 0.0074045  |
| Entropy              | -0.19741   |
| Perplexity           | 0.82085    |
| AveragePolicyStd     | 0.23749    |
| AveragePolicyStd[0]  | 0.26288    |
| AveragePolicyStd[1]  | 0.29664    |
| AveragePolicyStd[2]  | 0.19916    |
| AveragePolicyStd[3]  | 0.24278    |
| AveragePolicyStd[4]  | 0.17803    |
| AveragePolicyStd[5]  | 0.24543    |
| AverageReturn        | 1378.1     |
| MinReturn            | 128.5      |
| MaxReturn            | 1540.8     |
| StdReturn            | 320.67     |
| AverageEpisodeLength | 929.97     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.58     |
| TotalNEpisodes       | 17824      |
| TotalNSamples        | 3.2469e+06 |
| ExplainedVariance    | -0.0025392 |
-------------------------------------
[2018-12-22 11:13:29.915535 UTC] Saving snapshot
[2018-12-22 11:13:29.915799 UTC] Starting iteration 650
[2018-12-22 11:13:29.915927 UTC] Start collecting samples
[2018-12-22 11:13:32.900817 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:32.982470 UTC] Performing policy update
[2018-12-22 11:13:32.983155 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:33.073024 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:34.133476 UTC] Performing line search
[2018-12-22 11:13:34.260588 UTC] Updating baseline
[2018-12-22 11:13:35.705204 UTC] Computing logging information
-------------------------------------
| Iteration            | 650        |
| ExpectedImprovement  | 0.018879   |
| ActualImprovement    | 0.017932   |
| ImprovementRatio     | 0.94987    |
| MeanKL               | 0.0072375  |
| Entropy              | -0.19925   |
| Perplexity           | 0.81935    |
| AveragePolicyStd     | 0.23745    |
| AveragePolicyStd[0]  | 0.26358    |
| AveragePolicyStd[1]  | 0.29724    |
| AveragePolicyStd[2]  | 0.19956    |
| AveragePolicyStd[3]  | 0.24176    |
| AveragePolicyStd[4]  | 0.17757    |
| AveragePolicyStd[5]  | 0.24501    |
| AverageReturn        | 1358.4     |
| MinReturn            | 128.5      |
| MaxReturn            | 1540.8     |
| StdReturn            | 337.04     |
| AverageEpisodeLength | 917.27     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.75     |
| TotalNEpisodes       | 17833      |
| TotalNSamples        | 3.2547e+06 |
| ExplainedVariance    | 0.24166    |
-------------------------------------
[2018-12-22 11:13:36.081226 UTC] Saving snapshot
[2018-12-22 11:13:36.089322 UTC] Starting iteration 651
[2018-12-22 11:13:36.089539 UTC] Start collecting samples
[2018-12-22 11:13:39.021062 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:39.100754 UTC] Performing policy update
[2018-12-22 11:13:39.101349 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:39.191195 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:40.269249 UTC] Performing line search
[2018-12-22 11:13:40.397658 UTC] Updating baseline
[2018-12-22 11:13:41.679890 UTC] Computing logging information
------------------------------------
| Iteration            | 651       |
| ExpectedImprovement  | 0.018512  |
| ActualImprovement    | 0.017486  |
| ImprovementRatio     | 0.94459   |
| MeanKL               | 0.0070679 |
| Entropy              | -0.2103   |
| Perplexity           | 0.81034   |
| AveragePolicyStd     | 0.23706   |
| AveragePolicyStd[0]  | 0.26259   |
| AveragePolicyStd[1]  | 0.29729   |
| AveragePolicyStd[2]  | 0.19933   |
| AveragePolicyStd[3]  | 0.24117   |
| AveragePolicyStd[4]  | 0.17663   |
| AveragePolicyStd[5]  | 0.24537   |
| AverageReturn        | 1317.1    |
| MinReturn            | 76.951    |
| MaxReturn            | 1540.6    |
| StdReturn            | 390.58    |
| AverageEpisodeLength | 890.24    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 253.22    |
| TotalNEpisodes       | 17838     |
| TotalNSamples        | 3.257e+06 |
| ExplainedVariance    | 0.465     |
------------------------------------
[2018-12-22 11:13:42.055766 UTC] Saving snapshot
[2018-12-22 11:13:42.056007 UTC] Starting iteration 652
[2018-12-22 11:13:42.056122 UTC] Start collecting samples
[2018-12-22 11:13:45.012124 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:45.091573 UTC] Performing policy update
[2018-12-22 11:13:45.092292 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:45.184069 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:46.257147 UTC] Performing line search
[2018-12-22 11:13:46.386241 UTC] Updating baseline
[2018-12-22 11:13:47.756001 UTC] Computing logging information
-------------------------------------
| Iteration            | 652        |
| ExpectedImprovement  | 0.0169     |
| ActualImprovement    | 0.015739   |
| ImprovementRatio     | 0.93131    |
| MeanKL               | 0.0070571  |
| Entropy              | -0.22587   |
| Perplexity           | 0.79783    |
| AveragePolicyStd     | 0.23638    |
| AveragePolicyStd[0]  | 0.26172    |
| AveragePolicyStd[1]  | 0.2954     |
| AveragePolicyStd[2]  | 0.19911    |
| AveragePolicyStd[3]  | 0.24052    |
| AveragePolicyStd[4]  | 0.17659    |
| AveragePolicyStd[5]  | 0.24491    |
| AverageReturn        | 1318.7     |
| MinReturn            | 76.951     |
| MaxReturn            | 1551.3     |
| StdReturn            | 391.31     |
| AverageEpisodeLength | 890.16     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.19     |
| TotalNEpisodes       | 17842      |
| TotalNSamples        | 3.2609e+06 |
| ExplainedVariance    | 0.17335    |
-------------------------------------
[2018-12-22 11:13:48.124818 UTC] Saving snapshot
[2018-12-22 11:13:48.125060 UTC] Starting iteration 653
[2018-12-22 11:13:48.125177 UTC] Start collecting samples
[2018-12-22 11:13:51.132188 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:51.214826 UTC] Performing policy update
[2018-12-22 11:13:51.215426 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:51.305219 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:52.368339 UTC] Performing line search
[2018-12-22 11:13:52.495474 UTC] Updating baseline
[2018-12-22 11:13:53.685676 UTC] Computing logging information
-------------------------------------
| Iteration            | 653        |
| ExpectedImprovement  | 0.018155   |
| ActualImprovement    | 0.01715    |
| ImprovementRatio     | 0.94465    |
| MeanKL               | 0.0069859  |
| Entropy              | -0.23565   |
| Perplexity           | 0.79006    |
| AveragePolicyStd     | 0.23601    |
| AveragePolicyStd[0]  | 0.26099    |
| AveragePolicyStd[1]  | 0.29541    |
| AveragePolicyStd[2]  | 0.19903    |
| AveragePolicyStd[3]  | 0.23929    |
| AveragePolicyStd[4]  | 0.17615    |
| AveragePolicyStd[5]  | 0.24517    |
| AverageReturn        | 1310.8     |
| MinReturn            | 76.951     |
| MaxReturn            | 1556.8     |
| StdReturn            | 399.54     |
| AverageEpisodeLength | 883.93     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 258.03     |
| TotalNEpisodes       | 17851      |
| TotalNSamples        | 3.2693e+06 |
| ExplainedVariance    | 0.13436    |
-------------------------------------
[2018-12-22 11:13:54.058397 UTC] Saving snapshot
[2018-12-22 11:13:54.058659 UTC] Starting iteration 654
[2018-12-22 11:13:54.058788 UTC] Start collecting samples
[2018-12-22 11:13:56.991944 UTC] Computing input variables for policy optimization
[2018-12-22 11:13:57.071702 UTC] Performing policy update
[2018-12-22 11:13:57.072393 UTC] Computing gradient in Euclidean space
[2018-12-22 11:13:57.161858 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:13:58.231811 UTC] Performing line search
[2018-12-22 11:13:58.359293 UTC] Updating baseline
[2018-12-22 11:13:59.631196 UTC] Computing logging information
-------------------------------------
| Iteration            | 654        |
| ExpectedImprovement  | 0.016026   |
| ActualImprovement    | 0.015356   |
| ImprovementRatio     | 0.95819    |
| MeanKL               | 0.0074066  |
| Entropy              | -0.24461   |
| Perplexity           | 0.78301    |
| AveragePolicyStd     | 0.23563    |
| AveragePolicyStd[0]  | 0.26041    |
| AveragePolicyStd[1]  | 0.29459    |
| AveragePolicyStd[2]  | 0.19897    |
| AveragePolicyStd[3]  | 0.23828    |
| AveragePolicyStd[4]  | 0.176      |
| AveragePolicyStd[5]  | 0.24552    |
| AverageReturn        | 1303.4     |
| MinReturn            | 76.951     |
| MaxReturn            | 1556.8     |
| StdReturn            | 410.08     |
| AverageEpisodeLength | 878.69     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 264.47     |
| TotalNEpisodes       | 17855      |
| TotalNSamples        | 3.2726e+06 |
| ExplainedVariance    | 0.1948     |
-------------------------------------
[2018-12-22 11:14:00.011731 UTC] Saving snapshot
[2018-12-22 11:14:00.011980 UTC] Starting iteration 655
[2018-12-22 11:14:00.012099 UTC] Start collecting samples
[2018-12-22 11:14:02.972149 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:03.051974 UTC] Performing policy update
[2018-12-22 11:14:03.052664 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:03.144004 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:04.222032 UTC] Performing line search
[2018-12-22 11:14:04.350403 UTC] Updating baseline
[2018-12-22 11:14:05.891857 UTC] Computing logging information
-------------------------------------
| Iteration            | 655        |
| ExpectedImprovement  | 0.018235   |
| ActualImprovement    | 0.016984   |
| ImprovementRatio     | 0.93138    |
| MeanKL               | 0.007048   |
| Entropy              | -0.25643   |
| Perplexity           | 0.77381    |
| AveragePolicyStd     | 0.23513    |
| AveragePolicyStd[0]  | 0.2601     |
| AveragePolicyStd[1]  | 0.29362    |
| AveragePolicyStd[2]  | 0.19829    |
| AveragePolicyStd[3]  | 0.23811    |
| AveragePolicyStd[4]  | 0.17612    |
| AveragePolicyStd[5]  | 0.24457    |
| AverageReturn        | 1299.5     |
| MinReturn            | 76.951     |
| MaxReturn            | 1556.8     |
| StdReturn            | 410.03     |
| AverageEpisodeLength | 876.06     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 264.4      |
| TotalNEpisodes       | 17859      |
| TotalNSamples        | 3.2764e+06 |
| ExplainedVariance    | 0.30662    |
-------------------------------------
[2018-12-22 11:14:06.264289 UTC] Saving snapshot
[2018-12-22 11:14:06.264596 UTC] Starting iteration 656
[2018-12-22 11:14:06.264719 UTC] Start collecting samples
[2018-12-22 11:14:09.233113 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:09.314772 UTC] Performing policy update
[2018-12-22 11:14:09.315393 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:09.406005 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:10.463175 UTC] Performing line search
[2018-12-22 11:14:10.590975 UTC] Updating baseline
[2018-12-22 11:14:12.203674 UTC] Computing logging information
------------------------------------
| Iteration            | 656       |
| ExpectedImprovement  | 0.017514  |
| ActualImprovement    | 0.01694   |
| ImprovementRatio     | 0.96721   |
| MeanKL               | 0.0073943 |
| Entropy              | -0.25724  |
| Perplexity           | 0.77319   |
| AveragePolicyStd     | 0.23509   |
| AveragePolicyStd[0]  | 0.26      |
| AveragePolicyStd[1]  | 0.29367   |
| AveragePolicyStd[2]  | 0.198     |
| AveragePolicyStd[3]  | 0.23798   |
| AveragePolicyStd[4]  | 0.1765    |
| AveragePolicyStd[5]  | 0.2444    |
| AverageReturn        | 1300.8    |
| MinReturn            | 76.951    |
| MaxReturn            | 1556.8    |
| StdReturn            | 410.9     |
| AverageEpisodeLength | 875.32    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 264.36    |
| TotalNEpisodes       | 17867     |
| TotalNSamples        | 3.284e+06 |
| ExplainedVariance    | 0.048313  |
------------------------------------
[2018-12-22 11:14:12.570898 UTC] Saving snapshot
[2018-12-22 11:14:12.571130 UTC] Starting iteration 657
[2018-12-22 11:14:12.571246 UTC] Start collecting samples
[2018-12-22 11:14:15.532313 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:15.612697 UTC] Performing policy update
[2018-12-22 11:14:15.613580 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:15.704121 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:16.773904 UTC] Performing line search
[2018-12-22 11:14:16.901990 UTC] Updating baseline
[2018-12-22 11:14:18.184876 UTC] Computing logging information
-------------------------------------
| Iteration            | 657        |
| ExpectedImprovement  | 0.017606   |
| ActualImprovement    | 0.016705   |
| ImprovementRatio     | 0.94882    |
| MeanKL               | 0.0072602  |
| Entropy              | -0.25928   |
| Perplexity           | 0.77161    |
| AveragePolicyStd     | 0.235      |
| AveragePolicyStd[0]  | 0.26097    |
| AveragePolicyStd[1]  | 0.29344    |
| AveragePolicyStd[2]  | 0.19806    |
| AveragePolicyStd[3]  | 0.23772    |
| AveragePolicyStd[4]  | 0.17657    |
| AveragePolicyStd[5]  | 0.24327    |
| AverageReturn        | 1294.4     |
| MinReturn            | 76.951     |
| MaxReturn            | 1556.8     |
| StdReturn            | 421.34     |
| AverageEpisodeLength | 871.32     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 271.34     |
| TotalNEpisodes       | 17871      |
| TotalNSamples        | 3.2872e+06 |
| ExplainedVariance    | 0.18354    |
-------------------------------------
[2018-12-22 11:14:18.554267 UTC] Saving snapshot
[2018-12-22 11:14:18.554537 UTC] Starting iteration 658
[2018-12-22 11:14:18.554659 UTC] Start collecting samples
[2018-12-22 11:14:21.523552 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:21.605864 UTC] Performing policy update
[2018-12-22 11:14:21.606776 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:21.697558 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:22.763016 UTC] Performing line search
[2018-12-22 11:14:22.890798 UTC] Updating baseline
[2018-12-22 11:14:24.358021 UTC] Computing logging information
-------------------------------------
| Iteration            | 658        |
| ExpectedImprovement  | 0.015583   |
| ActualImprovement    | 0.015113   |
| ImprovementRatio     | 0.96987    |
| MeanKL               | 0.0073802  |
| Entropy              | -0.2563    |
| Perplexity           | 0.77391    |
| AveragePolicyStd     | 0.2351     |
| AveragePolicyStd[0]  | 0.26074    |
| AveragePolicyStd[1]  | 0.29289    |
| AveragePolicyStd[2]  | 0.19819    |
| AveragePolicyStd[3]  | 0.23847    |
| AveragePolicyStd[4]  | 0.17656    |
| AveragePolicyStd[5]  | 0.24375    |
| AverageReturn        | 1301.8     |
| MinReturn            | 76.951     |
| MaxReturn            | 1572.6     |
| StdReturn            | 418.62     |
| AverageEpisodeLength | 874.59     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 269.23     |
| TotalNEpisodes       | 17877      |
| TotalNSamples        | 3.2931e+06 |
| ExplainedVariance    | 0.11465    |
-------------------------------------
[2018-12-22 11:14:24.722335 UTC] Saving snapshot
[2018-12-22 11:14:24.722623 UTC] Starting iteration 659
[2018-12-22 11:14:24.722740 UTC] Start collecting samples
[2018-12-22 11:14:27.666145 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:27.746436 UTC] Performing policy update
[2018-12-22 11:14:27.747102 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:27.837716 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:28.906253 UTC] Performing line search
[2018-12-22 11:14:29.033201 UTC] Updating baseline
[2018-12-22 11:14:30.403176 UTC] Computing logging information
-------------------------------------
| Iteration            | 659        |
| ExpectedImprovement  | 0.015664   |
| ActualImprovement    | 0.014908   |
| ImprovementRatio     | 0.95176    |
| MeanKL               | 0.0076313  |
| Entropy              | -0.24503   |
| Perplexity           | 0.78268    |
| AveragePolicyStd     | 0.2355     |
| AveragePolicyStd[0]  | 0.26052    |
| AveragePolicyStd[1]  | 0.29272    |
| AveragePolicyStd[2]  | 0.19857    |
| AveragePolicyStd[3]  | 0.2395     |
| AveragePolicyStd[4]  | 0.17709    |
| AveragePolicyStd[5]  | 0.24459    |
| AverageReturn        | 1331       |
| MinReturn            | 76.951     |
| MaxReturn            | 1572.6     |
| StdReturn            | 385.37     |
| AverageEpisodeLength | 892.58     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.61     |
| TotalNEpisodes       | 17882      |
| TotalNSamples        | 3.2974e+06 |
| ExplainedVariance    | 0.13796    |
-------------------------------------
[2018-12-22 11:14:30.773257 UTC] Saving snapshot
[2018-12-22 11:14:30.773680 UTC] Starting iteration 660
[2018-12-22 11:14:30.773844 UTC] Start collecting samples
[2018-12-22 11:14:33.753227 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:33.835750 UTC] Performing policy update
[2018-12-22 11:14:33.836707 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:33.928128 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:35.000652 UTC] Performing line search
[2018-12-22 11:14:35.128463 UTC] Updating baseline
[2018-12-22 11:14:36.596653 UTC] Computing logging information
-------------------------------------
| Iteration            | 660        |
| ExpectedImprovement  | 0.018065   |
| ActualImprovement    | 0.016973   |
| ImprovementRatio     | 0.93955    |
| MeanKL               | 0.0071654  |
| Entropy              | -0.24472   |
| Perplexity           | 0.78292    |
| AveragePolicyStd     | 0.23549    |
| AveragePolicyStd[0]  | 0.26124    |
| AveragePolicyStd[1]  | 0.29191    |
| AveragePolicyStd[2]  | 0.19894    |
| AveragePolicyStd[3]  | 0.23858    |
| AveragePolicyStd[4]  | 0.17699    |
| AveragePolicyStd[5]  | 0.24531    |
| AverageReturn        | 1334.8     |
| MinReturn            | 76.951     |
| MaxReturn            | 1579.5     |
| StdReturn            | 386.86     |
| AverageEpisodeLength | 893.19     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.8      |
| TotalNEpisodes       | 17888      |
| TotalNSamples        | 3.3034e+06 |
| ExplainedVariance    | 0.0013847  |
-------------------------------------
[2018-12-22 11:14:36.965802 UTC] Saving snapshot
[2018-12-22 11:14:36.974175 UTC] Starting iteration 661
[2018-12-22 11:14:36.974393 UTC] Start collecting samples
[2018-12-22 11:14:39.903549 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:39.987334 UTC] Performing policy update
[2018-12-22 11:14:39.988001 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:40.079055 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:41.138941 UTC] Performing line search
[2018-12-22 11:14:41.267536 UTC] Updating baseline
[2018-12-22 11:14:42.642309 UTC] Computing logging information
-------------------------------------
| Iteration            | 661        |
| ExpectedImprovement  | 0.020532   |
| ActualImprovement    | 0.019037   |
| ImprovementRatio     | 0.92718    |
| MeanKL               | 0.0068749  |
| Entropy              | -0.24493   |
| Perplexity           | 0.78276    |
| AveragePolicyStd     | 0.2354     |
| AveragePolicyStd[0]  | 0.26068    |
| AveragePolicyStd[1]  | 0.29078    |
| AveragePolicyStd[2]  | 0.19913    |
| AveragePolicyStd[3]  | 0.23879    |
| AveragePolicyStd[4]  | 0.1777     |
| AveragePolicyStd[5]  | 0.2453     |
| AverageReturn        | 1325.7     |
| MinReturn            | 76.951     |
| MaxReturn            | 1579.5     |
| StdReturn            | 390.49     |
| AverageEpisodeLength | 886.61     |
| MinEpisodeLength     | 69         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 249.08     |
| TotalNEpisodes       | 17892      |
| TotalNSamples        | 3.3068e+06 |
| ExplainedVariance    | 0.43899    |
-------------------------------------
[2018-12-22 11:14:43.006954 UTC] Saving snapshot
[2018-12-22 11:14:43.007198 UTC] Starting iteration 662
[2018-12-22 11:14:43.007330 UTC] Start collecting samples
[2018-12-22 11:14:45.990988 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:46.073501 UTC] Performing policy update
[2018-12-22 11:14:46.074206 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:46.167009 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:47.226172 UTC] Performing line search
[2018-12-22 11:14:47.353461 UTC] Updating baseline
[2018-12-22 11:14:48.814204 UTC] Computing logging information
-------------------------------------
| Iteration            | 662        |
| ExpectedImprovement  | 0.01831    |
| ActualImprovement    | 0.0172     |
| ImprovementRatio     | 0.93938    |
| MeanKL               | 0.0073675  |
| Entropy              | -0.24205   |
| Perplexity           | 0.78502    |
| AveragePolicyStd     | 0.23555    |
| AveragePolicyStd[0]  | 0.26023    |
| AveragePolicyStd[1]  | 0.2912     |
| AveragePolicyStd[2]  | 0.19872    |
| AveragePolicyStd[3]  | 0.23988    |
| AveragePolicyStd[4]  | 0.17747    |
| AveragePolicyStd[5]  | 0.24578    |
| AverageReturn        | 1333.9     |
| MinReturn            | 53.817     |
| MaxReturn            | 1579.5     |
| StdReturn            | 388.8      |
| AverageEpisodeLength | 889.85     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 247.59     |
| TotalNEpisodes       | 17899      |
| TotalNSamples        | 3.3126e+06 |
| ExplainedVariance    | 0.2235     |
-------------------------------------
[2018-12-22 11:14:49.186531 UTC] Saving snapshot
[2018-12-22 11:14:49.186777 UTC] Starting iteration 663
[2018-12-22 11:14:49.186897 UTC] Start collecting samples
[2018-12-22 11:14:52.156485 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:52.235866 UTC] Performing policy update
[2018-12-22 11:14:52.236485 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:52.326087 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:53.385786 UTC] Performing line search
[2018-12-22 11:14:53.512317 UTC] Updating baseline
[2018-12-22 11:14:54.712025 UTC] Computing logging information
------------------------------------
| Iteration            | 663       |
| ExpectedImprovement  | 0.019489  |
| ActualImprovement    | 0.019253  |
| ImprovementRatio     | 0.98788   |
| MeanKL               | 0.0072246 |
| Entropy              | -0.25646  |
| Perplexity           | 0.77379   |
| AveragePolicyStd     | 0.23502   |
| AveragePolicyStd[0]  | 0.2601    |
| AveragePolicyStd[1]  | 0.29072   |
| AveragePolicyStd[2]  | 0.1984    |
| AveragePolicyStd[3]  | 0.23875   |
| AveragePolicyStd[4]  | 0.17659   |
| AveragePolicyStd[5]  | 0.24554   |
| AverageReturn        | 1322.4    |
| MinReturn            | 53.817    |
| MaxReturn            | 1579.5    |
| StdReturn            | 409.21    |
| AverageEpisodeLength | 880.13    |
| MinEpisodeLength     | 57        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 260.97    |
| TotalNEpisodes       | 17906     |
| TotalNSamples        | 3.318e+06 |
| ExplainedVariance    | 0.10419   |
------------------------------------
[2018-12-22 11:14:55.080758 UTC] Saving snapshot
[2018-12-22 11:14:55.081016 UTC] Starting iteration 664
[2018-12-22 11:14:55.081132 UTC] Start collecting samples
[2018-12-22 11:14:58.024789 UTC] Computing input variables for policy optimization
[2018-12-22 11:14:58.105086 UTC] Performing policy update
[2018-12-22 11:14:58.105725 UTC] Computing gradient in Euclidean space
[2018-12-22 11:14:58.195285 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:14:59.255986 UTC] Performing line search
[2018-12-22 11:14:59.382598 UTC] Updating baseline
[2018-12-22 11:15:00.759488 UTC] Computing logging information
-------------------------------------
| Iteration            | 664        |
| ExpectedImprovement  | 0.02011    |
| ActualImprovement    | 0.018286   |
| ImprovementRatio     | 0.90934    |
| MeanKL               | 0.006674   |
| Entropy              | -0.26087   |
| Perplexity           | 0.77038    |
| AveragePolicyStd     | 0.23486    |
| AveragePolicyStd[0]  | 0.25926    |
| AveragePolicyStd[1]  | 0.2905     |
| AveragePolicyStd[2]  | 0.19819    |
| AveragePolicyStd[3]  | 0.2385     |
| AveragePolicyStd[4]  | 0.17619    |
| AveragePolicyStd[5]  | 0.24652    |
| AverageReturn        | 1323.2     |
| MinReturn            | 53.817     |
| MaxReturn            | 1579.5     |
| StdReturn            | 409.94     |
| AverageEpisodeLength | 879.77     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 261.1      |
| TotalNEpisodes       | 17911      |
| TotalNSamples        | 3.3228e+06 |
| ExplainedVariance    | 0.18825    |
-------------------------------------
[2018-12-22 11:15:01.135253 UTC] Saving snapshot
[2018-12-22 11:15:01.135532 UTC] Starting iteration 665
[2018-12-22 11:15:01.135657 UTC] Start collecting samples
[2018-12-22 11:15:04.078648 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:04.157948 UTC] Performing policy update
[2018-12-22 11:15:04.158946 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:04.249725 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:05.308455 UTC] Performing line search
[2018-12-22 11:15:05.436973 UTC] Updating baseline
[2018-12-22 11:15:06.626283 UTC] Computing logging information
-------------------------------------
| Iteration            | 665        |
| ExpectedImprovement  | 0.017424   |
| ActualImprovement    | 0.016917   |
| ImprovementRatio     | 0.97093    |
| MeanKL               | 0.0075381  |
| Entropy              | -0.27476   |
| Perplexity           | 0.75976    |
| AveragePolicyStd     | 0.23426    |
| AveragePolicyStd[0]  | 0.25809    |
| AveragePolicyStd[1]  | 0.28935    |
| AveragePolicyStd[2]  | 0.19854    |
| AveragePolicyStd[3]  | 0.23685    |
| AveragePolicyStd[4]  | 0.17601    |
| AveragePolicyStd[5]  | 0.24672    |
| AverageReturn        | 1338.9     |
| MinReturn            | 53.817     |
| MaxReturn            | 1579.5     |
| StdReturn            | 394.04     |
| AverageEpisodeLength | 888.54     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.02     |
| TotalNEpisodes       | 17916      |
| TotalNSamples        | 3.3278e+06 |
| ExplainedVariance    | -0.03641   |
-------------------------------------
[2018-12-22 11:15:06.995337 UTC] Saving snapshot
[2018-12-22 11:15:06.995638 UTC] Starting iteration 666
[2018-12-22 11:15:06.995761 UTC] Start collecting samples
[2018-12-22 11:15:09.937764 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:10.016423 UTC] Performing policy update
[2018-12-22 11:15:10.017093 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:10.107479 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:11.176502 UTC] Performing line search
[2018-12-22 11:15:11.305173 UTC] Updating baseline
[2018-12-22 11:15:12.583162 UTC] Computing logging information
-------------------------------------
| Iteration            | 666        |
| ExpectedImprovement  | 0.016966   |
| ActualImprovement    | 0.016008   |
| ImprovementRatio     | 0.94351    |
| MeanKL               | 0.007815   |
| Entropy              | -0.28448   |
| Perplexity           | 0.75241    |
| AveragePolicyStd     | 0.23389    |
| AveragePolicyStd[0]  | 0.25929    |
| AveragePolicyStd[1]  | 0.28818    |
| AveragePolicyStd[2]  | 0.1987     |
| AveragePolicyStd[3]  | 0.23627    |
| AveragePolicyStd[4]  | 0.17524    |
| AveragePolicyStd[5]  | 0.24567    |
| AverageReturn        | 1337.6     |
| MinReturn            | 53.817     |
| MaxReturn            | 1579.5     |
| StdReturn            | 395.42     |
| AverageEpisodeLength | 886.24     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.04     |
| TotalNEpisodes       | 17921      |
| TotalNSamples        | 3.3325e+06 |
| ExplainedVariance    | 0.09572    |
-------------------------------------
[2018-12-22 11:15:12.952703 UTC] Saving snapshot
[2018-12-22 11:15:12.952967 UTC] Starting iteration 667
[2018-12-22 11:15:12.953084 UTC] Start collecting samples
[2018-12-22 11:15:15.900724 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:15.985425 UTC] Performing policy update
[2018-12-22 11:15:15.986347 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:16.078139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:17.135611 UTC] Performing line search
[2018-12-22 11:15:17.263087 UTC] Updating baseline
[2018-12-22 11:15:20.370758 UTC] Computing logging information
-------------------------------------
| Iteration            | 667        |
| ExpectedImprovement  | 0.016032   |
| ActualImprovement    | 0.015243   |
| ImprovementRatio     | 0.95075    |
| MeanKL               | 0.0077794  |
| Entropy              | -0.29057   |
| Perplexity           | 0.74784    |
| AveragePolicyStd     | 0.2336     |
| AveragePolicyStd[0]  | 0.25923    |
| AveragePolicyStd[1]  | 0.28751    |
| AveragePolicyStd[2]  | 0.19806    |
| AveragePolicyStd[3]  | 0.23505    |
| AveragePolicyStd[4]  | 0.17605    |
| AveragePolicyStd[5]  | 0.24573    |
| AverageReturn        | 1349.6     |
| MinReturn            | 53.817     |
| MaxReturn            | 1579.5     |
| StdReturn            | 391.01     |
| AverageEpisodeLength | 891.6      |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.66     |
| TotalNEpisodes       | 17927      |
| TotalNSamples        | 3.3385e+06 |
| ExplainedVariance    | -0.0035676 |
-------------------------------------
[2018-12-22 11:15:20.739581 UTC] Saving snapshot
[2018-12-22 11:15:20.739848 UTC] Starting iteration 668
[2018-12-22 11:15:20.739967 UTC] Start collecting samples
[2018-12-22 11:15:23.699573 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:23.783699 UTC] Performing policy update
[2018-12-22 11:15:23.784502 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:23.874742 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:24.941595 UTC] Performing line search
[2018-12-22 11:15:25.070562 UTC] Updating baseline
[2018-12-22 11:15:26.815008 UTC] Computing logging information
-------------------------------------
| Iteration            | 668        |
| ExpectedImprovement  | 0.01927    |
| ActualImprovement    | 0.017749   |
| ImprovementRatio     | 0.92108    |
| MeanKL               | 0.0069424  |
| Entropy              | -0.28108   |
| Perplexity           | 0.75497    |
| AveragePolicyStd     | 0.23397    |
| AveragePolicyStd[0]  | 0.26023    |
| AveragePolicyStd[1]  | 0.28805    |
| AveragePolicyStd[2]  | 0.1985     |
| AveragePolicyStd[3]  | 0.23555    |
| AveragePolicyStd[4]  | 0.17637    |
| AveragePolicyStd[5]  | 0.24514    |
| AverageReturn        | 1351.1     |
| MinReturn            | 53.817     |
| MaxReturn            | 1579.5     |
| StdReturn            | 389.33     |
| AverageEpisodeLength | 891.21     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 245.66     |
| TotalNEpisodes       | 17933      |
| TotalNSamples        | 3.3438e+06 |
| ExplainedVariance    | 0.12086    |
-------------------------------------
[2018-12-22 11:15:27.187764 UTC] Saving snapshot
[2018-12-22 11:15:27.188011 UTC] Starting iteration 669
[2018-12-22 11:15:27.188134 UTC] Start collecting samples
[2018-12-22 11:15:30.162536 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:30.241827 UTC] Performing policy update
[2018-12-22 11:15:30.242477 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:30.332122 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:31.404870 UTC] Performing line search
[2018-12-22 11:15:31.533871 UTC] Updating baseline
[2018-12-22 11:15:32.909552 UTC] Computing logging information
-------------------------------------
| Iteration            | 669        |
| ExpectedImprovement  | 0.018981   |
| ActualImprovement    | 0.018224   |
| ImprovementRatio     | 0.96013    |
| MeanKL               | 0.0078719  |
| Entropy              | -0.2923    |
| Perplexity           | 0.74655    |
| AveragePolicyStd     | 0.2336     |
| AveragePolicyStd[0]  | 0.25986    |
| AveragePolicyStd[1]  | 0.28823    |
| AveragePolicyStd[2]  | 0.19787    |
| AveragePolicyStd[3]  | 0.23508    |
| AveragePolicyStd[4]  | 0.1755     |
| AveragePolicyStd[5]  | 0.24507    |
| AverageReturn        | 1366       |
| MinReturn            | 53.817     |
| MaxReturn            | 1579.5     |
| StdReturn            | 378.39     |
| AverageEpisodeLength | 899.83     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.15     |
| TotalNEpisodes       | 17937      |
| TotalNSamples        | 3.3469e+06 |
| ExplainedVariance    | 0.12831    |
-------------------------------------
[2018-12-22 11:15:33.284570 UTC] Saving snapshot
[2018-12-22 11:15:33.284831 UTC] Starting iteration 670
[2018-12-22 11:15:33.284951 UTC] Start collecting samples
[2018-12-22 11:15:36.252449 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:36.333720 UTC] Performing policy update
[2018-12-22 11:15:36.334640 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:36.425461 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:37.488887 UTC] Performing line search
[2018-12-22 11:15:37.619113 UTC] Updating baseline
[2018-12-22 11:15:38.993810 UTC] Computing logging information
-------------------------------------
| Iteration            | 670        |
| ExpectedImprovement  | 0.017669   |
| ActualImprovement    | 0.016579   |
| ImprovementRatio     | 0.93827    |
| MeanKL               | 0.0075363  |
| Entropy              | -0.28528   |
| Perplexity           | 0.75181    |
| AveragePolicyStd     | 0.23385    |
| AveragePolicyStd[0]  | 0.25947    |
| AveragePolicyStd[1]  | 0.28858    |
| AveragePolicyStd[2]  | 0.19794    |
| AveragePolicyStd[3]  | 0.2356     |
| AveragePolicyStd[4]  | 0.17602    |
| AveragePolicyStd[5]  | 0.24551    |
| AverageReturn        | 1371.1     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 363.2      |
| AverageEpisodeLength | 902.61     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.56     |
| TotalNEpisodes       | 17944      |
| TotalNSamples        | 3.3532e+06 |
| ExplainedVariance    | 0.21592    |
-------------------------------------
[2018-12-22 11:15:39.365097 UTC] Saving snapshot
[2018-12-22 11:15:39.373284 UTC] Starting iteration 671
[2018-12-22 11:15:39.373492 UTC] Start collecting samples
[2018-12-22 11:15:42.295973 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:42.374272 UTC] Performing policy update
[2018-12-22 11:15:42.374992 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:42.465266 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:43.533096 UTC] Performing line search
[2018-12-22 11:15:43.661401 UTC] Updating baseline
[2018-12-22 11:15:45.463084 UTC] Computing logging information
-------------------------------------
| Iteration            | 671        |
| ExpectedImprovement  | 0.018384   |
| ActualImprovement    | 0.016648   |
| ImprovementRatio     | 0.90559    |
| MeanKL               | 0.0071522  |
| Entropy              | -0.28376   |
| Perplexity           | 0.75295    |
| AveragePolicyStd     | 0.2339     |
| AveragePolicyStd[0]  | 0.26015    |
| AveragePolicyStd[1]  | 0.28884    |
| AveragePolicyStd[2]  | 0.19825    |
| AveragePolicyStd[3]  | 0.23553    |
| AveragePolicyStd[4]  | 0.17631    |
| AveragePolicyStd[5]  | 0.2443     |
| AverageReturn        | 1372.3     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 363.69     |
| AverageEpisodeLength | 902.61     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.56     |
| TotalNEpisodes       | 17947      |
| TotalNSamples        | 3.3562e+06 |
| ExplainedVariance    | -0.10749   |
-------------------------------------
[2018-12-22 11:15:45.834541 UTC] Saving snapshot
[2018-12-22 11:15:45.834815 UTC] Starting iteration 672
[2018-12-22 11:15:45.834932 UTC] Start collecting samples
[2018-12-22 11:15:48.766994 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:48.847384 UTC] Performing policy update
[2018-12-22 11:15:48.848204 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:48.939464 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:50.016886 UTC] Performing line search
[2018-12-22 11:15:50.145679 UTC] Updating baseline
[2018-12-22 11:15:52.045713 UTC] Computing logging information
-------------------------------------
| Iteration            | 672        |
| ExpectedImprovement  | 0.016488   |
| ActualImprovement    | 0.015982   |
| ImprovementRatio     | 0.96933    |
| MeanKL               | 0.0073474  |
| Entropy              | -0.28055   |
| Perplexity           | 0.75537    |
| AveragePolicyStd     | 0.23406    |
| AveragePolicyStd[0]  | 0.26126    |
| AveragePolicyStd[1]  | 0.2888     |
| AveragePolicyStd[2]  | 0.19846    |
| AveragePolicyStd[3]  | 0.23551    |
| AveragePolicyStd[4]  | 0.17577    |
| AveragePolicyStd[5]  | 0.24458    |
| AverageReturn        | 1394.1     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 340.41     |
| AverageEpisodeLength | 915.86     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.09     |
| TotalNEpisodes       | 17952      |
| TotalNSamples        | 3.3612e+06 |
| ExplainedVariance    | 0.0030332  |
-------------------------------------
[2018-12-22 11:15:52.415287 UTC] Saving snapshot
[2018-12-22 11:15:52.415563 UTC] Starting iteration 673
[2018-12-22 11:15:52.415685 UTC] Start collecting samples
[2018-12-22 11:15:55.366792 UTC] Computing input variables for policy optimization
[2018-12-22 11:15:55.446809 UTC] Performing policy update
[2018-12-22 11:15:55.447379 UTC] Computing gradient in Euclidean space
[2018-12-22 11:15:55.537131 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:15:56.606293 UTC] Performing line search
[2018-12-22 11:15:56.734884 UTC] Updating baseline
[2018-12-22 11:15:58.450614 UTC] Computing logging information
-------------------------------------
| Iteration            | 673        |
| ExpectedImprovement  | 0.018782   |
| ActualImprovement    | 0.01711    |
| ImprovementRatio     | 0.91095    |
| MeanKL               | 0.0073579  |
| Entropy              | -0.27071   |
| Perplexity           | 0.76284    |
| AveragePolicyStd     | 0.23447    |
| AveragePolicyStd[0]  | 0.26226    |
| AveragePolicyStd[1]  | 0.28844    |
| AveragePolicyStd[2]  | 0.19854    |
| AveragePolicyStd[3]  | 0.23643    |
| AveragePolicyStd[4]  | 0.17557    |
| AveragePolicyStd[5]  | 0.2456     |
| AverageReturn        | 1399.6     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 339.91     |
| AverageEpisodeLength | 918.32     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.63     |
| TotalNEpisodes       | 17958      |
| TotalNSamples        | 3.3672e+06 |
| ExplainedVariance    | -0.0015518 |
-------------------------------------
[2018-12-22 11:15:58.823638 UTC] Saving snapshot
[2018-12-22 11:15:58.823900 UTC] Starting iteration 674
[2018-12-22 11:15:58.824031 UTC] Start collecting samples
[2018-12-22 11:16:01.769591 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:01.849927 UTC] Performing policy update
[2018-12-22 11:16:01.850703 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:01.945242 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:03.013793 UTC] Performing line search
[2018-12-22 11:16:03.142329 UTC] Updating baseline
[2018-12-22 11:16:04.420366 UTC] Computing logging information
-------------------------------------
| Iteration            | 674        |
| ExpectedImprovement  | 0.017818   |
| ActualImprovement    | 0.016545   |
| ImprovementRatio     | 0.92855    |
| MeanKL               | 0.0070862  |
| Entropy              | -0.27699   |
| Perplexity           | 0.75806    |
| AveragePolicyStd     | 0.23426    |
| AveragePolicyStd[0]  | 0.26241    |
| AveragePolicyStd[1]  | 0.28861    |
| AveragePolicyStd[2]  | 0.19842    |
| AveragePolicyStd[3]  | 0.23594    |
| AveragePolicyStd[4]  | 0.17516    |
| AveragePolicyStd[5]  | 0.245      |
| AverageReturn        | 1404       |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 338.04     |
| AverageEpisodeLength | 921.17     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.73     |
| TotalNEpisodes       | 17963      |
| TotalNSamples        | 3.3722e+06 |
| ExplainedVariance    | 0.089971   |
-------------------------------------
[2018-12-22 11:16:04.794069 UTC] Saving snapshot
[2018-12-22 11:16:04.794316 UTC] Starting iteration 675
[2018-12-22 11:16:04.794436 UTC] Start collecting samples
[2018-12-22 11:16:07.744752 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:07.824161 UTC] Performing policy update
[2018-12-22 11:16:07.825019 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:07.915666 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:08.975301 UTC] Performing line search
[2018-12-22 11:16:09.102733 UTC] Updating baseline
[2018-12-22 11:16:10.827151 UTC] Computing logging information
-------------------------------------
| Iteration            | 675        |
| ExpectedImprovement  | 0.015917   |
| ActualImprovement    | 0.015037   |
| ImprovementRatio     | 0.94473    |
| MeanKL               | 0.0074436  |
| Entropy              | -0.28052   |
| Perplexity           | 0.75539    |
| AveragePolicyStd     | 0.23409    |
| AveragePolicyStd[0]  | 0.26123    |
| AveragePolicyStd[1]  | 0.28837    |
| AveragePolicyStd[2]  | 0.19875    |
| AveragePolicyStd[3]  | 0.23588    |
| AveragePolicyStd[4]  | 0.17503    |
| AveragePolicyStd[5]  | 0.2453     |
| AverageReturn        | 1396.2     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 351.74     |
| AverageEpisodeLength | 914.61     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.64     |
| TotalNEpisodes       | 17968      |
| TotalNSamples        | 3.3765e+06 |
| ExplainedVariance    | 0.1298     |
-------------------------------------
[2018-12-22 11:16:11.194962 UTC] Saving snapshot
[2018-12-22 11:16:11.195202 UTC] Starting iteration 676
[2018-12-22 11:16:11.195333 UTC] Start collecting samples
[2018-12-22 11:16:14.189220 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:14.269565 UTC] Performing policy update
[2018-12-22 11:16:14.270143 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:14.359386 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:15.419799 UTC] Performing line search
[2018-12-22 11:16:15.547657 UTC] Updating baseline
[2018-12-22 11:16:16.913641 UTC] Computing logging information
-------------------------------------
| Iteration            | 676        |
| ExpectedImprovement  | 0.01957    |
| ActualImprovement    | 0.018839   |
| ImprovementRatio     | 0.96264    |
| MeanKL               | 0.006913   |
| Entropy              | -0.27767   |
| Perplexity           | 0.75754    |
| AveragePolicyStd     | 0.23417    |
| AveragePolicyStd[0]  | 0.26117    |
| AveragePolicyStd[1]  | 0.28796    |
| AveragePolicyStd[2]  | 0.19929    |
| AveragePolicyStd[3]  | 0.23544    |
| AveragePolicyStd[4]  | 0.17517    |
| AveragePolicyStd[5]  | 0.24598    |
| AverageReturn        | 1409.7     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 333.53     |
| AverageEpisodeLength | 922.55     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.03     |
| TotalNEpisodes       | 17975      |
| TotalNSamples        | 3.3835e+06 |
| ExplainedVariance    | -0.042772  |
-------------------------------------
[2018-12-22 11:16:17.286059 UTC] Saving snapshot
[2018-12-22 11:16:17.286331 UTC] Starting iteration 677
[2018-12-22 11:16:17.286456 UTC] Start collecting samples
[2018-12-22 11:16:20.230769 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:20.308807 UTC] Performing policy update
[2018-12-22 11:16:20.309382 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:20.397883 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:21.455545 UTC] Performing line search
[2018-12-22 11:16:21.581940 UTC] Updating baseline
[2018-12-22 11:16:22.867414 UTC] Computing logging information
-------------------------------------
| Iteration            | 677        |
| ExpectedImprovement  | 0.015974   |
| ActualImprovement    | 0.015059   |
| ImprovementRatio     | 0.9427     |
| MeanKL               | 0.0074695  |
| Entropy              | -0.28      |
| Perplexity           | 0.75578    |
| AveragePolicyStd     | 0.23405    |
| AveragePolicyStd[0]  | 0.26163    |
| AveragePolicyStd[1]  | 0.28743    |
| AveragePolicyStd[2]  | 0.19931    |
| AveragePolicyStd[3]  | 0.23545    |
| AveragePolicyStd[4]  | 0.17539    |
| AveragePolicyStd[5]  | 0.24509    |
| AverageReturn        | 1412.6     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 333.29     |
| AverageEpisodeLength | 923.7      |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.71     |
| TotalNEpisodes       | 17979      |
| TotalNSamples        | 3.3868e+06 |
| ExplainedVariance    | 0.2715     |
-------------------------------------
[2018-12-22 11:16:23.237197 UTC] Saving snapshot
[2018-12-22 11:16:23.237449 UTC] Starting iteration 678
[2018-12-22 11:16:23.237590 UTC] Start collecting samples
[2018-12-22 11:16:26.211938 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:26.292195 UTC] Performing policy update
[2018-12-22 11:16:26.292871 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:26.381567 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:27.331661 UTC] Performing line search
[2018-12-22 11:16:27.446860 UTC] Updating baseline
[2018-12-22 11:16:28.648129 UTC] Computing logging information
-------------------------------------
| Iteration            | 678        |
| ExpectedImprovement  | 0.017172   |
| ActualImprovement    | 0.016602   |
| ImprovementRatio     | 0.96681    |
| MeanKL               | 0.0080029  |
| Entropy              | -0.27824   |
| Perplexity           | 0.75711    |
| AveragePolicyStd     | 0.23418    |
| AveragePolicyStd[0]  | 0.26201    |
| AveragePolicyStd[1]  | 0.28819    |
| AveragePolicyStd[2]  | 0.19883    |
| AveragePolicyStd[3]  | 0.23531    |
| AveragePolicyStd[4]  | 0.17517    |
| AveragePolicyStd[5]  | 0.24556    |
| AverageReturn        | 1401.9     |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 340.94     |
| AverageEpisodeLength | 917.41     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.25     |
| TotalNEpisodes       | 17986      |
| TotalNSamples        | 3.3932e+06 |
| ExplainedVariance    | 0.21192    |
-------------------------------------
[2018-12-22 11:16:29.013791 UTC] Saving snapshot
[2018-12-22 11:16:29.014040 UTC] Starting iteration 679
[2018-12-22 11:16:29.014156 UTC] Start collecting samples
[2018-12-22 11:16:31.978128 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:32.059561 UTC] Performing policy update
[2018-12-22 11:16:32.061237 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:32.152146 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:33.223883 UTC] Performing line search
[2018-12-22 11:16:33.353349 UTC] Updating baseline
[2018-12-22 11:16:34.988603 UTC] Computing logging information
-------------------------------------
| Iteration            | 679        |
| ExpectedImprovement  | 0.01778    |
| ActualImprovement    | 0.017715   |
| ImprovementRatio     | 0.99637    |
| MeanKL               | 0.0072357  |
| Entropy              | -0.27764   |
| Perplexity           | 0.75757    |
| AveragePolicyStd     | 0.23424    |
| AveragePolicyStd[0]  | 0.26321    |
| AveragePolicyStd[1]  | 0.28835    |
| AveragePolicyStd[2]  | 0.19861    |
| AveragePolicyStd[3]  | 0.23409    |
| AveragePolicyStd[4]  | 0.17513    |
| AveragePolicyStd[5]  | 0.24606    |
| AverageReturn        | 1400       |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 355.11     |
| AverageEpisodeLength | 915.86     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.84     |
| TotalNEpisodes       | 17993      |
| TotalNSamples        | 3.3994e+06 |
| ExplainedVariance    | -0.02483   |
-------------------------------------
[2018-12-22 11:16:35.363350 UTC] Saving snapshot
[2018-12-22 11:16:35.363605 UTC] Starting iteration 680
[2018-12-22 11:16:35.363725 UTC] Start collecting samples
[2018-12-22 11:16:38.307256 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:38.386312 UTC] Performing policy update
[2018-12-22 11:16:38.387239 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:38.475224 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:39.529351 UTC] Performing line search
[2018-12-22 11:16:39.656215 UTC] Updating baseline
[2018-12-22 11:16:41.195140 UTC] Computing logging information
-------------------------------------
| Iteration            | 680        |
| ExpectedImprovement  | 0.018846   |
| ActualImprovement    | 0.017284   |
| ImprovementRatio     | 0.91714    |
| MeanKL               | 0.0070929  |
| Entropy              | -0.27519   |
| Perplexity           | 0.75943    |
| AveragePolicyStd     | 0.23433    |
| AveragePolicyStd[0]  | 0.26281    |
| AveragePolicyStd[1]  | 0.28892    |
| AveragePolicyStd[2]  | 0.19863    |
| AveragePolicyStd[3]  | 0.23401    |
| AveragePolicyStd[4]  | 0.17547    |
| AveragePolicyStd[5]  | 0.24614    |
| AverageReturn        | 1395       |
| MinReturn            | 53.817     |
| MaxReturn            | 1580.9     |
| StdReturn            | 358.96     |
| AverageEpisodeLength | 913.32     |
| MinEpisodeLength     | 57         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.55     |
| TotalNEpisodes       | 17997      |
| TotalNSamples        | 3.4029e+06 |
| ExplainedVariance    | 0.17105    |
-------------------------------------
[2018-12-22 11:16:41.565698 UTC] Saving snapshot
[2018-12-22 11:16:41.573874 UTC] Starting iteration 681
[2018-12-22 11:16:41.574085 UTC] Start collecting samples
[2018-12-22 11:16:44.533257 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:44.614441 UTC] Performing policy update
[2018-12-22 11:16:44.615375 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:44.704969 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:45.771524 UTC] Performing line search
[2018-12-22 11:16:45.899751 UTC] Updating baseline
[2018-12-22 11:16:47.176850 UTC] Computing logging information
-------------------------------------
| Iteration            | 681        |
| ExpectedImprovement  | 0.016099   |
| ActualImprovement    | 0.015854   |
| ImprovementRatio     | 0.98475    |
| MeanKL               | 0.0073461  |
| Entropy              | -0.28654   |
| Perplexity           | 0.75086    |
| AveragePolicyStd     | 0.23382    |
| AveragePolicyStd[0]  | 0.26187    |
| AveragePolicyStd[1]  | 0.28705    |
| AveragePolicyStd[2]  | 0.1986     |
| AveragePolicyStd[3]  | 0.23377    |
| AveragePolicyStd[4]  | 0.17528    |
| AveragePolicyStd[5]  | 0.24637    |
| AverageReturn        | 1412.4     |
| MinReturn            | 103.8      |
| MaxReturn            | 1580.9     |
| StdReturn            | 328.13     |
| AverageEpisodeLength | 923.89     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.21     |
| TotalNEpisodes       | 18003      |
| TotalNSamples        | 3.4083e+06 |
| ExplainedVariance    | 0.12561    |
-------------------------------------
[2018-12-22 11:16:47.550735 UTC] Saving snapshot
[2018-12-22 11:16:47.550998 UTC] Starting iteration 682
[2018-12-22 11:16:47.551116 UTC] Start collecting samples
[2018-12-22 11:16:50.490968 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:50.569073 UTC] Performing policy update
[2018-12-22 11:16:50.569821 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:50.660247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:51.730717 UTC] Performing line search
[2018-12-22 11:16:51.858699 UTC] Updating baseline
[2018-12-22 11:16:53.402727 UTC] Computing logging information
-------------------------------------
| Iteration            | 682        |
| ExpectedImprovement  | 0.018688   |
| ActualImprovement    | 0.017253   |
| ImprovementRatio     | 0.92322    |
| MeanKL               | 0.0071346  |
| Entropy              | -0.28379   |
| Perplexity           | 0.75293    |
| AveragePolicyStd     | 0.23391    |
| AveragePolicyStd[0]  | 0.261      |
| AveragePolicyStd[1]  | 0.28761    |
| AveragePolicyStd[2]  | 0.19895    |
| AveragePolicyStd[3]  | 0.23418    |
| AveragePolicyStd[4]  | 0.17549    |
| AveragePolicyStd[5]  | 0.24622    |
| AverageReturn        | 1426.2     |
| MinReturn            | 103.8      |
| MaxReturn            | 1580.9     |
| StdReturn            | 303.19     |
| AverageEpisodeLength | 932.6      |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.23     |
| TotalNEpisodes       | 18006      |
| TotalNSamples        | 3.4113e+06 |
| ExplainedVariance    | 0.13044    |
-------------------------------------
[2018-12-22 11:16:53.778649 UTC] Saving snapshot
[2018-12-22 11:16:53.778899 UTC] Starting iteration 683
[2018-12-22 11:16:53.779055 UTC] Start collecting samples
[2018-12-22 11:16:56.785533 UTC] Computing input variables for policy optimization
[2018-12-22 11:16:56.866406 UTC] Performing policy update
[2018-12-22 11:16:56.867051 UTC] Computing gradient in Euclidean space
[2018-12-22 11:16:56.957281 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:16:58.022338 UTC] Performing line search
[2018-12-22 11:16:58.150141 UTC] Updating baseline
[2018-12-22 11:16:59.524847 UTC] Computing logging information
-------------------------------------
| Iteration            | 683        |
| ExpectedImprovement  | 0.017144   |
| ActualImprovement    | 0.01658    |
| ImprovementRatio     | 0.96713    |
| MeanKL               | 0.0068339  |
| Entropy              | -0.28648   |
| Perplexity           | 0.75091    |
| AveragePolicyStd     | 0.23377    |
| AveragePolicyStd[0]  | 0.26138    |
| AveragePolicyStd[1]  | 0.2853     |
| AveragePolicyStd[2]  | 0.19909    |
| AveragePolicyStd[3]  | 0.23438    |
| AveragePolicyStd[4]  | 0.17511    |
| AveragePolicyStd[5]  | 0.24734    |
| AverageReturn        | 1398.8     |
| MinReturn            | 96.681     |
| MaxReturn            | 1580.9     |
| StdReturn            | 353.97     |
| AverageEpisodeLength | 914.4      |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.74     |
| TotalNEpisodes       | 18015      |
| TotalNSamples        | 3.4182e+06 |
| ExplainedVariance    | 0.28754    |
-------------------------------------
[2018-12-22 11:16:59.893210 UTC] Saving snapshot
[2018-12-22 11:16:59.893469 UTC] Starting iteration 684
[2018-12-22 11:16:59.893614 UTC] Start collecting samples
[2018-12-22 11:17:02.868461 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:02.950586 UTC] Performing policy update
[2018-12-22 11:17:02.951714 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:03.042716 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:04.104218 UTC] Performing line search
[2018-12-22 11:17:04.230985 UTC] Updating baseline
[2018-12-22 11:17:05.688395 UTC] Computing logging information
-------------------------------------
| Iteration            | 684        |
| ExpectedImprovement  | 0.018623   |
| ActualImprovement    | 0.018472   |
| ImprovementRatio     | 0.99187    |
| MeanKL               | 0.0073387  |
| Entropy              | -0.28657   |
| Perplexity           | 0.75083    |
| AveragePolicyStd     | 0.23378    |
| AveragePolicyStd[0]  | 0.26073    |
| AveragePolicyStd[1]  | 0.28599    |
| AveragePolicyStd[2]  | 0.19841    |
| AveragePolicyStd[3]  | 0.23483    |
| AveragePolicyStd[4]  | 0.17529    |
| AveragePolicyStd[5]  | 0.24746    |
| AverageReturn        | 1385.2     |
| MinReturn            | 96.681     |
| MaxReturn            | 1580.9     |
| StdReturn            | 373.93     |
| AverageEpisodeLength | 906.24     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.1      |
| TotalNEpisodes       | 18022      |
| TotalNSamples        | 3.4242e+06 |
| ExplainedVariance    | 0.095677   |
-------------------------------------
[2018-12-22 11:17:06.065912 UTC] Saving snapshot
[2018-12-22 11:17:06.066163 UTC] Starting iteration 685
[2018-12-22 11:17:06.066282 UTC] Start collecting samples
[2018-12-22 11:17:08.953495 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:09.032113 UTC] Performing policy update
[2018-12-22 11:17:09.033039 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:09.123004 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:10.197447 UTC] Performing line search
[2018-12-22 11:17:10.324690 UTC] Updating baseline
[2018-12-22 11:17:11.609490 UTC] Computing logging information
-------------------------------------
| Iteration            | 685        |
| ExpectedImprovement  | 0.016308   |
| ActualImprovement    | 0.015127   |
| ImprovementRatio     | 0.92759    |
| MeanKL               | 0.0071145  |
| Entropy              | -0.28588   |
| Perplexity           | 0.75136    |
| AveragePolicyStd     | 0.2338     |
| AveragePolicyStd[0]  | 0.26169    |
| AveragePolicyStd[1]  | 0.28586    |
| AveragePolicyStd[2]  | 0.19824    |
| AveragePolicyStd[3]  | 0.23485    |
| AveragePolicyStd[4]  | 0.17561    |
| AveragePolicyStd[5]  | 0.24657    |
| AverageReturn        | 1383.1     |
| MinReturn            | 96.681     |
| MaxReturn            | 1580.9     |
| StdReturn            | 373.44     |
| AverageEpisodeLength | 905.06     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 235.92     |
| TotalNEpisodes       | 18025      |
| TotalNSamples        | 3.4271e+06 |
| ExplainedVariance    | 0.18342    |
-------------------------------------
[2018-12-22 11:17:11.983832 UTC] Saving snapshot
[2018-12-22 11:17:11.984076 UTC] Starting iteration 686
[2018-12-22 11:17:11.984198 UTC] Start collecting samples
[2018-12-22 11:17:14.951908 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:15.033126 UTC] Performing policy update
[2018-12-22 11:17:15.033800 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:15.123119 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:16.195635 UTC] Performing line search
[2018-12-22 11:17:16.324377 UTC] Updating baseline
[2018-12-22 11:17:17.829485 UTC] Computing logging information
-------------------------------------
| Iteration            | 686        |
| ExpectedImprovement  | 0.018748   |
| ActualImprovement    | 0.017607   |
| ImprovementRatio     | 0.93916    |
| MeanKL               | 0.0070749  |
| Entropy              | -0.28724   |
| Perplexity           | 0.75033    |
| AveragePolicyStd     | 0.23374    |
| AveragePolicyStd[0]  | 0.26161    |
| AveragePolicyStd[1]  | 0.28497    |
| AveragePolicyStd[2]  | 0.19725    |
| AveragePolicyStd[3]  | 0.23439    |
| AveragePolicyStd[4]  | 0.17604    |
| AveragePolicyStd[5]  | 0.24819    |
| AverageReturn        | 1366.2     |
| MinReturn            | 96.681     |
| MaxReturn            | 1580.9     |
| StdReturn            | 397.94     |
| AverageEpisodeLength | 894.07     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 251.93     |
| TotalNEpisodes       | 18032      |
| TotalNSamples        | 3.4326e+06 |
| ExplainedVariance    | 0.17586    |
-------------------------------------
[2018-12-22 11:17:18.230801 UTC] Saving snapshot
[2018-12-22 11:17:18.231122 UTC] Starting iteration 687
[2018-12-22 11:17:18.231267 UTC] Start collecting samples
[2018-12-22 11:17:21.407165 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:21.491007 UTC] Performing policy update
[2018-12-22 11:17:21.491872 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:21.586881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:22.699191 UTC] Performing line search
[2018-12-22 11:17:22.832327 UTC] Updating baseline
[2018-12-22 11:17:24.344154 UTC] Computing logging information
-------------------------------------
| Iteration            | 687        |
| ExpectedImprovement  | 0.01713    |
| ActualImprovement    | 0.016564   |
| ImprovementRatio     | 0.96697    |
| MeanKL               | 0.0074611  |
| Entropy              | -0.28789   |
| Perplexity           | 0.74984    |
| AveragePolicyStd     | 0.23376    |
| AveragePolicyStd[0]  | 0.26286    |
| AveragePolicyStd[1]  | 0.28426    |
| AveragePolicyStd[2]  | 0.19643    |
| AveragePolicyStd[3]  | 0.23478    |
| AveragePolicyStd[4]  | 0.17587    |
| AveragePolicyStd[5]  | 0.24835    |
| AverageReturn        | 1373.1     |
| MinReturn            | 96.681     |
| MaxReturn            | 1598       |
| StdReturn            | 395.81     |
| AverageEpisodeLength | 897.92     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.57     |
| TotalNEpisodes       | 18036      |
| TotalNSamples        | 3.4366e+06 |
| ExplainedVariance    | -0.023922  |
-------------------------------------
[2018-12-22 11:17:24.737999 UTC] Saving snapshot
[2018-12-22 11:17:24.738270 UTC] Starting iteration 688
[2018-12-22 11:17:24.738410 UTC] Start collecting samples
[2018-12-22 11:17:27.879090 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:27.963811 UTC] Performing policy update
[2018-12-22 11:17:27.964541 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:28.058941 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:29.122714 UTC] Performing line search
[2018-12-22 11:17:29.251449 UTC] Updating baseline
[2018-12-22 11:17:30.629914 UTC] Computing logging information
------------------------------------
| Iteration            | 688       |
| ExpectedImprovement  | 0.017868  |
| ActualImprovement    | 0.017151  |
| ImprovementRatio     | 0.95986   |
| MeanKL               | 0.0072302 |
| Entropy              | -0.28775  |
| Perplexity           | 0.74995   |
| AveragePolicyStd     | 0.2338    |
| AveragePolicyStd[0]  | 0.26303   |
| AveragePolicyStd[1]  | 0.28465   |
| AveragePolicyStd[2]  | 0.19588   |
| AveragePolicyStd[3]  | 0.23516   |
| AveragePolicyStd[4]  | 0.17572   |
| AveragePolicyStd[5]  | 0.24837   |
| AverageReturn        | 1387.6    |
| MinReturn            | 96.681    |
| MaxReturn            | 1598      |
| StdReturn            | 373.58    |
| AverageEpisodeLength | 907.31    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 236.69    |
| TotalNEpisodes       | 18042     |
| TotalNSamples        | 3.442e+06 |
| ExplainedVariance    | 0.15854   |
------------------------------------
[2018-12-22 11:17:31.006371 UTC] Saving snapshot
[2018-12-22 11:17:31.006647 UTC] Starting iteration 689
[2018-12-22 11:17:31.006775 UTC] Start collecting samples
[2018-12-22 11:17:34.013336 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:34.095218 UTC] Performing policy update
[2018-12-22 11:17:34.095820 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:34.186889 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:35.253216 UTC] Performing line search
[2018-12-22 11:17:35.380882 UTC] Updating baseline
[2018-12-22 11:17:36.662683 UTC] Computing logging information
-------------------------------------
| Iteration            | 689        |
| ExpectedImprovement  | 0.020883   |
| ActualImprovement    | 0.018506   |
| ImprovementRatio     | 0.88616    |
| MeanKL               | 0.0072365  |
| Entropy              | -0.29218   |
| Perplexity           | 0.74663    |
| AveragePolicyStd     | 0.23365    |
| AveragePolicyStd[0]  | 0.26282    |
| AveragePolicyStd[1]  | 0.28444    |
| AveragePolicyStd[2]  | 0.19573    |
| AveragePolicyStd[3]  | 0.23524    |
| AveragePolicyStd[4]  | 0.17531    |
| AveragePolicyStd[5]  | 0.24835    |
| AverageReturn        | 1376       |
| MinReturn            | 96.681     |
| MaxReturn            | 1598       |
| StdReturn            | 394.98     |
| AverageEpisodeLength | 899.26     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.4      |
| TotalNEpisodes       | 18050      |
| TotalNSamples        | 3.4491e+06 |
| ExplainedVariance    | 0.046755   |
-------------------------------------
[2018-12-22 11:17:37.036589 UTC] Saving snapshot
[2018-12-22 11:17:37.036831 UTC] Starting iteration 690
[2018-12-22 11:17:37.036950 UTC] Start collecting samples
[2018-12-22 11:17:39.952144 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:40.031725 UTC] Performing policy update
[2018-12-22 11:17:40.032338 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:40.121566 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:41.187193 UTC] Performing line search
[2018-12-22 11:17:41.315813 UTC] Updating baseline
[2018-12-22 11:17:42.510244 UTC] Computing logging information
-------------------------------------
| Iteration            | 690        |
| ExpectedImprovement  | 0.021797   |
| ActualImprovement    | 0.020022   |
| ImprovementRatio     | 0.91856    |
| MeanKL               | 0.0068684  |
| Entropy              | -0.30095   |
| Perplexity           | 0.74012    |
| AveragePolicyStd     | 0.23327    |
| AveragePolicyStd[0]  | 0.26235    |
| AveragePolicyStd[1]  | 0.2827     |
| AveragePolicyStd[2]  | 0.19543    |
| AveragePolicyStd[3]  | 0.23551    |
| AveragePolicyStd[4]  | 0.17508    |
| AveragePolicyStd[5]  | 0.24856    |
| AverageReturn        | 1376.5     |
| MinReturn            | 96.681     |
| MaxReturn            | 1598       |
| StdReturn            | 395.2      |
| AverageEpisodeLength | 899.26     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.4      |
| TotalNEpisodes       | 18053      |
| TotalNSamples        | 3.4521e+06 |
| ExplainedVariance    | -0.012624  |
-------------------------------------
[2018-12-22 11:17:42.881941 UTC] Saving snapshot
[2018-12-22 11:17:42.890032 UTC] Starting iteration 691
[2018-12-22 11:17:42.890229 UTC] Start collecting samples
[2018-12-22 11:17:45.807826 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:45.887907 UTC] Performing policy update
[2018-12-22 11:17:45.888551 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:45.979836 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:47.044082 UTC] Performing line search
[2018-12-22 11:17:47.171879 UTC] Updating baseline
[2018-12-22 11:17:48.362665 UTC] Computing logging information
-------------------------------------
| Iteration            | 691        |
| ExpectedImprovement  | 0.018058   |
| ActualImprovement    | 0.017748   |
| ImprovementRatio     | 0.98283    |
| MeanKL               | 0.0075235  |
| Entropy              | -0.30586   |
| Perplexity           | 0.73649    |
| AveragePolicyStd     | 0.23304    |
| AveragePolicyStd[0]  | 0.26195    |
| AveragePolicyStd[1]  | 0.28162    |
| AveragePolicyStd[2]  | 0.19575    |
| AveragePolicyStd[3]  | 0.23585    |
| AveragePolicyStd[4]  | 0.17485    |
| AveragePolicyStd[5]  | 0.24822    |
| AverageReturn        | 1377.9     |
| MinReturn            | 96.681     |
| MaxReturn            | 1598       |
| StdReturn            | 395.74     |
| AverageEpisodeLength | 899.26     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.4      |
| TotalNEpisodes       | 18057      |
| TotalNSamples        | 3.4561e+06 |
| ExplainedVariance    | -0.044386  |
-------------------------------------
[2018-12-22 11:17:48.735491 UTC] Saving snapshot
[2018-12-22 11:17:48.735762 UTC] Starting iteration 692
[2018-12-22 11:17:48.736063 UTC] Start collecting samples
[2018-12-22 11:17:51.794408 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:51.881354 UTC] Performing policy update
[2018-12-22 11:17:51.882033 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:51.980923 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:53.105076 UTC] Performing line search
[2018-12-22 11:17:53.240056 UTC] Updating baseline
[2018-12-22 11:17:54.575880 UTC] Computing logging information
-------------------------------------
| Iteration            | 692        |
| ExpectedImprovement  | 0.020298   |
| ActualImprovement    | 0.018992   |
| ImprovementRatio     | 0.93567    |
| MeanKL               | 0.0073175  |
| Entropy              | -0.31641   |
| Perplexity           | 0.72876    |
| AveragePolicyStd     | 0.23264    |
| AveragePolicyStd[0]  | 0.26143    |
| AveragePolicyStd[1]  | 0.28094    |
| AveragePolicyStd[2]  | 0.1949     |
| AveragePolicyStd[3]  | 0.23605    |
| AveragePolicyStd[4]  | 0.17462    |
| AveragePolicyStd[5]  | 0.2479     |
| AverageReturn        | 1380.7     |
| MinReturn            | 96.681     |
| MaxReturn            | 1610.2     |
| StdReturn            | 396.81     |
| AverageEpisodeLength | 899.48     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.48     |
| TotalNEpisodes       | 18065      |
| TotalNSamples        | 3.4641e+06 |
| ExplainedVariance    | -0.0048212 |
-------------------------------------
[2018-12-22 11:17:54.978472 UTC] Saving snapshot
[2018-12-22 11:17:54.978770 UTC] Starting iteration 693
[2018-12-22 11:17:54.978889 UTC] Start collecting samples
[2018-12-22 11:17:58.020600 UTC] Computing input variables for policy optimization
[2018-12-22 11:17:58.101252 UTC] Performing policy update
[2018-12-22 11:17:58.101906 UTC] Computing gradient in Euclidean space
[2018-12-22 11:17:58.191398 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:17:59.258136 UTC] Performing line search
[2018-12-22 11:17:59.388999 UTC] Updating baseline
[2018-12-22 11:18:01.025600 UTC] Computing logging information
-------------------------------------
| Iteration            | 693        |
| ExpectedImprovement  | 0.019247   |
| ActualImprovement    | 0.017825   |
| ImprovementRatio     | 0.92612    |
| MeanKL               | 0.0072685  |
| Entropy              | -0.31416   |
| Perplexity           | 0.7304     |
| AveragePolicyStd     | 0.23273    |
| AveragePolicyStd[0]  | 0.2604     |
| AveragePolicyStd[1]  | 0.28125    |
| AveragePolicyStd[2]  | 0.19438    |
| AveragePolicyStd[3]  | 0.23654    |
| AveragePolicyStd[4]  | 0.17491    |
| AveragePolicyStd[5]  | 0.2489     |
| AverageReturn        | 1391.8     |
| MinReturn            | 96.681     |
| MaxReturn            | 1610.2     |
| StdReturn            | 386.11     |
| AverageEpisodeLength | 906.53     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 243.18     |
| TotalNEpisodes       | 18069      |
| TotalNSamples        | 3.4681e+06 |
| ExplainedVariance    | -0.0038116 |
-------------------------------------
[2018-12-22 11:18:01.403113 UTC] Saving snapshot
[2018-12-22 11:18:01.403383 UTC] Starting iteration 694
[2018-12-22 11:18:01.403533 UTC] Start collecting samples
[2018-12-22 11:18:04.344733 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:04.425347 UTC] Performing policy update
[2018-12-22 11:18:04.426052 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:04.516671 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:05.582370 UTC] Performing line search
[2018-12-22 11:18:05.710592 UTC] Updating baseline
[2018-12-22 11:18:07.505721 UTC] Computing logging information
-------------------------------------
| Iteration            | 694        |
| ExpectedImprovement  | 0.016333   |
| ActualImprovement    | 0.015464   |
| ImprovementRatio     | 0.94677    |
| MeanKL               | 0.0072421  |
| Entropy              | -0.31168   |
| Perplexity           | 0.73221    |
| AveragePolicyStd     | 0.23275    |
| AveragePolicyStd[0]  | 0.26012    |
| AveragePolicyStd[1]  | 0.2805     |
| AveragePolicyStd[2]  | 0.1947     |
| AveragePolicyStd[3]  | 0.23654    |
| AveragePolicyStd[4]  | 0.1756     |
| AveragePolicyStd[5]  | 0.24907    |
| AverageReturn        | 1393.9     |
| MinReturn            | 96.681     |
| MaxReturn            | 1615.4     |
| StdReturn            | 386.96     |
| AverageEpisodeLength | 906.53     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 243.18     |
| TotalNEpisodes       | 18073      |
| TotalNSamples        | 3.4721e+06 |
| ExplainedVariance    | 0.0052849  |
-------------------------------------
[2018-12-22 11:18:07.880401 UTC] Saving snapshot
[2018-12-22 11:18:07.880673 UTC] Starting iteration 695
[2018-12-22 11:18:07.880805 UTC] Start collecting samples
[2018-12-22 11:18:10.873925 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:10.958400 UTC] Performing policy update
[2018-12-22 11:18:10.959264 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:11.050691 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:12.124734 UTC] Performing line search
[2018-12-22 11:18:12.252397 UTC] Updating baseline
[2018-12-22 11:18:13.681393 UTC] Computing logging information
-------------------------------------
| Iteration            | 695        |
| ExpectedImprovement  | 0.018693   |
| ActualImprovement    | 0.017905   |
| ImprovementRatio     | 0.95782    |
| MeanKL               | 0.0073591  |
| Entropy              | -0.30962   |
| Perplexity           | 0.73372    |
| AveragePolicyStd     | 0.23277    |
| AveragePolicyStd[0]  | 0.25978    |
| AveragePolicyStd[1]  | 0.27967    |
| AveragePolicyStd[2]  | 0.19571    |
| AveragePolicyStd[3]  | 0.23665    |
| AveragePolicyStd[4]  | 0.17569    |
| AveragePolicyStd[5]  | 0.24913    |
| AverageReturn        | 1405.7     |
| MinReturn            | 96.681     |
| MaxReturn            | 1615.4     |
| StdReturn            | 377.37     |
| AverageEpisodeLength | 913.13     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.58     |
| TotalNEpisodes       | 18080      |
| TotalNSamples        | 3.4791e+06 |
| ExplainedVariance    | 0.0056252  |
-------------------------------------
[2018-12-22 11:18:14.059775 UTC] Saving snapshot
[2018-12-22 11:18:14.060029 UTC] Starting iteration 696
[2018-12-22 11:18:14.060150 UTC] Start collecting samples
[2018-12-22 11:18:17.007466 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:17.084958 UTC] Performing policy update
[2018-12-22 11:18:17.085624 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:17.175221 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:18.199529 UTC] Performing line search
[2018-12-22 11:18:18.326793 UTC] Updating baseline
[2018-12-22 11:18:19.969234 UTC] Computing logging information
-------------------------------------
| Iteration            | 696        |
| ExpectedImprovement  | 0.019338   |
| ActualImprovement    | 0.018045   |
| ImprovementRatio     | 0.93313    |
| MeanKL               | 0.0070425  |
| Entropy              | -0.31156   |
| Perplexity           | 0.7323     |
| AveragePolicyStd     | 0.23264    |
| AveragePolicyStd[0]  | 0.25861    |
| AveragePolicyStd[1]  | 0.27939    |
| AveragePolicyStd[2]  | 0.19609    |
| AveragePolicyStd[3]  | 0.23703    |
| AveragePolicyStd[4]  | 0.17594    |
| AveragePolicyStd[5]  | 0.24876    |
| AverageReturn        | 1399.4     |
| MinReturn            | 96.681     |
| MaxReturn            | 1615.4     |
| StdReturn            | 385.71     |
| AverageEpisodeLength | 908.55     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 241.11     |
| TotalNEpisodes       | 18084      |
| TotalNSamples        | 3.4826e+06 |
| ExplainedVariance    | 0.11568    |
-------------------------------------
[2018-12-22 11:18:20.339213 UTC] Saving snapshot
[2018-12-22 11:18:20.339467 UTC] Starting iteration 697
[2018-12-22 11:18:20.339603 UTC] Start collecting samples
[2018-12-22 11:18:23.295489 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:23.375694 UTC] Performing policy update
[2018-12-22 11:18:23.376357 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:23.464150 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:24.538094 UTC] Performing line search
[2018-12-22 11:18:24.665590 UTC] Updating baseline
[2018-12-22 11:18:26.021959 UTC] Computing logging information
-------------------------------------
| Iteration            | 697        |
| ExpectedImprovement  | 0.019797   |
| ActualImprovement    | 0.018648   |
| ImprovementRatio     | 0.94197    |
| MeanKL               | 0.0071866  |
| Entropy              | -0.31221   |
| Perplexity           | 0.73183    |
| AveragePolicyStd     | 0.23254    |
| AveragePolicyStd[0]  | 0.2579     |
| AveragePolicyStd[1]  | 0.27834    |
| AveragePolicyStd[2]  | 0.19652    |
| AveragePolicyStd[3]  | 0.23756    |
| AveragePolicyStd[4]  | 0.17628    |
| AveragePolicyStd[5]  | 0.24866    |
| AverageReturn        | 1418.1     |
| MinReturn            | 96.681     |
| MaxReturn            | 1615.4     |
| StdReturn            | 361.46     |
| AverageEpisodeLength | 919.18     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.52     |
| TotalNEpisodes       | 18090      |
| TotalNSamples        | 3.4883e+06 |
| ExplainedVariance    | 0.10363    |
-------------------------------------
[2018-12-22 11:18:26.393616 UTC] Saving snapshot
[2018-12-22 11:18:26.393905 UTC] Starting iteration 698
[2018-12-22 11:18:26.394029 UTC] Start collecting samples
[2018-12-22 11:18:29.378051 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:29.459961 UTC] Performing policy update
[2018-12-22 11:18:29.460799 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:29.553664 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:30.620035 UTC] Performing line search
[2018-12-22 11:18:30.750005 UTC] Updating baseline
[2018-12-22 11:18:32.220174 UTC] Computing logging information
-------------------------------------
| Iteration            | 698        |
| ExpectedImprovement  | 0.015476   |
| ActualImprovement    | 0.014551   |
| ImprovementRatio     | 0.94024    |
| MeanKL               | 0.0077252  |
| Entropy              | -0.31619   |
| Perplexity           | 0.72892    |
| AveragePolicyStd     | 0.23239    |
| AveragePolicyStd[0]  | 0.25747    |
| AveragePolicyStd[1]  | 0.27791    |
| AveragePolicyStd[2]  | 0.19635    |
| AveragePolicyStd[3]  | 0.23828    |
| AveragePolicyStd[4]  | 0.17599    |
| AveragePolicyStd[5]  | 0.24831    |
| AverageReturn        | 1405.2     |
| MinReturn            | 96.681     |
| MaxReturn            | 1640.7     |
| StdReturn            | 384.91     |
| AverageEpisodeLength | 909.44     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.62     |
| TotalNEpisodes       | 18096      |
| TotalNSamples        | 3.4929e+06 |
| ExplainedVariance    | 0.22595    |
-------------------------------------
[2018-12-22 11:18:32.597025 UTC] Saving snapshot
[2018-12-22 11:18:32.597267 UTC] Starting iteration 699
[2018-12-22 11:18:32.597400 UTC] Start collecting samples
[2018-12-22 11:18:35.687009 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:35.769039 UTC] Performing policy update
[2018-12-22 11:18:35.769760 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:35.860405 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:36.937603 UTC] Performing line search
[2018-12-22 11:18:37.065999 UTC] Updating baseline
[2018-12-22 11:18:38.614724 UTC] Computing logging information
-------------------------------------
| Iteration            | 699        |
| ExpectedImprovement  | 0.018865   |
| ActualImprovement    | 0.01798    |
| ImprovementRatio     | 0.9531     |
| MeanKL               | 0.0073747  |
| Entropy              | -0.31087   |
| Perplexity           | 0.73281    |
| AveragePolicyStd     | 0.23261    |
| AveragePolicyStd[0]  | 0.25755    |
| AveragePolicyStd[1]  | 0.27793    |
| AveragePolicyStd[2]  | 0.19568    |
| AveragePolicyStd[3]  | 0.23788    |
| AveragePolicyStd[4]  | 0.17645    |
| AveragePolicyStd[5]  | 0.25017    |
| AverageReturn        | 1402.9     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 396.83     |
| AverageEpisodeLength | 906.79     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.2      |
| TotalNEpisodes       | 18101      |
| TotalNSamples        | 3.4969e+06 |
| ExplainedVariance    | 0.1191     |
-------------------------------------
[2018-12-22 11:18:38.992978 UTC] Saving snapshot
[2018-12-22 11:18:38.993290 UTC] Starting iteration 700
[2018-12-22 11:18:38.993465 UTC] Start collecting samples
[2018-12-22 11:18:42.002176 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:42.086036 UTC] Performing policy update
[2018-12-22 11:18:42.086914 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:42.177813 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:43.239243 UTC] Performing line search
[2018-12-22 11:18:43.368137 UTC] Updating baseline
[2018-12-22 11:18:44.728576 UTC] Computing logging information
-------------------------------------
| Iteration            | 700        |
| ExpectedImprovement  | 0.016456   |
| ActualImprovement    | 0.015992   |
| ImprovementRatio     | 0.97181    |
| MeanKL               | 0.0075559  |
| Entropy              | -0.31414   |
| Perplexity           | 0.73042    |
| AveragePolicyStd     | 0.23248    |
| AveragePolicyStd[0]  | 0.25702    |
| AveragePolicyStd[1]  | 0.27731    |
| AveragePolicyStd[2]  | 0.19551    |
| AveragePolicyStd[3]  | 0.23814    |
| AveragePolicyStd[4]  | 0.17626    |
| AveragePolicyStd[5]  | 0.25064    |
| AverageReturn        | 1410.6     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 397.77     |
| AverageEpisodeLength | 908.93     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.07     |
| TotalNEpisodes       | 18108      |
| TotalNSamples        | 3.5039e+06 |
| ExplainedVariance    | -0.002671  |
-------------------------------------
[2018-12-22 11:18:45.105837 UTC] Saving snapshot
[2018-12-22 11:18:45.113969 UTC] Starting iteration 701
[2018-12-22 11:18:45.114180 UTC] Start collecting samples
[2018-12-22 11:18:48.053097 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:48.131903 UTC] Performing policy update
[2018-12-22 11:18:48.132566 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:48.223735 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:49.296391 UTC] Performing line search
[2018-12-22 11:18:49.425594 UTC] Updating baseline
[2018-12-22 11:18:50.980309 UTC] Computing logging information
-------------------------------------
| Iteration            | 701        |
| ExpectedImprovement  | 0.017304   |
| ActualImprovement    | 0.016317   |
| ImprovementRatio     | 0.94295    |
| MeanKL               | 0.0079855  |
| Entropy              | -0.31546   |
| Perplexity           | 0.72945    |
| AveragePolicyStd     | 0.23245    |
| AveragePolicyStd[0]  | 0.2578     |
| AveragePolicyStd[1]  | 0.27665    |
| AveragePolicyStd[2]  | 0.19531    |
| AveragePolicyStd[3]  | 0.23807    |
| AveragePolicyStd[4]  | 0.17602    |
| AveragePolicyStd[5]  | 0.25083    |
| AverageReturn        | 1411.4     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 398.05     |
| AverageEpisodeLength | 908.93     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.07     |
| TotalNEpisodes       | 18110      |
| TotalNSamples        | 3.5059e+06 |
| ExplainedVariance    | -0.020669  |
-------------------------------------
[2018-12-22 11:18:51.359129 UTC] Saving snapshot
[2018-12-22 11:18:51.359376 UTC] Starting iteration 702
[2018-12-22 11:18:51.359493 UTC] Start collecting samples
[2018-12-22 11:18:54.351115 UTC] Computing input variables for policy optimization
[2018-12-22 11:18:54.433657 UTC] Performing policy update
[2018-12-22 11:18:54.434261 UTC] Computing gradient in Euclidean space
[2018-12-22 11:18:54.526659 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:18:55.595740 UTC] Performing line search
[2018-12-22 11:18:55.723826 UTC] Updating baseline
[2018-12-22 11:18:57.004910 UTC] Computing logging information
-------------------------------------
| Iteration            | 702        |
| ExpectedImprovement  | 0.017894   |
| ActualImprovement    | 0.016892   |
| ImprovementRatio     | 0.94404    |
| MeanKL               | 0.0069962  |
| Entropy              | -0.32372   |
| Perplexity           | 0.72346    |
| AveragePolicyStd     | 0.23211    |
| AveragePolicyStd[0]  | 0.25719    |
| AveragePolicyStd[1]  | 0.2757     |
| AveragePolicyStd[2]  | 0.19492    |
| AveragePolicyStd[3]  | 0.23837    |
| AveragePolicyStd[4]  | 0.17592    |
| AveragePolicyStd[5]  | 0.25054    |
| AverageReturn        | 1458.3     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 328.16     |
| AverageEpisodeLength | 937.71     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.88     |
| TotalNEpisodes       | 18117      |
| TotalNSamples        | 3.5129e+06 |
| ExplainedVariance    | -0.016097  |
-------------------------------------
[2018-12-22 11:18:57.378814 UTC] Saving snapshot
[2018-12-22 11:18:57.379052 UTC] Starting iteration 703
[2018-12-22 11:18:57.379168 UTC] Start collecting samples
[2018-12-22 11:19:00.360991 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:00.439017 UTC] Performing policy update
[2018-12-22 11:19:00.439595 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:00.528055 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:01.589561 UTC] Performing line search
[2018-12-22 11:19:01.717437 UTC] Updating baseline
[2018-12-22 11:19:03.700254 UTC] Computing logging information
-------------------------------------
| Iteration            | 703        |
| ExpectedImprovement  | 0.019041   |
| ActualImprovement    | 0.017091   |
| ImprovementRatio     | 0.89762    |
| MeanKL               | 0.0073093  |
| Entropy              | -0.3206    |
| Perplexity           | 0.72571    |
| AveragePolicyStd     | 0.23221    |
| AveragePolicyStd[0]  | 0.25672    |
| AveragePolicyStd[1]  | 0.27565    |
| AveragePolicyStd[2]  | 0.19561    |
| AveragePolicyStd[3]  | 0.23899    |
| AveragePolicyStd[4]  | 0.17573    |
| AveragePolicyStd[5]  | 0.25056    |
| AverageReturn        | 1460.3     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 328.72     |
| AverageEpisodeLength | 936.99     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.79     |
| TotalNEpisodes       | 18123      |
| TotalNSamples        | 3.5189e+06 |
| ExplainedVariance    | 0.08592    |
-------------------------------------
[2018-12-22 11:19:04.077897 UTC] Saving snapshot
[2018-12-22 11:19:04.078142 UTC] Starting iteration 704
[2018-12-22 11:19:04.078259 UTC] Start collecting samples
[2018-12-22 11:19:07.009025 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:07.087147 UTC] Performing policy update
[2018-12-22 11:19:07.088027 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:07.177857 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:08.200495 UTC] Performing line search
[2018-12-22 11:19:08.329208 UTC] Updating baseline
[2018-12-22 11:19:10.239862 UTC] Computing logging information
------------------------------------
| Iteration            | 704       |
| ExpectedImprovement  | 0.016773  |
| ActualImprovement    | 0.015445  |
| ImprovementRatio     | 0.92084   |
| MeanKL               | 0.0070196 |
| Entropy              | -0.32753  |
| Perplexity           | 0.7207    |
| AveragePolicyStd     | 0.23187   |
| AveragePolicyStd[0]  | 0.2564    |
| AveragePolicyStd[1]  | 0.27469   |
| AveragePolicyStd[2]  | 0.19558   |
| AveragePolicyStd[3]  | 0.23889   |
| AveragePolicyStd[4]  | 0.17622   |
| AveragePolicyStd[5]  | 0.24945   |
| AverageReturn        | 1449.7    |
| MinReturn            | 91.888    |
| MaxReturn            | 1640.7    |
| StdReturn            | 350.69    |
| AverageEpisodeLength | 929.96    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 217.24    |
| TotalNEpisodes       | 18126     |
| TotalNSamples        | 3.521e+06 |
| ExplainedVariance    | 0.026545  |
------------------------------------
[2018-12-22 11:19:10.611400 UTC] Saving snapshot
[2018-12-22 11:19:10.611660 UTC] Starting iteration 705
[2018-12-22 11:19:10.611789 UTC] Start collecting samples
[2018-12-22 11:19:13.615942 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:13.697932 UTC] Performing policy update
[2018-12-22 11:19:13.698577 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:13.788117 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:14.857889 UTC] Performing line search
[2018-12-22 11:19:14.988006 UTC] Updating baseline
[2018-12-22 11:19:16.547696 UTC] Computing logging information
------------------------------------
| Iteration            | 705       |
| ExpectedImprovement  | 0.01793   |
| ActualImprovement    | 0.017145  |
| ImprovementRatio     | 0.95619   |
| MeanKL               | 0.0076272 |
| Entropy              | -0.33899  |
| Perplexity           | 0.71249   |
| AveragePolicyStd     | 0.23149   |
| AveragePolicyStd[0]  | 0.25575   |
| AveragePolicyStd[1]  | 0.27422   |
| AveragePolicyStd[2]  | 0.19432   |
| AveragePolicyStd[3]  | 0.23923   |
| AveragePolicyStd[4]  | 0.17549   |
| AveragePolicyStd[5]  | 0.24995   |
| AverageReturn        | 1476.8    |
| MinReturn            | 91.888    |
| MaxReturn            | 1640.7    |
| StdReturn            | 314.77    |
| AverageEpisodeLength | 944.83    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 193.74    |
| TotalNEpisodes       | 18134     |
| TotalNSamples        | 3.529e+06 |
| ExplainedVariance    | -0.016582 |
------------------------------------
[2018-12-22 11:19:16.922584 UTC] Saving snapshot
[2018-12-22 11:19:16.922829 UTC] Starting iteration 706
[2018-12-22 11:19:16.922946 UTC] Start collecting samples
[2018-12-22 11:19:19.848595 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:19.928236 UTC] Performing policy update
[2018-12-22 11:19:19.929033 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:20.020718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:21.089443 UTC] Performing line search
[2018-12-22 11:19:21.219810 UTC] Updating baseline
[2018-12-22 11:19:22.951749 UTC] Computing logging information
------------------------------------
| Iteration            | 706       |
| ExpectedImprovement  | 0.017757  |
| ActualImprovement    | 0.016379  |
| ImprovementRatio     | 0.9224    |
| MeanKL               | 0.0076268 |
| Entropy              | -0.34113  |
| Perplexity           | 0.71097   |
| AveragePolicyStd     | 0.23146   |
| AveragePolicyStd[0]  | 0.25532   |
| AveragePolicyStd[1]  | 0.27446   |
| AveragePolicyStd[2]  | 0.19353   |
| AveragePolicyStd[3]  | 0.2389    |
| AveragePolicyStd[4]  | 0.17529   |
| AveragePolicyStd[5]  | 0.25126   |
| AverageReturn        | 1477.8    |
| MinReturn            | 91.888    |
| MaxReturn            | 1640.7    |
| StdReturn            | 315.06    |
| AverageEpisodeLength | 944.83    |
| MinEpisodeLength     | 77        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 193.74    |
| TotalNEpisodes       | 18137     |
| TotalNSamples        | 3.532e+06 |
| ExplainedVariance    | -0.015036 |
------------------------------------
[2018-12-22 11:19:23.329768 UTC] Saving snapshot
[2018-12-22 11:19:23.330025 UTC] Starting iteration 707
[2018-12-22 11:19:23.330144 UTC] Start collecting samples
[2018-12-22 11:19:26.294174 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:26.373218 UTC] Performing policy update
[2018-12-22 11:19:26.374165 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:26.465227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:27.526854 UTC] Performing line search
[2018-12-22 11:19:27.654653 UTC] Updating baseline
[2018-12-22 11:19:29.468901 UTC] Computing logging information
-------------------------------------
| Iteration            | 707        |
| ExpectedImprovement  | 0.020135   |
| ActualImprovement    | 0.018677   |
| ImprovementRatio     | 0.9276     |
| MeanKL               | 0.0069687  |
| Entropy              | -0.34579   |
| Perplexity           | 0.70766    |
| AveragePolicyStd     | 0.23125    |
| AveragePolicyStd[0]  | 0.25528    |
| AveragePolicyStd[1]  | 0.2734     |
| AveragePolicyStd[2]  | 0.19358    |
| AveragePolicyStd[3]  | 0.23864    |
| AveragePolicyStd[4]  | 0.17526    |
| AveragePolicyStd[5]  | 0.25137    |
| AverageReturn        | 1486.9     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 305.93     |
| AverageEpisodeLength | 950.06     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.01     |
| TotalNEpisodes       | 18141      |
| TotalNSamples        | 3.536e+06  |
| ExplainedVariance    | 0.00034158 |
-------------------------------------
[2018-12-22 11:19:29.842307 UTC] Saving snapshot
[2018-12-22 11:19:29.842570 UTC] Starting iteration 708
[2018-12-22 11:19:29.842687 UTC] Start collecting samples
[2018-12-22 11:19:32.830722 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:32.912614 UTC] Performing policy update
[2018-12-22 11:19:32.913189 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:33.004723 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:34.077496 UTC] Performing line search
[2018-12-22 11:19:34.206840 UTC] Updating baseline
[2018-12-22 11:19:35.574830 UTC] Computing logging information
-------------------------------------
| Iteration            | 708        |
| ExpectedImprovement  | 0.017243   |
| ActualImprovement    | 0.016799   |
| ImprovementRatio     | 0.97423    |
| MeanKL               | 0.0072847  |
| Entropy              | -0.3496    |
| Perplexity           | 0.70497    |
| AveragePolicyStd     | 0.23109    |
| AveragePolicyStd[0]  | 0.25526    |
| AveragePolicyStd[1]  | 0.27312    |
| AveragePolicyStd[2]  | 0.19345    |
| AveragePolicyStd[3]  | 0.23767    |
| AveragePolicyStd[4]  | 0.1755     |
| AveragePolicyStd[5]  | 0.25154    |
| AverageReturn        | 1502.3     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 274.68     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.92     |
| TotalNEpisodes       | 18149      |
| TotalNSamples        | 3.5439e+06 |
| ExplainedVariance    | 0.051051   |
-------------------------------------
[2018-12-22 11:19:35.956811 UTC] Saving snapshot
[2018-12-22 11:19:35.957057 UTC] Starting iteration 709
[2018-12-22 11:19:35.957183 UTC] Start collecting samples
[2018-12-22 11:19:38.900456 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:38.979287 UTC] Performing policy update
[2018-12-22 11:19:38.980015 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:39.068815 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:40.142055 UTC] Performing line search
[2018-12-22 11:19:40.270368 UTC] Updating baseline
[2018-12-22 11:19:41.463852 UTC] Computing logging information
-------------------------------------
| Iteration            | 709        |
| ExpectedImprovement  | 0.017171   |
| ActualImprovement    | 0.016038   |
| ImprovementRatio     | 0.934      |
| MeanKL               | 0.007891   |
| Entropy              | -0.34288   |
| Perplexity           | 0.70972    |
| AveragePolicyStd     | 0.2314     |
| AveragePolicyStd[0]  | 0.25583    |
| AveragePolicyStd[1]  | 0.27443    |
| AveragePolicyStd[2]  | 0.19274    |
| AveragePolicyStd[3]  | 0.2381     |
| AveragePolicyStd[4]  | 0.17573    |
| AveragePolicyStd[5]  | 0.25159    |
| AverageReturn        | 1504.5     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 275.35     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.92     |
| TotalNEpisodes       | 18153      |
| TotalNSamples        | 3.5479e+06 |
| ExplainedVariance    | -0.051299  |
-------------------------------------
[2018-12-22 11:19:41.838226 UTC] Saving snapshot
[2018-12-22 11:19:41.838483 UTC] Starting iteration 710
[2018-12-22 11:19:41.838635 UTC] Start collecting samples
[2018-12-22 11:19:44.783881 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:44.863608 UTC] Performing policy update
[2018-12-22 11:19:44.864234 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:44.955868 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:46.024242 UTC] Performing line search
[2018-12-22 11:19:46.152054 UTC] Updating baseline
[2018-12-22 11:19:47.779040 UTC] Computing logging information
-------------------------------------
| Iteration            | 710        |
| ExpectedImprovement  | 0.016318   |
| ActualImprovement    | 0.015682   |
| ImprovementRatio     | 0.96103    |
| MeanKL               | 0.0076436  |
| Entropy              | -0.35568   |
| Perplexity           | 0.7007     |
| AveragePolicyStd     | 0.23093    |
| AveragePolicyStd[0]  | 0.25516    |
| AveragePolicyStd[1]  | 0.27347    |
| AveragePolicyStd[2]  | 0.19268    |
| AveragePolicyStd[3]  | 0.23807    |
| AveragePolicyStd[4]  | 0.17478    |
| AveragePolicyStd[5]  | 0.25141    |
| AverageReturn        | 1505.6     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.7     |
| StdReturn            | 275.66     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.92     |
| TotalNEpisodes       | 18157      |
| TotalNSamples        | 3.5519e+06 |
| ExplainedVariance    | 0.0039404  |
-------------------------------------
[2018-12-22 11:19:48.159363 UTC] Saving snapshot
[2018-12-22 11:19:48.166632 UTC] Starting iteration 711
[2018-12-22 11:19:48.166844 UTC] Start collecting samples
[2018-12-22 11:19:51.128425 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:51.207356 UTC] Performing policy update
[2018-12-22 11:19:51.208325 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:51.299216 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:52.377727 UTC] Performing line search
[2018-12-22 11:19:52.506774 UTC] Updating baseline
[2018-12-22 11:19:54.041344 UTC] Computing logging information
-------------------------------------
| Iteration            | 711        |
| ExpectedImprovement  | 0.018077   |
| ActualImprovement    | 0.017317   |
| ImprovementRatio     | 0.95799    |
| MeanKL               | 0.0072355  |
| Entropy              | -0.35666   |
| Perplexity           | 0.70001    |
| AveragePolicyStd     | 0.23089    |
| AveragePolicyStd[0]  | 0.2546     |
| AveragePolicyStd[1]  | 0.27327    |
| AveragePolicyStd[2]  | 0.19224    |
| AveragePolicyStd[3]  | 0.23883    |
| AveragePolicyStd[4]  | 0.17486    |
| AveragePolicyStd[5]  | 0.25154    |
| AverageReturn        | 1508.7     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.9     |
| StdReturn            | 276.55     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.92     |
| TotalNEpisodes       | 18163      |
| TotalNSamples        | 3.5579e+06 |
| ExplainedVariance    | -0.01443   |
-------------------------------------
[2018-12-22 11:19:54.417610 UTC] Saving snapshot
[2018-12-22 11:19:54.417900 UTC] Starting iteration 712
[2018-12-22 11:19:54.418019 UTC] Start collecting samples
[2018-12-22 11:19:57.379313 UTC] Computing input variables for policy optimization
[2018-12-22 11:19:57.456453 UTC] Performing policy update
[2018-12-22 11:19:57.457385 UTC] Computing gradient in Euclidean space
[2018-12-22 11:19:57.548905 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:19:58.615951 UTC] Performing line search
[2018-12-22 11:19:58.744879 UTC] Updating baseline
[2018-12-22 11:20:00.191681 UTC] Computing logging information
-------------------------------------
| Iteration            | 712        |
| ExpectedImprovement  | 0.018639   |
| ActualImprovement    | 0.017493   |
| ImprovementRatio     | 0.93853    |
| MeanKL               | 0.0067341  |
| Entropy              | -0.35831   |
| Perplexity           | 0.69886    |
| AveragePolicyStd     | 0.23082    |
| AveragePolicyStd[0]  | 0.25427    |
| AveragePolicyStd[1]  | 0.27301    |
| AveragePolicyStd[2]  | 0.19184    |
| AveragePolicyStd[3]  | 0.23841    |
| AveragePolicyStd[4]  | 0.1751     |
| AveragePolicyStd[5]  | 0.25231    |
| AverageReturn        | 1512.2     |
| MinReturn            | 91.888     |
| MaxReturn            | 1640.9     |
| StdReturn            | 277.36     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.92     |
| TotalNEpisodes       | 18168      |
| TotalNSamples        | 3.5629e+06 |
| ExplainedVariance    | 0.018687   |
-------------------------------------
[2018-12-22 11:20:00.566646 UTC] Saving snapshot
[2018-12-22 11:20:00.566888 UTC] Starting iteration 713
[2018-12-22 11:20:00.567030 UTC] Start collecting samples
[2018-12-22 11:20:03.549659 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:03.628084 UTC] Performing policy update
[2018-12-22 11:20:03.628798 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:03.717885 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:04.781875 UTC] Performing line search
[2018-12-22 11:20:04.909397 UTC] Updating baseline
[2018-12-22 11:20:06.439996 UTC] Computing logging information
-------------------------------------
| Iteration            | 713        |
| ExpectedImprovement  | 0.019555   |
| ActualImprovement    | 0.017374   |
| ImprovementRatio     | 0.8885     |
| MeanKL               | 0.00695    |
| Entropy              | -0.35828   |
| Perplexity           | 0.69888    |
| AveragePolicyStd     | 0.23082    |
| AveragePolicyStd[0]  | 0.25553    |
| AveragePolicyStd[1]  | 0.27239    |
| AveragePolicyStd[2]  | 0.19138    |
| AveragePolicyStd[3]  | 0.23771    |
| AveragePolicyStd[4]  | 0.17554    |
| AveragePolicyStd[5]  | 0.25236    |
| AverageReturn        | 1514.1     |
| MinReturn            | 91.888     |
| MaxReturn            | 1643.2     |
| StdReturn            | 277.87     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.92     |
| TotalNEpisodes       | 18173      |
| TotalNSamples        | 3.5679e+06 |
| ExplainedVariance    | -0.027756  |
-------------------------------------
[2018-12-22 11:20:06.815293 UTC] Saving snapshot
[2018-12-22 11:20:06.815712 UTC] Starting iteration 714
[2018-12-22 11:20:06.816078 UTC] Start collecting samples
[2018-12-22 11:20:09.778921 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:09.858752 UTC] Performing policy update
[2018-12-22 11:20:09.859323 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:09.953228 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:11.015627 UTC] Performing line search
[2018-12-22 11:20:11.143420 UTC] Updating baseline
[2018-12-22 11:20:13.017341 UTC] Computing logging information
-------------------------------------
| Iteration            | 714        |
| ExpectedImprovement  | 0.019468   |
| ActualImprovement    | 0.018498   |
| ImprovementRatio     | 0.95015    |
| MeanKL               | 0.0072574  |
| Entropy              | -0.37719   |
| Perplexity           | 0.68579    |
| AveragePolicyStd     | 0.2301     |
| AveragePolicyStd[0]  | 0.25552    |
| AveragePolicyStd[1]  | 0.27109    |
| AveragePolicyStd[2]  | 0.18999    |
| AveragePolicyStd[3]  | 0.2372     |
| AveragePolicyStd[4]  | 0.17536    |
| AveragePolicyStd[5]  | 0.25145    |
| AverageReturn        | 1518.7     |
| MinReturn            | 91.888     |
| MaxReturn            | 1674.4     |
| StdReturn            | 279.09     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.92     |
| TotalNEpisodes       | 18179      |
| TotalNSamples        | 3.5739e+06 |
| ExplainedVariance    | 0.00107    |
-------------------------------------
[2018-12-22 11:20:13.391405 UTC] Saving snapshot
[2018-12-22 11:20:13.391671 UTC] Starting iteration 715
[2018-12-22 11:20:13.391803 UTC] Start collecting samples
[2018-12-22 11:20:16.364397 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:16.443290 UTC] Performing policy update
[2018-12-22 11:20:16.444048 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:16.533286 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:17.596880 UTC] Performing line search
[2018-12-22 11:20:17.724167 UTC] Updating baseline
[2018-12-22 11:20:19.462851 UTC] Computing logging information
-------------------------------------
| Iteration            | 715        |
| ExpectedImprovement  | 0.022498   |
| ActualImprovement    | 0.021254   |
| ImprovementRatio     | 0.94473    |
| MeanKL               | 0.0071241  |
| Entropy              | -0.37666   |
| Perplexity           | 0.68615    |
| AveragePolicyStd     | 0.23012    |
| AveragePolicyStd[0]  | 0.25586    |
| AveragePolicyStd[1]  | 0.27093    |
| AveragePolicyStd[2]  | 0.18986    |
| AveragePolicyStd[3]  | 0.23713    |
| AveragePolicyStd[4]  | 0.17547    |
| AveragePolicyStd[5]  | 0.25148    |
| AverageReturn        | 1508.9     |
| MinReturn            | 91.888     |
| MaxReturn            | 1674.4     |
| StdReturn            | 304.03     |
| AverageEpisodeLength | 950.33     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.76     |
| TotalNEpisodes       | 18184      |
| TotalNSamples        | 3.5776e+06 |
| ExplainedVariance    | 0.12991    |
-------------------------------------
[2018-12-22 11:20:19.839354 UTC] Saving snapshot
[2018-12-22 11:20:19.839616 UTC] Starting iteration 716
[2018-12-22 11:20:19.839737 UTC] Start collecting samples
[2018-12-22 11:20:22.776683 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:22.856114 UTC] Performing policy update
[2018-12-22 11:20:22.856879 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:22.944290 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:24.018725 UTC] Performing line search
[2018-12-22 11:20:24.146000 UTC] Updating baseline
[2018-12-22 11:20:25.414243 UTC] Computing logging information
-------------------------------------
| Iteration            | 716        |
| ExpectedImprovement  | 0.017852   |
| ActualImprovement    | 0.016806   |
| ImprovementRatio     | 0.94142    |
| MeanKL               | 0.0072392  |
| Entropy              | -0.37574   |
| Perplexity           | 0.68678    |
| AveragePolicyStd     | 0.23018    |
| AveragePolicyStd[0]  | 0.25593    |
| AveragePolicyStd[1]  | 0.27061    |
| AveragePolicyStd[2]  | 0.18966    |
| AveragePolicyStd[3]  | 0.23736    |
| AveragePolicyStd[4]  | 0.17527    |
| AveragePolicyStd[5]  | 0.25226    |
| AverageReturn        | 1516.8     |
| MinReturn            | 91.888     |
| MaxReturn            | 1674.4     |
| StdReturn            | 302.23     |
| AverageEpisodeLength | 953.16     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.32     |
| TotalNEpisodes       | 18190      |
| TotalNSamples        | 3.5836e+06 |
| ExplainedVariance    | -0.02338   |
-------------------------------------
[2018-12-22 11:20:25.795354 UTC] Saving snapshot
[2018-12-22 11:20:25.795634 UTC] Starting iteration 717
[2018-12-22 11:20:25.795759 UTC] Start collecting samples
[2018-12-22 11:20:28.707560 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:28.784735 UTC] Performing policy update
[2018-12-22 11:20:28.785400 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:28.876602 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:29.947617 UTC] Performing line search
[2018-12-22 11:20:30.076684 UTC] Updating baseline
[2018-12-22 11:20:31.444170 UTC] Computing logging information
-------------------------------------
| Iteration            | 717        |
| ExpectedImprovement  | 0.018535   |
| ActualImprovement    | 0.016848   |
| ImprovementRatio     | 0.90898    |
| MeanKL               | 0.0073795  |
| Entropy              | -0.37644   |
| Perplexity           | 0.6863     |
| AveragePolicyStd     | 0.23019    |
| AveragePolicyStd[0]  | 0.25609    |
| AveragePolicyStd[1]  | 0.27039    |
| AveragePolicyStd[2]  | 0.18917    |
| AveragePolicyStd[3]  | 0.23749    |
| AveragePolicyStd[4]  | 0.17501    |
| AveragePolicyStd[5]  | 0.25302    |
| AverageReturn        | 1530.3     |
| MinReturn            | 91.888     |
| MaxReturn            | 1674.4     |
| StdReturn            | 275.31     |
| AverageEpisodeLength | 960.95     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.2      |
| TotalNEpisodes       | 18193      |
| TotalNSamples        | 3.5866e+06 |
| ExplainedVariance    | 0.2159     |
-------------------------------------
[2018-12-22 11:20:31.822141 UTC] Saving snapshot
[2018-12-22 11:20:31.822398 UTC] Starting iteration 718
[2018-12-22 11:20:31.822549 UTC] Start collecting samples
[2018-12-22 11:20:34.820983 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:34.903293 UTC] Performing policy update
[2018-12-22 11:20:34.904009 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:34.996161 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:36.068856 UTC] Performing line search
[2018-12-22 11:20:36.197403 UTC] Updating baseline
[2018-12-22 11:20:37.576709 UTC] Computing logging information
-------------------------------------
| Iteration            | 718        |
| ExpectedImprovement  | 0.017609   |
| ActualImprovement    | 0.016521   |
| ImprovementRatio     | 0.93822    |
| MeanKL               | 0.0074532  |
| Entropy              | -0.3739    |
| Perplexity           | 0.68805    |
| AveragePolicyStd     | 0.23029    |
| AveragePolicyStd[0]  | 0.25621    |
| AveragePolicyStd[1]  | 0.27019    |
| AveragePolicyStd[2]  | 0.18903    |
| AveragePolicyStd[3]  | 0.23812    |
| AveragePolicyStd[4]  | 0.1751     |
| AveragePolicyStd[5]  | 0.25312    |
| AverageReturn        | 1528       |
| MinReturn            | 71.046     |
| MaxReturn            | 1674.4     |
| StdReturn            | 295.37     |
| AverageEpisodeLength | 957.77     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.16     |
| TotalNEpisodes       | 18202      |
| TotalNSamples        | 3.5937e+06 |
| ExplainedVariance    | 0.26586    |
-------------------------------------
[2018-12-22 11:20:37.959004 UTC] Saving snapshot
[2018-12-22 11:20:37.959248 UTC] Starting iteration 719
[2018-12-22 11:20:37.959389 UTC] Start collecting samples
[2018-12-22 11:20:40.947140 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:41.028994 UTC] Performing policy update
[2018-12-22 11:20:41.029676 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:41.121822 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:42.203860 UTC] Performing line search
[2018-12-22 11:20:42.332480 UTC] Updating baseline
[2018-12-22 11:20:43.714581 UTC] Computing logging information
-------------------------------------
| Iteration            | 719        |
| ExpectedImprovement  | 0.016827   |
| ActualImprovement    | 0.01618    |
| ImprovementRatio     | 0.96154    |
| MeanKL               | 0.0072425  |
| Entropy              | -0.37631   |
| Perplexity           | 0.68639    |
| AveragePolicyStd     | 0.23019    |
| AveragePolicyStd[0]  | 0.25635    |
| AveragePolicyStd[1]  | 0.27045    |
| AveragePolicyStd[2]  | 0.1885     |
| AveragePolicyStd[3]  | 0.23842    |
| AveragePolicyStd[4]  | 0.17549    |
| AveragePolicyStd[5]  | 0.25195    |
| AverageReturn        | 1506.4     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.2     |
| StdReturn            | 334.12     |
| AverageEpisodeLength | 944.01     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.3      |
| TotalNEpisodes       | 18209      |
| TotalNSamples        | 3.5993e+06 |
| ExplainedVariance    | 0.16747    |
-------------------------------------
[2018-12-22 11:20:44.095729 UTC] Saving snapshot
[2018-12-22 11:20:44.095969 UTC] Starting iteration 720
[2018-12-22 11:20:44.096086 UTC] Start collecting samples
[2018-12-22 11:20:47.033421 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:47.112092 UTC] Performing policy update
[2018-12-22 11:20:47.113025 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:47.203643 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:48.278734 UTC] Performing line search
[2018-12-22 11:20:48.408423 UTC] Updating baseline
[2018-12-22 11:20:50.048648 UTC] Computing logging information
-------------------------------------
| Iteration            | 720        |
| ExpectedImprovement  | 0.017359   |
| ActualImprovement    | 0.016541   |
| ImprovementRatio     | 0.95289    |
| MeanKL               | 0.0074072  |
| Entropy              | -0.38786   |
| Perplexity           | 0.6785     |
| AveragePolicyStd     | 0.22978    |
| AveragePolicyStd[0]  | 0.25575    |
| AveragePolicyStd[1]  | 0.27062    |
| AveragePolicyStd[2]  | 0.18854    |
| AveragePolicyStd[3]  | 0.23711    |
| AveragePolicyStd[4]  | 0.17467    |
| AveragePolicyStd[5]  | 0.25198    |
| AverageReturn        | 1507.7     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.2     |
| StdReturn            | 334.4      |
| AverageEpisodeLength | 944.01     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.3      |
| TotalNEpisodes       | 18211      |
| TotalNSamples        | 3.6013e+06 |
| ExplainedVariance    | -0.13694   |
-------------------------------------
[2018-12-22 11:20:50.421423 UTC] Saving snapshot
[2018-12-22 11:20:50.429463 UTC] Starting iteration 721
[2018-12-22 11:20:50.429675 UTC] Start collecting samples
[2018-12-22 11:20:53.444049 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:53.527028 UTC] Performing policy update
[2018-12-22 11:20:53.527793 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:53.618903 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:20:54.696066 UTC] Performing line search
[2018-12-22 11:20:54.824242 UTC] Updating baseline
[2018-12-22 11:20:56.019488 UTC] Computing logging information
-------------------------------------
| Iteration            | 721        |
| ExpectedImprovement  | 0.019013   |
| ActualImprovement    | 0.018463   |
| ImprovementRatio     | 0.97106    |
| MeanKL               | 0.0070512  |
| Entropy              | -0.39116   |
| Perplexity           | 0.67627    |
| AveragePolicyStd     | 0.2296     |
| AveragePolicyStd[0]  | 0.25449    |
| AveragePolicyStd[1]  | 0.27006    |
| AveragePolicyStd[2]  | 0.18884    |
| AveragePolicyStd[3]  | 0.23727    |
| AveragePolicyStd[4]  | 0.17488    |
| AveragePolicyStd[5]  | 0.25204    |
| AverageReturn        | 1504.6     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 340.16     |
| AverageEpisodeLength | 939.86     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.36     |
| TotalNEpisodes       | 18220      |
| TotalNSamples        | 3.6099e+06 |
| ExplainedVariance    | 0.084418   |
-------------------------------------
[2018-12-22 11:20:56.396372 UTC] Saving snapshot
[2018-12-22 11:20:56.396638 UTC] Starting iteration 722
[2018-12-22 11:20:56.396761 UTC] Start collecting samples
[2018-12-22 11:20:59.327373 UTC] Computing input variables for policy optimization
[2018-12-22 11:20:59.406885 UTC] Performing policy update
[2018-12-22 11:20:59.407612 UTC] Computing gradient in Euclidean space
[2018-12-22 11:20:59.496681 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:00.576073 UTC] Performing line search
[2018-12-22 11:21:00.705048 UTC] Updating baseline
[2018-12-22 11:21:02.259338 UTC] Computing logging information
-------------------------------------
| Iteration            | 722        |
| ExpectedImprovement  | 0.015704   |
| ActualImprovement    | 0.015328   |
| ImprovementRatio     | 0.97607    |
| MeanKL               | 0.0072261  |
| Entropy              | -0.39237   |
| Perplexity           | 0.67546    |
| AveragePolicyStd     | 0.22954    |
| AveragePolicyStd[0]  | 0.25507    |
| AveragePolicyStd[1]  | 0.26984    |
| AveragePolicyStd[2]  | 0.18862    |
| AveragePolicyStd[3]  | 0.23683    |
| AveragePolicyStd[4]  | 0.17517    |
| AveragePolicyStd[5]  | 0.25172    |
| AverageReturn        | 1521.5     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 316.02     |
| AverageEpisodeLength | 948.79     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.64     |
| TotalNEpisodes       | 18224      |
| TotalNSamples        | 3.6139e+06 |
| ExplainedVariance    | -0.042229  |
-------------------------------------
[2018-12-22 11:21:02.641986 UTC] Saving snapshot
[2018-12-22 11:21:02.642236 UTC] Starting iteration 723
[2018-12-22 11:21:02.642373 UTC] Start collecting samples
[2018-12-22 11:21:05.567420 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:05.644791 UTC] Performing policy update
[2018-12-22 11:21:05.645457 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:05.734910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:06.801833 UTC] Performing line search
[2018-12-22 11:21:06.930053 UTC] Updating baseline
[2018-12-22 11:21:08.833621 UTC] Computing logging information
-------------------------------------
| Iteration            | 723        |
| ExpectedImprovement  | 0.019432   |
| ActualImprovement    | 0.01717    |
| ImprovementRatio     | 0.88361    |
| MeanKL               | 0.0071876  |
| Entropy              | -0.38887   |
| Perplexity           | 0.67782    |
| AveragePolicyStd     | 0.22968    |
| AveragePolicyStd[0]  | 0.25554    |
| AveragePolicyStd[1]  | 0.26979    |
| AveragePolicyStd[2]  | 0.18798    |
| AveragePolicyStd[3]  | 0.23667    |
| AveragePolicyStd[4]  | 0.17583    |
| AveragePolicyStd[5]  | 0.25227    |
| AverageReturn        | 1522.1     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 316.1      |
| AverageEpisodeLength | 948.79     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.64     |
| TotalNEpisodes       | 18226      |
| TotalNSamples        | 3.6159e+06 |
| ExplainedVariance    | 0.0029075  |
-------------------------------------
[2018-12-22 11:21:09.207194 UTC] Saving snapshot
[2018-12-22 11:21:09.207438 UTC] Starting iteration 724
[2018-12-22 11:21:09.207573 UTC] Start collecting samples
[2018-12-22 11:21:12.193568 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:12.274065 UTC] Performing policy update
[2018-12-22 11:21:12.274885 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:12.363788 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:13.429178 UTC] Performing line search
[2018-12-22 11:21:13.557179 UTC] Updating baseline
[2018-12-22 11:21:15.169012 UTC] Computing logging information
-------------------------------------
| Iteration            | 724        |
| ExpectedImprovement  | 0.015662   |
| ActualImprovement    | 0.015142   |
| ImprovementRatio     | 0.96679    |
| MeanKL               | 0.0075234  |
| Entropy              | -0.39442   |
| Perplexity           | 0.67407    |
| AveragePolicyStd     | 0.22952    |
| AveragePolicyStd[0]  | 0.25598    |
| AveragePolicyStd[1]  | 0.26952    |
| AveragePolicyStd[2]  | 0.18721    |
| AveragePolicyStd[3]  | 0.2367     |
| AveragePolicyStd[4]  | 0.17538    |
| AveragePolicyStd[5]  | 0.25233    |
| AverageReturn        | 1525.4     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 317.01     |
| AverageEpisodeLength | 948.79     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.64     |
| TotalNEpisodes       | 18234      |
| TotalNSamples        | 3.6239e+06 |
| ExplainedVariance    | 0.0040906  |
-------------------------------------
[2018-12-22 11:21:15.544788 UTC] Saving snapshot
[2018-12-22 11:21:15.545032 UTC] Starting iteration 725
[2018-12-22 11:21:15.545147 UTC] Start collecting samples
[2018-12-22 11:21:18.505697 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:18.584991 UTC] Performing policy update
[2018-12-22 11:21:18.585659 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:18.677154 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:19.738293 UTC] Performing line search
[2018-12-22 11:21:19.866294 UTC] Updating baseline
[2018-12-22 11:21:21.551457 UTC] Computing logging information
-------------------------------------
| Iteration            | 725        |
| ExpectedImprovement  | 0.0174     |
| ActualImprovement    | 0.016696   |
| ImprovementRatio     | 0.95954    |
| MeanKL               | 0.0070835  |
| Entropy              | -0.39142   |
| Perplexity           | 0.6761     |
| AveragePolicyStd     | 0.22961    |
| AveragePolicyStd[0]  | 0.2561     |
| AveragePolicyStd[1]  | 0.26906    |
| AveragePolicyStd[2]  | 0.18762    |
| AveragePolicyStd[3]  | 0.23645    |
| AveragePolicyStd[4]  | 0.17548    |
| AveragePolicyStd[5]  | 0.25298    |
| AverageReturn        | 1503.5     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 358.76     |
| AverageEpisodeLength | 933.51     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.14     |
| TotalNEpisodes       | 18241      |
| TotalNSamples        | 3.6294e+06 |
| ExplainedVariance    | 0.15595    |
-------------------------------------
[2018-12-22 11:21:21.932634 UTC] Saving snapshot
[2018-12-22 11:21:21.932881 UTC] Starting iteration 726
[2018-12-22 11:21:21.932996 UTC] Start collecting samples
[2018-12-22 11:21:24.873726 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:24.952625 UTC] Performing policy update
[2018-12-22 11:21:24.953411 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:25.043349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:26.110665 UTC] Performing line search
[2018-12-22 11:21:26.240782 UTC] Updating baseline
[2018-12-22 11:21:27.579185 UTC] Computing logging information
-------------------------------------
| Iteration            | 726        |
| ExpectedImprovement  | 0.017351   |
| ActualImprovement    | 0.016668   |
| ImprovementRatio     | 0.96066    |
| MeanKL               | 0.0071431  |
| Entropy              | -0.3971    |
| Perplexity           | 0.67227    |
| AveragePolicyStd     | 0.22943    |
| AveragePolicyStd[0]  | 0.25515    |
| AveragePolicyStd[1]  | 0.2691     |
| AveragePolicyStd[2]  | 0.18757    |
| AveragePolicyStd[3]  | 0.23642    |
| AveragePolicyStd[4]  | 0.1748     |
| AveragePolicyStd[5]  | 0.25352    |
| AverageReturn        | 1494.8     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 379.55     |
| AverageEpisodeLength | 926.56     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.55     |
| TotalNEpisodes       | 18245      |
| TotalNSamples        | 3.6326e+06 |
| ExplainedVariance    | 0.15634    |
-------------------------------------
[2018-12-22 11:21:27.957013 UTC] Saving snapshot
[2018-12-22 11:21:27.957284 UTC] Starting iteration 727
[2018-12-22 11:21:27.957409 UTC] Start collecting samples
[2018-12-22 11:21:30.918693 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:30.997757 UTC] Performing policy update
[2018-12-22 11:21:30.998406 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:31.088432 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:32.168307 UTC] Performing line search
[2018-12-22 11:21:32.297897 UTC] Updating baseline
[2018-12-22 11:21:33.951872 UTC] Computing logging information
-------------------------------------
| Iteration            | 727        |
| ExpectedImprovement  | 0.017073   |
| ActualImprovement    | 0.016233   |
| ImprovementRatio     | 0.95085    |
| MeanKL               | 0.0076456  |
| Entropy              | -0.40227   |
| Perplexity           | 0.6688     |
| AveragePolicyStd     | 0.22924    |
| AveragePolicyStd[0]  | 0.25536    |
| AveragePolicyStd[1]  | 0.26935    |
| AveragePolicyStd[2]  | 0.18744    |
| AveragePolicyStd[3]  | 0.23588    |
| AveragePolicyStd[4]  | 0.17456    |
| AveragePolicyStd[5]  | 0.25288    |
| AverageReturn        | 1494       |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 379.8      |
| AverageEpisodeLength | 925.65     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.43     |
| TotalNEpisodes       | 18250      |
| TotalNSamples        | 3.6375e+06 |
| ExplainedVariance    | 0.13442    |
-------------------------------------
[2018-12-22 11:21:34.328746 UTC] Saving snapshot
[2018-12-22 11:21:34.328996 UTC] Starting iteration 728
[2018-12-22 11:21:34.329115 UTC] Start collecting samples
[2018-12-22 11:21:37.306804 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:37.388610 UTC] Performing policy update
[2018-12-22 11:21:37.389374 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:37.479057 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:38.539842 UTC] Performing line search
[2018-12-22 11:21:38.670232 UTC] Updating baseline
[2018-12-22 11:21:40.035246 UTC] Computing logging information
-------------------------------------
| Iteration            | 728        |
| ExpectedImprovement  | 0.018981   |
| ActualImprovement    | 0.017837   |
| ImprovementRatio     | 0.93972    |
| MeanKL               | 0.0074214  |
| Entropy              | -0.4003    |
| Perplexity           | 0.67012    |
| AveragePolicyStd     | 0.22934    |
| AveragePolicyStd[0]  | 0.25529    |
| AveragePolicyStd[1]  | 0.26921    |
| AveragePolicyStd[2]  | 0.18737    |
| AveragePolicyStd[3]  | 0.23598    |
| AveragePolicyStd[4]  | 0.17437    |
| AveragePolicyStd[5]  | 0.25384    |
| AverageReturn        | 1482.7     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 394.49     |
| AverageEpisodeLength | 918.49     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 235.12     |
| TotalNEpisodes       | 18257      |
| TotalNSamples        | 3.6438e+06 |
| ExplainedVariance    | 0.13539    |
-------------------------------------
[2018-12-22 11:21:40.411228 UTC] Saving snapshot
[2018-12-22 11:21:40.411476 UTC] Starting iteration 729
[2018-12-22 11:21:40.411613 UTC] Start collecting samples
[2018-12-22 11:21:43.349174 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:43.429251 UTC] Performing policy update
[2018-12-22 11:21:43.430069 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:43.519802 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:44.485591 UTC] Performing line search
[2018-12-22 11:21:44.602278 UTC] Updating baseline
[2018-12-22 11:21:45.977419 UTC] Computing logging information
-------------------------------------
| Iteration            | 729        |
| ExpectedImprovement  | 0.017825   |
| ActualImprovement    | 0.017143   |
| ImprovementRatio     | 0.96176    |
| MeanKL               | 0.0075796  |
| Entropy              | -0.39551   |
| Perplexity           | 0.67334    |
| AveragePolicyStd     | 0.22956    |
| AveragePolicyStd[0]  | 0.25529    |
| AveragePolicyStd[1]  | 0.2693     |
| AveragePolicyStd[2]  | 0.18731    |
| AveragePolicyStd[3]  | 0.23639    |
| AveragePolicyStd[4]  | 0.17413    |
| AveragePolicyStd[5]  | 0.25496    |
| AverageReturn        | 1475.2     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 399.24     |
| AverageEpisodeLength | 913.82     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 238.08     |
| TotalNEpisodes       | 18262      |
| TotalNSamples        | 3.6483e+06 |
| ExplainedVariance    | 0.10852    |
-------------------------------------
[2018-12-22 11:21:46.351931 UTC] Saving snapshot
[2018-12-22 11:21:46.352169 UTC] Starting iteration 730
[2018-12-22 11:21:46.352303 UTC] Start collecting samples
[2018-12-22 11:21:49.243847 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:49.322250 UTC] Performing policy update
[2018-12-22 11:21:49.323815 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:49.414735 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:50.488614 UTC] Performing line search
[2018-12-22 11:21:50.616093 UTC] Updating baseline
[2018-12-22 11:21:51.906557 UTC] Computing logging information
-------------------------------------
| Iteration            | 730        |
| ExpectedImprovement  | 0.017139   |
| ActualImprovement    | 0.016397   |
| ImprovementRatio     | 0.95669    |
| MeanKL               | 0.007067   |
| Entropy              | -0.39739   |
| Perplexity           | 0.67207    |
| AveragePolicyStd     | 0.22957    |
| AveragePolicyStd[0]  | 0.25527    |
| AveragePolicyStd[1]  | 0.26983    |
| AveragePolicyStd[2]  | 0.18664    |
| AveragePolicyStd[3]  | 0.23597    |
| AveragePolicyStd[4]  | 0.17374    |
| AveragePolicyStd[5]  | 0.25596    |
| AverageReturn        | 1471.9     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 399        |
| AverageEpisodeLength | 912.09     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 238.07     |
| TotalNEpisodes       | 18266      |
| TotalNSamples        | 3.6521e+06 |
| ExplainedVariance    | 0.20233    |
-------------------------------------
[2018-12-22 11:21:52.285358 UTC] Saving snapshot
[2018-12-22 11:21:52.293687 UTC] Starting iteration 731
[2018-12-22 11:21:52.293913 UTC] Start collecting samples
[2018-12-22 11:21:55.220902 UTC] Computing input variables for policy optimization
[2018-12-22 11:21:55.301241 UTC] Performing policy update
[2018-12-22 11:21:55.301868 UTC] Computing gradient in Euclidean space
[2018-12-22 11:21:55.390487 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:21:56.413641 UTC] Performing line search
[2018-12-22 11:21:56.540929 UTC] Updating baseline
[2018-12-22 11:21:58.003375 UTC] Computing logging information
-------------------------------------
| Iteration            | 731        |
| ExpectedImprovement  | 0.016349   |
| ActualImprovement    | 0.015936   |
| ImprovementRatio     | 0.97472    |
| MeanKL               | 0.007508   |
| Entropy              | -0.40149   |
| Perplexity           | 0.66932    |
| AveragePolicyStd     | 0.22936    |
| AveragePolicyStd[0]  | 0.25361    |
| AveragePolicyStd[1]  | 0.27005    |
| AveragePolicyStd[2]  | 0.18666    |
| AveragePolicyStd[3]  | 0.23684    |
| AveragePolicyStd[4]  | 0.17405    |
| AveragePolicyStd[5]  | 0.25493    |
| AverageReturn        | 1469.5     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 398.6      |
| AverageEpisodeLength | 911.35     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.91     |
| TotalNEpisodes       | 18272      |
| TotalNSamples        | 3.6581e+06 |
| ExplainedVariance    | 0.035356   |
-------------------------------------
[2018-12-22 11:21:58.377018 UTC] Saving snapshot
[2018-12-22 11:21:58.377262 UTC] Starting iteration 732
[2018-12-22 11:21:58.377383 UTC] Start collecting samples
[2018-12-22 11:22:01.391809 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:01.475122 UTC] Performing policy update
[2018-12-22 11:22:01.477045 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:01.565649 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:02.637529 UTC] Performing line search
[2018-12-22 11:22:02.765334 UTC] Updating baseline
[2018-12-22 11:22:04.139057 UTC] Computing logging information
-------------------------------------
| Iteration            | 732        |
| ExpectedImprovement  | 0.020837   |
| ActualImprovement    | 0.019432   |
| ImprovementRatio     | 0.93258    |
| MeanKL               | 0.0067761  |
| Entropy              | -0.41113   |
| Perplexity           | 0.6629     |
| AveragePolicyStd     | 0.22896    |
| AveragePolicyStd[0]  | 0.2525     |
| AveragePolicyStd[1]  | 0.26928    |
| AveragePolicyStd[2]  | 0.18629    |
| AveragePolicyStd[3]  | 0.2375     |
| AveragePolicyStd[4]  | 0.174      |
| AveragePolicyStd[5]  | 0.25418    |
| AverageReturn        | 1446.4     |
| MinReturn            | 71.046     |
| MaxReturn            | 1686.3     |
| StdReturn            | 425.79     |
| AverageEpisodeLength | 897.72     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.82     |
| TotalNEpisodes       | 18281      |
| TotalNSamples        | 3.6644e+06 |
| ExplainedVariance    | 0.23335    |
-------------------------------------
[2018-12-22 11:22:04.516008 UTC] Saving snapshot
[2018-12-22 11:22:04.516251 UTC] Starting iteration 733
[2018-12-22 11:22:04.516382 UTC] Start collecting samples
[2018-12-22 11:22:07.454524 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:07.533049 UTC] Performing policy update
[2018-12-22 11:22:07.533690 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:07.623626 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:08.697394 UTC] Performing line search
[2018-12-22 11:22:08.827076 UTC] Updating baseline
[2018-12-22 11:22:10.293454 UTC] Computing logging information
-------------------------------------
| Iteration            | 733        |
| ExpectedImprovement  | 0.018003   |
| ActualImprovement    | 0.019498   |
| ImprovementRatio     | 1.083      |
| MeanKL               | 0.0070217  |
| Entropy              | -0.40987   |
| Perplexity           | 0.66373    |
| AveragePolicyStd     | 0.22905    |
| AveragePolicyStd[0]  | 0.25398    |
| AveragePolicyStd[1]  | 0.26972    |
| AveragePolicyStd[2]  | 0.18578    |
| AveragePolicyStd[3]  | 0.23625    |
| AveragePolicyStd[4]  | 0.17412    |
| AveragePolicyStd[5]  | 0.25448    |
| AverageReturn        | 1447.5     |
| MinReturn            | 71.046     |
| MaxReturn            | 1715.1     |
| StdReturn            | 426.37     |
| AverageEpisodeLength | 897.72     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.82     |
| TotalNEpisodes       | 18284      |
| TotalNSamples        | 3.6674e+06 |
| ExplainedVariance    | -0.1267    |
-------------------------------------
[2018-12-22 11:22:10.669981 UTC] Saving snapshot
[2018-12-22 11:22:10.670228 UTC] Starting iteration 734
[2018-12-22 11:22:10.670344 UTC] Start collecting samples
[2018-12-22 11:22:13.620398 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:13.700187 UTC] Performing policy update
[2018-12-22 11:22:13.701124 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:13.794611 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:14.869904 UTC] Performing line search
[2018-12-22 11:22:14.997686 UTC] Updating baseline
[2018-12-22 11:22:16.717006 UTC] Computing logging information
-------------------------------------
| Iteration            | 734        |
| ExpectedImprovement  | 0.017106   |
| ActualImprovement    | 0.015781   |
| ImprovementRatio     | 0.92255    |
| MeanKL               | 0.0080078  |
| Entropy              | -0.41451   |
| Perplexity           | 0.66067    |
| AveragePolicyStd     | 0.22896    |
| AveragePolicyStd[0]  | 0.25487    |
| AveragePolicyStd[1]  | 0.2704     |
| AveragePolicyStd[2]  | 0.18413    |
| AveragePolicyStd[3]  | 0.23528    |
| AveragePolicyStd[4]  | 0.17433    |
| AveragePolicyStd[5]  | 0.25479    |
| AverageReturn        | 1448.2     |
| MinReturn            | 71.046     |
| MaxReturn            | 1715.1     |
| StdReturn            | 426.64     |
| AverageEpisodeLength | 897.72     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 253.82     |
| TotalNEpisodes       | 18288      |
| TotalNSamples        | 3.6714e+06 |
| ExplainedVariance    | -0.0012455 |
-------------------------------------
[2018-12-22 11:22:17.093858 UTC] Saving snapshot
[2018-12-22 11:22:17.094106 UTC] Starting iteration 735
[2018-12-22 11:22:17.094240 UTC] Start collecting samples
[2018-12-22 11:22:20.080533 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:20.160476 UTC] Performing policy update
[2018-12-22 11:22:20.161059 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:20.252149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:21.323112 UTC] Performing line search
[2018-12-22 11:22:21.453684 UTC] Updating baseline
[2018-12-22 11:22:23.005805 UTC] Computing logging information
-------------------------------------
| Iteration            | 735        |
| ExpectedImprovement  | 0.017947   |
| ActualImprovement    | 0.016766   |
| ImprovementRatio     | 0.93421    |
| MeanKL               | 0.0071868  |
| Entropy              | -0.4182    |
| Perplexity           | 0.65823    |
| AveragePolicyStd     | 0.22881    |
| AveragePolicyStd[0]  | 0.25458    |
| AveragePolicyStd[1]  | 0.27007    |
| AveragePolicyStd[2]  | 0.18364    |
| AveragePolicyStd[3]  | 0.23474    |
| AveragePolicyStd[4]  | 0.1747     |
| AveragePolicyStd[5]  | 0.25514    |
| AverageReturn        | 1435.5     |
| MinReturn            | 71.046     |
| MaxReturn            | 1715.1     |
| StdReturn            | 446.69     |
| AverageEpisodeLength | 889.15     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 264.92     |
| TotalNEpisodes       | 18295      |
| TotalNSamples        | 3.6775e+06 |
| ExplainedVariance    | 0.096103   |
-------------------------------------
[2018-12-22 11:22:23.385320 UTC] Saving snapshot
[2018-12-22 11:22:23.385585 UTC] Starting iteration 736
[2018-12-22 11:22:23.385705 UTC] Start collecting samples
[2018-12-22 11:22:26.340385 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:26.418964 UTC] Performing policy update
[2018-12-22 11:22:26.419633 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:26.510385 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:27.579094 UTC] Performing line search
[2018-12-22 11:22:27.712347 UTC] Updating baseline
[2018-12-22 11:22:29.120931 UTC] Computing logging information
-------------------------------------
| Iteration            | 736        |
| ExpectedImprovement  | 0.020075   |
| ActualImprovement    | 0.019065   |
| ImprovementRatio     | 0.94969    |
| MeanKL               | 0.0068541  |
| Entropy              | -0.42197   |
| Perplexity           | 0.65575    |
| AveragePolicyStd     | 0.22871    |
| AveragePolicyStd[0]  | 0.25353    |
| AveragePolicyStd[1]  | 0.2707     |
| AveragePolicyStd[2]  | 0.1829     |
| AveragePolicyStd[3]  | 0.23483    |
| AveragePolicyStd[4]  | 0.17465    |
| AveragePolicyStd[5]  | 0.25567    |
| AverageReturn        | 1465.2     |
| MinReturn            | 110.96     |
| MaxReturn            | 1715.1     |
| StdReturn            | 403.46     |
| AverageEpisodeLength | 907.9      |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.65     |
| TotalNEpisodes       | 18300      |
| TotalNSamples        | 3.6825e+06 |
| ExplainedVariance    | -0.028066  |
-------------------------------------
[2018-12-22 11:22:29.523751 UTC] Saving snapshot
[2018-12-22 11:22:29.524024 UTC] Starting iteration 737
[2018-12-22 11:22:29.524144 UTC] Start collecting samples
[2018-12-22 11:22:32.687110 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:32.772027 UTC] Performing policy update
[2018-12-22 11:22:32.772729 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:32.866811 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:33.988256 UTC] Performing line search
[2018-12-22 11:22:34.126458 UTC] Updating baseline
[2018-12-22 11:22:36.059662 UTC] Computing logging information
-------------------------------------
| Iteration            | 737        |
| ExpectedImprovement  | 0.018028   |
| ActualImprovement    | 0.016645   |
| ImprovementRatio     | 0.92328    |
| MeanKL               | 0.0076224  |
| Entropy              | -0.43071   |
| Perplexity           | 0.65005    |
| AveragePolicyStd     | 0.22833    |
| AveragePolicyStd[0]  | 0.25396    |
| AveragePolicyStd[1]  | 0.26953    |
| AveragePolicyStd[2]  | 0.18336    |
| AveragePolicyStd[3]  | 0.23477    |
| AveragePolicyStd[4]  | 0.1742     |
| AveragePolicyStd[5]  | 0.2542     |
| AverageReturn        | 1480.1     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 380.05     |
| AverageEpisodeLength | 917.04     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.12     |
| TotalNEpisodes       | 18305      |
| TotalNSamples        | 3.6875e+06 |
| ExplainedVariance    | -5.418e-05 |
-------------------------------------
[2018-12-22 11:22:36.455243 UTC] Saving snapshot
[2018-12-22 11:22:36.455560 UTC] Starting iteration 738
[2018-12-22 11:22:36.455706 UTC] Start collecting samples
[2018-12-22 11:22:39.474078 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:39.552905 UTC] Performing policy update
[2018-12-22 11:22:39.553630 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:39.643904 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:40.715928 UTC] Performing line search
[2018-12-22 11:22:40.844733 UTC] Updating baseline
[2018-12-22 11:22:42.594700 UTC] Computing logging information
-------------------------------------
| Iteration            | 738        |
| ExpectedImprovement  | 0.020365   |
| ActualImprovement    | 0.018874   |
| ImprovementRatio     | 0.92677    |
| MeanKL               | 0.0069782  |
| Entropy              | -0.43532   |
| Perplexity           | 0.64706    |
| AveragePolicyStd     | 0.22819    |
| AveragePolicyStd[0]  | 0.25402    |
| AveragePolicyStd[1]  | 0.26994    |
| AveragePolicyStd[2]  | 0.18255    |
| AveragePolicyStd[3]  | 0.23418    |
| AveragePolicyStd[4]  | 0.17441    |
| AveragePolicyStd[5]  | 0.25404    |
| AverageReturn        | 1473       |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 398.14     |
| AverageEpisodeLength | 912.65     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.46     |
| TotalNEpisodes       | 18310      |
| TotalNSamples        | 3.6916e+06 |
| ExplainedVariance    | 0.079502   |
-------------------------------------
[2018-12-22 11:22:42.972444 UTC] Saving snapshot
[2018-12-22 11:22:42.972701 UTC] Starting iteration 739
[2018-12-22 11:22:42.972822 UTC] Start collecting samples
[2018-12-22 11:22:45.925557 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:46.010914 UTC] Performing policy update
[2018-12-22 11:22:46.011512 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:46.101153 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:47.166380 UTC] Performing line search
[2018-12-22 11:22:47.294824 UTC] Updating baseline
[2018-12-22 11:22:48.921410 UTC] Computing logging information
-------------------------------------
| Iteration            | 739        |
| ExpectedImprovement  | 0.019597   |
| ActualImprovement    | 0.018494   |
| ImprovementRatio     | 0.94371    |
| MeanKL               | 0.0071562  |
| Entropy              | -0.43226   |
| Perplexity           | 0.64904    |
| AveragePolicyStd     | 0.22831    |
| AveragePolicyStd[0]  | 0.25315    |
| AveragePolicyStd[1]  | 0.27079    |
| AveragePolicyStd[2]  | 0.18277    |
| AveragePolicyStd[3]  | 0.23448    |
| AveragePolicyStd[4]  | 0.17441    |
| AveragePolicyStd[5]  | 0.25426    |
| AverageReturn        | 1472.2     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 397.92     |
| AverageEpisodeLength | 912.65     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.46     |
| TotalNEpisodes       | 18317      |
| TotalNSamples        | 3.6986e+06 |
| ExplainedVariance    | 0.0047436  |
-------------------------------------
[2018-12-22 11:22:49.296400 UTC] Saving snapshot
[2018-12-22 11:22:49.296661 UTC] Starting iteration 740
[2018-12-22 11:22:49.296794 UTC] Start collecting samples
[2018-12-22 11:22:52.249360 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:52.328132 UTC] Performing policy update
[2018-12-22 11:22:52.328818 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:52.417841 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:53.480665 UTC] Performing line search
[2018-12-22 11:22:53.607909 UTC] Updating baseline
[2018-12-22 11:22:54.975597 UTC] Computing logging information
-------------------------------------
| Iteration            | 740        |
| ExpectedImprovement  | 0.015791   |
| ActualImprovement    | 0.015192   |
| ImprovementRatio     | 0.96208    |
| MeanKL               | 0.0073104  |
| Entropy              | -0.43563   |
| Perplexity           | 0.64685    |
| AveragePolicyStd     | 0.2282     |
| AveragePolicyStd[0]  | 0.25295    |
| AveragePolicyStd[1]  | 0.27142    |
| AveragePolicyStd[2]  | 0.18239    |
| AveragePolicyStd[3]  | 0.23468    |
| AveragePolicyStd[4]  | 0.17437    |
| AveragePolicyStd[5]  | 0.25337    |
| AverageReturn        | 1478.7     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 394.3      |
| AverageEpisodeLength | 916.8      |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 234.3      |
| TotalNEpisodes       | 18322      |
| TotalNSamples        | 3.7036e+06 |
| ExplainedVariance    | -0.016885  |
-------------------------------------
[2018-12-22 11:22:55.356014 UTC] Saving snapshot
[2018-12-22 11:22:55.363723 UTC] Starting iteration 741
[2018-12-22 11:22:55.363927 UTC] Start collecting samples
[2018-12-22 11:22:58.416293 UTC] Computing input variables for policy optimization
[2018-12-22 11:22:58.500390 UTC] Performing policy update
[2018-12-22 11:22:58.501106 UTC] Computing gradient in Euclidean space
[2018-12-22 11:22:58.596553 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:22:59.726241 UTC] Performing line search
[2018-12-22 11:22:59.865426 UTC] Updating baseline
[2018-12-22 11:23:01.485841 UTC] Computing logging information
-------------------------------------
| Iteration            | 741        |
| ExpectedImprovement  | 0.019289   |
| ActualImprovement    | 0.017511   |
| ImprovementRatio     | 0.90783    |
| MeanKL               | 0.0073066  |
| Entropy              | -0.42966   |
| Perplexity           | 0.65073    |
| AveragePolicyStd     | 0.22834    |
| AveragePolicyStd[0]  | 0.25197    |
| AveragePolicyStd[1]  | 0.27015    |
| AveragePolicyStd[2]  | 0.18284    |
| AveragePolicyStd[3]  | 0.23632    |
| AveragePolicyStd[4]  | 0.17497    |
| AveragePolicyStd[5]  | 0.25379    |
| AverageReturn        | 1479.9     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 394.7      |
| AverageEpisodeLength | 916.8      |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 234.3      |
| TotalNEpisodes       | 18325      |
| TotalNSamples        | 3.7066e+06 |
| ExplainedVariance    | -0.0037859 |
-------------------------------------
[2018-12-22 11:23:01.913307 UTC] Saving snapshot
[2018-12-22 11:23:01.913619 UTC] Starting iteration 742
[2018-12-22 11:23:01.913787 UTC] Start collecting samples
[2018-12-22 11:23:04.948883 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:05.031977 UTC] Performing policy update
[2018-12-22 11:23:05.032749 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:05.123633 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:06.197661 UTC] Performing line search
[2018-12-22 11:23:06.325587 UTC] Updating baseline
[2018-12-22 11:23:07.776315 UTC] Computing logging information
-------------------------------------
| Iteration            | 742        |
| ExpectedImprovement  | 0.017939   |
| ActualImprovement    | 0.017115   |
| ImprovementRatio     | 0.95406    |
| MeanKL               | 0.0069922  |
| Entropy              | -0.43103   |
| Perplexity           | 0.64984    |
| AveragePolicyStd     | 0.2283     |
| AveragePolicyStd[0]  | 0.25192    |
| AveragePolicyStd[1]  | 0.26954    |
| AveragePolicyStd[2]  | 0.18286    |
| AveragePolicyStd[3]  | 0.23688    |
| AveragePolicyStd[4]  | 0.17462    |
| AveragePolicyStd[5]  | 0.25395    |
| AverageReturn        | 1472.4     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 397.54     |
| AverageEpisodeLength | 912.87     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.16     |
| TotalNEpisodes       | 18334      |
| TotalNSamples        | 3.7152e+06 |
| ExplainedVariance    | 0.098984   |
-------------------------------------
[2018-12-22 11:23:08.157462 UTC] Saving snapshot
[2018-12-22 11:23:08.157729 UTC] Starting iteration 743
[2018-12-22 11:23:08.157880 UTC] Start collecting samples
[2018-12-22 11:23:11.111975 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:11.192244 UTC] Performing policy update
[2018-12-22 11:23:11.193145 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:11.283872 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:12.356870 UTC] Performing line search
[2018-12-22 11:23:12.485638 UTC] Updating baseline
[2018-12-22 11:23:13.863949 UTC] Computing logging information
-------------------------------------
| Iteration            | 743        |
| ExpectedImprovement  | 0.020654   |
| ActualImprovement    | 0.019449   |
| ImprovementRatio     | 0.94168    |
| MeanKL               | 0.0075548  |
| Entropy              | -0.43368   |
| Perplexity           | 0.64812    |
| AveragePolicyStd     | 0.22819    |
| AveragePolicyStd[0]  | 0.251      |
| AveragePolicyStd[1]  | 0.26979    |
| AveragePolicyStd[2]  | 0.18318    |
| AveragePolicyStd[3]  | 0.23695    |
| AveragePolicyStd[4]  | 0.17433    |
| AveragePolicyStd[5]  | 0.25388    |
| AverageReturn        | 1485.1     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 379.29     |
| AverageEpisodeLength | 920.77     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.49     |
| TotalNEpisodes       | 18337      |
| TotalNSamples        | 3.7182e+06 |
| ExplainedVariance    | 0.076585   |
-------------------------------------
[2018-12-22 11:23:14.245909 UTC] Saving snapshot
[2018-12-22 11:23:14.246155 UTC] Starting iteration 744
[2018-12-22 11:23:14.246282 UTC] Start collecting samples
[2018-12-22 11:23:17.171940 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:17.250948 UTC] Performing policy update
[2018-12-22 11:23:17.251716 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:17.341523 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:18.421338 UTC] Performing line search
[2018-12-22 11:23:18.550624 UTC] Updating baseline
[2018-12-22 11:23:20.359657 UTC] Computing logging information
-------------------------------------
| Iteration            | 744        |
| ExpectedImprovement  | 0.018575   |
| ActualImprovement    | 0.017079   |
| ImprovementRatio     | 0.9195     |
| MeanKL               | 0.0070745  |
| Entropy              | -0.44299   |
| Perplexity           | 0.64211    |
| AveragePolicyStd     | 0.22782    |
| AveragePolicyStd[0]  | 0.25114    |
| AveragePolicyStd[1]  | 0.26918    |
| AveragePolicyStd[2]  | 0.18303    |
| AveragePolicyStd[3]  | 0.23668    |
| AveragePolicyStd[4]  | 0.17414    |
| AveragePolicyStd[5]  | 0.25273    |
| AverageReturn        | 1496.9     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 362.83     |
| AverageEpisodeLength | 928.15     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.67     |
| TotalNEpisodes       | 18340      |
| TotalNSamples        | 3.7212e+06 |
| ExplainedVariance    | -0.022359  |
-------------------------------------
[2018-12-22 11:23:20.736997 UTC] Saving snapshot
[2018-12-22 11:23:20.737315 UTC] Starting iteration 745
[2018-12-22 11:23:20.737459 UTC] Start collecting samples
[2018-12-22 11:23:23.773941 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:23.855897 UTC] Performing policy update
[2018-12-22 11:23:23.856605 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:23.951372 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:25.013787 UTC] Performing line search
[2018-12-22 11:23:25.142734 UTC] Updating baseline
[2018-12-22 11:23:26.346376 UTC] Computing logging information
-------------------------------------
| Iteration            | 745        |
| ExpectedImprovement  | 0.017928   |
| ActualImprovement    | 0.017115   |
| ImprovementRatio     | 0.95468    |
| MeanKL               | 0.0073051  |
| Entropy              | -0.44882   |
| Perplexity           | 0.63838    |
| AveragePolicyStd     | 0.2276     |
| AveragePolicyStd[0]  | 0.25126    |
| AveragePolicyStd[1]  | 0.26903    |
| AveragePolicyStd[2]  | 0.18311    |
| AveragePolicyStd[3]  | 0.23673    |
| AveragePolicyStd[4]  | 0.17365    |
| AveragePolicyStd[5]  | 0.25182    |
| AverageReturn        | 1498.9     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 349.97     |
| AverageEpisodeLength | 928.92     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.25     |
| TotalNEpisodes       | 18351      |
| TotalNSamples        | 3.7314e+06 |
| ExplainedVariance    | 0.1133     |
-------------------------------------
[2018-12-22 11:23:26.726357 UTC] Saving snapshot
[2018-12-22 11:23:26.726649 UTC] Starting iteration 746
[2018-12-22 11:23:26.726787 UTC] Start collecting samples
[2018-12-22 11:23:29.658044 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:29.735713 UTC] Performing policy update
[2018-12-22 11:23:29.736287 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:29.825069 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:30.882710 UTC] Performing line search
[2018-12-22 11:23:31.011220 UTC] Updating baseline
[2018-12-22 11:23:32.632951 UTC] Computing logging information
-------------------------------------
| Iteration            | 746        |
| ExpectedImprovement  | 0.017905   |
| ActualImprovement    | 0.016344   |
| ImprovementRatio     | 0.91283    |
| MeanKL               | 0.0074054  |
| Entropy              | -0.45227   |
| Perplexity           | 0.63618    |
| AveragePolicyStd     | 0.22749    |
| AveragePolicyStd[0]  | 0.25142    |
| AveragePolicyStd[1]  | 0.26954    |
| AveragePolicyStd[2]  | 0.18239    |
| AveragePolicyStd[3]  | 0.23667    |
| AveragePolicyStd[4]  | 0.17382    |
| AveragePolicyStd[5]  | 0.25113    |
| AverageReturn        | 1510.9     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 333.13     |
| AverageEpisodeLength | 936.08     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.01     |
| TotalNEpisodes       | 18353      |
| TotalNSamples        | 3.7334e+06 |
| ExplainedVariance    | -0.0014073 |
-------------------------------------
[2018-12-22 11:23:33.006944 UTC] Saving snapshot
[2018-12-22 11:23:33.007194 UTC] Starting iteration 747
[2018-12-22 11:23:33.007310 UTC] Start collecting samples
[2018-12-22 11:23:35.948053 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:36.026786 UTC] Performing policy update
[2018-12-22 11:23:36.027390 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:36.118105 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:37.176862 UTC] Performing line search
[2018-12-22 11:23:37.305052 UTC] Updating baseline
[2018-12-22 11:23:39.118461 UTC] Computing logging information
-------------------------------------
| Iteration            | 747        |
| ExpectedImprovement  | 0.017418   |
| ActualImprovement    | 0.016226   |
| ImprovementRatio     | 0.9316     |
| MeanKL               | 0.0074507  |
| Entropy              | -0.4549    |
| Perplexity           | 0.63451    |
| AveragePolicyStd     | 0.22741    |
| AveragePolicyStd[0]  | 0.25085    |
| AveragePolicyStd[1]  | 0.26994    |
| AveragePolicyStd[2]  | 0.18285    |
| AveragePolicyStd[3]  | 0.23717    |
| AveragePolicyStd[4]  | 0.17314    |
| AveragePolicyStd[5]  | 0.25048    |
| AverageReturn        | 1511.5     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 333.19     |
| AverageEpisodeLength | 936.08     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.01     |
| TotalNEpisodes       | 18355      |
| TotalNSamples        | 3.7354e+06 |
| ExplainedVariance    | 0.010944   |
-------------------------------------
[2018-12-22 11:23:39.491894 UTC] Saving snapshot
[2018-12-22 11:23:39.492145 UTC] Starting iteration 748
[2018-12-22 11:23:39.492319 UTC] Start collecting samples
[2018-12-22 11:23:42.503421 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:42.588152 UTC] Performing policy update
[2018-12-22 11:23:42.588825 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:42.680188 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:43.756332 UTC] Performing line search
[2018-12-22 11:23:43.885727 UTC] Updating baseline
[2018-12-22 11:23:45.478240 UTC] Computing logging information
-------------------------------------
| Iteration            | 748        |
| ExpectedImprovement  | 0.018384   |
| ActualImprovement    | 0.017611   |
| ImprovementRatio     | 0.95795    |
| MeanKL               | 0.0073545  |
| Entropy              | -0.4654    |
| Perplexity           | 0.62788    |
| AveragePolicyStd     | 0.22698    |
| AveragePolicyStd[0]  | 0.2505     |
| AveragePolicyStd[1]  | 0.27056    |
| AveragePolicyStd[2]  | 0.18282    |
| AveragePolicyStd[3]  | 0.23608    |
| AveragePolicyStd[4]  | 0.17319    |
| AveragePolicyStd[5]  | 0.24876    |
| AverageReturn        | 1524       |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 326.93     |
| AverageEpisodeLength | 942.48     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.67     |
| TotalNEpisodes       | 18365      |
| TotalNSamples        | 3.7454e+06 |
| ExplainedVariance    | -0.0023535 |
-------------------------------------
[2018-12-22 11:23:45.862779 UTC] Saving snapshot
[2018-12-22 11:23:45.863062 UTC] Starting iteration 749
[2018-12-22 11:23:45.863178 UTC] Start collecting samples
[2018-12-22 11:23:48.814460 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:48.896375 UTC] Performing policy update
[2018-12-22 11:23:48.897031 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:48.987258 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:50.072104 UTC] Performing line search
[2018-12-22 11:23:50.199369 UTC] Updating baseline
[2018-12-22 11:23:51.586002 UTC] Computing logging information
-------------------------------------
| Iteration            | 749        |
| ExpectedImprovement  | 0.01765    |
| ActualImprovement    | 0.017329   |
| ImprovementRatio     | 0.98178    |
| MeanKL               | 0.0073485  |
| Entropy              | -0.47761   |
| Perplexity           | 0.62026    |
| AveragePolicyStd     | 0.22641    |
| AveragePolicyStd[0]  | 0.24954    |
| AveragePolicyStd[1]  | 0.26885    |
| AveragePolicyStd[2]  | 0.18358    |
| AveragePolicyStd[3]  | 0.23513    |
| AveragePolicyStd[4]  | 0.17335    |
| AveragePolicyStd[5]  | 0.24801    |
| AverageReturn        | 1512.6     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 351.84     |
| AverageEpisodeLength | 934.96     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.21     |
| TotalNEpisodes       | 18370      |
| TotalNSamples        | 3.7496e+06 |
| ExplainedVariance    | 0.1802     |
-------------------------------------
[2018-12-22 11:23:51.965483 UTC] Saving snapshot
[2018-12-22 11:23:51.965764 UTC] Starting iteration 750
[2018-12-22 11:23:51.965890 UTC] Start collecting samples
[2018-12-22 11:23:54.844438 UTC] Computing input variables for policy optimization
[2018-12-22 11:23:54.922966 UTC] Performing policy update
[2018-12-22 11:23:54.923754 UTC] Computing gradient in Euclidean space
[2018-12-22 11:23:55.013024 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:23:56.073025 UTC] Performing line search
[2018-12-22 11:23:56.201164 UTC] Updating baseline
[2018-12-22 11:23:57.830533 UTC] Computing logging information
-------------------------------------
| Iteration            | 750        |
| ExpectedImprovement  | 0.019731   |
| ActualImprovement    | 0.017205   |
| ImprovementRatio     | 0.87197    |
| MeanKL               | 0.0068855  |
| Entropy              | -0.48277   |
| Perplexity           | 0.61707    |
| AveragePolicyStd     | 0.2262     |
| AveragePolicyStd[0]  | 0.24912    |
| AveragePolicyStd[1]  | 0.26876    |
| AveragePolicyStd[2]  | 0.18371    |
| AveragePolicyStd[3]  | 0.23467    |
| AveragePolicyStd[4]  | 0.17326    |
| AveragePolicyStd[5]  | 0.24767    |
| AverageReturn        | 1512.2     |
| MinReturn            | 121.02     |
| MaxReturn            | 1715.1     |
| StdReturn            | 351.69     |
| AverageEpisodeLength | 934.96     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.21     |
| TotalNEpisodes       | 18371      |
| TotalNSamples        | 3.7506e+06 |
| ExplainedVariance    | 0.021585   |
-------------------------------------
[2018-12-22 11:23:58.209815 UTC] Saving snapshot
[2018-12-22 11:23:58.217823 UTC] Starting iteration 751
[2018-12-22 11:23:58.218020 UTC] Start collecting samples
[2018-12-22 11:24:01.210130 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:01.292992 UTC] Performing policy update
[2018-12-22 11:24:01.293639 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:01.385042 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:02.452985 UTC] Performing line search
[2018-12-22 11:24:02.580844 UTC] Updating baseline
[2018-12-22 11:24:03.867692 UTC] Computing logging information
-------------------------------------
| Iteration            | 751        |
| ExpectedImprovement  | 0.015412   |
| ActualImprovement    | 0.015135   |
| ImprovementRatio     | 0.98199    |
| MeanKL               | 0.0078209  |
| Entropy              | -0.49069   |
| Perplexity           | 0.6122     |
| AveragePolicyStd     | 0.22595    |
| AveragePolicyStd[0]  | 0.24803    |
| AveragePolicyStd[1]  | 0.26862    |
| AveragePolicyStd[2]  | 0.18373    |
| AveragePolicyStd[3]  | 0.2348     |
| AveragePolicyStd[4]  | 0.17207    |
| AveragePolicyStd[5]  | 0.24846    |
| AverageReturn        | 1541.1     |
| MinReturn            | 130.34     |
| MaxReturn            | 1715.1     |
| StdReturn            | 304.4      |
| AverageEpisodeLength | 951.65     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.06     |
| TotalNEpisodes       | 18379      |
| TotalNSamples        | 3.7583e+06 |
| ExplainedVariance    | 0.071067   |
-------------------------------------
[2018-12-22 11:24:04.248903 UTC] Saving snapshot
[2018-12-22 11:24:04.249176 UTC] Starting iteration 752
[2018-12-22 11:24:04.249292 UTC] Start collecting samples
[2018-12-22 11:24:07.199633 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:07.280424 UTC] Performing policy update
[2018-12-22 11:24:07.281197 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:07.371646 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:08.442595 UTC] Performing line search
[2018-12-22 11:24:08.569251 UTC] Updating baseline
[2018-12-22 11:24:09.950000 UTC] Computing logging information
-------------------------------------
| Iteration            | 752        |
| ExpectedImprovement  | 0.017235   |
| ActualImprovement    | 0.016406   |
| ImprovementRatio     | 0.95193    |
| MeanKL               | 0.0078215  |
| Entropy              | -0.4888    |
| Perplexity           | 0.61336    |
| AveragePolicyStd     | 0.22602    |
| AveragePolicyStd[0]  | 0.24779    |
| AveragePolicyStd[1]  | 0.26892    |
| AveragePolicyStd[2]  | 0.18418    |
| AveragePolicyStd[3]  | 0.23456    |
| AveragePolicyStd[4]  | 0.17191    |
| AveragePolicyStd[5]  | 0.24877    |
| AverageReturn        | 1539.7     |
| MinReturn            | 130.34     |
| MaxReturn            | 1694.6     |
| StdReturn            | 302.7      |
| AverageEpisodeLength | 951.83     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.31     |
| TotalNEpisodes       | 18386      |
| TotalNSamples        | 3.7646e+06 |
| ExplainedVariance    | 0.14405    |
-------------------------------------
[2018-12-22 11:24:10.325535 UTC] Saving snapshot
[2018-12-22 11:24:10.325882 UTC] Starting iteration 753
[2018-12-22 11:24:10.326020 UTC] Start collecting samples
[2018-12-22 11:24:13.230126 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:13.307960 UTC] Performing policy update
[2018-12-22 11:24:13.308769 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:13.398650 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:14.467809 UTC] Performing line search
[2018-12-22 11:24:14.598741 UTC] Updating baseline
[2018-12-22 11:24:15.790081 UTC] Computing logging information
--------------------------------------
| Iteration            | 753         |
| ExpectedImprovement  | 0.017216    |
| ActualImprovement    | 0.015842    |
| ImprovementRatio     | 0.9202      |
| MeanKL               | 0.0072895   |
| Entropy              | -0.49098    |
| Perplexity           | 0.61203     |
| AveragePolicyStd     | 0.22591     |
| AveragePolicyStd[0]  | 0.24855     |
| AveragePolicyStd[1]  | 0.26874     |
| AveragePolicyStd[2]  | 0.18392     |
| AveragePolicyStd[3]  | 0.23362     |
| AveragePolicyStd[4]  | 0.17255     |
| AveragePolicyStd[5]  | 0.24806     |
| AverageReturn        | 1538        |
| MinReturn            | 130.34      |
| MaxReturn            | 1694.6      |
| StdReturn            | 302.38      |
| AverageEpisodeLength | 951.83      |
| MinEpisodeLength     | 99          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 178.31      |
| TotalNEpisodes       | 18388       |
| TotalNSamples        | 3.7666e+06  |
| ExplainedVariance    | -8.1904e-05 |
--------------------------------------
[2018-12-22 11:24:16.167133 UTC] Saving snapshot
[2018-12-22 11:24:16.167416 UTC] Starting iteration 754
[2018-12-22 11:24:16.167556 UTC] Start collecting samples
[2018-12-22 11:24:19.122545 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:19.200972 UTC] Performing policy update
[2018-12-22 11:24:19.201795 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:19.291713 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:20.367350 UTC] Performing line search
[2018-12-22 11:24:20.494611 UTC] Updating baseline
[2018-12-22 11:24:22.203407 UTC] Computing logging information
-------------------------------------
| Iteration            | 754        |
| ExpectedImprovement  | 0.019672   |
| ActualImprovement    | 0.018272   |
| ImprovementRatio     | 0.92883    |
| MeanKL               | 0.0072615  |
| Entropy              | -0.48857   |
| Perplexity           | 0.6135     |
| AveragePolicyStd     | 0.22604    |
| AveragePolicyStd[0]  | 0.24919    |
| AveragePolicyStd[1]  | 0.2692     |
| AveragePolicyStd[2]  | 0.184      |
| AveragePolicyStd[3]  | 0.23374    |
| AveragePolicyStd[4]  | 0.17209    |
| AveragePolicyStd[5]  | 0.24801    |
| AverageReturn        | 1536.7     |
| MinReturn            | 130.34     |
| MaxReturn            | 1690.1     |
| StdReturn            | 302.02     |
| AverageEpisodeLength | 951.83     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.31     |
| TotalNEpisodes       | 18393      |
| TotalNSamples        | 3.7716e+06 |
| ExplainedVariance    | -0.0068555 |
-------------------------------------
[2018-12-22 11:24:22.577931 UTC] Saving snapshot
[2018-12-22 11:24:22.578177 UTC] Starting iteration 755
[2018-12-22 11:24:22.578291 UTC] Start collecting samples
[2018-12-22 11:24:25.583887 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:25.665771 UTC] Performing policy update
[2018-12-22 11:24:25.666568 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:25.762356 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:26.827624 UTC] Performing line search
[2018-12-22 11:24:26.954910 UTC] Updating baseline
[2018-12-22 11:24:28.323777 UTC] Computing logging information
-------------------------------------
| Iteration            | 755        |
| ExpectedImprovement  | 0.020057   |
| ActualImprovement    | 0.01907    |
| ImprovementRatio     | 0.9508     |
| MeanKL               | 0.0072502  |
| Entropy              | -0.49505   |
| Perplexity           | 0.60954    |
| AveragePolicyStd     | 0.22583    |
| AveragePolicyStd[0]  | 0.249      |
| AveragePolicyStd[1]  | 0.26912    |
| AveragePolicyStd[2]  | 0.18384    |
| AveragePolicyStd[3]  | 0.23328    |
| AveragePolicyStd[4]  | 0.17151    |
| AveragePolicyStd[5]  | 0.24821    |
| AverageReturn        | 1551.5     |
| MinReturn            | 130.34     |
| MaxReturn            | 1690.1     |
| StdReturn            | 267.34     |
| AverageEpisodeLength | 960.54     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.03     |
| TotalNEpisodes       | 18401      |
| TotalNSamples        | 3.7796e+06 |
| ExplainedVariance    | 0.00035694 |
-------------------------------------
[2018-12-22 11:24:28.700410 UTC] Saving snapshot
[2018-12-22 11:24:28.700698 UTC] Starting iteration 756
[2018-12-22 11:24:28.700831 UTC] Start collecting samples
[2018-12-22 11:24:31.666084 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:31.746459 UTC] Performing policy update
[2018-12-22 11:24:31.747134 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:31.836349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:32.895763 UTC] Performing line search
[2018-12-22 11:24:33.023021 UTC] Updating baseline
[2018-12-22 11:24:34.383716 UTC] Computing logging information
-------------------------------------
| Iteration            | 756        |
| ExpectedImprovement  | 0.017699   |
| ActualImprovement    | 0.016622   |
| ImprovementRatio     | 0.93917    |
| MeanKL               | 0.0075496  |
| Entropy              | -0.51016   |
| Perplexity           | 0.6004     |
| AveragePolicyStd     | 0.22524    |
| AveragePolicyStd[0]  | 0.24826    |
| AveragePolicyStd[1]  | 0.26794    |
| AveragePolicyStd[2]  | 0.18294    |
| AveragePolicyStd[3]  | 0.23305    |
| AveragePolicyStd[4]  | 0.17151    |
| AveragePolicyStd[5]  | 0.24774    |
| AverageReturn        | 1536.7     |
| MinReturn            | 130.34     |
| MaxReturn            | 1690.1     |
| StdReturn            | 285.88     |
| AverageEpisodeLength | 951.11     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.12     |
| TotalNEpisodes       | 18406      |
| TotalNSamples        | 3.7836e+06 |
| ExplainedVariance    | 0.27086    |
-------------------------------------
[2018-12-22 11:24:34.765437 UTC] Saving snapshot
[2018-12-22 11:24:34.765808 UTC] Starting iteration 757
[2018-12-22 11:24:34.765932 UTC] Start collecting samples
[2018-12-22 11:24:37.666963 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:37.744205 UTC] Performing policy update
[2018-12-22 11:24:37.745076 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:37.834178 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:38.893696 UTC] Performing line search
[2018-12-22 11:24:39.020179 UTC] Updating baseline
[2018-12-22 11:24:40.383852 UTC] Computing logging information
-------------------------------------
| Iteration            | 757        |
| ExpectedImprovement  | 0.016533   |
| ActualImprovement    | 0.0153     |
| ImprovementRatio     | 0.9254     |
| MeanKL               | 0.0074687  |
| Entropy              | -0.51654   |
| Perplexity           | 0.59658    |
| AveragePolicyStd     | 0.22499    |
| AveragePolicyStd[0]  | 0.24634    |
| AveragePolicyStd[1]  | 0.26836    |
| AveragePolicyStd[2]  | 0.18284    |
| AveragePolicyStd[3]  | 0.23284    |
| AveragePolicyStd[4]  | 0.17156    |
| AveragePolicyStd[5]  | 0.24799    |
| AverageReturn        | 1534.1     |
| MinReturn            | 130.34     |
| MaxReturn            | 1690.1     |
| StdReturn            | 285.73     |
| AverageEpisodeLength | 950.41     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.06     |
| TotalNEpisodes       | 18408      |
| TotalNSamples        | 3.7856e+06 |
| ExplainedVariance    | 0.30927    |
-------------------------------------
[2018-12-22 11:24:40.754279 UTC] Saving snapshot
[2018-12-22 11:24:40.754551 UTC] Starting iteration 758
[2018-12-22 11:24:40.754674 UTC] Start collecting samples
[2018-12-22 11:24:43.741859 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:43.824860 UTC] Performing policy update
[2018-12-22 11:24:43.825489 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:43.916157 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:44.981238 UTC] Performing line search
[2018-12-22 11:24:45.108908 UTC] Updating baseline
[2018-12-22 11:24:46.399716 UTC] Computing logging information
-------------------------------------
| Iteration            | 758        |
| ExpectedImprovement  | 0.016927   |
| ActualImprovement    | 0.015976   |
| ImprovementRatio     | 0.94386    |
| MeanKL               | 0.0071208  |
| Entropy              | -0.52227   |
| Perplexity           | 0.59317    |
| AveragePolicyStd     | 0.22481    |
| AveragePolicyStd[0]  | 0.24638    |
| AveragePolicyStd[1]  | 0.26828    |
| AveragePolicyStd[2]  | 0.18227    |
| AveragePolicyStd[3]  | 0.23327    |
| AveragePolicyStd[4]  | 0.17119    |
| AveragePolicyStd[5]  | 0.24747    |
| AverageReturn        | 1547.7     |
| MinReturn            | 220.42     |
| MaxReturn            | 1690.1     |
| StdReturn            | 248.32     |
| AverageEpisodeLength | 959.42     |
| MinEpisodeLength     | 174        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.87     |
| TotalNEpisodes       | 18417      |
| TotalNSamples        | 3.7946e+06 |
| ExplainedVariance    | 0.013891   |
-------------------------------------
[2018-12-22 11:24:46.776052 UTC] Saving snapshot
[2018-12-22 11:24:46.776316 UTC] Starting iteration 759
[2018-12-22 11:24:46.776437 UTC] Start collecting samples
[2018-12-22 11:24:49.700071 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:49.779693 UTC] Performing policy update
[2018-12-22 11:24:49.780291 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:49.871309 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:50.940454 UTC] Performing line search
[2018-12-22 11:24:51.068848 UTC] Updating baseline
[2018-12-22 11:24:52.543426 UTC] Computing logging information
-------------------------------------
| Iteration            | 759        |
| ExpectedImprovement  | 0.016841   |
| ActualImprovement    | 0.016398   |
| ImprovementRatio     | 0.97372    |
| MeanKL               | 0.0074858  |
| Entropy              | -0.52814   |
| Perplexity           | 0.5897     |
| AveragePolicyStd     | 0.22457    |
| AveragePolicyStd[0]  | 0.24624    |
| AveragePolicyStd[1]  | 0.26756    |
| AveragePolicyStd[2]  | 0.1819     |
| AveragePolicyStd[3]  | 0.23282    |
| AveragePolicyStd[4]  | 0.17144    |
| AveragePolicyStd[5]  | 0.24743    |
| AverageReturn        | 1532.5     |
| MinReturn            | 141.72     |
| MaxReturn            | 1690.1     |
| StdReturn            | 284.7      |
| AverageEpisodeLength | 950.41     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.06     |
| TotalNEpisodes       | 18422      |
| TotalNSamples        | 3.7986e+06 |
| ExplainedVariance    | 0.086836   |
-------------------------------------
[2018-12-22 11:24:52.923768 UTC] Saving snapshot
[2018-12-22 11:24:52.924011 UTC] Starting iteration 760
[2018-12-22 11:24:52.924131 UTC] Start collecting samples
[2018-12-22 11:24:55.840191 UTC] Computing input variables for policy optimization
[2018-12-22 11:24:55.920985 UTC] Performing policy update
[2018-12-22 11:24:55.921900 UTC] Computing gradient in Euclidean space
[2018-12-22 11:24:56.013524 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:24:57.078048 UTC] Performing line search
[2018-12-22 11:24:57.207384 UTC] Updating baseline
[2018-12-22 11:24:58.483885 UTC] Computing logging information
-------------------------------------
| Iteration            | 760        |
| ExpectedImprovement  | 0.017208   |
| ActualImprovement    | 0.016063   |
| ImprovementRatio     | 0.93347    |
| MeanKL               | 0.007574   |
| Entropy              | -0.5235    |
| Perplexity           | 0.59244    |
| AveragePolicyStd     | 0.22474    |
| AveragePolicyStd[0]  | 0.24681    |
| AveragePolicyStd[1]  | 0.26769    |
| AveragePolicyStd[2]  | 0.18204    |
| AveragePolicyStd[3]  | 0.23272    |
| AveragePolicyStd[4]  | 0.17163    |
| AveragePolicyStd[5]  | 0.24753    |
| AverageReturn        | 1532.2     |
| MinReturn            | 141.72     |
| MaxReturn            | 1690.1     |
| StdReturn            | 284.61     |
| AverageEpisodeLength | 950.41     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.06     |
| TotalNEpisodes       | 18425      |
| TotalNSamples        | 3.8016e+06 |
| ExplainedVariance    | -0.0054816 |
-------------------------------------
[2018-12-22 11:24:58.861524 UTC] Saving snapshot
[2018-12-22 11:24:58.869875 UTC] Starting iteration 761
[2018-12-22 11:24:58.870092 UTC] Start collecting samples
[2018-12-22 11:25:01.860968 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:01.946462 UTC] Performing policy update
[2018-12-22 11:25:01.947375 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:02.038219 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:03.099756 UTC] Performing line search
[2018-12-22 11:25:03.228065 UTC] Updating baseline
[2018-12-22 11:25:05.112003 UTC] Computing logging information
-------------------------------------
| Iteration            | 761        |
| ExpectedImprovement  | 0.018692   |
| ActualImprovement    | 0.017131   |
| ImprovementRatio     | 0.91649    |
| MeanKL               | 0.0071647  |
| Entropy              | -0.5251    |
| Perplexity           | 0.5915     |
| AveragePolicyStd     | 0.22465    |
| AveragePolicyStd[0]  | 0.24731    |
| AveragePolicyStd[1]  | 0.26686    |
| AveragePolicyStd[2]  | 0.1818     |
| AveragePolicyStd[3]  | 0.23277    |
| AveragePolicyStd[4]  | 0.17199    |
| AveragePolicyStd[5]  | 0.24716    |
| AverageReturn        | 1538.5     |
| MinReturn            | 141.72     |
| MaxReturn            | 1690.1     |
| StdReturn            | 278.81     |
| AverageEpisodeLength | 954.34     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.57     |
| TotalNEpisodes       | 18432      |
| TotalNSamples        | 3.8086e+06 |
| ExplainedVariance    | -0.0019743 |
-------------------------------------
[2018-12-22 11:25:05.490376 UTC] Saving snapshot
[2018-12-22 11:25:05.490646 UTC] Starting iteration 762
[2018-12-22 11:25:05.490780 UTC] Start collecting samples
[2018-12-22 11:25:08.482982 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:08.563335 UTC] Performing policy update
[2018-12-22 11:25:08.564112 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:08.654454 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:09.715079 UTC] Performing line search
[2018-12-22 11:25:09.845979 UTC] Updating baseline
[2018-12-22 11:25:11.117955 UTC] Computing logging information
-------------------------------------
| Iteration            | 762        |
| ExpectedImprovement  | 0.018213   |
| ActualImprovement    | 0.017312   |
| ImprovementRatio     | 0.95053    |
| MeanKL               | 0.0072483  |
| Entropy              | -0.53928   |
| Perplexity           | 0.58317    |
| AveragePolicyStd     | 0.2241     |
| AveragePolicyStd[0]  | 0.24634    |
| AveragePolicyStd[1]  | 0.26585    |
| AveragePolicyStd[2]  | 0.18163    |
| AveragePolicyStd[3]  | 0.23276    |
| AveragePolicyStd[4]  | 0.17148    |
| AveragePolicyStd[5]  | 0.24654    |
| AverageReturn        | 1523.3     |
| MinReturn            | 102.49     |
| MaxReturn            | 1690.1     |
| StdReturn            | 313.11     |
| AverageEpisodeLength | 945.18     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.77     |
| TotalNEpisodes       | 18439      |
| TotalNSamples        | 3.8147e+06 |
| ExplainedVariance    | 0.067539   |
-------------------------------------
[2018-12-22 11:25:11.498778 UTC] Saving snapshot
[2018-12-22 11:25:11.499034 UTC] Starting iteration 763
[2018-12-22 11:25:11.499151 UTC] Start collecting samples
[2018-12-22 11:25:14.411583 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:14.490580 UTC] Performing policy update
[2018-12-22 11:25:14.491302 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:14.581387 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:15.639519 UTC] Performing line search
[2018-12-22 11:25:15.770520 UTC] Updating baseline
[2018-12-22 11:25:17.135540 UTC] Computing logging information
-------------------------------------
| Iteration            | 763        |
| ExpectedImprovement  | 0.014291   |
| ActualImprovement    | 0.013872   |
| ImprovementRatio     | 0.97068    |
| MeanKL               | 0.0079069  |
| Entropy              | -0.54303   |
| Perplexity           | 0.58099    |
| AveragePolicyStd     | 0.22403    |
| AveragePolicyStd[0]  | 0.24617    |
| AveragePolicyStd[1]  | 0.26677    |
| AveragePolicyStd[2]  | 0.18122    |
| AveragePolicyStd[3]  | 0.23266    |
| AveragePolicyStd[4]  | 0.17087    |
| AveragePolicyStd[5]  | 0.24648    |
| AverageReturn        | 1523.6     |
| MinReturn            | 102.49     |
| MaxReturn            | 1690.1     |
| StdReturn            | 313.2      |
| AverageEpisodeLength | 945.18     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.77     |
| TotalNEpisodes       | 18441      |
| TotalNSamples        | 3.8167e+06 |
| ExplainedVariance    | -0.086049  |
-------------------------------------
[2018-12-22 11:25:17.512515 UTC] Saving snapshot
[2018-12-22 11:25:17.512763 UTC] Starting iteration 764
[2018-12-22 11:25:17.512898 UTC] Start collecting samples
[2018-12-22 11:25:20.506010 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:20.587683 UTC] Performing policy update
[2018-12-22 11:25:20.588539 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:20.679493 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:21.744289 UTC] Performing line search
[2018-12-22 11:25:21.877362 UTC] Updating baseline
[2018-12-22 11:25:23.747802 UTC] Computing logging information
-------------------------------------
| Iteration            | 764        |
| ExpectedImprovement  | 0.01654    |
| ActualImprovement    | 0.015965   |
| ImprovementRatio     | 0.96519    |
| MeanKL               | 0.008065   |
| Entropy              | -0.53192   |
| Perplexity           | 0.58748    |
| AveragePolicyStd     | 0.22449    |
| AveragePolicyStd[0]  | 0.24611    |
| AveragePolicyStd[1]  | 0.26801    |
| AveragePolicyStd[2]  | 0.18122    |
| AveragePolicyStd[3]  | 0.23414    |
| AveragePolicyStd[4]  | 0.17076    |
| AveragePolicyStd[5]  | 0.24672    |
| AverageReturn        | 1528.1     |
| MinReturn            | 102.49     |
| MaxReturn            | 1690.1     |
| StdReturn            | 306.96     |
| AverageEpisodeLength | 949.25     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.46     |
| TotalNEpisodes       | 18449      |
| TotalNSamples        | 3.8247e+06 |
| ExplainedVariance    | -0.0019715 |
-------------------------------------
[2018-12-22 11:25:24.127439 UTC] Saving snapshot
[2018-12-22 11:25:24.127725 UTC] Starting iteration 765
[2018-12-22 11:25:24.127845 UTC] Start collecting samples
[2018-12-22 11:25:27.099101 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:27.179338 UTC] Performing policy update
[2018-12-22 11:25:27.180038 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:27.270187 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:28.338151 UTC] Performing line search
[2018-12-22 11:25:28.465911 UTC] Updating baseline
[2018-12-22 11:25:29.932011 UTC] Computing logging information
-------------------------------------
| Iteration            | 765        |
| ExpectedImprovement  | 0.016028   |
| ActualImprovement    | 0.015274   |
| ImprovementRatio     | 0.95298    |
| MeanKL               | 0.0080804  |
| Entropy              | -0.52816   |
| Perplexity           | 0.58969    |
| AveragePolicyStd     | 0.22462    |
| AveragePolicyStd[0]  | 0.24615    |
| AveragePolicyStd[1]  | 0.26833    |
| AveragePolicyStd[2]  | 0.18163    |
| AveragePolicyStd[3]  | 0.23388    |
| AveragePolicyStd[4]  | 0.17093    |
| AveragePolicyStd[5]  | 0.24679    |
| AverageReturn        | 1533.5     |
| MinReturn            | 102.49     |
| MaxReturn            | 1690.1     |
| StdReturn            | 300.63     |
| AverageEpisodeLength | 953.43     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.76     |
| TotalNEpisodes       | 18455      |
| TotalNSamples        | 3.8307e+06 |
| ExplainedVariance    | 0.029309   |
-------------------------------------
[2018-12-22 11:25:30.317963 UTC] Saving snapshot
[2018-12-22 11:25:30.318425 UTC] Starting iteration 766
[2018-12-22 11:25:30.318798 UTC] Start collecting samples
[2018-12-22 11:25:33.239709 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:33.318266 UTC] Performing policy update
[2018-12-22 11:25:33.319020 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:33.410108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:34.474418 UTC] Performing line search
[2018-12-22 11:25:34.602011 UTC] Updating baseline
[2018-12-22 11:25:36.351068 UTC] Computing logging information
------------------------------------
| Iteration            | 766       |
| ExpectedImprovement  | 0.017874  |
| ActualImprovement    | 0.016783  |
| ImprovementRatio     | 0.93897   |
| MeanKL               | 0.007229  |
| Entropy              | -0.52864  |
| Perplexity           | 0.58941   |
| AveragePolicyStd     | 0.22455   |
| AveragePolicyStd[0]  | 0.2463    |
| AveragePolicyStd[1]  | 0.26628   |
| AveragePolicyStd[2]  | 0.18168   |
| AveragePolicyStd[3]  | 0.23465   |
| AveragePolicyStd[4]  | 0.17117   |
| AveragePolicyStd[5]  | 0.24719   |
| AverageReturn        | 1520.1    |
| MinReturn            | 102.49    |
| MaxReturn            | 1690.1    |
| StdReturn            | 323.24    |
| AverageEpisodeLength | 945.85    |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 193.13    |
| TotalNEpisodes       | 18457     |
| TotalNSamples        | 3.832e+06 |
| ExplainedVariance    | 0.22183   |
------------------------------------
[2018-12-22 11:25:36.729883 UTC] Saving snapshot
[2018-12-22 11:25:36.730160 UTC] Starting iteration 767
[2018-12-22 11:25:36.730278 UTC] Start collecting samples
[2018-12-22 11:25:39.717168 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:39.799902 UTC] Performing policy update
[2018-12-22 11:25:39.800565 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:39.890624 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:40.951579 UTC] Performing line search
[2018-12-22 11:25:41.080114 UTC] Updating baseline
[2018-12-22 11:25:42.631228 UTC] Computing logging information
-------------------------------------
| Iteration            | 767        |
| ExpectedImprovement  | 0.018981   |
| ActualImprovement    | 0.01804    |
| ImprovementRatio     | 0.95039    |
| MeanKL               | 0.0075433  |
| Entropy              | -0.52869   |
| Perplexity           | 0.58937    |
| AveragePolicyStd     | 0.22452    |
| AveragePolicyStd[0]  | 0.24663    |
| AveragePolicyStd[1]  | 0.26485    |
| AveragePolicyStd[2]  | 0.18224    |
| AveragePolicyStd[3]  | 0.23467    |
| AveragePolicyStd[4]  | 0.17075    |
| AveragePolicyStd[5]  | 0.24799    |
| AverageReturn        | 1507.3     |
| MinReturn            | 102.49     |
| MaxReturn            | 1690.1     |
| StdReturn            | 336.21     |
| AverageEpisodeLength | 939.7      |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.94     |
| TotalNEpisodes       | 18465      |
| TotalNSamples        | 3.8394e+06 |
| ExplainedVariance    | 0.12687    |
-------------------------------------
[2018-12-22 11:25:43.013304 UTC] Saving snapshot
[2018-12-22 11:25:43.013586 UTC] Starting iteration 768
[2018-12-22 11:25:43.013707 UTC] Start collecting samples
[2018-12-22 11:25:45.974636 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:46.056397 UTC] Performing policy update
[2018-12-22 11:25:46.057276 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:46.148156 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:47.218820 UTC] Performing line search
[2018-12-22 11:25:47.348618 UTC] Updating baseline
[2018-12-22 11:25:48.619044 UTC] Computing logging information
-------------------------------------
| Iteration            | 768        |
| ExpectedImprovement  | 0.017132   |
| ActualImprovement    | 0.016285   |
| ImprovementRatio     | 0.95055    |
| MeanKL               | 0.0077365  |
| Entropy              | -0.52941   |
| Perplexity           | 0.58895    |
| AveragePolicyStd     | 0.22447    |
| AveragePolicyStd[0]  | 0.24649    |
| AveragePolicyStd[1]  | 0.26474    |
| AveragePolicyStd[2]  | 0.18284    |
| AveragePolicyStd[3]  | 0.23386    |
| AveragePolicyStd[4]  | 0.17069    |
| AveragePolicyStd[5]  | 0.24819    |
| AverageReturn        | 1521       |
| MinReturn            | 102.49     |
| MaxReturn            | 1690.1     |
| StdReturn            | 310.52     |
| AverageEpisodeLength | 947.96     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.69     |
| TotalNEpisodes       | 18470      |
| TotalNSamples        | 3.8444e+06 |
| ExplainedVariance    | 0.047251   |
-------------------------------------
[2018-12-22 11:25:49.000923 UTC] Saving snapshot
[2018-12-22 11:25:49.001218 UTC] Starting iteration 769
[2018-12-22 11:25:49.001353 UTC] Start collecting samples
[2018-12-22 11:25:51.962712 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:52.042085 UTC] Performing policy update
[2018-12-22 11:25:52.042687 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:52.132746 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:53.196364 UTC] Performing line search
[2018-12-22 11:25:53.323659 UTC] Updating baseline
[2018-12-22 11:25:54.704082 UTC] Computing logging information
-------------------------------------
| Iteration            | 769        |
| ExpectedImprovement  | 0.017672   |
| ActualImprovement    | 0.016694   |
| ImprovementRatio     | 0.94463    |
| MeanKL               | 0.0074803  |
| Entropy              | -0.5239    |
| Perplexity           | 0.59221    |
| AveragePolicyStd     | 0.22462    |
| AveragePolicyStd[0]  | 0.24597    |
| AveragePolicyStd[1]  | 0.26443    |
| AveragePolicyStd[2]  | 0.18262    |
| AveragePolicyStd[3]  | 0.2346     |
| AveragePolicyStd[4]  | 0.17176    |
| AveragePolicyStd[5]  | 0.24836    |
| AverageReturn        | 1510.5     |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.6     |
| StdReturn            | 321.93     |
| AverageEpisodeLength | 942.14     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.95     |
| TotalNEpisodes       | 18474      |
| TotalNSamples        | 3.8478e+06 |
| ExplainedVariance    | 0.28342    |
-------------------------------------
[2018-12-22 11:25:55.086256 UTC] Saving snapshot
[2018-12-22 11:25:55.086557 UTC] Starting iteration 770
[2018-12-22 11:25:55.086684 UTC] Start collecting samples
[2018-12-22 11:25:58.074085 UTC] Computing input variables for policy optimization
[2018-12-22 11:25:58.156135 UTC] Performing policy update
[2018-12-22 11:25:58.156756 UTC] Computing gradient in Euclidean space
[2018-12-22 11:25:58.247816 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:25:59.311667 UTC] Performing line search
[2018-12-22 11:25:59.440284 UTC] Updating baseline
[2018-12-22 11:26:00.729517 UTC] Computing logging information
-------------------------------------
| Iteration            | 770        |
| ExpectedImprovement  | 0.018685   |
| ActualImprovement    | 0.017721   |
| ImprovementRatio     | 0.9484     |
| MeanKL               | 0.0069104  |
| Entropy              | -0.51924   |
| Perplexity           | 0.59497    |
| AveragePolicyStd     | 0.22474    |
| AveragePolicyStd[0]  | 0.24624    |
| AveragePolicyStd[1]  | 0.26521    |
| AveragePolicyStd[2]  | 0.18262    |
| AveragePolicyStd[3]  | 0.23462    |
| AveragePolicyStd[4]  | 0.17284    |
| AveragePolicyStd[5]  | 0.24693    |
| AverageReturn        | 1513.4     |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.6     |
| StdReturn            | 320.63     |
| AverageEpisodeLength | 944.29     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.38     |
| TotalNEpisodes       | 18478      |
| TotalNSamples        | 3.8518e+06 |
| ExplainedVariance    | 0.13431    |
-------------------------------------
[2018-12-22 11:26:01.112894 UTC] Saving snapshot
[2018-12-22 11:26:01.121261 UTC] Starting iteration 771
[2018-12-22 11:26:01.121486 UTC] Start collecting samples
[2018-12-22 11:26:04.086630 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:04.168550 UTC] Performing policy update
[2018-12-22 11:26:04.169237 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:04.257709 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:05.313843 UTC] Performing line search
[2018-12-22 11:26:05.441280 UTC] Updating baseline
[2018-12-22 11:26:06.978059 UTC] Computing logging information
-------------------------------------
| Iteration            | 771        |
| ExpectedImprovement  | 0.017811   |
| ActualImprovement    | 0.018338   |
| ImprovementRatio     | 1.0296     |
| MeanKL               | 0.0072064  |
| Entropy              | -0.51906   |
| Perplexity           | 0.59508    |
| AveragePolicyStd     | 0.22479    |
| AveragePolicyStd[0]  | 0.24696    |
| AveragePolicyStd[1]  | 0.26525    |
| AveragePolicyStd[2]  | 0.18298    |
| AveragePolicyStd[3]  | 0.23432    |
| AveragePolicyStd[4]  | 0.17198    |
| AveragePolicyStd[5]  | 0.24728    |
| AverageReturn        | 1513.9     |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.7     |
| StdReturn            | 320.76     |
| AverageEpisodeLength | 944.29     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.38     |
| TotalNEpisodes       | 18484      |
| TotalNSamples        | 3.8578e+06 |
| ExplainedVariance    | -0.039222  |
-------------------------------------
[2018-12-22 11:26:07.353259 UTC] Saving snapshot
[2018-12-22 11:26:07.353526 UTC] Starting iteration 772
[2018-12-22 11:26:07.353649 UTC] Start collecting samples
[2018-12-22 11:26:10.315909 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:10.395782 UTC] Performing policy update
[2018-12-22 11:26:10.396559 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:10.486972 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:11.547357 UTC] Performing line search
[2018-12-22 11:26:11.676520 UTC] Updating baseline
[2018-12-22 11:26:13.223627 UTC] Computing logging information
-------------------------------------
| Iteration            | 772        |
| ExpectedImprovement  | 0.017675   |
| ActualImprovement    | 0.016686   |
| ImprovementRatio     | 0.94406    |
| MeanKL               | 0.0075161  |
| Entropy              | -0.51014   |
| Perplexity           | 0.60041    |
| AveragePolicyStd     | 0.2251     |
| AveragePolicyStd[0]  | 0.24739    |
| AveragePolicyStd[1]  | 0.26547    |
| AveragePolicyStd[2]  | 0.18337    |
| AveragePolicyStd[3]  | 0.23429    |
| AveragePolicyStd[4]  | 0.17261    |
| AveragePolicyStd[5]  | 0.24745    |
| AverageReturn        | 1528.2     |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.7     |
| StdReturn            | 296.29     |
| AverageEpisodeLength | 952.15     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.9      |
| TotalNEpisodes       | 18489      |
| TotalNSamples        | 3.8628e+06 |
| ExplainedVariance    | 0.02321    |
-------------------------------------
[2018-12-22 11:26:13.597187 UTC] Saving snapshot
[2018-12-22 11:26:13.597574 UTC] Starting iteration 773
[2018-12-22 11:26:13.597697 UTC] Start collecting samples
[2018-12-22 11:26:16.575186 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:16.654799 UTC] Performing policy update
[2018-12-22 11:26:16.655635 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:16.745897 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:17.816122 UTC] Performing line search
[2018-12-22 11:26:17.947703 UTC] Updating baseline
[2018-12-22 11:26:19.408832 UTC] Computing logging information
-------------------------------------
| Iteration            | 773        |
| ExpectedImprovement  | 0.017353   |
| ActualImprovement    | 0.01652    |
| ImprovementRatio     | 0.95198    |
| MeanKL               | 0.0080029  |
| Entropy              | -0.51329   |
| Perplexity           | 0.59852    |
| AveragePolicyStd     | 0.22499    |
| AveragePolicyStd[0]  | 0.24792    |
| AveragePolicyStd[1]  | 0.2647     |
| AveragePolicyStd[2]  | 0.18347    |
| AveragePolicyStd[3]  | 0.23439    |
| AveragePolicyStd[4]  | 0.17212    |
| AveragePolicyStd[5]  | 0.24731    |
| AverageReturn        | 1523.9     |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.7     |
| StdReturn            | 299.55     |
| AverageEpisodeLength | 949.04     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.74     |
| TotalNEpisodes       | 18493      |
| TotalNSamples        | 3.8665e+06 |
| ExplainedVariance    | 0.1836     |
-------------------------------------
[2018-12-22 11:26:19.796953 UTC] Saving snapshot
[2018-12-22 11:26:19.797219 UTC] Starting iteration 774
[2018-12-22 11:26:19.797358 UTC] Start collecting samples
[2018-12-22 11:26:22.763848 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:22.844887 UTC] Performing policy update
[2018-12-22 11:26:22.845725 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:22.935523 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:23.994017 UTC] Performing line search
[2018-12-22 11:26:24.120814 UTC] Updating baseline
[2018-12-22 11:26:25.397703 UTC] Computing logging information
-------------------------------------
| Iteration            | 774        |
| ExpectedImprovement  | 0.017793   |
| ActualImprovement    | 0.017102   |
| ImprovementRatio     | 0.96115    |
| MeanKL               | 0.007175   |
| Entropy              | -0.519     |
| Perplexity           | 0.59511    |
| AveragePolicyStd     | 0.2248     |
| AveragePolicyStd[0]  | 0.2479     |
| AveragePolicyStd[1]  | 0.26474    |
| AveragePolicyStd[2]  | 0.18344    |
| AveragePolicyStd[3]  | 0.23409    |
| AveragePolicyStd[4]  | 0.17146    |
| AveragePolicyStd[5]  | 0.24719    |
| AverageReturn        | 1523.5     |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.7     |
| StdReturn            | 299.32     |
| AverageEpisodeLength | 949.04     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.74     |
| TotalNEpisodes       | 18500      |
| TotalNSamples        | 3.8735e+06 |
| ExplainedVariance    | -0.0042843 |
-------------------------------------
[2018-12-22 11:26:25.778961 UTC] Saving snapshot
[2018-12-22 11:26:25.779241 UTC] Starting iteration 775
[2018-12-22 11:26:25.779377 UTC] Start collecting samples
[2018-12-22 11:26:28.718601 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:28.800396 UTC] Performing policy update
[2018-12-22 11:26:28.801419 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:28.891572 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:29.964895 UTC] Performing line search
[2018-12-22 11:26:30.092702 UTC] Updating baseline
[2018-12-22 11:26:31.466216 UTC] Computing logging information
-------------------------------------
| Iteration            | 775        |
| ExpectedImprovement  | 0.018      |
| ActualImprovement    | 0.016865   |
| ImprovementRatio     | 0.93693    |
| MeanKL               | 0.0073223  |
| Entropy              | -0.51819   |
| Perplexity           | 0.5956     |
| AveragePolicyStd     | 0.22485    |
| AveragePolicyStd[0]  | 0.24778    |
| AveragePolicyStd[1]  | 0.26547    |
| AveragePolicyStd[2]  | 0.18366    |
| AveragePolicyStd[3]  | 0.23402    |
| AveragePolicyStd[4]  | 0.1713     |
| AveragePolicyStd[5]  | 0.24685    |
| AverageReturn        | 1537.9     |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.7     |
| StdReturn            | 282.13     |
| AverageEpisodeLength | 958.47     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.46     |
| TotalNEpisodes       | 18505      |
| TotalNSamples        | 3.8785e+06 |
| ExplainedVariance    | -0.0083641 |
-------------------------------------
[2018-12-22 11:26:31.849327 UTC] Saving snapshot
[2018-12-22 11:26:31.849595 UTC] Starting iteration 776
[2018-12-22 11:26:31.849717 UTC] Start collecting samples
[2018-12-22 11:26:34.827607 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:34.907249 UTC] Performing policy update
[2018-12-22 11:26:34.908042 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:35.000445 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:36.076957 UTC] Performing line search
[2018-12-22 11:26:36.206531 UTC] Updating baseline
[2018-12-22 11:26:37.473560 UTC] Computing logging information
-------------------------------------
| Iteration            | 776        |
| ExpectedImprovement  | 0.018472   |
| ActualImprovement    | 0.017048   |
| ImprovementRatio     | 0.92291    |
| MeanKL               | 0.0072916  |
| Entropy              | -0.52567   |
| Perplexity           | 0.59116    |
| AveragePolicyStd     | 0.22462    |
| AveragePolicyStd[0]  | 0.24778    |
| AveragePolicyStd[1]  | 0.26497    |
| AveragePolicyStd[2]  | 0.18382    |
| AveragePolicyStd[3]  | 0.23441    |
| AveragePolicyStd[4]  | 0.16998    |
| AveragePolicyStd[5]  | 0.24674    |
| AverageReturn        | 1538       |
| MinReturn            | 102.49     |
| MaxReturn            | 1666.7     |
| StdReturn            | 282.77     |
| AverageEpisodeLength | 957.49     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.9      |
| TotalNEpisodes       | 18510      |
| TotalNSamples        | 3.8833e+06 |
| ExplainedVariance    | 0.082092   |
-------------------------------------
[2018-12-22 11:26:37.852989 UTC] Saving snapshot
[2018-12-22 11:26:37.853237 UTC] Starting iteration 777
[2018-12-22 11:26:37.853370 UTC] Start collecting samples
[2018-12-22 11:26:40.833055 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:40.913691 UTC] Performing policy update
[2018-12-22 11:26:40.914490 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:41.005085 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:42.072881 UTC] Performing line search
[2018-12-22 11:26:42.199731 UTC] Updating baseline
[2018-12-22 11:26:43.469916 UTC] Computing logging information
------------------------------------
| Iteration            | 777       |
| ExpectedImprovement  | 0.018825  |
| ActualImprovement    | 0.017628  |
| ImprovementRatio     | 0.93641   |
| MeanKL               | 0.0075276 |
| Entropy              | -0.52888  |
| Perplexity           | 0.58926   |
| AveragePolicyStd     | 0.22453   |
| AveragePolicyStd[0]  | 0.24724   |
| AveragePolicyStd[1]  | 0.26491   |
| AveragePolicyStd[2]  | 0.1829    |
| AveragePolicyStd[3]  | 0.23496   |
| AveragePolicyStd[4]  | 0.16998   |
| AveragePolicyStd[5]  | 0.24721   |
| AverageReturn        | 1534.8    |
| MinReturn            | 102.49    |
| MaxReturn            | 1667.9    |
| StdReturn            | 285.75    |
| AverageEpisodeLength | 954.74    |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 171.41    |
| TotalNEpisodes       | 18516     |
| TotalNSamples        | 3.889e+06 |
| ExplainedVariance    | 0.15241   |
------------------------------------
[2018-12-22 11:26:43.853912 UTC] Saving snapshot
[2018-12-22 11:26:43.854132 UTC] Starting iteration 778
[2018-12-22 11:26:43.854282 UTC] Start collecting samples
[2018-12-22 11:26:46.815265 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:46.895995 UTC] Performing policy update
[2018-12-22 11:26:46.896745 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:46.987109 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:48.050131 UTC] Performing line search
[2018-12-22 11:26:48.176372 UTC] Updating baseline
[2018-12-22 11:26:49.536652 UTC] Computing logging information
-------------------------------------
| Iteration            | 778        |
| ExpectedImprovement  | 0.016799   |
| ActualImprovement    | 0.016023   |
| ImprovementRatio     | 0.95384    |
| MeanKL               | 0.0076567  |
| Entropy              | -0.53157   |
| Perplexity           | 0.58768    |
| AveragePolicyStd     | 0.22444    |
| AveragePolicyStd[0]  | 0.2475     |
| AveragePolicyStd[1]  | 0.26507    |
| AveragePolicyStd[2]  | 0.18285    |
| AveragePolicyStd[3]  | 0.23378    |
| AveragePolicyStd[4]  | 0.16993    |
| AveragePolicyStd[5]  | 0.24751    |
| AverageReturn        | 1521.4     |
| MinReturn            | 102.49     |
| MaxReturn            | 1667.9     |
| StdReturn            | 297.78     |
| AverageEpisodeLength | 947.03     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.18     |
| TotalNEpisodes       | 18523      |
| TotalNSamples        | 3.8944e+06 |
| ExplainedVariance    | 0.23182    |
-------------------------------------
[2018-12-22 11:26:49.916151 UTC] Saving snapshot
[2018-12-22 11:26:49.916436 UTC] Starting iteration 779
[2018-12-22 11:26:49.916573 UTC] Start collecting samples
[2018-12-22 11:26:52.818920 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:52.897191 UTC] Performing policy update
[2018-12-22 11:26:52.897805 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:52.986584 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:26:54.055569 UTC] Performing line search
[2018-12-22 11:26:54.184444 UTC] Updating baseline
[2018-12-22 11:26:55.805786 UTC] Computing logging information
-------------------------------------
| Iteration            | 779        |
| ExpectedImprovement  | 0.017031   |
| ActualImprovement    | 0.016089   |
| ImprovementRatio     | 0.94473    |
| MeanKL               | 0.0071557  |
| Entropy              | -0.5326    |
| Perplexity           | 0.58708    |
| AveragePolicyStd     | 0.22443    |
| AveragePolicyStd[0]  | 0.24809    |
| AveragePolicyStd[1]  | 0.26472    |
| AveragePolicyStd[2]  | 0.18244    |
| AveragePolicyStd[3]  | 0.23403    |
| AveragePolicyStd[4]  | 0.16979    |
| AveragePolicyStd[5]  | 0.24748    |
| AverageReturn        | 1520.7     |
| MinReturn            | 102.49     |
| MaxReturn            | 1667.9     |
| StdReturn            | 297.61     |
| AverageEpisodeLength | 947.03     |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.18     |
| TotalNEpisodes       | 18526      |
| TotalNSamples        | 3.8974e+06 |
| ExplainedVariance    | -0.015026  |
-------------------------------------
[2018-12-22 11:26:56.189776 UTC] Saving snapshot
[2018-12-22 11:26:56.190031 UTC] Starting iteration 780
[2018-12-22 11:26:56.190148 UTC] Start collecting samples
[2018-12-22 11:26:59.155545 UTC] Computing input variables for policy optimization
[2018-12-22 11:26:59.238809 UTC] Performing policy update
[2018-12-22 11:26:59.239413 UTC] Computing gradient in Euclidean space
[2018-12-22 11:26:59.329249 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:00.402927 UTC] Performing line search
[2018-12-22 11:27:00.532162 UTC] Updating baseline
[2018-12-22 11:27:01.967290 UTC] Computing logging information
-------------------------------------
| Iteration            | 780        |
| ExpectedImprovement  | 0.018273   |
| ActualImprovement    | 0.017521   |
| ImprovementRatio     | 0.95888    |
| MeanKL               | 0.0072424  |
| Entropy              | -0.53344   |
| Perplexity           | 0.58659    |
| AveragePolicyStd     | 0.2244     |
| AveragePolicyStd[0]  | 0.24688    |
| AveragePolicyStd[1]  | 0.26503    |
| AveragePolicyStd[2]  | 0.18244    |
| AveragePolicyStd[3]  | 0.23428    |
| AveragePolicyStd[4]  | 0.16967    |
| AveragePolicyStd[5]  | 0.24812    |
| AverageReturn        | 1517.8     |
| MinReturn            | 102.49     |
| MaxReturn            | 1673       |
| StdReturn            | 298.16     |
| AverageEpisodeLength | 945.5      |
| MinEpisodeLength     | 84         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.38     |
| TotalNEpisodes       | 18534      |
| TotalNSamples        | 3.9052e+06 |
| ExplainedVariance    | 0.080808   |
-------------------------------------
[2018-12-22 11:27:02.341590 UTC] Saving snapshot
[2018-12-22 11:27:02.349776 UTC] Starting iteration 781
[2018-12-22 11:27:02.349981 UTC] Start collecting samples
[2018-12-22 11:27:05.299994 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:05.380310 UTC] Performing policy update
[2018-12-22 11:27:05.381030 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:05.472962 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:06.550922 UTC] Performing line search
[2018-12-22 11:27:06.678114 UTC] Updating baseline
[2018-12-22 11:27:08.042974 UTC] Computing logging information
-------------------------------------
| Iteration            | 781        |
| ExpectedImprovement  | 0.017942   |
| ActualImprovement    | 0.016858   |
| ImprovementRatio     | 0.93957    |
| MeanKL               | 0.0072772  |
| Entropy              | -0.52944   |
| Perplexity           | 0.58894    |
| AveragePolicyStd     | 0.22455    |
| AveragePolicyStd[0]  | 0.24655    |
| AveragePolicyStd[1]  | 0.26496    |
| AveragePolicyStd[2]  | 0.18247    |
| AveragePolicyStd[3]  | 0.23502    |
| AveragePolicyStd[4]  | 0.16975    |
| AveragePolicyStd[5]  | 0.24856    |
| AverageReturn        | 1523.1     |
| MinReturn            | 147.15     |
| MaxReturn            | 1673       |
| StdReturn            | 276.38     |
| AverageEpisodeLength | 948.87     |
| MinEpisodeLength     | 110        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.73     |
| TotalNEpisodes       | 18540      |
| TotalNSamples        | 3.9106e+06 |
| ExplainedVariance    | 0.1014     |
-------------------------------------
[2018-12-22 11:27:08.427704 UTC] Saving snapshot
[2018-12-22 11:27:08.427955 UTC] Starting iteration 782
[2018-12-22 11:27:08.428088 UTC] Start collecting samples
[2018-12-22 11:27:11.355986 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:11.433373 UTC] Performing policy update
[2018-12-22 11:27:11.434195 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:11.523689 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:12.583244 UTC] Performing line search
[2018-12-22 11:27:12.710215 UTC] Updating baseline
[2018-12-22 11:27:14.244290 UTC] Computing logging information
-------------------------------------
| Iteration            | 782        |
| ExpectedImprovement  | 0.018859   |
| ActualImprovement    | 0.017157   |
| ImprovementRatio     | 0.90976    |
| MeanKL               | 0.0070346  |
| Entropy              | -0.52953   |
| Perplexity           | 0.58888    |
| AveragePolicyStd     | 0.22453    |
| AveragePolicyStd[0]  | 0.24614    |
| AveragePolicyStd[1]  | 0.26583    |
| AveragePolicyStd[2]  | 0.18257    |
| AveragePolicyStd[3]  | 0.23495    |
| AveragePolicyStd[4]  | 0.17002    |
| AveragePolicyStd[5]  | 0.24771    |
| AverageReturn        | 1507.6     |
| MinReturn            | 102.89     |
| MaxReturn            | 1673       |
| StdReturn            | 310.1      |
| AverageEpisodeLength | 939.75     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.57     |
| TotalNEpisodes       | 18543      |
| TotalNSamples        | 3.9127e+06 |
| ExplainedVariance    | 0.18262    |
-------------------------------------
[2018-12-22 11:27:14.625533 UTC] Saving snapshot
[2018-12-22 11:27:14.625824 UTC] Starting iteration 783
[2018-12-22 11:27:14.625949 UTC] Start collecting samples
[2018-12-22 11:27:17.564082 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:17.643784 UTC] Performing policy update
[2018-12-22 11:27:17.644406 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:17.733018 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:18.803742 UTC] Performing line search
[2018-12-22 11:27:18.933404 UTC] Updating baseline
[2018-12-22 11:27:20.389465 UTC] Computing logging information
-------------------------------------
| Iteration            | 783        |
| ExpectedImprovement  | 0.018593   |
| ActualImprovement    | 0.017622   |
| ImprovementRatio     | 0.94778    |
| MeanKL               | 0.0072792  |
| Entropy              | -0.53451   |
| Perplexity           | 0.58596    |
| AveragePolicyStd     | 0.22439    |
| AveragePolicyStd[0]  | 0.24675    |
| AveragePolicyStd[1]  | 0.26464    |
| AveragePolicyStd[2]  | 0.18201    |
| AveragePolicyStd[3]  | 0.23578    |
| AveragePolicyStd[4]  | 0.16937    |
| AveragePolicyStd[5]  | 0.24779    |
| AverageReturn        | 1511.2     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 311.42     |
| AverageEpisodeLength | 939.75     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.57     |
| TotalNEpisodes       | 18549      |
| TotalNSamples        | 3.9187e+06 |
| ExplainedVariance    | 0.023349   |
-------------------------------------
[2018-12-22 11:27:20.767985 UTC] Saving snapshot
[2018-12-22 11:27:20.768236 UTC] Starting iteration 784
[2018-12-22 11:27:20.768355 UTC] Start collecting samples
[2018-12-22 11:27:23.712825 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:23.792800 UTC] Performing policy update
[2018-12-22 11:27:23.793528 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:23.883511 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:24.937396 UTC] Performing line search
[2018-12-22 11:27:25.065630 UTC] Updating baseline
[2018-12-22 11:27:26.325561 UTC] Computing logging information
-------------------------------------
| Iteration            | 784        |
| ExpectedImprovement  | 0.019541   |
| ActualImprovement    | 0.01831    |
| ImprovementRatio     | 0.937      |
| MeanKL               | 0.0070804  |
| Entropy              | -0.55438   |
| Perplexity           | 0.57443    |
| AveragePolicyStd     | 0.22369    |
| AveragePolicyStd[0]  | 0.24661    |
| AveragePolicyStd[1]  | 0.2639     |
| AveragePolicyStd[2]  | 0.18138    |
| AveragePolicyStd[3]  | 0.235      |
| AveragePolicyStd[4]  | 0.16827    |
| AveragePolicyStd[5]  | 0.24699    |
| AverageReturn        | 1512.6     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 311.76     |
| AverageEpisodeLength | 939.75     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.57     |
| TotalNEpisodes       | 18555      |
| TotalNSamples        | 3.9247e+06 |
| ExplainedVariance    | 0.01805    |
-------------------------------------
[2018-12-22 11:27:26.703534 UTC] Saving snapshot
[2018-12-22 11:27:26.703796 UTC] Starting iteration 785
[2018-12-22 11:27:26.703919 UTC] Start collecting samples
[2018-12-22 11:27:29.579314 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:29.655367 UTC] Performing policy update
[2018-12-22 11:27:29.656085 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:29.747534 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:30.809927 UTC] Performing line search
[2018-12-22 11:27:30.939362 UTC] Updating baseline
[2018-12-22 11:27:32.358649 UTC] Computing logging information
-------------------------------------
| Iteration            | 785        |
| ExpectedImprovement  | 0.018623   |
| ActualImprovement    | 0.016867   |
| ImprovementRatio     | 0.90571    |
| MeanKL               | 0.007292   |
| Entropy              | -0.54824   |
| Perplexity           | 0.57796    |
| AveragePolicyStd     | 0.22394    |
| AveragePolicyStd[0]  | 0.24749    |
| AveragePolicyStd[1]  | 0.26367    |
| AveragePolicyStd[2]  | 0.1816     |
| AveragePolicyStd[3]  | 0.23517    |
| AveragePolicyStd[4]  | 0.16816    |
| AveragePolicyStd[5]  | 0.24754    |
| AverageReturn        | 1525.3     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 288.38     |
| AverageEpisodeLength | 947.33     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.9      |
| TotalNEpisodes       | 18557      |
| TotalNSamples        | 3.9267e+06 |
| ExplainedVariance    | 0.071353   |
-------------------------------------
[2018-12-22 11:27:32.737617 UTC] Saving snapshot
[2018-12-22 11:27:32.737889 UTC] Starting iteration 786
[2018-12-22 11:27:32.738023 UTC] Start collecting samples
[2018-12-22 11:27:35.673854 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:35.756250 UTC] Performing policy update
[2018-12-22 11:27:35.757009 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:35.847548 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:36.901937 UTC] Performing line search
[2018-12-22 11:27:37.028439 UTC] Updating baseline
[2018-12-22 11:27:38.557685 UTC] Computing logging information
-------------------------------------
| Iteration            | 786        |
| ExpectedImprovement  | 0.01864    |
| ActualImprovement    | 0.017236   |
| ImprovementRatio     | 0.92472    |
| MeanKL               | 0.0070959  |
| Entropy              | -0.54927   |
| Perplexity           | 0.57737    |
| AveragePolicyStd     | 0.22388    |
| AveragePolicyStd[0]  | 0.24641    |
| AveragePolicyStd[1]  | 0.26365    |
| AveragePolicyStd[2]  | 0.18181    |
| AveragePolicyStd[3]  | 0.23509    |
| AveragePolicyStd[4]  | 0.16818    |
| AveragePolicyStd[5]  | 0.24816    |
| AverageReturn        | 1536.6     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 272        |
| AverageEpisodeLength | 953.48     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.41     |
| TotalNEpisodes       | 18562      |
| TotalNSamples        | 3.9317e+06 |
| ExplainedVariance    | 0.0070371  |
-------------------------------------
[2018-12-22 11:27:38.966419 UTC] Saving snapshot
[2018-12-22 11:27:38.966734 UTC] Starting iteration 787
[2018-12-22 11:27:38.966867 UTC] Start collecting samples
[2018-12-22 11:27:42.141332 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:42.225692 UTC] Performing policy update
[2018-12-22 11:27:42.226431 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:42.320580 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:43.432917 UTC] Performing line search
[2018-12-22 11:27:43.567888 UTC] Updating baseline
[2018-12-22 11:27:44.994566 UTC] Computing logging information
-------------------------------------
| Iteration            | 787        |
| ExpectedImprovement  | 0.017951   |
| ActualImprovement    | 0.016845   |
| ImprovementRatio     | 0.93842    |
| MeanKL               | 0.00752    |
| Entropy              | -0.55113   |
| Perplexity           | 0.5763     |
| AveragePolicyStd     | 0.22385    |
| AveragePolicyStd[0]  | 0.24579    |
| AveragePolicyStd[1]  | 0.26386    |
| AveragePolicyStd[2]  | 0.18159    |
| AveragePolicyStd[3]  | 0.23589    |
| AveragePolicyStd[4]  | 0.16768    |
| AveragePolicyStd[5]  | 0.24831    |
| AverageReturn        | 1538.7     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 272.5      |
| AverageEpisodeLength | 953.48     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.41     |
| TotalNEpisodes       | 18569      |
| TotalNSamples        | 3.9387e+06 |
| ExplainedVariance    | 0.0012073  |
-------------------------------------
[2018-12-22 11:27:45.396625 UTC] Saving snapshot
[2018-12-22 11:27:45.396881 UTC] Starting iteration 788
[2018-12-22 11:27:45.397009 UTC] Start collecting samples
[2018-12-22 11:27:48.529406 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:48.610190 UTC] Performing policy update
[2018-12-22 11:27:48.610807 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:48.703546 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:49.758487 UTC] Performing line search
[2018-12-22 11:27:49.887058 UTC] Updating baseline
[2018-12-22 11:27:51.840635 UTC] Computing logging information
-------------------------------------
| Iteration            | 788        |
| ExpectedImprovement  | 0.01752    |
| ActualImprovement    | 0.016252   |
| ImprovementRatio     | 0.92762    |
| MeanKL               | 0.0073638  |
| Entropy              | -0.55291   |
| Perplexity           | 0.57527    |
| AveragePolicyStd     | 0.22379    |
| AveragePolicyStd[0]  | 0.24509    |
| AveragePolicyStd[1]  | 0.26364    |
| AveragePolicyStd[2]  | 0.18174    |
| AveragePolicyStd[3]  | 0.23622    |
| AveragePolicyStd[4]  | 0.16736    |
| AveragePolicyStd[5]  | 0.24872    |
| AverageReturn        | 1549.2     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 257.66     |
| AverageEpisodeLength | 959.3      |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.28     |
| TotalNEpisodes       | 18573      |
| TotalNSamples        | 3.9427e+06 |
| ExplainedVariance    | 0.0097669  |
-------------------------------------
[2018-12-22 11:27:52.217149 UTC] Saving snapshot
[2018-12-22 11:27:52.217396 UTC] Starting iteration 789
[2018-12-22 11:27:52.217534 UTC] Start collecting samples
[2018-12-22 11:27:55.112824 UTC] Computing input variables for policy optimization
[2018-12-22 11:27:55.188685 UTC] Performing policy update
[2018-12-22 11:27:55.189321 UTC] Computing gradient in Euclidean space
[2018-12-22 11:27:55.278315 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:27:56.333051 UTC] Performing line search
[2018-12-22 11:27:56.459918 UTC] Updating baseline
[2018-12-22 11:27:57.888754 UTC] Computing logging information
-------------------------------------
| Iteration            | 789        |
| ExpectedImprovement  | 0.018582   |
| ActualImprovement    | 0.016642   |
| ImprovementRatio     | 0.89561    |
| MeanKL               | 0.0071812  |
| Entropy              | -0.55215   |
| Perplexity           | 0.57571    |
| AveragePolicyStd     | 0.22381    |
| AveragePolicyStd[0]  | 0.24435    |
| AveragePolicyStd[1]  | 0.26375    |
| AveragePolicyStd[2]  | 0.18207    |
| AveragePolicyStd[3]  | 0.23649    |
| AveragePolicyStd[4]  | 0.16729    |
| AveragePolicyStd[5]  | 0.24892    |
| AverageReturn        | 1549.8     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 257.82     |
| AverageEpisodeLength | 959.3      |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.28     |
| TotalNEpisodes       | 18576      |
| TotalNSamples        | 3.9457e+06 |
| ExplainedVariance    | -0.016826  |
-------------------------------------
[2018-12-22 11:27:58.274582 UTC] Saving snapshot
[2018-12-22 11:27:58.274845 UTC] Starting iteration 790
[2018-12-22 11:27:58.274965 UTC] Start collecting samples
[2018-12-22 11:28:01.239936 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:01.321337 UTC] Performing policy update
[2018-12-22 11:28:01.322000 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:01.412869 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:02.483817 UTC] Performing line search
[2018-12-22 11:28:02.611944 UTC] Updating baseline
[2018-12-22 11:28:04.434370 UTC] Computing logging information
-------------------------------------
| Iteration            | 790        |
| ExpectedImprovement  | 0.017843   |
| ActualImprovement    | 0.017112   |
| ImprovementRatio     | 0.95903    |
| MeanKL               | 0.0076676  |
| Entropy              | -0.55737   |
| Perplexity           | 0.57271    |
| AveragePolicyStd     | 0.22362    |
| AveragePolicyStd[0]  | 0.24391    |
| AveragePolicyStd[1]  | 0.26343    |
| AveragePolicyStd[2]  | 0.18199    |
| AveragePolicyStd[3]  | 0.23664    |
| AveragePolicyStd[4]  | 0.16695    |
| AveragePolicyStd[5]  | 0.24884    |
| AverageReturn        | 1551.5     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 258.29     |
| AverageEpisodeLength | 959.3      |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.28     |
| TotalNEpisodes       | 18584      |
| TotalNSamples        | 3.9537e+06 |
| ExplainedVariance    | -0.0033169 |
-------------------------------------
[2018-12-22 11:28:04.847842 UTC] Saving snapshot
[2018-12-22 11:28:04.857142 UTC] Starting iteration 791
[2018-12-22 11:28:04.857407 UTC] Start collecting samples
[2018-12-22 11:28:08.062132 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:08.146774 UTC] Performing policy update
[2018-12-22 11:28:08.147451 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:08.241365 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:09.310467 UTC] Performing line search
[2018-12-22 11:28:09.439038 UTC] Updating baseline
[2018-12-22 11:28:10.893781 UTC] Computing logging information
-------------------------------------
| Iteration            | 791        |
| ExpectedImprovement  | 0.019056   |
| ActualImprovement    | 0.017905   |
| ImprovementRatio     | 0.93963    |
| MeanKL               | 0.0069391  |
| Entropy              | -0.55982   |
| Perplexity           | 0.57131    |
| AveragePolicyStd     | 0.22349    |
| AveragePolicyStd[0]  | 0.24327    |
| AveragePolicyStd[1]  | 0.26311    |
| AveragePolicyStd[2]  | 0.1818     |
| AveragePolicyStd[3]  | 0.23595    |
| AveragePolicyStd[4]  | 0.16752    |
| AveragePolicyStd[5]  | 0.2493     |
| AverageReturn        | 1539.9     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 277.7      |
| AverageEpisodeLength | 952.95     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.23     |
| TotalNEpisodes       | 18590      |
| TotalNSamples        | 3.9591e+06 |
| ExplainedVariance    | 0.069086   |
-------------------------------------
[2018-12-22 11:28:11.278127 UTC] Saving snapshot
[2018-12-22 11:28:11.278394 UTC] Starting iteration 792
[2018-12-22 11:28:11.278528 UTC] Start collecting samples
[2018-12-22 11:28:14.227395 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:14.306370 UTC] Performing policy update
[2018-12-22 11:28:14.306957 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:14.396251 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:15.449913 UTC] Performing line search
[2018-12-22 11:28:15.577814 UTC] Updating baseline
[2018-12-22 11:28:16.963566 UTC] Computing logging information
------------------------------------
| Iteration            | 792       |
| ExpectedImprovement  | 0.01557   |
| ActualImprovement    | 0.014878  |
| ImprovementRatio     | 0.95555   |
| MeanKL               | 0.0077925 |
| Entropy              | -0.5585   |
| Perplexity           | 0.57207   |
| AveragePolicyStd     | 0.22359   |
| AveragePolicyStd[0]  | 0.24369   |
| AveragePolicyStd[1]  | 0.26375   |
| AveragePolicyStd[2]  | 0.18211   |
| AveragePolicyStd[3]  | 0.2348    |
| AveragePolicyStd[4]  | 0.16703   |
| AveragePolicyStd[5]  | 0.25014   |
| AverageReturn        | 1545.6    |
| MinReturn            | 102.89    |
| MaxReturn            | 1696.5    |
| StdReturn            | 274.56    |
| AverageEpisodeLength | 955.54    |
| MinEpisodeLength     | 88        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.07    |
| TotalNEpisodes       | 18593     |
| TotalNSamples        | 3.962e+06 |
| ExplainedVariance    | 0.17514   |
------------------------------------
[2018-12-22 11:28:17.340148 UTC] Saving snapshot
[2018-12-22 11:28:17.340693 UTC] Starting iteration 793
[2018-12-22 11:28:17.340883 UTC] Start collecting samples
[2018-12-22 11:28:20.291515 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:20.371922 UTC] Performing policy update
[2018-12-22 11:28:20.372635 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:20.462545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:21.534876 UTC] Performing line search
[2018-12-22 11:28:21.663410 UTC] Updating baseline
[2018-12-22 11:28:23.204518 UTC] Computing logging information
-------------------------------------
| Iteration            | 793        |
| ExpectedImprovement  | 0.018814   |
| ActualImprovement    | 0.017221   |
| ImprovementRatio     | 0.91531    |
| MeanKL               | 0.0073946  |
| Entropy              | -0.57102   |
| Perplexity           | 0.56495    |
| AveragePolicyStd     | 0.22308    |
| AveragePolicyStd[0]  | 0.243      |
| AveragePolicyStd[1]  | 0.26232    |
| AveragePolicyStd[2]  | 0.18196    |
| AveragePolicyStd[3]  | 0.23377    |
| AveragePolicyStd[4]  | 0.16699    |
| AveragePolicyStd[5]  | 0.25042    |
| AverageReturn        | 1534.5     |
| MinReturn            | 102.89     |
| MaxReturn            | 1696.5     |
| StdReturn            | 308.09     |
| AverageEpisodeLength | 946.85     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.58     |
| TotalNEpisodes       | 18600      |
| TotalNSamples        | 3.9682e+06 |
| ExplainedVariance    | 0.085119   |
-------------------------------------
[2018-12-22 11:28:23.586203 UTC] Saving snapshot
[2018-12-22 11:28:23.586453 UTC] Starting iteration 794
[2018-12-22 11:28:23.586590 UTC] Start collecting samples
[2018-12-22 11:28:26.556341 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:26.637756 UTC] Performing policy update
[2018-12-22 11:28:26.638341 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:26.728593 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:27.794560 UTC] Performing line search
[2018-12-22 11:28:27.925968 UTC] Updating baseline
[2018-12-22 11:28:29.483682 UTC] Computing logging information
-------------------------------------
| Iteration            | 794        |
| ExpectedImprovement  | 0.01846    |
| ActualImprovement    | 0.017396   |
| ImprovementRatio     | 0.94238    |
| MeanKL               | 0.0074078  |
| Entropy              | -0.56963   |
| Perplexity           | 0.56574    |
| AveragePolicyStd     | 0.22307    |
| AveragePolicyStd[0]  | 0.24317    |
| AveragePolicyStd[1]  | 0.2618     |
| AveragePolicyStd[2]  | 0.1822     |
| AveragePolicyStd[3]  | 0.23444    |
| AveragePolicyStd[4]  | 0.16744    |
| AveragePolicyStd[5]  | 0.24939    |
| AverageReturn        | 1537.9     |
| MinReturn            | 102.89     |
| MaxReturn            | 1715.8     |
| StdReturn            | 309.2      |
| AverageEpisodeLength | 946.85     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.58     |
| TotalNEpisodes       | 18607      |
| TotalNSamples        | 3.9752e+06 |
| ExplainedVariance    | -0.0081567 |
-------------------------------------
[2018-12-22 11:28:29.866136 UTC] Saving snapshot
[2018-12-22 11:28:29.866397 UTC] Starting iteration 795
[2018-12-22 11:28:29.866531 UTC] Start collecting samples
[2018-12-22 11:28:32.776523 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:32.854433 UTC] Performing policy update
[2018-12-22 11:28:32.855154 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:32.946550 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:34.025384 UTC] Performing line search
[2018-12-22 11:28:34.152730 UTC] Updating baseline
[2018-12-22 11:28:35.423409 UTC] Computing logging information
-------------------------------------
| Iteration            | 795        |
| ExpectedImprovement  | 0.018877   |
| ActualImprovement    | 0.017416   |
| ImprovementRatio     | 0.92259    |
| MeanKL               | 0.0069145  |
| Entropy              | -0.57912   |
| Perplexity           | 0.56039    |
| AveragePolicyStd     | 0.22276    |
| AveragePolicyStd[0]  | 0.24316    |
| AveragePolicyStd[1]  | 0.26068    |
| AveragePolicyStd[2]  | 0.18191    |
| AveragePolicyStd[3]  | 0.23476    |
| AveragePolicyStd[4]  | 0.16648    |
| AveragePolicyStd[5]  | 0.24959    |
| AverageReturn        | 1542.5     |
| MinReturn            | 102.89     |
| MaxReturn            | 1715.8     |
| StdReturn            | 309.25     |
| AverageEpisodeLength | 948.53     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.29     |
| TotalNEpisodes       | 18610      |
| TotalNSamples        | 3.9782e+06 |
| ExplainedVariance    | -0.046871  |
-------------------------------------
[2018-12-22 11:28:35.810927 UTC] Saving snapshot
[2018-12-22 11:28:35.811204 UTC] Starting iteration 796
[2018-12-22 11:28:35.811325 UTC] Start collecting samples
[2018-12-22 11:28:38.770989 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:38.851584 UTC] Performing policy update
[2018-12-22 11:28:38.852370 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:38.941981 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:40.012174 UTC] Performing line search
[2018-12-22 11:28:40.139699 UTC] Updating baseline
[2018-12-22 11:28:41.771820 UTC] Computing logging information
-------------------------------------
| Iteration            | 796        |
| ExpectedImprovement  | 0.016833   |
| ActualImprovement    | 0.015942   |
| ImprovementRatio     | 0.94707    |
| MeanKL               | 0.0070139  |
| Entropy              | -0.57883   |
| Perplexity           | 0.56056    |
| AveragePolicyStd     | 0.22277    |
| AveragePolicyStd[0]  | 0.24383    |
| AveragePolicyStd[1]  | 0.26008    |
| AveragePolicyStd[2]  | 0.18183    |
| AveragePolicyStd[3]  | 0.2347     |
| AveragePolicyStd[4]  | 0.16648    |
| AveragePolicyStd[5]  | 0.24973    |
| AverageReturn        | 1541.5     |
| MinReturn            | 102.89     |
| MaxReturn            | 1722.3     |
| StdReturn            | 330.7      |
| AverageEpisodeLength | 946.18     |
| MinEpisodeLength     | 88         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.07     |
| TotalNEpisodes       | 18617      |
| TotalNSamples        | 3.9844e+06 |
| ExplainedVariance    | 0.081279   |
-------------------------------------
[2018-12-22 11:28:42.153975 UTC] Saving snapshot
[2018-12-22 11:28:42.154412 UTC] Starting iteration 797
[2018-12-22 11:28:42.154746 UTC] Start collecting samples
[2018-12-22 11:28:45.115686 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:45.195109 UTC] Performing policy update
[2018-12-22 11:28:45.195881 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:45.287157 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:46.361239 UTC] Performing line search
[2018-12-22 11:28:46.488667 UTC] Updating baseline
[2018-12-22 11:28:47.845033 UTC] Computing logging information
--------------------------------------
| Iteration            | 797         |
| ExpectedImprovement  | 0.016255    |
| ActualImprovement    | 0.015617    |
| ImprovementRatio     | 0.96074     |
| MeanKL               | 0.0081532   |
| Entropy              | -0.58452    |
| Perplexity           | 0.55737     |
| AveragePolicyStd     | 0.22255     |
| AveragePolicyStd[0]  | 0.24284     |
| AveragePolicyStd[1]  | 0.25928     |
| AveragePolicyStd[2]  | 0.18179     |
| AveragePolicyStd[3]  | 0.2344      |
| AveragePolicyStd[4]  | 0.16635     |
| AveragePolicyStd[5]  | 0.25065     |
| AverageReturn        | 1569        |
| MinReturn            | 102.89      |
| MaxReturn            | 1722.3      |
| StdReturn            | 289.63      |
| AverageEpisodeLength | 960.14      |
| MinEpisodeLength     | 88          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 167.64      |
| TotalNEpisodes       | 18624       |
| TotalNSamples        | 3.9914e+06  |
| ExplainedVariance    | -0.00030765 |
--------------------------------------
[2018-12-22 11:28:48.232385 UTC] Saving snapshot
[2018-12-22 11:28:48.232651 UTC] Starting iteration 798
[2018-12-22 11:28:48.232776 UTC] Start collecting samples
[2018-12-22 11:28:51.159636 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:51.238165 UTC] Performing policy update
[2018-12-22 11:28:51.238763 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:51.329770 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:52.395411 UTC] Performing line search
[2018-12-22 11:28:52.521762 UTC] Updating baseline
[2018-12-22 11:28:53.802113 UTC] Computing logging information
------------------------------------
| Iteration            | 798       |
| ExpectedImprovement  | 0.019069  |
| ActualImprovement    | 0.018186  |
| ImprovementRatio     | 0.95369   |
| MeanKL               | 0.007517  |
| Entropy              | -0.58315  |
| Perplexity           | 0.55813   |
| AveragePolicyStd     | 0.2226    |
| AveragePolicyStd[0]  | 0.2429    |
| AveragePolicyStd[1]  | 0.2595    |
| AveragePolicyStd[2]  | 0.18165   |
| AveragePolicyStd[3]  | 0.2339    |
| AveragePolicyStd[4]  | 0.16668   |
| AveragePolicyStd[5]  | 0.25096   |
| AverageReturn        | 1547.6    |
| MinReturn            | 102.89    |
| MaxReturn            | 1722.3    |
| StdReturn            | 326.89    |
| AverageEpisodeLength | 946.77    |
| MinEpisodeLength     | 88        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 190.06    |
| TotalNEpisodes       | 18628     |
| TotalNSamples        | 3.994e+06 |
| ExplainedVariance    | 0.2614    |
------------------------------------
[2018-12-22 11:28:54.194912 UTC] Saving snapshot
[2018-12-22 11:28:54.195156 UTC] Starting iteration 799
[2018-12-22 11:28:54.195291 UTC] Start collecting samples
[2018-12-22 11:28:57.160294 UTC] Computing input variables for policy optimization
[2018-12-22 11:28:57.239304 UTC] Performing policy update
[2018-12-22 11:28:57.239909 UTC] Computing gradient in Euclidean space
[2018-12-22 11:28:57.331242 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:28:58.407083 UTC] Performing line search
[2018-12-22 11:28:58.535417 UTC] Updating baseline
[2018-12-22 11:29:00.159811 UTC] Computing logging information
------------------------------------
| Iteration            | 799       |
| ExpectedImprovement  | 0.0157    |
| ActualImprovement    | 0.015445  |
| ImprovementRatio     | 0.98375   |
| MeanKL               | 0.0074548 |
| Entropy              | -0.57759  |
| Perplexity           | 0.56125   |
| AveragePolicyStd     | 0.22279   |
| AveragePolicyStd[0]  | 0.24337   |
| AveragePolicyStd[1]  | 0.25917   |
| AveragePolicyStd[2]  | 0.18163   |
| AveragePolicyStd[3]  | 0.23396   |
| AveragePolicyStd[4]  | 0.1671    |
| AveragePolicyStd[5]  | 0.25152   |
| AverageReturn        | 1554.3    |
| MinReturn            | 102.89    |
| MaxReturn            | 1722.3    |
| StdReturn            | 327       |
| AverageEpisodeLength | 948.3     |
| MinEpisodeLength     | 88        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 189.87    |
| TotalNEpisodes       | 18633     |
| TotalNSamples        | 3.999e+06 |
| ExplainedVariance    | -0.040615 |
------------------------------------
[2018-12-22 11:29:00.544514 UTC] Saving snapshot
[2018-12-22 11:29:00.544757 UTC] Starting iteration 800
[2018-12-22 11:29:00.544875 UTC] Start collecting samples
[2018-12-22 11:29:03.512881 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:03.590209 UTC] Performing policy update
[2018-12-22 11:29:03.590803 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:03.680948 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:04.750779 UTC] Performing line search
[2018-12-22 11:29:04.879914 UTC] Updating baseline
[2018-12-22 11:29:06.215794 UTC] Computing logging information
------------------------------------
| Iteration            | 800       |
| ExpectedImprovement  | 0.019286  |
| ActualImprovement    | 0.018044  |
| ImprovementRatio     | 0.93562   |
| MeanKL               | 0.0069601 |
| Entropy              | -0.57418  |
| Perplexity           | 0.56317   |
| AveragePolicyStd     | 0.22289   |
| AveragePolicyStd[0]  | 0.24431   |
| AveragePolicyStd[1]  | 0.25878   |
| AveragePolicyStd[2]  | 0.18167   |
| AveragePolicyStd[3]  | 0.23394   |
| AveragePolicyStd[4]  | 0.16747   |
| AveragePolicyStd[5]  | 0.25121   |
| AverageReturn        | 1565.6    |
| MinReturn            | 102.89    |
| MaxReturn            | 1722.3    |
| StdReturn            | 314.39    |
| AverageEpisodeLength | 954.09    |
| MinEpisodeLength     | 88        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 182.38    |
| TotalNEpisodes       | 18638     |
| TotalNSamples        | 4.004e+06 |
| ExplainedVariance    | 0.015626  |
------------------------------------
[2018-12-22 11:29:06.598133 UTC] Saving snapshot
[2018-12-22 11:29:06.606382 UTC] Starting iteration 801
[2018-12-22 11:29:06.606606 UTC] Start collecting samples
[2018-12-22 11:29:09.559870 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:09.638002 UTC] Performing policy update
[2018-12-22 11:29:09.638593 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:09.728965 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:10.792534 UTC] Performing line search
[2018-12-22 11:29:10.920144 UTC] Updating baseline
[2018-12-22 11:29:12.447233 UTC] Computing logging information
-------------------------------------
| Iteration            | 801        |
| ExpectedImprovement  | 0.017439   |
| ActualImprovement    | 0.016222   |
| ImprovementRatio     | 0.93024    |
| MeanKL               | 0.00752    |
| Entropy              | -0.57702   |
| Perplexity           | 0.56157    |
| AveragePolicyStd     | 0.22278    |
| AveragePolicyStd[0]  | 0.24477    |
| AveragePolicyStd[1]  | 0.25782    |
| AveragePolicyStd[2]  | 0.18116    |
| AveragePolicyStd[3]  | 0.23375    |
| AveragePolicyStd[4]  | 0.16778    |
| AveragePolicyStd[5]  | 0.25139    |
| AverageReturn        | 1567.9     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 314.16     |
| AverageEpisodeLength | 954.37     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.06     |
| TotalNEpisodes       | 18643      |
| TotalNSamples        | 4.0081e+06 |
| ExplainedVariance    | 0.22057    |
-------------------------------------
[2018-12-22 11:29:12.827551 UTC] Saving snapshot
[2018-12-22 11:29:12.827819 UTC] Starting iteration 802
[2018-12-22 11:29:12.827958 UTC] Start collecting samples
[2018-12-22 11:29:15.753473 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:15.833350 UTC] Performing policy update
[2018-12-22 11:29:15.833996 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:15.928348 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:16.974918 UTC] Performing line search
[2018-12-22 11:29:17.101817 UTC] Updating baseline
[2018-12-22 11:29:18.622201 UTC] Computing logging information
-------------------------------------
| Iteration            | 802        |
| ExpectedImprovement  | 0.018621   |
| ActualImprovement    | 0.0172     |
| ImprovementRatio     | 0.9237     |
| MeanKL               | 0.0073777  |
| Entropy              | -0.57375   |
| Perplexity           | 0.56341    |
| AveragePolicyStd     | 0.2229     |
| AveragePolicyStd[0]  | 0.2443     |
| AveragePolicyStd[1]  | 0.25805    |
| AveragePolicyStd[2]  | 0.18164    |
| AveragePolicyStd[3]  | 0.23349    |
| AveragePolicyStd[4]  | 0.16769    |
| AveragePolicyStd[5]  | 0.25221    |
| AverageReturn        | 1554.1     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 335.68     |
| AverageEpisodeLength | 946.99     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.66     |
| TotalNEpisodes       | 18647      |
| TotalNSamples        | 4.0114e+06 |
| ExplainedVariance    | 0.081134   |
-------------------------------------
[2018-12-22 11:29:18.998703 UTC] Saving snapshot
[2018-12-22 11:29:18.998961 UTC] Starting iteration 803
[2018-12-22 11:29:18.999081 UTC] Start collecting samples
[2018-12-22 11:29:21.956646 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:22.037755 UTC] Performing policy update
[2018-12-22 11:29:22.038473 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:22.128543 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:23.188475 UTC] Performing line search
[2018-12-22 11:29:23.317338 UTC] Updating baseline
[2018-12-22 11:29:24.758662 UTC] Computing logging information
-------------------------------------
| Iteration            | 803        |
| ExpectedImprovement  | 0.015847   |
| ActualImprovement    | 0.015209   |
| ImprovementRatio     | 0.9597     |
| MeanKL               | 0.0076925  |
| Entropy              | -0.5743    |
| Perplexity           | 0.5631     |
| AveragePolicyStd     | 0.22291    |
| AveragePolicyStd[0]  | 0.24462    |
| AveragePolicyStd[1]  | 0.25812    |
| AveragePolicyStd[2]  | 0.18133    |
| AveragePolicyStd[3]  | 0.23332    |
| AveragePolicyStd[4]  | 0.16758    |
| AveragePolicyStd[5]  | 0.25247    |
| AverageReturn        | 1555.3     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 335.94     |
| AverageEpisodeLength | 946.99     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.66     |
| TotalNEpisodes       | 18653      |
| TotalNSamples        | 4.0174e+06 |
| ExplainedVariance    | -0.012313  |
-------------------------------------
[2018-12-22 11:29:25.143324 UTC] Saving snapshot
[2018-12-22 11:29:25.143590 UTC] Starting iteration 804
[2018-12-22 11:29:25.143712 UTC] Start collecting samples
[2018-12-22 11:29:28.121003 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:28.201889 UTC] Performing policy update
[2018-12-22 11:29:28.202600 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:28.293478 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:29.356795 UTC] Performing line search
[2018-12-22 11:29:29.485574 UTC] Updating baseline
[2018-12-22 11:29:30.834380 UTC] Computing logging information
-------------------------------------
| Iteration            | 804        |
| ExpectedImprovement  | 0.019346   |
| ActualImprovement    | 0.018125   |
| ImprovementRatio     | 0.93689    |
| MeanKL               | 0.0073799  |
| Entropy              | -0.56361   |
| Perplexity           | 0.56915    |
| AveragePolicyStd     | 0.22331    |
| AveragePolicyStd[0]  | 0.24535    |
| AveragePolicyStd[1]  | 0.25836    |
| AveragePolicyStd[2]  | 0.18192    |
| AveragePolicyStd[3]  | 0.23395    |
| AveragePolicyStd[4]  | 0.16751    |
| AveragePolicyStd[5]  | 0.25279    |
| AverageReturn        | 1555.9     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 338.6      |
| AverageEpisodeLength | 944.5      |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.56     |
| TotalNEpisodes       | 18659      |
| TotalNSamples        | 4.0232e+06 |
| ExplainedVariance    | 0.13806    |
-------------------------------------
[2018-12-22 11:29:31.213886 UTC] Saving snapshot
[2018-12-22 11:29:31.214135 UTC] Starting iteration 805
[2018-12-22 11:29:31.214254 UTC] Start collecting samples
[2018-12-22 11:29:34.162885 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:34.242877 UTC] Performing policy update
[2018-12-22 11:29:34.243825 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:34.333941 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:35.392785 UTC] Performing line search
[2018-12-22 11:29:35.521181 UTC] Updating baseline
[2018-12-22 11:29:36.714735 UTC] Computing logging information
-------------------------------------
| Iteration            | 805        |
| ExpectedImprovement  | 0.017881   |
| ActualImprovement    | 0.017196   |
| ImprovementRatio     | 0.96171    |
| MeanKL               | 0.0078621  |
| Entropy              | -0.56463   |
| Perplexity           | 0.56857    |
| AveragePolicyStd     | 0.22324    |
| AveragePolicyStd[0]  | 0.24568    |
| AveragePolicyStd[1]  | 0.25673    |
| AveragePolicyStd[2]  | 0.18226    |
| AveragePolicyStd[3]  | 0.23442    |
| AveragePolicyStd[4]  | 0.16748    |
| AveragePolicyStd[5]  | 0.25285    |
| AverageReturn        | 1552.3     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 340.45     |
| AverageEpisodeLength | 941.86     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.57     |
| TotalNEpisodes       | 18663      |
| TotalNSamples        | 4.0269e+06 |
| ExplainedVariance    | 0.19422    |
-------------------------------------
[2018-12-22 11:29:37.095206 UTC] Saving snapshot
[2018-12-22 11:29:37.095447 UTC] Starting iteration 806
[2018-12-22 11:29:37.095590 UTC] Start collecting samples
[2018-12-22 11:29:40.055961 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:40.136896 UTC] Performing policy update
[2018-12-22 11:29:40.137705 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:40.227849 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:41.279847 UTC] Performing line search
[2018-12-22 11:29:41.406198 UTC] Updating baseline
[2018-12-22 11:29:42.745669 UTC] Computing logging information
-------------------------------------
| Iteration            | 806        |
| ExpectedImprovement  | 0.018802   |
| ActualImprovement    | 0.017989   |
| ImprovementRatio     | 0.95674    |
| MeanKL               | 0.0071202  |
| Entropy              | -0.57299   |
| Perplexity           | 0.56384    |
| AveragePolicyStd     | 0.22294    |
| AveragePolicyStd[0]  | 0.24544    |
| AveragePolicyStd[1]  | 0.25731    |
| AveragePolicyStd[2]  | 0.1816     |
| AveragePolicyStd[3]  | 0.23328    |
| AveragePolicyStd[4]  | 0.16754    |
| AveragePolicyStd[5]  | 0.25248    |
| AverageReturn        | 1543       |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 360.89     |
| AverageEpisodeLength | 934.38     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.17     |
| TotalNEpisodes       | 18670      |
| TotalNSamples        | 4.0331e+06 |
| ExplainedVariance    | 0.15783    |
-------------------------------------
[2018-12-22 11:29:43.120146 UTC] Saving snapshot
[2018-12-22 11:29:43.120396 UTC] Starting iteration 807
[2018-12-22 11:29:43.120530 UTC] Start collecting samples
[2018-12-22 11:29:46.082818 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:46.163378 UTC] Performing policy update
[2018-12-22 11:29:46.163993 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:46.255633 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:47.321618 UTC] Performing line search
[2018-12-22 11:29:47.450369 UTC] Updating baseline
[2018-12-22 11:29:48.826978 UTC] Computing logging information
-------------------------------------
| Iteration            | 807        |
| ExpectedImprovement  | 0.016181   |
| ActualImprovement    | 0.015573   |
| ImprovementRatio     | 0.96242    |
| MeanKL               | 0.0076351  |
| Entropy              | -0.56589   |
| Perplexity           | 0.56785    |
| AveragePolicyStd     | 0.2232     |
| AveragePolicyStd[0]  | 0.24613    |
| AveragePolicyStd[1]  | 0.2579     |
| AveragePolicyStd[2]  | 0.18179    |
| AveragePolicyStd[3]  | 0.23322    |
| AveragePolicyStd[4]  | 0.16781    |
| AveragePolicyStd[5]  | 0.25237    |
| AverageReturn        | 1546.3     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 362.01     |
| AverageEpisodeLength | 934.38     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.17     |
| TotalNEpisodes       | 18676      |
| TotalNSamples        | 4.0391e+06 |
| ExplainedVariance    | 0.012174   |
-------------------------------------
[2018-12-22 11:29:49.211241 UTC] Saving snapshot
[2018-12-22 11:29:49.211508 UTC] Starting iteration 808
[2018-12-22 11:29:49.211636 UTC] Start collecting samples
[2018-12-22 11:29:52.167099 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:52.247418 UTC] Performing policy update
[2018-12-22 11:29:52.248257 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:52.339192 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:53.401700 UTC] Performing line search
[2018-12-22 11:29:53.531805 UTC] Updating baseline
[2018-12-22 11:29:54.911576 UTC] Computing logging information
-------------------------------------
| Iteration            | 808        |
| ExpectedImprovement  | 0.019029   |
| ActualImprovement    | 0.017918   |
| ImprovementRatio     | 0.94159    |
| MeanKL               | 0.0081593  |
| Entropy              | -0.55357   |
| Perplexity           | 0.57489    |
| AveragePolicyStd     | 0.2237     |
| AveragePolicyStd[0]  | 0.24743    |
| AveragePolicyStd[1]  | 0.25779    |
| AveragePolicyStd[2]  | 0.18216    |
| AveragePolicyStd[3]  | 0.23343    |
| AveragePolicyStd[4]  | 0.16769    |
| AveragePolicyStd[5]  | 0.2537     |
| AverageReturn        | 1538.5     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 370.79     |
| AverageEpisodeLength | 929.13     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.03     |
| TotalNEpisodes       | 18680      |
| TotalNSamples        | 4.0426e+06 |
| ExplainedVariance    | 0.028488   |
-------------------------------------
[2018-12-22 11:29:55.295282 UTC] Saving snapshot
[2018-12-22 11:29:55.295554 UTC] Starting iteration 809
[2018-12-22 11:29:55.295683 UTC] Start collecting samples
[2018-12-22 11:29:58.283800 UTC] Computing input variables for policy optimization
[2018-12-22 11:29:58.364206 UTC] Performing policy update
[2018-12-22 11:29:58.365177 UTC] Computing gradient in Euclidean space
[2018-12-22 11:29:58.454636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:29:59.511606 UTC] Performing line search
[2018-12-22 11:29:59.640295 UTC] Updating baseline
[2018-12-22 11:30:00.817605 UTC] Computing logging information
-------------------------------------
| Iteration            | 809        |
| ExpectedImprovement  | 0.015965   |
| ActualImprovement    | 0.015139   |
| ImprovementRatio     | 0.94822    |
| MeanKL               | 0.0072018  |
| Entropy              | -0.554     |
| Perplexity           | 0.57465    |
| AveragePolicyStd     | 0.22369    |
| AveragePolicyStd[0]  | 0.24756    |
| AveragePolicyStd[1]  | 0.25737    |
| AveragePolicyStd[2]  | 0.18252    |
| AveragePolicyStd[3]  | 0.23383    |
| AveragePolicyStd[4]  | 0.16724    |
| AveragePolicyStd[5]  | 0.25362    |
| AverageReturn        | 1539.5     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 371.15     |
| AverageEpisodeLength | 929.13     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.03     |
| TotalNEpisodes       | 18686      |
| TotalNSamples        | 4.0486e+06 |
| ExplainedVariance    | 0.0033423  |
-------------------------------------
[2018-12-22 11:30:01.201239 UTC] Saving snapshot
[2018-12-22 11:30:01.201487 UTC] Starting iteration 810
[2018-12-22 11:30:01.201629 UTC] Start collecting samples
[2018-12-22 11:30:04.181659 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:04.260947 UTC] Performing policy update
[2018-12-22 11:30:04.261893 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:04.352529 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:05.416749 UTC] Performing line search
[2018-12-22 11:30:05.545006 UTC] Updating baseline
[2018-12-22 11:30:07.270201 UTC] Computing logging information
-------------------------------------
| Iteration            | 810        |
| ExpectedImprovement  | 0.016848   |
| ActualImprovement    | 0.016119   |
| ImprovementRatio     | 0.95676    |
| MeanKL               | 0.0079532  |
| Entropy              | -0.55963   |
| Perplexity           | 0.57142    |
| AveragePolicyStd     | 0.22352    |
| AveragePolicyStd[0]  | 0.24791    |
| AveragePolicyStd[1]  | 0.25742    |
| AveragePolicyStd[2]  | 0.18231    |
| AveragePolicyStd[3]  | 0.23343    |
| AveragePolicyStd[4]  | 0.16671    |
| AveragePolicyStd[5]  | 0.25331    |
| AverageReturn        | 1553       |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 357.16     |
| AverageEpisodeLength | 936        |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.51     |
| TotalNEpisodes       | 18692      |
| TotalNSamples        | 4.0546e+06 |
| ExplainedVariance    | -0.0014453 |
-------------------------------------
[2018-12-22 11:30:07.648310 UTC] Saving snapshot
[2018-12-22 11:30:07.656410 UTC] Starting iteration 811
[2018-12-22 11:30:07.656638 UTC] Start collecting samples
[2018-12-22 11:30:10.582570 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:10.661287 UTC] Performing policy update
[2018-12-22 11:30:10.661940 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:10.751008 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:11.824351 UTC] Performing line search
[2018-12-22 11:30:11.955676 UTC] Updating baseline
[2018-12-22 11:30:13.237779 UTC] Computing logging information
-------------------------------------
| Iteration            | 811        |
| ExpectedImprovement  | 0.016955   |
| ActualImprovement    | 0.015388   |
| ImprovementRatio     | 0.9076     |
| MeanKL               | 0.0070544  |
| Entropy              | -0.55479   |
| Perplexity           | 0.57419    |
| AveragePolicyStd     | 0.22362    |
| AveragePolicyStd[0]  | 0.24772    |
| AveragePolicyStd[1]  | 0.25689    |
| AveragePolicyStd[2]  | 0.18292    |
| AveragePolicyStd[3]  | 0.23307    |
| AveragePolicyStd[4]  | 0.16742    |
| AveragePolicyStd[5]  | 0.25372    |
| AverageReturn        | 1552.1     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 357.02     |
| AverageEpisodeLength | 935.73     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.45     |
| TotalNEpisodes       | 18695      |
| TotalNSamples        | 4.0576e+06 |
| ExplainedVariance    | 0.16632    |
-------------------------------------
[2018-12-22 11:30:13.623483 UTC] Saving snapshot
[2018-12-22 11:30:13.623742 UTC] Starting iteration 812
[2018-12-22 11:30:13.623863 UTC] Start collecting samples
[2018-12-22 11:30:16.619635 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:16.701765 UTC] Performing policy update
[2018-12-22 11:30:16.702374 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:16.793153 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:17.866415 UTC] Performing line search
[2018-12-22 11:30:17.998517 UTC] Updating baseline
[2018-12-22 11:30:19.368627 UTC] Computing logging information
-------------------------------------
| Iteration            | 812        |
| ExpectedImprovement  | 0.018761   |
| ActualImprovement    | 0.018338   |
| ImprovementRatio     | 0.97744    |
| MeanKL               | 0.0078851  |
| Entropy              | -0.55798   |
| Perplexity           | 0.57236    |
| AveragePolicyStd     | 0.22349    |
| AveragePolicyStd[0]  | 0.24709    |
| AveragePolicyStd[1]  | 0.25691    |
| AveragePolicyStd[2]  | 0.18245    |
| AveragePolicyStd[3]  | 0.23319    |
| AveragePolicyStd[4]  | 0.16775    |
| AveragePolicyStd[5]  | 0.25357    |
| AverageReturn        | 1564.3     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 328.56     |
| AverageEpisodeLength | 943.78     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18702      |
| TotalNSamples        | 4.0645e+06 |
| ExplainedVariance    | 0.06909    |
-------------------------------------
[2018-12-22 11:30:19.757158 UTC] Saving snapshot
[2018-12-22 11:30:19.757424 UTC] Starting iteration 813
[2018-12-22 11:30:19.757563 UTC] Start collecting samples
[2018-12-22 11:30:22.709142 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:22.789790 UTC] Performing policy update
[2018-12-22 11:30:22.790599 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:22.879327 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:23.953661 UTC] Performing line search
[2018-12-22 11:30:24.081660 UTC] Updating baseline
[2018-12-22 11:30:25.348935 UTC] Computing logging information
-------------------------------------
| Iteration            | 813        |
| ExpectedImprovement  | 0.017577   |
| ActualImprovement    | 0.016649   |
| ImprovementRatio     | 0.94719    |
| MeanKL               | 0.0076146  |
| Entropy              | -0.55256   |
| Perplexity           | 0.57547    |
| AveragePolicyStd     | 0.22371    |
| AveragePolicyStd[0]  | 0.24713    |
| AveragePolicyStd[1]  | 0.25728    |
| AveragePolicyStd[2]  | 0.18274    |
| AveragePolicyStd[3]  | 0.23322    |
| AveragePolicyStd[4]  | 0.16767    |
| AveragePolicyStd[5]  | 0.25424    |
| AverageReturn        | 1563.4     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 328.25     |
| AverageEpisodeLength | 943.78     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18708      |
| TotalNSamples        | 4.0705e+06 |
| ExplainedVariance    | -0.01624   |
-------------------------------------
[2018-12-22 11:30:25.734366 UTC] Saving snapshot
[2018-12-22 11:30:25.734648 UTC] Starting iteration 814
[2018-12-22 11:30:25.734863 UTC] Start collecting samples
[2018-12-22 11:30:28.626598 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:28.703359 UTC] Performing policy update
[2018-12-22 11:30:28.704207 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:28.793339 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:29.860637 UTC] Performing line search
[2018-12-22 11:30:29.991085 UTC] Updating baseline
[2018-12-22 11:30:31.694788 UTC] Computing logging information
-------------------------------------
| Iteration            | 814        |
| ExpectedImprovement  | 0.016969   |
| ActualImprovement    | 0.014679   |
| ImprovementRatio     | 0.86502    |
| MeanKL               | 0.0076889  |
| Entropy              | -0.55008   |
| Perplexity           | 0.5769     |
| AveragePolicyStd     | 0.22385    |
| AveragePolicyStd[0]  | 0.24801    |
| AveragePolicyStd[1]  | 0.25799    |
| AveragePolicyStd[2]  | 0.18233    |
| AveragePolicyStd[3]  | 0.23379    |
| AveragePolicyStd[4]  | 0.16746    |
| AveragePolicyStd[5]  | 0.25351    |
| AverageReturn        | 1563.8     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 328.36     |
| AverageEpisodeLength | 943.78     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18710      |
| TotalNSamples        | 4.0725e+06 |
| ExplainedVariance    | 0.0050385  |
-------------------------------------
[2018-12-22 11:30:32.084530 UTC] Saving snapshot
[2018-12-22 11:30:32.084776 UTC] Starting iteration 815
[2018-12-22 11:30:32.084902 UTC] Start collecting samples
[2018-12-22 11:30:35.062695 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:35.142542 UTC] Performing policy update
[2018-12-22 11:30:35.143370 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:35.233272 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:36.296422 UTC] Performing line search
[2018-12-22 11:30:36.423348 UTC] Updating baseline
[2018-12-22 11:30:38.095222 UTC] Computing logging information
-------------------------------------
| Iteration            | 815        |
| ExpectedImprovement  | 0.018117   |
| ActualImprovement    | 0.017045   |
| ImprovementRatio     | 0.94078    |
| MeanKL               | 0.0078716  |
| Entropy              | -0.55767   |
| Perplexity           | 0.57254    |
| AveragePolicyStd     | 0.22357    |
| AveragePolicyStd[0]  | 0.24867    |
| AveragePolicyStd[1]  | 0.25673    |
| AveragePolicyStd[2]  | 0.18236    |
| AveragePolicyStd[3]  | 0.23399    |
| AveragePolicyStd[4]  | 0.16685    |
| AveragePolicyStd[5]  | 0.25284    |
| AverageReturn        | 1562.6     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 330.94     |
| AverageEpisodeLength | 943.07     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.64     |
| TotalNEpisodes       | 18718      |
| TotalNSamples        | 4.0797e+06 |
| ExplainedVariance    | 0.047816   |
-------------------------------------
[2018-12-22 11:30:38.475173 UTC] Saving snapshot
[2018-12-22 11:30:38.475433 UTC] Starting iteration 816
[2018-12-22 11:30:38.475576 UTC] Start collecting samples
[2018-12-22 11:30:41.385343 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:41.463080 UTC] Performing policy update
[2018-12-22 11:30:41.463962 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:41.553201 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:42.618006 UTC] Performing line search
[2018-12-22 11:30:42.746356 UTC] Updating baseline
[2018-12-22 11:30:44.271064 UTC] Computing logging information
-------------------------------------
| Iteration            | 816        |
| ExpectedImprovement  | 0.015887   |
| ActualImprovement    | 0.015007   |
| ImprovementRatio     | 0.94457    |
| MeanKL               | 0.0075363  |
| Entropy              | -0.5705    |
| Perplexity           | 0.56524    |
| AveragePolicyStd     | 0.22314    |
| AveragePolicyStd[0]  | 0.24879    |
| AveragePolicyStd[1]  | 0.25597    |
| AveragePolicyStd[2]  | 0.1817     |
| AveragePolicyStd[3]  | 0.23366    |
| AveragePolicyStd[4]  | 0.16612    |
| AveragePolicyStd[5]  | 0.25259    |
| AverageReturn        | 1562.6     |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 330.95     |
| AverageEpisodeLength | 943.07     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.64     |
| TotalNEpisodes       | 18722      |
| TotalNSamples        | 4.0837e+06 |
| ExplainedVariance    | 0.0043164  |
-------------------------------------
[2018-12-22 11:30:44.656129 UTC] Saving snapshot
[2018-12-22 11:30:44.656378 UTC] Starting iteration 817
[2018-12-22 11:30:44.656513 UTC] Start collecting samples
[2018-12-22 11:30:47.570219 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:47.647345 UTC] Performing policy update
[2018-12-22 11:30:47.648053 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:47.737629 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:48.798194 UTC] Performing line search
[2018-12-22 11:30:48.927008 UTC] Updating baseline
[2018-12-22 11:30:50.282907 UTC] Computing logging information
-------------------------------------
| Iteration            | 817        |
| ExpectedImprovement  | 0.019374   |
| ActualImprovement    | 0.017381   |
| ImprovementRatio     | 0.89712    |
| MeanKL               | 0.0071354  |
| Entropy              | -0.56912   |
| Perplexity           | 0.56602    |
| AveragePolicyStd     | 0.22315    |
| AveragePolicyStd[0]  | 0.24906    |
| AveragePolicyStd[1]  | 0.25537    |
| AveragePolicyStd[2]  | 0.18194    |
| AveragePolicyStd[3]  | 0.23332    |
| AveragePolicyStd[4]  | 0.16648    |
| AveragePolicyStd[5]  | 0.25274    |
| AverageReturn        | 1572       |
| MinReturn            | 122.74     |
| MaxReturn            | 1734.8     |
| StdReturn            | 319.63     |
| AverageEpisodeLength | 948.48     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.4      |
| TotalNEpisodes       | 18726      |
| TotalNSamples        | 4.0877e+06 |
| ExplainedVariance    | 0.066597   |
-------------------------------------
[2018-12-22 11:30:50.661095 UTC] Saving snapshot
[2018-12-22 11:30:50.661331 UTC] Starting iteration 818
[2018-12-22 11:30:50.661447 UTC] Start collecting samples
[2018-12-22 11:30:53.615062 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:53.695098 UTC] Performing policy update
[2018-12-22 11:30:53.695769 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:53.789909 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:30:54.852130 UTC] Performing line search
[2018-12-22 11:30:54.980102 UTC] Updating baseline
[2018-12-22 11:30:56.162540 UTC] Computing logging information
-------------------------------------
| Iteration            | 818        |
| ExpectedImprovement  | 0.017216   |
| ActualImprovement    | 0.016734   |
| ImprovementRatio     | 0.97204    |
| MeanKL               | 0.0074405  |
| Entropy              | -0.56837   |
| Perplexity           | 0.56645    |
| AveragePolicyStd     | 0.22315    |
| AveragePolicyStd[0]  | 0.24844    |
| AveragePolicyStd[1]  | 0.25595    |
| AveragePolicyStd[2]  | 0.1818     |
| AveragePolicyStd[3]  | 0.23283    |
| AveragePolicyStd[4]  | 0.1671     |
| AveragePolicyStd[5]  | 0.2528     |
| AverageReturn        | 1562.7     |
| MinReturn            | 122.74     |
| MaxReturn            | 1737       |
| StdReturn            | 333.08     |
| AverageEpisodeLength | 942.84     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.61     |
| TotalNEpisodes       | 18734      |
| TotalNSamples        | 4.0943e+06 |
| ExplainedVariance    | 0.1902     |
-------------------------------------
[2018-12-22 11:30:56.544876 UTC] Saving snapshot
[2018-12-22 11:30:56.545129 UTC] Starting iteration 819
[2018-12-22 11:30:56.545247 UTC] Start collecting samples
[2018-12-22 11:30:59.465412 UTC] Computing input variables for policy optimization
[2018-12-22 11:30:59.544820 UTC] Performing policy update
[2018-12-22 11:30:59.545462 UTC] Computing gradient in Euclidean space
[2018-12-22 11:30:59.634658 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:00.692060 UTC] Performing line search
[2018-12-22 11:31:00.818178 UTC] Updating baseline
[2018-12-22 11:31:02.267516 UTC] Computing logging information
-------------------------------------
| Iteration            | 819        |
| ExpectedImprovement  | 0.018361   |
| ActualImprovement    | 0.01739    |
| ImprovementRatio     | 0.94714    |
| MeanKL               | 0.0070401  |
| Entropy              | -0.55032   |
| Perplexity           | 0.57677    |
| AveragePolicyStd     | 0.22381    |
| AveragePolicyStd[0]  | 0.24968    |
| AveragePolicyStd[1]  | 0.25571    |
| AveragePolicyStd[2]  | 0.18291    |
| AveragePolicyStd[3]  | 0.23393    |
| AveragePolicyStd[4]  | 0.16723    |
| AveragePolicyStd[5]  | 0.2534     |
| AverageReturn        | 1547.9     |
| MinReturn            | 122.74     |
| MaxReturn            | 1737       |
| StdReturn            | 353.98     |
| AverageEpisodeLength | 935.19     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.1      |
| TotalNEpisodes       | 18740      |
| TotalNSamples        | 4.0995e+06 |
| ExplainedVariance    | 0.10787    |
-------------------------------------
[2018-12-22 11:31:02.654511 UTC] Saving snapshot
[2018-12-22 11:31:02.654769 UTC] Starting iteration 820
[2018-12-22 11:31:02.654901 UTC] Start collecting samples
[2018-12-22 11:31:05.551002 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:05.627960 UTC] Performing policy update
[2018-12-22 11:31:05.628561 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:05.718277 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:06.783713 UTC] Performing line search
[2018-12-22 11:31:06.911768 UTC] Updating baseline
[2018-12-22 11:31:08.186412 UTC] Computing logging information
-------------------------------------
| Iteration            | 820        |
| ExpectedImprovement  | 0.015999   |
| ActualImprovement    | 0.015176   |
| ImprovementRatio     | 0.9485     |
| MeanKL               | 0.007445   |
| Entropy              | -0.55643   |
| Perplexity           | 0.57325    |
| AveragePolicyStd     | 0.22355    |
| AveragePolicyStd[0]  | 0.24934    |
| AveragePolicyStd[1]  | 0.2548     |
| AveragePolicyStd[2]  | 0.18311    |
| AveragePolicyStd[3]  | 0.23284    |
| AveragePolicyStd[4]  | 0.1673     |
| AveragePolicyStd[5]  | 0.2539     |
| AverageReturn        | 1562.8     |
| MinReturn            | 204.8      |
| MaxReturn            | 1737       |
| StdReturn            | 323.73     |
| AverageEpisodeLength | 944.03     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.75     |
| TotalNEpisodes       | 18743      |
| TotalNSamples        | 4.1025e+06 |
| ExplainedVariance    | 0.13331    |
-------------------------------------
[2018-12-22 11:31:08.566334 UTC] Saving snapshot
[2018-12-22 11:31:08.574545 UTC] Starting iteration 821
[2018-12-22 11:31:08.574734 UTC] Start collecting samples
[2018-12-22 11:31:11.533310 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:11.613398 UTC] Performing policy update
[2018-12-22 11:31:11.614161 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:11.704232 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:12.763514 UTC] Performing line search
[2018-12-22 11:31:12.891242 UTC] Updating baseline
[2018-12-22 11:31:14.308141 UTC] Computing logging information
-------------------------------------
| Iteration            | 821        |
| ExpectedImprovement  | 0.017271   |
| ActualImprovement    | 0.016269   |
| ImprovementRatio     | 0.942      |
| MeanKL               | 0.0071917  |
| Entropy              | -0.55263   |
| Perplexity           | 0.57543    |
| AveragePolicyStd     | 0.22366    |
| AveragePolicyStd[0]  | 0.24983    |
| AveragePolicyStd[1]  | 0.25438    |
| AveragePolicyStd[2]  | 0.18324    |
| AveragePolicyStd[3]  | 0.23222    |
| AveragePolicyStd[4]  | 0.16782    |
| AveragePolicyStd[5]  | 0.25449    |
| AverageReturn        | 1574.9     |
| MinReturn            | 204.8      |
| MaxReturn            | 1737       |
| StdReturn            | 301.13     |
| AverageEpisodeLength | 950.52     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.69     |
| TotalNEpisodes       | 18749      |
| TotalNSamples        | 4.1085e+06 |
| ExplainedVariance    | 0.12118    |
-------------------------------------
[2018-12-22 11:31:14.695388 UTC] Saving snapshot
[2018-12-22 11:31:14.695698 UTC] Starting iteration 822
[2018-12-22 11:31:14.695797 UTC] Start collecting samples
[2018-12-22 11:31:17.643191 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:17.721685 UTC] Performing policy update
[2018-12-22 11:31:17.722409 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:17.814687 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:18.886165 UTC] Performing line search
[2018-12-22 11:31:19.015696 UTC] Updating baseline
[2018-12-22 11:31:20.531095 UTC] Computing logging information
-------------------------------------
| Iteration            | 822        |
| ExpectedImprovement  | 0.016074   |
| ActualImprovement    | 0.015362   |
| ImprovementRatio     | 0.95571    |
| MeanKL               | 0.0077516  |
| Entropy              | -0.55403   |
| Perplexity           | 0.57463    |
| AveragePolicyStd     | 0.22363    |
| AveragePolicyStd[0]  | 0.24991    |
| AveragePolicyStd[1]  | 0.2545     |
| AveragePolicyStd[2]  | 0.18316    |
| AveragePolicyStd[3]  | 0.23178    |
| AveragePolicyStd[4]  | 0.16766    |
| AveragePolicyStd[5]  | 0.25479    |
| AverageReturn        | 1575.4     |
| MinReturn            | 204.8      |
| MaxReturn            | 1737       |
| StdReturn            | 301.26     |
| AverageEpisodeLength | 950.52     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.69     |
| TotalNEpisodes       | 18755      |
| TotalNSamples        | 4.1145e+06 |
| ExplainedVariance    | -0.019349  |
-------------------------------------
[2018-12-22 11:31:20.915286 UTC] Saving snapshot
[2018-12-22 11:31:20.915597 UTC] Starting iteration 823
[2018-12-22 11:31:20.915723 UTC] Start collecting samples
[2018-12-22 11:31:23.835610 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:23.916282 UTC] Performing policy update
[2018-12-22 11:31:23.917162 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:24.009418 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:25.069152 UTC] Performing line search
[2018-12-22 11:31:25.195931 UTC] Updating baseline
[2018-12-22 11:31:26.458287 UTC] Computing logging information
-------------------------------------
| Iteration            | 823        |
| ExpectedImprovement  | 0.017345   |
| ActualImprovement    | 0.016452   |
| ImprovementRatio     | 0.94851    |
| MeanKL               | 0.0071317  |
| Entropy              | -0.55356   |
| Perplexity           | 0.5749     |
| AveragePolicyStd     | 0.22365    |
| AveragePolicyStd[0]  | 0.24954    |
| AveragePolicyStd[1]  | 0.25373    |
| AveragePolicyStd[2]  | 0.18328    |
| AveragePolicyStd[3]  | 0.23201    |
| AveragePolicyStd[4]  | 0.16756    |
| AveragePolicyStd[5]  | 0.25577    |
| AverageReturn        | 1563.2     |
| MinReturn            | 99.537     |
| MaxReturn            | 1737       |
| StdReturn            | 333.16     |
| AverageEpisodeLength | 943.84     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.1      |
| TotalNEpisodes       | 18760      |
| TotalNSamples        | 4.1185e+06 |
| ExplainedVariance    | 0.098532   |
-------------------------------------
[2018-12-22 11:31:26.841281 UTC] Saving snapshot
[2018-12-22 11:31:26.841548 UTC] Starting iteration 824
[2018-12-22 11:31:26.841671 UTC] Start collecting samples
[2018-12-22 11:31:29.727643 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:29.804622 UTC] Performing policy update
[2018-12-22 11:31:29.805202 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:29.895497 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:30.970995 UTC] Performing line search
[2018-12-22 11:31:31.099206 UTC] Updating baseline
[2018-12-22 11:31:32.635524 UTC] Computing logging information
-------------------------------------
| Iteration            | 824        |
| ExpectedImprovement  | 0.016293   |
| ActualImprovement    | 0.015316   |
| ImprovementRatio     | 0.94004    |
| MeanKL               | 0.0078252  |
| Entropy              | -0.56082   |
| Perplexity           | 0.57074    |
| AveragePolicyStd     | 0.2234     |
| AveragePolicyStd[0]  | 0.24969    |
| AveragePolicyStd[1]  | 0.25338    |
| AveragePolicyStd[2]  | 0.18333    |
| AveragePolicyStd[3]  | 0.23133    |
| AveragePolicyStd[4]  | 0.16703    |
| AveragePolicyStd[5]  | 0.25562    |
| AverageReturn        | 1568.6     |
| MinReturn            | 99.537     |
| MaxReturn            | 1737       |
| StdReturn            | 331.62     |
| AverageEpisodeLength | 946.48     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.04     |
| TotalNEpisodes       | 18763      |
| TotalNSamples        | 4.1215e+06 |
| ExplainedVariance    | -0.036305  |
-------------------------------------
[2018-12-22 11:31:33.021442 UTC] Saving snapshot
[2018-12-22 11:31:33.021700 UTC] Starting iteration 825
[2018-12-22 11:31:33.021867 UTC] Start collecting samples
[2018-12-22 11:31:35.991743 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:36.072161 UTC] Performing policy update
[2018-12-22 11:31:36.072839 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:36.163953 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:37.226922 UTC] Performing line search
[2018-12-22 11:31:37.354982 UTC] Updating baseline
[2018-12-22 11:31:38.795806 UTC] Computing logging information
-------------------------------------
| Iteration            | 825        |
| ExpectedImprovement  | 0.016706   |
| ActualImprovement    | 0.015769   |
| ImprovementRatio     | 0.9439     |
| MeanKL               | 0.0071088  |
| Entropy              | -0.56487   |
| Perplexity           | 0.56843    |
| AveragePolicyStd     | 0.22327    |
| AveragePolicyStd[0]  | 0.24953    |
| AveragePolicyStd[1]  | 0.25303    |
| AveragePolicyStd[2]  | 0.18377    |
| AveragePolicyStd[3]  | 0.23128    |
| AveragePolicyStd[4]  | 0.16619    |
| AveragePolicyStd[5]  | 0.25582    |
| AverageReturn        | 1581.4     |
| MinReturn            | 99.537     |
| MaxReturn            | 1737       |
| StdReturn            | 309.25     |
| AverageEpisodeLength | 953.96     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.89     |
| TotalNEpisodes       | 18771      |
| TotalNSamples        | 4.1295e+06 |
| ExplainedVariance    | -0.011041  |
-------------------------------------
[2018-12-22 11:31:39.177950 UTC] Saving snapshot
[2018-12-22 11:31:39.178205 UTC] Starting iteration 826
[2018-12-22 11:31:39.178327 UTC] Start collecting samples
[2018-12-22 11:31:42.107100 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:42.185084 UTC] Performing policy update
[2018-12-22 11:31:42.185680 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:42.274194 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:43.335812 UTC] Performing line search
[2018-12-22 11:31:43.464065 UTC] Updating baseline
[2018-12-22 11:31:45.679727 UTC] Computing logging information
-------------------------------------
| Iteration            | 826        |
| ExpectedImprovement  | 0.015168   |
| ActualImprovement    | 0.014322   |
| ImprovementRatio     | 0.94418    |
| MeanKL               | 0.0079072  |
| Entropy              | -0.55777   |
| Perplexity           | 0.57248    |
| AveragePolicyStd     | 0.22349    |
| AveragePolicyStd[0]  | 0.24994    |
| AveragePolicyStd[1]  | 0.25214    |
| AveragePolicyStd[2]  | 0.18474    |
| AveragePolicyStd[3]  | 0.23188    |
| AveragePolicyStd[4]  | 0.16626    |
| AveragePolicyStd[5]  | 0.25601    |
| AverageReturn        | 1580.5     |
| MinReturn            | 99.537     |
| MaxReturn            | 1737       |
| StdReturn            | 308.98     |
| AverageEpisodeLength | 953.96     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.89     |
| TotalNEpisodes       | 18776      |
| TotalNSamples        | 4.1345e+06 |
| ExplainedVariance    | 0.00081675 |
-------------------------------------
[2018-12-22 11:31:46.065773 UTC] Saving snapshot
[2018-12-22 11:31:46.066024 UTC] Starting iteration 827
[2018-12-22 11:31:46.066162 UTC] Start collecting samples
[2018-12-22 11:31:48.966151 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:49.043818 UTC] Performing policy update
[2018-12-22 11:31:49.044694 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:49.135138 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:50.210143 UTC] Performing line search
[2018-12-22 11:31:50.337477 UTC] Updating baseline
[2018-12-22 11:31:52.130246 UTC] Computing logging information
-------------------------------------
| Iteration            | 827        |
| ExpectedImprovement  | 0.017332   |
| ActualImprovement    | 0.015962   |
| ImprovementRatio     | 0.92093    |
| MeanKL               | 0.0072896  |
| Entropy              | -0.55484   |
| Perplexity           | 0.57416    |
| AveragePolicyStd     | 0.22364    |
| AveragePolicyStd[0]  | 0.25065    |
| AveragePolicyStd[1]  | 0.25229    |
| AveragePolicyStd[2]  | 0.1848     |
| AveragePolicyStd[3]  | 0.23222    |
| AveragePolicyStd[4]  | 0.16593    |
| AveragePolicyStd[5]  | 0.25593    |
| AverageReturn        | 1568.9     |
| MinReturn            | 99.537     |
| MaxReturn            | 1737       |
| StdReturn            | 329.9      |
| AverageEpisodeLength | 946.79     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.94     |
| TotalNEpisodes       | 18779      |
| TotalNSamples        | 4.1368e+06 |
| ExplainedVariance    | 0.13884    |
-------------------------------------
[2018-12-22 11:31:52.515967 UTC] Saving snapshot
[2018-12-22 11:31:52.516211 UTC] Starting iteration 828
[2018-12-22 11:31:52.516352 UTC] Start collecting samples
[2018-12-22 11:31:55.458130 UTC] Computing input variables for policy optimization
[2018-12-22 11:31:55.539893 UTC] Performing policy update
[2018-12-22 11:31:55.540847 UTC] Computing gradient in Euclidean space
[2018-12-22 11:31:55.630025 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:31:56.706176 UTC] Performing line search
[2018-12-22 11:31:56.833183 UTC] Updating baseline
[2018-12-22 11:31:58.613774 UTC] Computing logging information
-------------------------------------
| Iteration            | 828        |
| ExpectedImprovement  | 0.018278   |
| ActualImprovement    | 0.017038   |
| ImprovementRatio     | 0.93213    |
| MeanKL               | 0.0073782  |
| Entropy              | -0.5509    |
| Perplexity           | 0.57643    |
| AveragePolicyStd     | 0.22381    |
| AveragePolicyStd[0]  | 0.25166    |
| AveragePolicyStd[1]  | 0.25161    |
| AveragePolicyStd[2]  | 0.18495    |
| AveragePolicyStd[3]  | 0.23271    |
| AveragePolicyStd[4]  | 0.1657     |
| AveragePolicyStd[5]  | 0.2562     |
| AverageReturn        | 1579.1     |
| MinReturn            | 99.537     |
| MaxReturn            | 1737       |
| StdReturn            | 320.46     |
| AverageEpisodeLength | 950.85     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.06     |
| TotalNEpisodes       | 18788      |
| TotalNSamples        | 4.1457e+06 |
| ExplainedVariance    | 0.06837    |
-------------------------------------
[2018-12-22 11:31:58.998500 UTC] Saving snapshot
[2018-12-22 11:31:58.998762 UTC] Starting iteration 829
[2018-12-22 11:31:58.998884 UTC] Start collecting samples
[2018-12-22 11:32:01.915557 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:01.997125 UTC] Performing policy update
[2018-12-22 11:32:01.997802 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:02.089527 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:03.150100 UTC] Performing line search
[2018-12-22 11:32:03.276663 UTC] Updating baseline
[2018-12-22 11:32:05.057343 UTC] Computing logging information
-------------------------------------
| Iteration            | 829        |
| ExpectedImprovement  | 0.017751   |
| ActualImprovement    | 0.016887   |
| ImprovementRatio     | 0.95136    |
| MeanKL               | 0.0073818  |
| Entropy              | -0.56004   |
| Perplexity           | 0.57119    |
| AveragePolicyStd     | 0.2235     |
| AveragePolicyStd[0]  | 0.2514     |
| AveragePolicyStd[1]  | 0.2511     |
| AveragePolicyStd[2]  | 0.18453    |
| AveragePolicyStd[3]  | 0.23254    |
| AveragePolicyStd[4]  | 0.16516    |
| AveragePolicyStd[5]  | 0.25625    |
| AverageReturn        | 1580.9     |
| MinReturn            | 99.537     |
| MaxReturn            | 1742.1     |
| StdReturn            | 321.04     |
| AverageEpisodeLength | 950.85     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.06     |
| TotalNEpisodes       | 18791      |
| TotalNSamples        | 4.1487e+06 |
| ExplainedVariance    | -0.0036518 |
-------------------------------------
[2018-12-22 11:32:05.441035 UTC] Saving snapshot
[2018-12-22 11:32:05.441275 UTC] Starting iteration 830
[2018-12-22 11:32:05.441400 UTC] Start collecting samples
[2018-12-22 11:32:08.383986 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:08.464077 UTC] Performing policy update
[2018-12-22 11:32:08.464833 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:08.555029 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:09.608745 UTC] Performing line search
[2018-12-22 11:32:09.737158 UTC] Updating baseline
[2018-12-22 11:32:11.408707 UTC] Computing logging information
------------------------------------
| Iteration            | 830       |
| ExpectedImprovement  | 0.016404  |
| ActualImprovement    | 0.015437  |
| ImprovementRatio     | 0.94105   |
| MeanKL               | 0.0076967 |
| Entropy              | -0.56149  |
| Perplexity           | 0.57036   |
| AveragePolicyStd     | 0.22343   |
| AveragePolicyStd[0]  | 0.25193   |
| AveragePolicyStd[1]  | 0.25025   |
| AveragePolicyStd[2]  | 0.18393   |
| AveragePolicyStd[3]  | 0.2331    |
| AveragePolicyStd[4]  | 0.16558   |
| AveragePolicyStd[5]  | 0.25577   |
| AverageReturn        | 1556.7    |
| MinReturn            | 99.537    |
| MaxReturn            | 1748.4    |
| StdReturn            | 360.04    |
| AverageEpisodeLength | 934.66    |
| MinEpisodeLength     | 83        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 207.04    |
| TotalNEpisodes       | 18798     |
| TotalNSamples        | 4.154e+06 |
| ExplainedVariance    | 0.2717    |
------------------------------------
[2018-12-22 11:32:11.789275 UTC] Saving snapshot
[2018-12-22 11:32:11.797413 UTC] Starting iteration 831
[2018-12-22 11:32:11.797625 UTC] Start collecting samples
[2018-12-22 11:32:14.735083 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:14.815598 UTC] Performing policy update
[2018-12-22 11:32:14.816175 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:14.906188 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:15.960841 UTC] Performing line search
[2018-12-22 11:32:16.088666 UTC] Updating baseline
[2018-12-22 11:32:17.356223 UTC] Computing logging information
-------------------------------------
| Iteration            | 831        |
| ExpectedImprovement  | 0.017177   |
| ActualImprovement    | 0.016274   |
| ImprovementRatio     | 0.94741    |
| MeanKL               | 0.0074548  |
| Entropy              | -0.56199   |
| Perplexity           | 0.57007    |
| AveragePolicyStd     | 0.22341    |
| AveragePolicyStd[0]  | 0.25181    |
| AveragePolicyStd[1]  | 0.25112    |
| AveragePolicyStd[2]  | 0.18437    |
| AveragePolicyStd[3]  | 0.23323    |
| AveragePolicyStd[4]  | 0.16515    |
| AveragePolicyStd[5]  | 0.2548     |
| AverageReturn        | 1553.1     |
| MinReturn            | 99.537     |
| MaxReturn            | 1748.4     |
| StdReturn            | 367.4      |
| AverageEpisodeLength | 930.11     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.52     |
| TotalNEpisodes       | 18805      |
| TotalNSamples        | 4.1605e+06 |
| ExplainedVariance    | 0.085772   |
-------------------------------------
[2018-12-22 11:32:17.738516 UTC] Saving snapshot
[2018-12-22 11:32:17.738774 UTC] Starting iteration 832
[2018-12-22 11:32:17.738896 UTC] Start collecting samples
[2018-12-22 11:32:20.632345 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:20.708924 UTC] Performing policy update
[2018-12-22 11:32:20.709515 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:20.798659 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:21.862962 UTC] Performing line search
[2018-12-22 11:32:21.991104 UTC] Updating baseline
[2018-12-22 11:32:23.343468 UTC] Computing logging information
-------------------------------------
| Iteration            | 832        |
| ExpectedImprovement  | 0.017114   |
| ActualImprovement    | 0.016098   |
| ImprovementRatio     | 0.94064    |
| MeanKL               | 0.006986   |
| Entropy              | -0.5703    |
| Perplexity           | 0.56536    |
| AveragePolicyStd     | 0.22312    |
| AveragePolicyStd[0]  | 0.25163    |
| AveragePolicyStd[1]  | 0.25044    |
| AveragePolicyStd[2]  | 0.18488    |
| AveragePolicyStd[3]  | 0.23279    |
| AveragePolicyStd[4]  | 0.16421    |
| AveragePolicyStd[5]  | 0.25478    |
| AverageReturn        | 1549.8     |
| MinReturn            | 99.537     |
| MaxReturn            | 1748.4     |
| StdReturn            | 368.03     |
| AverageEpisodeLength | 928.09     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.81     |
| TotalNEpisodes       | 18808      |
| TotalNSamples        | 4.1633e+06 |
| ExplainedVariance    | 0.34716    |
-------------------------------------
[2018-12-22 11:32:23.725282 UTC] Saving snapshot
[2018-12-22 11:32:23.725554 UTC] Starting iteration 833
[2018-12-22 11:32:23.725681 UTC] Start collecting samples
[2018-12-22 11:32:26.677391 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:26.757855 UTC] Performing policy update
[2018-12-22 11:32:26.758413 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:26.847248 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:27.904788 UTC] Performing line search
[2018-12-22 11:32:28.036732 UTC] Updating baseline
[2018-12-22 11:32:29.394484 UTC] Computing logging information
-------------------------------------
| Iteration            | 833        |
| ExpectedImprovement  | 0.018443   |
| ActualImprovement    | 0.017643   |
| ImprovementRatio     | 0.95666    |
| MeanKL               | 0.0076939  |
| Entropy              | -0.583     |
| Perplexity           | 0.55822    |
| AveragePolicyStd     | 0.22269    |
| AveragePolicyStd[0]  | 0.25139    |
| AveragePolicyStd[1]  | 0.2507     |
| AveragePolicyStd[2]  | 0.18454    |
| AveragePolicyStd[3]  | 0.23129    |
| AveragePolicyStd[4]  | 0.16352    |
| AveragePolicyStd[5]  | 0.25472    |
| AverageReturn        | 1551.7     |
| MinReturn            | 99.537     |
| MaxReturn            | 1748.4     |
| StdReturn            | 351.58     |
| AverageEpisodeLength | 928.74     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.88     |
| TotalNEpisodes       | 18815      |
| TotalNSamples        | 4.1695e+06 |
| ExplainedVariance    | 0.16818    |
-------------------------------------
[2018-12-22 11:32:29.784818 UTC] Saving snapshot
[2018-12-22 11:32:29.785061 UTC] Starting iteration 834
[2018-12-22 11:32:29.785176 UTC] Start collecting samples
[2018-12-22 11:32:32.715989 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:32.794113 UTC] Performing policy update
[2018-12-22 11:32:32.794817 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:32.884037 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:33.943850 UTC] Performing line search
[2018-12-22 11:32:34.070922 UTC] Updating baseline
[2018-12-22 11:32:35.598385 UTC] Computing logging information
-------------------------------------
| Iteration            | 834        |
| ExpectedImprovement  | 0.017568   |
| ActualImprovement    | 0.016293   |
| ImprovementRatio     | 0.92737    |
| MeanKL               | 0.0077221  |
| Entropy              | -0.58451   |
| Perplexity           | 0.55738    |
| AveragePolicyStd     | 0.22258    |
| AveragePolicyStd[0]  | 0.25129    |
| AveragePolicyStd[1]  | 0.24983    |
| AveragePolicyStd[2]  | 0.18477    |
| AveragePolicyStd[3]  | 0.23135    |
| AveragePolicyStd[4]  | 0.16396    |
| AveragePolicyStd[5]  | 0.25426    |
| AverageReturn        | 1553.3     |
| MinReturn            | 99.537     |
| MaxReturn            | 1748.4     |
| StdReturn            | 352.11     |
| AverageEpisodeLength | 928.74     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.88     |
| TotalNEpisodes       | 18819      |
| TotalNSamples        | 4.1735e+06 |
| ExplainedVariance    | -0.011769  |
-------------------------------------
[2018-12-22 11:32:35.989591 UTC] Saving snapshot
[2018-12-22 11:32:35.989886 UTC] Starting iteration 835
[2018-12-22 11:32:35.990020 UTC] Start collecting samples
[2018-12-22 11:32:38.932045 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:39.013104 UTC] Performing policy update
[2018-12-22 11:32:39.014963 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:39.104246 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:40.165227 UTC] Performing line search
[2018-12-22 11:32:40.291829 UTC] Updating baseline
[2018-12-22 11:32:41.645796 UTC] Computing logging information
------------------------------------
| Iteration            | 835       |
| ExpectedImprovement  | 0.017909  |
| ActualImprovement    | 0.017356  |
| ImprovementRatio     | 0.96916   |
| MeanKL               | 0.007301  |
| Entropy              | -0.58184  |
| Perplexity           | 0.55887   |
| AveragePolicyStd     | 0.22272   |
| AveragePolicyStd[0]  | 0.25141   |
| AveragePolicyStd[1]  | 0.25017   |
| AveragePolicyStd[2]  | 0.18377   |
| AveragePolicyStd[3]  | 0.23265   |
| AveragePolicyStd[4]  | 0.16407   |
| AveragePolicyStd[5]  | 0.25425   |
| AverageReturn        | 1511.7    |
| MinReturn            | 99.537    |
| MaxReturn            | 1748.4    |
| StdReturn            | 421.32    |
| AverageEpisodeLength | 903.3     |
| MinEpisodeLength     | 83        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 240.2     |
| TotalNEpisodes       | 18827     |
| TotalNSamples        | 4.179e+06 |
| ExplainedVariance    | 0.29774   |
------------------------------------
[2018-12-22 11:32:42.027896 UTC] Saving snapshot
[2018-12-22 11:32:42.028141 UTC] Starting iteration 836
[2018-12-22 11:32:42.028280 UTC] Start collecting samples
[2018-12-22 11:32:44.962944 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:45.044008 UTC] Performing policy update
[2018-12-22 11:32:45.044657 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:45.133372 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:46.206924 UTC] Performing line search
[2018-12-22 11:32:46.335790 UTC] Updating baseline
[2018-12-22 11:32:47.786977 UTC] Computing logging information
------------------------------------
| Iteration            | 836       |
| ExpectedImprovement  | 0.019694  |
| ActualImprovement    | 0.019394  |
| ImprovementRatio     | 0.98474   |
| MeanKL               | 0.0077662 |
| Entropy              | -0.58467  |
| Perplexity           | 0.55729   |
| AveragePolicyStd     | 0.22259   |
| AveragePolicyStd[0]  | 0.25164   |
| AveragePolicyStd[1]  | 0.2494    |
| AveragePolicyStd[2]  | 0.18397   |
| AveragePolicyStd[3]  | 0.23267   |
| AveragePolicyStd[4]  | 0.16403   |
| AveragePolicyStd[5]  | 0.25386   |
| AverageReturn        | 1536.4    |
| MinReturn            | 99.537    |
| MaxReturn            | 1748.4    |
| StdReturn            | 394.41    |
| AverageEpisodeLength | 916.9     |
| MinEpisodeLength     | 83        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 225.53    |
| TotalNEpisodes       | 18833     |
| TotalNSamples        | 4.185e+06 |
| ExplainedVariance    | -0.065182 |
------------------------------------
[2018-12-22 11:32:48.179894 UTC] Saving snapshot
[2018-12-22 11:32:48.180170 UTC] Starting iteration 837
[2018-12-22 11:32:48.180302 UTC] Start collecting samples
[2018-12-22 11:32:51.240283 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:51.323424 UTC] Performing policy update
[2018-12-22 11:32:51.324171 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:51.419351 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:52.537575 UTC] Performing line search
[2018-12-22 11:32:52.670681 UTC] Updating baseline
[2018-12-22 11:32:54.064223 UTC] Computing logging information
-------------------------------------
| Iteration            | 837        |
| ExpectedImprovement  | 0.017642   |
| ActualImprovement    | 0.017383   |
| ImprovementRatio     | 0.9853     |
| MeanKL               | 0.0073978  |
| Entropy              | -0.58427   |
| Perplexity           | 0.55751    |
| AveragePolicyStd     | 0.22263    |
| AveragePolicyStd[0]  | 0.25197    |
| AveragePolicyStd[1]  | 0.24872    |
| AveragePolicyStd[2]  | 0.18447    |
| AveragePolicyStd[3]  | 0.23294    |
| AveragePolicyStd[4]  | 0.1634     |
| AveragePolicyStd[5]  | 0.25429    |
| AverageReturn        | 1530.3     |
| MinReturn            | 99.537     |
| MaxReturn            | 1749.1     |
| StdReturn            | 399.17     |
| AverageEpisodeLength | 912.62     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.96     |
| TotalNEpisodes       | 18836      |
| TotalNSamples        | 4.1876e+06 |
| ExplainedVariance    | 0.23047    |
-------------------------------------
[2018-12-22 11:32:54.482482 UTC] Saving snapshot
[2018-12-22 11:32:54.482816 UTC] Starting iteration 838
[2018-12-22 11:32:54.482937 UTC] Start collecting samples
[2018-12-22 11:32:57.655786 UTC] Computing input variables for policy optimization
[2018-12-22 11:32:57.739901 UTC] Performing policy update
[2018-12-22 11:32:57.740464 UTC] Computing gradient in Euclidean space
[2018-12-22 11:32:57.833651 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:32:58.942897 UTC] Performing line search
[2018-12-22 11:32:59.076692 UTC] Updating baseline
[2018-12-22 11:33:00.532307 UTC] Computing logging information
-------------------------------------
| Iteration            | 838        |
| ExpectedImprovement  | 0.017689   |
| ActualImprovement    | 0.016531   |
| ImprovementRatio     | 0.93451    |
| MeanKL               | 0.0071809  |
| Entropy              | -0.58416   |
| Perplexity           | 0.55758    |
| AveragePolicyStd     | 0.22261    |
| AveragePolicyStd[0]  | 0.25166    |
| AveragePolicyStd[1]  | 0.24874    |
| AveragePolicyStd[2]  | 0.18505    |
| AveragePolicyStd[3]  | 0.23307    |
| AveragePolicyStd[4]  | 0.16325    |
| AveragePolicyStd[5]  | 0.25391    |
| AverageReturn        | 1545.9     |
| MinReturn            | 99.537     |
| MaxReturn            | 1749.1     |
| StdReturn            | 381.71     |
| AverageEpisodeLength | 920.27     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.7      |
| TotalNEpisodes       | 18843      |
| TotalNSamples        | 4.1946e+06 |
| ExplainedVariance    | 0.013956   |
-------------------------------------
[2018-12-22 11:33:00.915835 UTC] Saving snapshot
[2018-12-22 11:33:00.916081 UTC] Starting iteration 839
[2018-12-22 11:33:00.916197 UTC] Start collecting samples
[2018-12-22 11:33:03.808878 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:03.887211 UTC] Performing policy update
[2018-12-22 11:33:03.888096 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:03.981606 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:05.033594 UTC] Performing line search
[2018-12-22 11:33:05.160891 UTC] Updating baseline
[2018-12-22 11:33:06.519172 UTC] Computing logging information
--------------------------------------
| Iteration            | 839         |
| ExpectedImprovement  | 0.018465    |
| ActualImprovement    | 0.017319    |
| ImprovementRatio     | 0.93795     |
| MeanKL               | 0.0068697   |
| Entropy              | -0.58766    |
| Perplexity           | 0.55563     |
| AveragePolicyStd     | 0.22247     |
| AveragePolicyStd[0]  | 0.25111     |
| AveragePolicyStd[1]  | 0.24773     |
| AveragePolicyStd[2]  | 0.18465     |
| AveragePolicyStd[3]  | 0.23399     |
| AveragePolicyStd[4]  | 0.16337     |
| AveragePolicyStd[5]  | 0.25399     |
| AverageReturn        | 1549.3      |
| MinReturn            | 99.537      |
| MaxReturn            | 1749.1      |
| StdReturn            | 382.26      |
| AverageEpisodeLength | 921.16      |
| MinEpisodeLength     | 83          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 217.84      |
| TotalNEpisodes       | 18847       |
| TotalNSamples        | 4.1986e+06  |
| ExplainedVariance    | -0.00015577 |
--------------------------------------
[2018-12-22 11:33:06.898797 UTC] Saving snapshot
[2018-12-22 11:33:06.899040 UTC] Starting iteration 840
[2018-12-22 11:33:06.899158 UTC] Start collecting samples
[2018-12-22 11:33:09.905541 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:09.993106 UTC] Performing policy update
[2018-12-22 11:33:09.993872 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:10.088646 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:11.211894 UTC] Performing line search
[2018-12-22 11:33:11.348320 UTC] Updating baseline
[2018-12-22 11:33:13.118958 UTC] Computing logging information
-------------------------------------
| Iteration            | 840        |
| ExpectedImprovement  | 0.017203   |
| ActualImprovement    | 0.016376   |
| ImprovementRatio     | 0.9519     |
| MeanKL               | 0.0083485  |
| Entropy              | -0.58808   |
| Perplexity           | 0.55539    |
| AveragePolicyStd     | 0.2225     |
| AveragePolicyStd[0]  | 0.25206    |
| AveragePolicyStd[1]  | 0.2478     |
| AveragePolicyStd[2]  | 0.18457    |
| AveragePolicyStd[3]  | 0.23371    |
| AveragePolicyStd[4]  | 0.16294    |
| AveragePolicyStd[5]  | 0.25394    |
| AverageReturn        | 1550.6     |
| MinReturn            | 99.537     |
| MaxReturn            | 1749.1     |
| StdReturn            | 382.75     |
| AverageEpisodeLength | 921.16     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.84     |
| TotalNEpisodes       | 18852      |
| TotalNSamples        | 4.2036e+06 |
| ExplainedVariance    | -0.0037244 |
-------------------------------------
[2018-12-22 11:33:13.547174 UTC] Saving snapshot
[2018-12-22 11:33:13.555091 UTC] Starting iteration 841
[2018-12-22 11:33:13.555308 UTC] Start collecting samples
[2018-12-22 11:33:16.548447 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:16.626662 UTC] Performing policy update
[2018-12-22 11:33:16.627310 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:16.717778 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:17.780074 UTC] Performing line search
[2018-12-22 11:33:17.907911 UTC] Updating baseline
[2018-12-22 11:33:19.255901 UTC] Computing logging information
-------------------------------------
| Iteration            | 841        |
| ExpectedImprovement  | 0.017981   |
| ActualImprovement    | 0.016904   |
| ImprovementRatio     | 0.94011    |
| MeanKL               | 0.0074161  |
| Entropy              | -0.59855   |
| Perplexity           | 0.54961    |
| AveragePolicyStd     | 0.22208    |
| AveragePolicyStd[0]  | 0.25174    |
| AveragePolicyStd[1]  | 0.24714    |
| AveragePolicyStd[2]  | 0.18455    |
| AveragePolicyStd[3]  | 0.23374    |
| AveragePolicyStd[4]  | 0.16274    |
| AveragePolicyStd[5]  | 0.25257    |
| AverageReturn        | 1521.8     |
| MinReturn            | 99.537     |
| MaxReturn            | 1749.1     |
| StdReturn            | 422.54     |
| AverageEpisodeLength | 903.75     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.3      |
| TotalNEpisodes       | 18859      |
| TotalNSamples        | 4.2088e+06 |
| ExplainedVariance    | 0.23281    |
-------------------------------------
[2018-12-22 11:33:19.639778 UTC] Saving snapshot
[2018-12-22 11:33:19.640050 UTC] Starting iteration 842
[2018-12-22 11:33:19.640168 UTC] Start collecting samples
[2018-12-22 11:33:22.572919 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:22.649799 UTC] Performing policy update
[2018-12-22 11:33:22.650691 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:22.740163 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:23.806960 UTC] Performing line search
[2018-12-22 11:33:23.938283 UTC] Updating baseline
[2018-12-22 11:33:25.458508 UTC] Computing logging information
-------------------------------------
| Iteration            | 842        |
| ExpectedImprovement  | 0.018027   |
| ActualImprovement    | 0.017439   |
| ImprovementRatio     | 0.9674     |
| MeanKL               | 0.0073614  |
| Entropy              | -0.59714   |
| Perplexity           | 0.55038    |
| AveragePolicyStd     | 0.22211    |
| AveragePolicyStd[0]  | 0.25146    |
| AveragePolicyStd[1]  | 0.24692    |
| AveragePolicyStd[2]  | 0.1845     |
| AveragePolicyStd[3]  | 0.23435    |
| AveragePolicyStd[4]  | 0.16301    |
| AveragePolicyStd[5]  | 0.25244    |
| AverageReturn        | 1538       |
| MinReturn            | 115.37     |
| MaxReturn            | 1749.1     |
| StdReturn            | 398.06     |
| AverageEpisodeLength | 912.92     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.87     |
| TotalNEpisodes       | 18864      |
| TotalNSamples        | 4.2138e+06 |
| ExplainedVariance    | -0.0037285 |
-------------------------------------
[2018-12-22 11:33:25.844460 UTC] Saving snapshot
[2018-12-22 11:33:25.844717 UTC] Starting iteration 843
[2018-12-22 11:33:25.844838 UTC] Start collecting samples
[2018-12-22 11:33:28.771024 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:28.849202 UTC] Performing policy update
[2018-12-22 11:33:28.849832 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:28.938522 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:29.998523 UTC] Performing line search
[2018-12-22 11:33:30.127050 UTC] Updating baseline
[2018-12-22 11:33:32.144943 UTC] Computing logging information
-------------------------------------
| Iteration            | 843        |
| ExpectedImprovement  | 0.016786   |
| ActualImprovement    | 0.01601    |
| ImprovementRatio     | 0.95379    |
| MeanKL               | 0.0073339  |
| Entropy              | -0.6002    |
| Perplexity           | 0.5487     |
| AveragePolicyStd     | 0.22201    |
| AveragePolicyStd[0]  | 0.25093    |
| AveragePolicyStd[1]  | 0.24757    |
| AveragePolicyStd[2]  | 0.18408    |
| AveragePolicyStd[3]  | 0.23433    |
| AveragePolicyStd[4]  | 0.16301    |
| AveragePolicyStd[5]  | 0.25213    |
| AverageReturn        | 1539.8     |
| MinReturn            | 115.37     |
| MaxReturn            | 1749.1     |
| StdReturn            | 398.72     |
| AverageEpisodeLength | 912.92     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.87     |
| TotalNEpisodes       | 18869      |
| TotalNSamples        | 4.2188e+06 |
| ExplainedVariance    | -0.0002651 |
-------------------------------------
[2018-12-22 11:33:32.529591 UTC] Saving snapshot
[2018-12-22 11:33:32.529881 UTC] Starting iteration 844
[2018-12-22 11:33:32.530005 UTC] Start collecting samples
[2018-12-22 11:33:35.458032 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:35.535719 UTC] Performing policy update
[2018-12-22 11:33:35.536594 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:35.626178 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:36.692653 UTC] Performing line search
[2018-12-22 11:33:36.819998 UTC] Updating baseline
[2018-12-22 11:33:38.070903 UTC] Computing logging information
-------------------------------------
| Iteration            | 844        |
| ExpectedImprovement  | 0.016117   |
| ActualImprovement    | 0.014993   |
| ImprovementRatio     | 0.93029    |
| MeanKL               | 0.0080204  |
| Entropy              | -0.61008   |
| Perplexity           | 0.54331    |
| AveragePolicyStd     | 0.22169    |
| AveragePolicyStd[0]  | 0.25132    |
| AveragePolicyStd[1]  | 0.24727    |
| AveragePolicyStd[2]  | 0.18245    |
| AveragePolicyStd[3]  | 0.23374    |
| AveragePolicyStd[4]  | 0.16313    |
| AveragePolicyStd[5]  | 0.25223    |
| AverageReturn        | 1541.2     |
| MinReturn            | 115.37     |
| MaxReturn            | 1749.1     |
| StdReturn            | 399.22     |
| AverageEpisodeLength | 912.92     |
| MinEpisodeLength     | 102        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.87     |
| TotalNEpisodes       | 18874      |
| TotalNSamples        | 4.2238e+06 |
| ExplainedVariance    | -0.022446  |
-------------------------------------
[2018-12-22 11:33:38.450923 UTC] Saving snapshot
[2018-12-22 11:33:38.451349 UTC] Starting iteration 845
[2018-12-22 11:33:38.451570 UTC] Start collecting samples
[2018-12-22 11:33:41.376595 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:41.454045 UTC] Performing policy update
[2018-12-22 11:33:41.454702 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:41.543903 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:42.611898 UTC] Performing line search
[2018-12-22 11:33:42.738347 UTC] Updating baseline
[2018-12-22 11:33:44.101173 UTC] Computing logging information
-------------------------------------
| Iteration            | 845        |
| ExpectedImprovement  | 0.018778   |
| ActualImprovement    | 0.01773    |
| ImprovementRatio     | 0.94422    |
| MeanKL               | 0.0070928  |
| Entropy              | -0.61243   |
| Perplexity           | 0.54203    |
| AveragePolicyStd     | 0.22164    |
| AveragePolicyStd[0]  | 0.2527     |
| AveragePolicyStd[1]  | 0.24614    |
| AveragePolicyStd[2]  | 0.18201    |
| AveragePolicyStd[3]  | 0.23476    |
| AveragePolicyStd[4]  | 0.16285    |
| AveragePolicyStd[5]  | 0.25137    |
| AverageReturn        | 1537.3     |
| MinReturn            | 101.25     |
| MaxReturn            | 1749.1     |
| StdReturn            | 409.09     |
| AverageEpisodeLength | 911.06     |
| MinEpisodeLength     | 97         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.74     |
| TotalNEpisodes       | 18878      |
| TotalNSamples        | 4.2269e+06 |
| ExplainedVariance    | 0.20413    |
-------------------------------------
[2018-12-22 11:33:44.488466 UTC] Saving snapshot
[2018-12-22 11:33:44.488730 UTC] Starting iteration 846
[2018-12-22 11:33:44.488854 UTC] Start collecting samples
[2018-12-22 11:33:47.470048 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:47.552252 UTC] Performing policy update
[2018-12-22 11:33:47.552899 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:47.641672 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:48.707692 UTC] Performing line search
[2018-12-22 11:33:48.834534 UTC] Updating baseline
[2018-12-22 11:33:50.009216 UTC] Computing logging information
------------------------------------
| Iteration            | 846       |
| ExpectedImprovement  | 0.016801  |
| ActualImprovement    | 0.015987  |
| ImprovementRatio     | 0.95158   |
| MeanKL               | 0.007481  |
| Entropy              | -0.61423  |
| Perplexity           | 0.54106   |
| AveragePolicyStd     | 0.22157   |
| AveragePolicyStd[0]  | 0.25287   |
| AveragePolicyStd[1]  | 0.24592   |
| AveragePolicyStd[2]  | 0.18126   |
| AveragePolicyStd[3]  | 0.2346    |
| AveragePolicyStd[4]  | 0.1633    |
| AveragePolicyStd[5]  | 0.25149   |
| AverageReturn        | 1522.2    |
| MinReturn            | 101.25    |
| MaxReturn            | 1749.1    |
| StdReturn            | 432.35    |
| AverageEpisodeLength | 903.11    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 245.84    |
| TotalNEpisodes       | 18887     |
| TotalNSamples        | 4.235e+06 |
| ExplainedVariance    | 0.037675  |
------------------------------------
[2018-12-22 11:33:50.392942 UTC] Saving snapshot
[2018-12-22 11:33:50.393216 UTC] Starting iteration 847
[2018-12-22 11:33:50.393338 UTC] Start collecting samples
[2018-12-22 11:33:53.281997 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:53.361303 UTC] Performing policy update
[2018-12-22 11:33:53.362104 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:53.454663 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:33:54.521608 UTC] Performing line search
[2018-12-22 11:33:54.648720 UTC] Updating baseline
[2018-12-22 11:33:56.266295 UTC] Computing logging information
------------------------------------
| Iteration            | 847       |
| ExpectedImprovement  | 0.014849  |
| ActualImprovement    | 0.014255  |
| ImprovementRatio     | 0.95998   |
| MeanKL               | 0.0082379 |
| Entropy              | -0.61488  |
| Perplexity           | 0.5407    |
| AveragePolicyStd     | 0.22157   |
| AveragePolicyStd[0]  | 0.25302   |
| AveragePolicyStd[1]  | 0.24605   |
| AveragePolicyStd[2]  | 0.18094   |
| AveragePolicyStd[3]  | 0.23472   |
| AveragePolicyStd[4]  | 0.16318   |
| AveragePolicyStd[5]  | 0.25154   |
| AverageReturn        | 1522.6    |
| MinReturn            | 101.25    |
| MaxReturn            | 1749.1    |
| StdReturn            | 432.5     |
| AverageEpisodeLength | 903.11    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 245.84    |
| TotalNEpisodes       | 18890     |
| TotalNSamples        | 4.238e+06 |
| ExplainedVariance    | -0.041476 |
------------------------------------
[2018-12-22 11:33:56.651135 UTC] Saving snapshot
[2018-12-22 11:33:56.651374 UTC] Starting iteration 848
[2018-12-22 11:33:56.651511 UTC] Start collecting samples
[2018-12-22 11:33:59.579983 UTC] Computing input variables for policy optimization
[2018-12-22 11:33:59.657183 UTC] Performing policy update
[2018-12-22 11:33:59.658070 UTC] Computing gradient in Euclidean space
[2018-12-22 11:33:59.749225 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:00.811062 UTC] Performing line search
[2018-12-22 11:34:00.940214 UTC] Updating baseline
[2018-12-22 11:34:03.137826 UTC] Computing logging information
------------------------------------
| Iteration            | 848       |
| ExpectedImprovement  | 0.0183    |
| ActualImprovement    | 0.017375  |
| ImprovementRatio     | 0.94945   |
| MeanKL               | 0.0077798 |
| Entropy              | -0.60963  |
| Perplexity           | 0.54355   |
| AveragePolicyStd     | 0.22175   |
| AveragePolicyStd[0]  | 0.25335   |
| AveragePolicyStd[1]  | 0.24654   |
| AveragePolicyStd[2]  | 0.18148   |
| AveragePolicyStd[3]  | 0.2351    |
| AveragePolicyStd[4]  | 0.16318   |
| AveragePolicyStd[5]  | 0.25087   |
| AverageReturn        | 1550.1    |
| MinReturn            | 101.25    |
| MaxReturn            | 1749.1    |
| StdReturn            | 403.03    |
| AverageEpisodeLength | 920.21    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 229.28    |
| TotalNEpisodes       | 18895     |
| TotalNSamples        | 4.243e+06 |
| ExplainedVariance    | 0.0012089 |
------------------------------------
[2018-12-22 11:34:03.522410 UTC] Saving snapshot
[2018-12-22 11:34:03.522686 UTC] Starting iteration 849
[2018-12-22 11:34:03.522818 UTC] Start collecting samples
[2018-12-22 11:34:06.494485 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:06.573864 UTC] Performing policy update
[2018-12-22 11:34:06.574439 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:06.665905 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:07.726095 UTC] Performing line search
[2018-12-22 11:34:07.854468 UTC] Updating baseline
[2018-12-22 11:34:09.363725 UTC] Computing logging information
------------------------------------
| Iteration            | 849       |
| ExpectedImprovement  | 0.017112  |
| ActualImprovement    | 0.015987  |
| ImprovementRatio     | 0.93429   |
| MeanKL               | 0.0076941 |
| Entropy              | -0.60961  |
| Perplexity           | 0.54356   |
| AveragePolicyStd     | 0.22177   |
| AveragePolicyStd[0]  | 0.25404   |
| AveragePolicyStd[1]  | 0.24698   |
| AveragePolicyStd[2]  | 0.18173   |
| AveragePolicyStd[3]  | 0.23446   |
| AveragePolicyStd[4]  | 0.16291   |
| AveragePolicyStd[5]  | 0.25051   |
| AverageReturn        | 1547.6    |
| MinReturn            | 101.25    |
| MaxReturn            | 1749.1    |
| StdReturn            | 402.12    |
| AverageEpisodeLength | 920.21    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 229.28    |
| TotalNEpisodes       | 18902     |
| TotalNSamples        | 4.25e+06  |
| ExplainedVariance    | -0.010741 |
------------------------------------
[2018-12-22 11:34:09.748832 UTC] Saving snapshot
[2018-12-22 11:34:09.749078 UTC] Starting iteration 850
[2018-12-22 11:34:09.749198 UTC] Start collecting samples
[2018-12-22 11:34:12.677561 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:12.755240 UTC] Performing policy update
[2018-12-22 11:34:12.756197 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:12.845424 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:13.911370 UTC] Performing line search
[2018-12-22 11:34:14.042883 UTC] Updating baseline
[2018-12-22 11:34:15.443176 UTC] Computing logging information
------------------------------------
| Iteration            | 850       |
| ExpectedImprovement  | 0.016477  |
| ActualImprovement    | 0.014989  |
| ImprovementRatio     | 0.90973   |
| MeanKL               | 0.0072334 |
| Entropy              | -0.6119   |
| Perplexity           | 0.54232   |
| AveragePolicyStd     | 0.22175   |
| AveragePolicyStd[0]  | 0.2549    |
| AveragePolicyStd[1]  | 0.24595   |
| AveragePolicyStd[2]  | 0.18146   |
| AveragePolicyStd[3]  | 0.23479   |
| AveragePolicyStd[4]  | 0.16234   |
| AveragePolicyStd[5]  | 0.25103   |
| AverageReturn        | 1556.1    |
| MinReturn            | 101.25    |
| MaxReturn            | 1749.1    |
| StdReturn            | 396.85    |
| AverageEpisodeLength | 924.54    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 226.22    |
| TotalNEpisodes       | 18906     |
| TotalNSamples        | 4.254e+06 |
| ExplainedVariance    | 0.14231   |
------------------------------------
[2018-12-22 11:34:15.834012 UTC] Saving snapshot
[2018-12-22 11:34:15.842168 UTC] Starting iteration 851
[2018-12-22 11:34:15.842383 UTC] Start collecting samples
[2018-12-22 11:34:18.797629 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:18.877879 UTC] Performing policy update
[2018-12-22 11:34:18.878551 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:18.969028 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:20.031457 UTC] Performing line search
[2018-12-22 11:34:20.159556 UTC] Updating baseline
[2018-12-22 11:34:21.311437 UTC] Computing logging information
-------------------------------------
| Iteration            | 851        |
| ExpectedImprovement  | 0.015497   |
| ActualImprovement    | 0.014805   |
| ImprovementRatio     | 0.95534    |
| MeanKL               | 0.0079839  |
| Entropy              | -0.6046    |
| Perplexity           | 0.54629    |
| AveragePolicyStd     | 0.22203    |
| AveragePolicyStd[0]  | 0.25577    |
| AveragePolicyStd[1]  | 0.24588    |
| AveragePolicyStd[2]  | 0.18187    |
| AveragePolicyStd[3]  | 0.23535    |
| AveragePolicyStd[4]  | 0.16227    |
| AveragePolicyStd[5]  | 0.25101    |
| AverageReturn        | 1565.7     |
| MinReturn            | 101.25     |
| MaxReturn            | 1749.1     |
| StdReturn            | 393.65     |
| AverageEpisodeLength | 930.2      |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.19     |
| TotalNEpisodes       | 18912      |
| TotalNSamples        | 4.2596e+06 |
| ExplainedVariance    | 0.12368    |
-------------------------------------
[2018-12-22 11:34:21.695475 UTC] Saving snapshot
[2018-12-22 11:34:21.695744 UTC] Starting iteration 852
[2018-12-22 11:34:21.695866 UTC] Start collecting samples
[2018-12-22 11:34:24.668403 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:24.748733 UTC] Performing policy update
[2018-12-22 11:34:24.749526 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:24.840813 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:25.924841 UTC] Performing line search
[2018-12-22 11:34:26.052864 UTC] Updating baseline
[2018-12-22 11:34:27.487911 UTC] Computing logging information
------------------------------------
| Iteration            | 852       |
| ExpectedImprovement  | 0.017265  |
| ActualImprovement    | 0.016076  |
| ImprovementRatio     | 0.93113   |
| MeanKL               | 0.0081697 |
| Entropy              | -0.60531  |
| Perplexity           | 0.54591   |
| AveragePolicyStd     | 0.22208   |
| AveragePolicyStd[0]  | 0.25562   |
| AveragePolicyStd[1]  | 0.24633   |
| AveragePolicyStd[2]  | 0.18194   |
| AveragePolicyStd[3]  | 0.23563   |
| AveragePolicyStd[4]  | 0.16126   |
| AveragePolicyStd[5]  | 0.25171   |
| AverageReturn        | 1556.7    |
| MinReturn            | 101.25    |
| MaxReturn            | 1749.1    |
| StdReturn            | 402.31    |
| AverageEpisodeLength | 924.95    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 228.59    |
| TotalNEpisodes       | 18918     |
| TotalNSamples        | 4.265e+06 |
| ExplainedVariance    | 0.10997   |
------------------------------------
[2018-12-22 11:34:27.872144 UTC] Saving snapshot
[2018-12-22 11:34:27.872401 UTC] Starting iteration 853
[2018-12-22 11:34:27.872534 UTC] Start collecting samples
[2018-12-22 11:34:30.787390 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:30.865162 UTC] Performing policy update
[2018-12-22 11:34:30.865794 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:30.954588 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:32.018921 UTC] Performing line search
[2018-12-22 11:34:32.144619 UTC] Updating baseline
[2018-12-22 11:34:33.476191 UTC] Computing logging information
--------------------------------------
| Iteration            | 853         |
| ExpectedImprovement  | 0.015786    |
| ActualImprovement    | 0.014692    |
| ImprovementRatio     | 0.93071     |
| MeanKL               | 0.0074005   |
| Entropy              | -0.61097    |
| Perplexity           | 0.54282     |
| AveragePolicyStd     | 0.22191     |
| AveragePolicyStd[0]  | 0.25654     |
| AveragePolicyStd[1]  | 0.24556     |
| AveragePolicyStd[2]  | 0.18189     |
| AveragePolicyStd[3]  | 0.23499     |
| AveragePolicyStd[4]  | 0.16077     |
| AveragePolicyStd[5]  | 0.25167     |
| AverageReturn        | 1556.1      |
| MinReturn            | 101.25      |
| MaxReturn            | 1749.1      |
| StdReturn            | 402.13      |
| AverageEpisodeLength | 924.95      |
| MinEpisodeLength     | 86          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 228.59      |
| TotalNEpisodes       | 18921       |
| TotalNSamples        | 4.268e+06   |
| ExplainedVariance    | -0.00085877 |
--------------------------------------
[2018-12-22 11:34:33.866636 UTC] Saving snapshot
[2018-12-22 11:34:33.866899 UTC] Starting iteration 854
[2018-12-22 11:34:33.867022 UTC] Start collecting samples
[2018-12-22 11:34:36.854100 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:36.933525 UTC] Performing policy update
[2018-12-22 11:34:36.934231 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:37.023723 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:38.078015 UTC] Performing line search
[2018-12-22 11:34:38.205234 UTC] Updating baseline
[2018-12-22 11:34:39.460648 UTC] Computing logging information
------------------------------------
| Iteration            | 854       |
| ExpectedImprovement  | 0.015796  |
| ActualImprovement    | 0.015076  |
| ImprovementRatio     | 0.95444   |
| MeanKL               | 0.0075456 |
| Entropy              | -0.61512  |
| Perplexity           | 0.54058   |
| AveragePolicyStd     | 0.22174   |
| AveragePolicyStd[0]  | 0.25699   |
| AveragePolicyStd[1]  | 0.24563   |
| AveragePolicyStd[2]  | 0.1815    |
| AveragePolicyStd[3]  | 0.23439   |
| AveragePolicyStd[4]  | 0.16104   |
| AveragePolicyStd[5]  | 0.2509    |
| AverageReturn        | 1599.8    |
| MinReturn            | 101.25    |
| MaxReturn            | 1749.1    |
| StdReturn            | 323.82    |
| AverageEpisodeLength | 950.39    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 183.88    |
| TotalNEpisodes       | 18928     |
| TotalNSamples        | 4.275e+06 |
| ExplainedVariance    | -0.005745 |
------------------------------------
[2018-12-22 11:34:39.849912 UTC] Saving snapshot
[2018-12-22 11:34:39.850225 UTC] Starting iteration 855
[2018-12-22 11:34:39.850352 UTC] Start collecting samples
[2018-12-22 11:34:42.792377 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:42.869189 UTC] Performing policy update
[2018-12-22 11:34:42.869866 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:42.958885 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:44.023200 UTC] Performing line search
[2018-12-22 11:34:44.151228 UTC] Updating baseline
[2018-12-22 11:34:45.957541 UTC] Computing logging information
-------------------------------------
| Iteration            | 855        |
| ExpectedImprovement  | 0.018231   |
| ActualImprovement    | 0.01715    |
| ImprovementRatio     | 0.94067    |
| MeanKL               | 0.007129   |
| Entropy              | -0.61293   |
| Perplexity           | 0.54176    |
| AveragePolicyStd     | 0.2218     |
| AveragePolicyStd[0]  | 0.25705    |
| AveragePolicyStd[1]  | 0.24525    |
| AveragePolicyStd[2]  | 0.18183    |
| AveragePolicyStd[3]  | 0.23438    |
| AveragePolicyStd[4]  | 0.16119    |
| AveragePolicyStd[5]  | 0.25107    |
| AverageReturn        | 1594.4     |
| MinReturn            | 101.25     |
| MaxReturn            | 1749.1     |
| StdReturn            | 325.58     |
| AverageEpisodeLength | 947.84     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.94     |
| TotalNEpisodes       | 18933      |
| TotalNSamples        | 4.2798e+06 |
| ExplainedVariance    | 0.10345    |
-------------------------------------
[2018-12-22 11:34:46.347562 UTC] Saving snapshot
[2018-12-22 11:34:46.347813 UTC] Starting iteration 856
[2018-12-22 11:34:46.347938 UTC] Start collecting samples
[2018-12-22 11:34:49.276920 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:49.354699 UTC] Performing policy update
[2018-12-22 11:34:49.355285 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:49.444858 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:50.505122 UTC] Performing line search
[2018-12-22 11:34:50.631652 UTC] Updating baseline
[2018-12-22 11:34:51.816355 UTC] Computing logging information
------------------------------------
| Iteration            | 856       |
| ExpectedImprovement  | 0.017615  |
| ActualImprovement    | 0.016608  |
| ImprovementRatio     | 0.94278   |
| MeanKL               | 0.0077912 |
| Entropy              | -0.61981  |
| Perplexity           | 0.53805   |
| AveragePolicyStd     | 0.22162   |
| AveragePolicyStd[0]  | 0.25698   |
| AveragePolicyStd[1]  | 0.24548   |
| AveragePolicyStd[2]  | 0.18135   |
| AveragePolicyStd[3]  | 0.23408   |
| AveragePolicyStd[4]  | 0.16034   |
| AveragePolicyStd[5]  | 0.25153   |
| AverageReturn        | 1587.2    |
| MinReturn            | 101.25    |
| MaxReturn            | 1747.3    |
| StdReturn            | 345.18    |
| AverageEpisodeLength | 944.02    |
| MinEpisodeLength     | 86        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 196.26    |
| TotalNEpisodes       | 18937     |
| TotalNSamples        | 4.283e+06 |
| ExplainedVariance    | 0.18146   |
------------------------------------
[2018-12-22 11:34:52.199982 UTC] Saving snapshot
[2018-12-22 11:34:52.200226 UTC] Starting iteration 857
[2018-12-22 11:34:52.200354 UTC] Start collecting samples
[2018-12-22 11:34:55.179801 UTC] Computing input variables for policy optimization
[2018-12-22 11:34:55.258214 UTC] Performing policy update
[2018-12-22 11:34:55.259057 UTC] Computing gradient in Euclidean space
[2018-12-22 11:34:55.349210 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:34:56.412397 UTC] Performing line search
[2018-12-22 11:34:56.540486 UTC] Updating baseline
[2018-12-22 11:34:57.978621 UTC] Computing logging information
-------------------------------------
| Iteration            | 857        |
| ExpectedImprovement  | 0.017877   |
| ActualImprovement    | 0.017037   |
| ImprovementRatio     | 0.95302    |
| MeanKL               | 0.0074327  |
| Entropy              | -0.62503   |
| Perplexity           | 0.53524    |
| AveragePolicyStd     | 0.22138    |
| AveragePolicyStd[0]  | 0.25606    |
| AveragePolicyStd[1]  | 0.24486    |
| AveragePolicyStd[2]  | 0.18076    |
| AveragePolicyStd[3]  | 0.23471    |
| AveragePolicyStd[4]  | 0.16092    |
| AveragePolicyStd[5]  | 0.25098    |
| AverageReturn        | 1582       |
| MinReturn            | 101.25     |
| MaxReturn            | 1747.3     |
| StdReturn            | 346.83     |
| AverageEpisodeLength | 941.58     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.07     |
| TotalNEpisodes       | 18945      |
| TotalNSamples        | 4.2907e+06 |
| ExplainedVariance    | 0.068771   |
-------------------------------------
[2018-12-22 11:34:58.365268 UTC] Saving snapshot
[2018-12-22 11:34:58.365527 UTC] Starting iteration 858
[2018-12-22 11:34:58.365649 UTC] Start collecting samples
[2018-12-22 11:35:01.277713 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:01.356998 UTC] Performing policy update
[2018-12-22 11:35:01.357589 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:01.447519 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:02.519918 UTC] Performing line search
[2018-12-22 11:35:02.645785 UTC] Updating baseline
[2018-12-22 11:35:04.174843 UTC] Computing logging information
-------------------------------------
| Iteration            | 858        |
| ExpectedImprovement  | 0.017295   |
| ActualImprovement    | 0.016187   |
| ImprovementRatio     | 0.93592    |
| MeanKL               | 0.0075873  |
| Entropy              | -0.62525   |
| Perplexity           | 0.53513    |
| AveragePolicyStd     | 0.22143    |
| AveragePolicyStd[0]  | 0.25682    |
| AveragePolicyStd[1]  | 0.24546    |
| AveragePolicyStd[2]  | 0.18024    |
| AveragePolicyStd[3]  | 0.23482    |
| AveragePolicyStd[4]  | 0.16062    |
| AveragePolicyStd[5]  | 0.25063    |
| AverageReturn        | 1581.3     |
| MinReturn            | 101.25     |
| MaxReturn            | 1747.3     |
| StdReturn            | 346.6      |
| AverageEpisodeLength | 941.58     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.07     |
| TotalNEpisodes       | 18948      |
| TotalNSamples        | 4.2937e+06 |
| ExplainedVariance    | 0.033881   |
-------------------------------------
[2018-12-22 11:35:04.564153 UTC] Saving snapshot
[2018-12-22 11:35:04.564407 UTC] Starting iteration 859
[2018-12-22 11:35:04.564540 UTC] Start collecting samples
[2018-12-22 11:35:07.507697 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:07.586197 UTC] Performing policy update
[2018-12-22 11:35:07.587043 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:07.675782 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:08.735209 UTC] Performing line search
[2018-12-22 11:35:08.863103 UTC] Updating baseline
[2018-12-22 11:35:10.370709 UTC] Computing logging information
-------------------------------------
| Iteration            | 859        |
| ExpectedImprovement  | 0.018461   |
| ActualImprovement    | 0.017711   |
| ImprovementRatio     | 0.95935    |
| MeanKL               | 0.0076999  |
| Entropy              | -0.62619   |
| Perplexity           | 0.53462    |
| AveragePolicyStd     | 0.22138    |
| AveragePolicyStd[0]  | 0.25684    |
| AveragePolicyStd[1]  | 0.24539    |
| AveragePolicyStd[2]  | 0.18047    |
| AveragePolicyStd[3]  | 0.23483    |
| AveragePolicyStd[4]  | 0.16059    |
| AveragePolicyStd[5]  | 0.25017    |
| AverageReturn        | 1584.7     |
| MinReturn            | 101.25     |
| MaxReturn            | 1747.3     |
| StdReturn            | 333.31     |
| AverageEpisodeLength | 944.7      |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.95     |
| TotalNEpisodes       | 18953      |
| TotalNSamples        | 4.2983e+06 |
| ExplainedVariance    | 0.14294    |
-------------------------------------
[2018-12-22 11:35:10.754303 UTC] Saving snapshot
[2018-12-22 11:35:10.754564 UTC] Starting iteration 860
[2018-12-22 11:35:10.754687 UTC] Start collecting samples
[2018-12-22 11:35:13.716890 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:13.798176 UTC] Performing policy update
[2018-12-22 11:35:13.799154 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:13.890287 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:14.957791 UTC] Performing line search
[2018-12-22 11:35:15.086137 UTC] Updating baseline
[2018-12-22 11:35:16.441677 UTC] Computing logging information
-------------------------------------
| Iteration            | 860        |
| ExpectedImprovement  | 0.018528   |
| ActualImprovement    | 0.017112   |
| ImprovementRatio     | 0.92355    |
| MeanKL               | 0.0074239  |
| Entropy              | -0.64073   |
| Perplexity           | 0.52691    |
| AveragePolicyStd     | 0.22081    |
| AveragePolicyStd[0]  | 0.25533    |
| AveragePolicyStd[1]  | 0.24483    |
| AveragePolicyStd[2]  | 0.18037    |
| AveragePolicyStd[3]  | 0.23468    |
| AveragePolicyStd[4]  | 0.16023    |
| AveragePolicyStd[5]  | 0.24945    |
| AverageReturn        | 1602.8     |
| MinReturn            | 101.25     |
| MaxReturn            | 1747.3     |
| StdReturn            | 298.73     |
| AverageEpisodeLength | 954.96     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.83     |
| TotalNEpisodes       | 18959      |
| TotalNSamples        | 4.3043e+06 |
| ExplainedVariance    | -0.041203  |
-------------------------------------
[2018-12-22 11:35:16.827911 UTC] Saving snapshot
[2018-12-22 11:35:16.836686 UTC] Starting iteration 861
[2018-12-22 11:35:16.836883 UTC] Start collecting samples
[2018-12-22 11:35:19.780286 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:19.859555 UTC] Performing policy update
[2018-12-22 11:35:19.860292 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:19.954028 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:21.016563 UTC] Performing line search
[2018-12-22 11:35:21.143373 UTC] Updating baseline
[2018-12-22 11:35:22.924120 UTC] Computing logging information
-------------------------------------
| Iteration            | 861        |
| ExpectedImprovement  | 0.020137   |
| ActualImprovement    | 0.018161   |
| ImprovementRatio     | 0.90191    |
| MeanKL               | 0.0071066  |
| Entropy              | -0.642     |
| Perplexity           | 0.52624    |
| AveragePolicyStd     | 0.2208     |
| AveragePolicyStd[0]  | 0.25573    |
| AveragePolicyStd[1]  | 0.24507    |
| AveragePolicyStd[2]  | 0.17999    |
| AveragePolicyStd[3]  | 0.2343     |
| AveragePolicyStd[4]  | 0.16013    |
| AveragePolicyStd[5]  | 0.24957    |
| AverageReturn        | 1601.1     |
| MinReturn            | 101.25     |
| MaxReturn            | 1747.3     |
| StdReturn            | 298.26     |
| AverageEpisodeLength | 954.96     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.83     |
| TotalNEpisodes       | 18963      |
| TotalNSamples        | 4.3083e+06 |
| ExplainedVariance    | 0.029976   |
-------------------------------------
[2018-12-22 11:35:23.306751 UTC] Saving snapshot
[2018-12-22 11:35:23.306996 UTC] Starting iteration 862
[2018-12-22 11:35:23.307111 UTC] Start collecting samples
[2018-12-22 11:35:26.277856 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:26.361089 UTC] Performing policy update
[2018-12-22 11:35:26.361688 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:26.452400 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:27.518325 UTC] Performing line search
[2018-12-22 11:35:27.646413 UTC] Updating baseline
[2018-12-22 11:35:29.186152 UTC] Computing logging information
-------------------------------------
| Iteration            | 862        |
| ExpectedImprovement  | 0.01848    |
| ActualImprovement    | 0.017586   |
| ImprovementRatio     | 0.95161    |
| MeanKL               | 0.0072709  |
| Entropy              | -0.64558   |
| Perplexity           | 0.52436    |
| AveragePolicyStd     | 0.22065    |
| AveragePolicyStd[0]  | 0.25574    |
| AveragePolicyStd[1]  | 0.24509    |
| AveragePolicyStd[2]  | 0.17972    |
| AveragePolicyStd[3]  | 0.23374    |
| AveragePolicyStd[4]  | 0.16036    |
| AveragePolicyStd[5]  | 0.24927    |
| AverageReturn        | 1588.3     |
| MinReturn            | 101.25     |
| MaxReturn            | 1747.3     |
| StdReturn            | 317.96     |
| AverageEpisodeLength | 948.04     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.61     |
| TotalNEpisodes       | 18969      |
| TotalNSamples        | 4.3136e+06 |
| ExplainedVariance    | 0.092799   |
-------------------------------------
[2018-12-22 11:35:29.571148 UTC] Saving snapshot
[2018-12-22 11:35:29.571394 UTC] Starting iteration 863
[2018-12-22 11:35:29.571532 UTC] Start collecting samples
[2018-12-22 11:35:32.560691 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:32.640944 UTC] Performing policy update
[2018-12-22 11:35:32.641706 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:32.730739 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:33.780626 UTC] Performing line search
[2018-12-22 11:35:33.908847 UTC] Updating baseline
[2018-12-22 11:35:35.255020 UTC] Computing logging information
-------------------------------------
| Iteration            | 863        |
| ExpectedImprovement  | 0.017846   |
| ActualImprovement    | 0.016855   |
| ImprovementRatio     | 0.94447    |
| MeanKL               | 0.0078069  |
| Entropy              | -0.64417   |
| Perplexity           | 0.5251     |
| AveragePolicyStd     | 0.22073    |
| AveragePolicyStd[0]  | 0.25666    |
| AveragePolicyStd[1]  | 0.24546    |
| AveragePolicyStd[2]  | 0.17975    |
| AveragePolicyStd[3]  | 0.23319    |
| AveragePolicyStd[4]  | 0.1602     |
| AveragePolicyStd[5]  | 0.24914    |
| AverageReturn        | 1588.5     |
| MinReturn            | 101.25     |
| MaxReturn            | 1747.3     |
| StdReturn            | 318.04     |
| AverageEpisodeLength | 948.04     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.61     |
| TotalNEpisodes       | 18975      |
| TotalNSamples        | 4.3196e+06 |
| ExplainedVariance    | -0.036002  |
-------------------------------------
[2018-12-22 11:35:35.634951 UTC] Saving snapshot
[2018-12-22 11:35:35.635189 UTC] Starting iteration 864
[2018-12-22 11:35:35.635308 UTC] Start collecting samples
[2018-12-22 11:35:38.564868 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:38.642891 UTC] Performing policy update
[2018-12-22 11:35:38.643658 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:38.735321 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:39.803580 UTC] Performing line search
[2018-12-22 11:35:39.930974 UTC] Updating baseline
[2018-12-22 11:35:41.464950 UTC] Computing logging information
-------------------------------------
| Iteration            | 864        |
| ExpectedImprovement  | 0.017723   |
| ActualImprovement    | 0.016676   |
| ImprovementRatio     | 0.94092    |
| MeanKL               | 0.0072973  |
| Entropy              | -0.64899   |
| Perplexity           | 0.52257    |
| AveragePolicyStd     | 0.22056    |
| AveragePolicyStd[0]  | 0.25686    |
| AveragePolicyStd[1]  | 0.24539    |
| AveragePolicyStd[2]  | 0.17912    |
| AveragePolicyStd[3]  | 0.23303    |
| AveragePolicyStd[4]  | 0.16039    |
| AveragePolicyStd[5]  | 0.24855    |
| AverageReturn        | 1604.7     |
| MinReturn            | 114.47     |
| MaxReturn            | 1747.3     |
| StdReturn            | 280.95     |
| AverageEpisodeLength | 957.07     |
| MinEpisodeLength     | 86         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.13     |
| TotalNEpisodes       | 18978      |
| TotalNSamples        | 4.3226e+06 |
| ExplainedVariance    | -0.0010578 |
-------------------------------------
[2018-12-22 11:35:41.851380 UTC] Saving snapshot
[2018-12-22 11:35:41.851645 UTC] Starting iteration 865
[2018-12-22 11:35:41.851783 UTC] Start collecting samples
[2018-12-22 11:35:44.800149 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:44.879282 UTC] Performing policy update
[2018-12-22 11:35:44.880002 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:44.971736 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:46.040585 UTC] Performing line search
[2018-12-22 11:35:46.169042 UTC] Updating baseline
[2018-12-22 11:35:47.696542 UTC] Computing logging information
-------------------------------------
| Iteration            | 865        |
| ExpectedImprovement  | 0.017023   |
| ActualImprovement    | 0.016339   |
| ImprovementRatio     | 0.95983    |
| MeanKL               | 0.0072886  |
| Entropy              | -0.64919   |
| Perplexity           | 0.52247    |
| AveragePolicyStd     | 0.22058    |
| AveragePolicyStd[0]  | 0.25677    |
| AveragePolicyStd[1]  | 0.24465    |
| AveragePolicyStd[2]  | 0.1792     |
| AveragePolicyStd[3]  | 0.23335    |
| AveragePolicyStd[4]  | 0.16002    |
| AveragePolicyStd[5]  | 0.24949    |
| AverageReturn        | 1622       |
| MinReturn            | 265.59     |
| MaxReturn            | 1747.3     |
| StdReturn            | 238.18     |
| AverageEpisodeLength | 966.21     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.93     |
| TotalNEpisodes       | 18984      |
| TotalNSamples        | 4.3286e+06 |
| ExplainedVariance    | 0.0033708  |
-------------------------------------
[2018-12-22 11:35:48.088228 UTC] Saving snapshot
[2018-12-22 11:35:48.088483 UTC] Starting iteration 866
[2018-12-22 11:35:48.088622 UTC] Start collecting samples
[2018-12-22 11:35:51.065502 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:51.146192 UTC] Performing policy update
[2018-12-22 11:35:51.146914 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:51.237726 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:52.308436 UTC] Performing line search
[2018-12-22 11:35:52.435829 UTC] Updating baseline
[2018-12-22 11:35:53.881149 UTC] Computing logging information
-------------------------------------
| Iteration            | 866        |
| ExpectedImprovement  | 0.018269   |
| ActualImprovement    | 0.017166   |
| ImprovementRatio     | 0.9396     |
| MeanKL               | 0.0074204  |
| Entropy              | -0.65641   |
| Perplexity           | 0.51871    |
| AveragePolicyStd     | 0.22034    |
| AveragePolicyStd[0]  | 0.2565     |
| AveragePolicyStd[1]  | 0.24423    |
| AveragePolicyStd[2]  | 0.17899    |
| AveragePolicyStd[3]  | 0.23287    |
| AveragePolicyStd[4]  | 0.15958    |
| AveragePolicyStd[5]  | 0.24985    |
| AverageReturn        | 1612.9     |
| MinReturn            | 265.59     |
| MaxReturn            | 1747.9     |
| StdReturn            | 257.13     |
| AverageEpisodeLength | 960.48     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.3      |
| TotalNEpisodes       | 18991      |
| TotalNSamples        | 4.3351e+06 |
| ExplainedVariance    | 0.18431    |
-------------------------------------
[2018-12-22 11:35:54.271827 UTC] Saving snapshot
[2018-12-22 11:35:54.272067 UTC] Starting iteration 867
[2018-12-22 11:35:54.272181 UTC] Start collecting samples
[2018-12-22 11:35:57.225644 UTC] Computing input variables for policy optimization
[2018-12-22 11:35:57.304441 UTC] Performing policy update
[2018-12-22 11:35:57.305108 UTC] Computing gradient in Euclidean space
[2018-12-22 11:35:57.396088 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:35:58.452113 UTC] Performing line search
[2018-12-22 11:35:58.583221 UTC] Updating baseline
[2018-12-22 11:35:59.943869 UTC] Computing logging information
-------------------------------------
| Iteration            | 867        |
| ExpectedImprovement  | 0.01709    |
| ActualImprovement    | 0.015689   |
| ImprovementRatio     | 0.91803    |
| MeanKL               | 0.0074379  |
| Entropy              | -0.65167   |
| Perplexity           | 0.52118    |
| AveragePolicyStd     | 0.22053    |
| AveragePolicyStd[0]  | 0.25722    |
| AveragePolicyStd[1]  | 0.24425    |
| AveragePolicyStd[2]  | 0.17917    |
| AveragePolicyStd[3]  | 0.233      |
| AveragePolicyStd[4]  | 0.15952    |
| AveragePolicyStd[5]  | 0.25003    |
| AverageReturn        | 1591.9     |
| MinReturn            | 146.17     |
| MaxReturn            | 1747.9     |
| StdReturn            | 301.27     |
| AverageEpisodeLength | 948.2      |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.88     |
| TotalNEpisodes       | 18996      |
| TotalNSamples        | 4.3388e+06 |
| ExplainedVariance    | 0.27821    |
-------------------------------------
[2018-12-22 11:36:00.329514 UTC] Saving snapshot
[2018-12-22 11:36:00.329791 UTC] Starting iteration 868
[2018-12-22 11:36:00.329923 UTC] Start collecting samples
[2018-12-22 11:36:03.304323 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:03.381047 UTC] Performing policy update
[2018-12-22 11:36:03.381637 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:03.472956 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:04.543931 UTC] Performing line search
[2018-12-22 11:36:04.672221 UTC] Updating baseline
[2018-12-22 11:36:06.636108 UTC] Computing logging information
-------------------------------------
| Iteration            | 868        |
| ExpectedImprovement  | 0.018467   |
| ActualImprovement    | 0.017784   |
| ImprovementRatio     | 0.96301    |
| MeanKL               | 0.0077235  |
| Entropy              | -0.66024   |
| Perplexity           | 0.51672    |
| AveragePolicyStd     | 0.22031    |
| AveragePolicyStd[0]  | 0.25739    |
| AveragePolicyStd[1]  | 0.24386    |
| AveragePolicyStd[2]  | 0.17829    |
| AveragePolicyStd[3]  | 0.23289    |
| AveragePolicyStd[4]  | 0.15871    |
| AveragePolicyStd[5]  | 0.25074    |
| AverageReturn        | 1592.1     |
| MinReturn            | 146.17     |
| MaxReturn            | 1747.9     |
| StdReturn            | 301.36     |
| AverageEpisodeLength | 948.2      |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.88     |
| TotalNEpisodes       | 19001      |
| TotalNSamples        | 4.3438e+06 |
| ExplainedVariance    | -0.036073  |
-------------------------------------
[2018-12-22 11:36:07.020434 UTC] Saving snapshot
[2018-12-22 11:36:07.020697 UTC] Starting iteration 869
[2018-12-22 11:36:07.020830 UTC] Start collecting samples
[2018-12-22 11:36:09.958684 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:10.037625 UTC] Performing policy update
[2018-12-22 11:36:10.038628 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:10.127712 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:11.185121 UTC] Performing line search
[2018-12-22 11:36:11.313580 UTC] Updating baseline
[2018-12-22 11:36:12.845786 UTC] Computing logging information
-------------------------------------
| Iteration            | 869        |
| ExpectedImprovement  | 0.016973   |
| ActualImprovement    | 0.015914   |
| ImprovementRatio     | 0.93761    |
| MeanKL               | 0.0080638  |
| Entropy              | -0.66304   |
| Perplexity           | 0.51528    |
| AveragePolicyStd     | 0.2202     |
| AveragePolicyStd[0]  | 0.257      |
| AveragePolicyStd[1]  | 0.24386    |
| AveragePolicyStd[2]  | 0.17839    |
| AveragePolicyStd[3]  | 0.23304    |
| AveragePolicyStd[4]  | 0.15854    |
| AveragePolicyStd[5]  | 0.25038    |
| AverageReturn        | 1592.1     |
| MinReturn            | 146.17     |
| MaxReturn            | 1747.9     |
| StdReturn            | 301.28     |
| AverageEpisodeLength | 948.42     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.93     |
| TotalNEpisodes       | 19006      |
| TotalNSamples        | 4.3488e+06 |
| ExplainedVariance    | 0.0049527  |
-------------------------------------
[2018-12-22 11:36:13.237112 UTC] Saving snapshot
[2018-12-22 11:36:13.237373 UTC] Starting iteration 870
[2018-12-22 11:36:13.237517 UTC] Start collecting samples
[2018-12-22 11:36:16.207579 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:16.288897 UTC] Performing policy update
[2018-12-22 11:36:16.289609 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:16.380193 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:17.436311 UTC] Performing line search
[2018-12-22 11:36:17.563700 UTC] Updating baseline
[2018-12-22 11:36:18.928845 UTC] Computing logging information
-------------------------------------
| Iteration            | 870        |
| ExpectedImprovement  | 0.017195   |
| ActualImprovement    | 0.016736   |
| ImprovementRatio     | 0.9733     |
| MeanKL               | 0.0074386  |
| Entropy              | -0.67056   |
| Perplexity           | 0.51142    |
| AveragePolicyStd     | 0.21992    |
| AveragePolicyStd[0]  | 0.25675    |
| AveragePolicyStd[1]  | 0.24322    |
| AveragePolicyStd[2]  | 0.17776    |
| AveragePolicyStd[3]  | 0.2329     |
| AveragePolicyStd[4]  | 0.15874    |
| AveragePolicyStd[5]  | 0.25013    |
| AverageReturn        | 1603.5     |
| MinReturn            | 146.17     |
| MaxReturn            | 1747.9     |
| StdReturn            | 286.68     |
| AverageEpisodeLength | 954.48     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.44     |
| TotalNEpisodes       | 19013      |
| TotalNSamples        | 4.3555e+06 |
| ExplainedVariance    | 0.18827    |
-------------------------------------
[2018-12-22 11:36:19.308217 UTC] Saving snapshot
[2018-12-22 11:36:19.316541 UTC] Starting iteration 871
[2018-12-22 11:36:19.316735 UTC] Start collecting samples
[2018-12-22 11:36:22.255509 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:22.333092 UTC] Performing policy update
[2018-12-22 11:36:22.333723 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:22.422202 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:23.470524 UTC] Performing line search
[2018-12-22 11:36:23.596038 UTC] Updating baseline
[2018-12-22 11:36:24.957576 UTC] Computing logging information
-------------------------------------
| Iteration            | 871        |
| ExpectedImprovement  | 0.017755   |
| ActualImprovement    | 0.016789   |
| ImprovementRatio     | 0.94562    |
| MeanKL               | 0.0075927  |
| Entropy              | -0.6698    |
| Perplexity           | 0.51181    |
| AveragePolicyStd     | 0.21989    |
| AveragePolicyStd[0]  | 0.25708    |
| AveragePolicyStd[1]  | 0.24249    |
| AveragePolicyStd[2]  | 0.17775    |
| AveragePolicyStd[3]  | 0.23366    |
| AveragePolicyStd[4]  | 0.15934    |
| AveragePolicyStd[5]  | 0.24901    |
| AverageReturn        | 1576.1     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 341.41     |
| AverageEpisodeLength | 938.61     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.16     |
| TotalNEpisodes       | 19018      |
| TotalNSamples        | 4.3589e+06 |
| ExplainedVariance    | 0.22363    |
-------------------------------------
[2018-12-22 11:36:25.344587 UTC] Saving snapshot
[2018-12-22 11:36:25.344874 UTC] Starting iteration 872
[2018-12-22 11:36:25.344992 UTC] Start collecting samples
[2018-12-22 11:36:28.305786 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:28.385509 UTC] Performing policy update
[2018-12-22 11:36:28.386207 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:28.476538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:29.550122 UTC] Performing line search
[2018-12-22 11:36:29.678774 UTC] Updating baseline
[2018-12-22 11:36:31.035420 UTC] Computing logging information
-------------------------------------
| Iteration            | 872        |
| ExpectedImprovement  | 0.018076   |
| ActualImprovement    | 0.016509   |
| ImprovementRatio     | 0.91334    |
| MeanKL               | 0.0071299  |
| Entropy              | -0.66834   |
| Perplexity           | 0.51256    |
| AveragePolicyStd     | 0.21993    |
| AveragePolicyStd[0]  | 0.25688    |
| AveragePolicyStd[1]  | 0.24215    |
| AveragePolicyStd[2]  | 0.17758    |
| AveragePolicyStd[3]  | 0.23375    |
| AveragePolicyStd[4]  | 0.15966    |
| AveragePolicyStd[5]  | 0.24954    |
| AverageReturn        | 1575.6     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 341.23     |
| AverageEpisodeLength | 938.61     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.16     |
| TotalNEpisodes       | 19023      |
| TotalNSamples        | 4.3639e+06 |
| ExplainedVariance    | -0.0014302 |
-------------------------------------
[2018-12-22 11:36:31.424960 UTC] Saving snapshot
[2018-12-22 11:36:31.425206 UTC] Starting iteration 873
[2018-12-22 11:36:31.425339 UTC] Start collecting samples
[2018-12-22 11:36:34.393939 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:34.474206 UTC] Performing policy update
[2018-12-22 11:36:34.474963 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:34.563633 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:35.634111 UTC] Performing line search
[2018-12-22 11:36:35.765927 UTC] Updating baseline
[2018-12-22 11:36:37.124774 UTC] Computing logging information
-------------------------------------
| Iteration            | 873        |
| ExpectedImprovement  | 0.017382   |
| ActualImprovement    | 0.016302   |
| ImprovementRatio     | 0.93786    |
| MeanKL               | 0.0076015  |
| Entropy              | -0.6753    |
| Perplexity           | 0.509      |
| AveragePolicyStd     | 0.21973    |
| AveragePolicyStd[0]  | 0.25695    |
| AveragePolicyStd[1]  | 0.24174    |
| AveragePolicyStd[2]  | 0.17745    |
| AveragePolicyStd[3]  | 0.23325    |
| AveragePolicyStd[4]  | 0.15886    |
| AveragePolicyStd[5]  | 0.25016    |
| AverageReturn        | 1574.9     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 344.49     |
| AverageEpisodeLength | 937.22     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.05     |
| TotalNEpisodes       | 19030      |
| TotalNSamples        | 4.3705e+06 |
| ExplainedVariance    | 0.07658    |
-------------------------------------
[2018-12-22 11:36:37.512800 UTC] Saving snapshot
[2018-12-22 11:36:37.513111 UTC] Starting iteration 874
[2018-12-22 11:36:37.513258 UTC] Start collecting samples
[2018-12-22 11:36:40.446082 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:40.523844 UTC] Performing policy update
[2018-12-22 11:36:40.524445 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:40.613788 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:41.671038 UTC] Performing line search
[2018-12-22 11:36:41.800423 UTC] Updating baseline
[2018-12-22 11:36:43.042605 UTC] Computing logging information
-------------------------------------
| Iteration            | 874        |
| ExpectedImprovement  | 0.019912   |
| ActualImprovement    | 0.018678   |
| ImprovementRatio     | 0.93799    |
| MeanKL               | 0.0074072  |
| Entropy              | -0.67369   |
| Perplexity           | 0.50982    |
| AveragePolicyStd     | 0.21979    |
| AveragePolicyStd[0]  | 0.25749    |
| AveragePolicyStd[1]  | 0.24164    |
| AveragePolicyStd[2]  | 0.17783    |
| AveragePolicyStd[3]  | 0.23284    |
| AveragePolicyStd[4]  | 0.15877    |
| AveragePolicyStd[5]  | 0.25018    |
| AverageReturn        | 1572.6     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 344.61     |
| AverageEpisodeLength | 936.29     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.97     |
| TotalNEpisodes       | 19032      |
| TotalNSamples        | 4.3724e+06 |
| ExplainedVariance    | 0.25412    |
-------------------------------------
[2018-12-22 11:36:43.435594 UTC] Saving snapshot
[2018-12-22 11:36:43.435852 UTC] Starting iteration 875
[2018-12-22 11:36:43.435975 UTC] Start collecting samples
[2018-12-22 11:36:46.394982 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:46.473528 UTC] Performing policy update
[2018-12-22 11:36:46.474160 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:46.563689 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:47.619217 UTC] Performing line search
[2018-12-22 11:36:47.746673 UTC] Updating baseline
[2018-12-22 11:36:49.072251 UTC] Computing logging information
-------------------------------------
| Iteration            | 875        |
| ExpectedImprovement  | 0.018986   |
| ActualImprovement    | 0.017893   |
| ImprovementRatio     | 0.9424     |
| MeanKL               | 0.0073957  |
| Entropy              | -0.67919   |
| Perplexity           | 0.50703    |
| AveragePolicyStd     | 0.21957    |
| AveragePolicyStd[0]  | 0.25715    |
| AveragePolicyStd[1]  | 0.24127    |
| AveragePolicyStd[2]  | 0.17721    |
| AveragePolicyStd[3]  | 0.23271    |
| AveragePolicyStd[4]  | 0.15913    |
| AveragePolicyStd[5]  | 0.24995    |
| AverageReturn        | 1591.4     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 316.66     |
| AverageEpisodeLength | 946.83     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.04     |
| TotalNEpisodes       | 19038      |
| TotalNSamples        | 4.3784e+06 |
| ExplainedVariance    | 0.0020773  |
-------------------------------------
[2018-12-22 11:36:49.455059 UTC] Saving snapshot
[2018-12-22 11:36:49.455336 UTC] Starting iteration 876
[2018-12-22 11:36:49.455457 UTC] Start collecting samples
[2018-12-22 11:36:52.437559 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:52.517310 UTC] Performing policy update
[2018-12-22 11:36:52.518082 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:52.608130 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:53.659341 UTC] Performing line search
[2018-12-22 11:36:53.789929 UTC] Updating baseline
[2018-12-22 11:36:55.135917 UTC] Computing logging information
-------------------------------------
| Iteration            | 876        |
| ExpectedImprovement  | 0.018239   |
| ActualImprovement    | 0.017645   |
| ImprovementRatio     | 0.96739    |
| MeanKL               | 0.0073402  |
| Entropy              | -0.68951   |
| Perplexity           | 0.50182    |
| AveragePolicyStd     | 0.2192     |
| AveragePolicyStd[0]  | 0.25663    |
| AveragePolicyStd[1]  | 0.24028    |
| AveragePolicyStd[2]  | 0.17697    |
| AveragePolicyStd[3]  | 0.23238    |
| AveragePolicyStd[4]  | 0.15879    |
| AveragePolicyStd[5]  | 0.25013    |
| AverageReturn        | 1594.1     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 317.48     |
| AverageEpisodeLength | 946.83     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.04     |
| TotalNEpisodes       | 19046      |
| TotalNSamples        | 4.3864e+06 |
| ExplainedVariance    | -0.019842  |
-------------------------------------
[2018-12-22 11:36:55.516404 UTC] Saving snapshot
[2018-12-22 11:36:55.516736 UTC] Starting iteration 877
[2018-12-22 11:36:55.516870 UTC] Start collecting samples
[2018-12-22 11:36:58.442730 UTC] Computing input variables for policy optimization
[2018-12-22 11:36:58.521111 UTC] Performing policy update
[2018-12-22 11:36:58.521847 UTC] Computing gradient in Euclidean space
[2018-12-22 11:36:58.611557 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:36:59.669312 UTC] Performing line search
[2018-12-22 11:36:59.801795 UTC] Updating baseline
[2018-12-22 11:37:01.494523 UTC] Computing logging information
-------------------------------------
| Iteration            | 877        |
| ExpectedImprovement  | 0.018197   |
| ActualImprovement    | 0.016347   |
| ImprovementRatio     | 0.89831    |
| MeanKL               | 0.0073704  |
| Entropy              | -0.69924   |
| Perplexity           | 0.49696    |
| AveragePolicyStd     | 0.21893    |
| AveragePolicyStd[0]  | 0.25682    |
| AveragePolicyStd[1]  | 0.23919    |
| AveragePolicyStd[2]  | 0.17648    |
| AveragePolicyStd[3]  | 0.23209    |
| AveragePolicyStd[4]  | 0.15787    |
| AveragePolicyStd[5]  | 0.25112    |
| AverageReturn        | 1589.9     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 319.3      |
| AverageEpisodeLength | 944.38     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.97     |
| TotalNEpisodes       | 19049      |
| TotalNSamples        | 4.3892e+06 |
| ExplainedVariance    | 0.17871    |
-------------------------------------
[2018-12-22 11:37:01.883532 UTC] Saving snapshot
[2018-12-22 11:37:01.883794 UTC] Starting iteration 878
[2018-12-22 11:37:01.883912 UTC] Start collecting samples
[2018-12-22 11:37:04.827886 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:04.906047 UTC] Performing policy update
[2018-12-22 11:37:04.906807 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:04.996158 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:06.066944 UTC] Performing line search
[2018-12-22 11:37:06.195404 UTC] Updating baseline
[2018-12-22 11:37:07.784692 UTC] Computing logging information
-------------------------------------
| Iteration            | 878        |
| ExpectedImprovement  | 0.017292   |
| ActualImprovement    | 0.016501   |
| ImprovementRatio     | 0.95421    |
| MeanKL               | 0.0076227  |
| Entropy              | -0.7043    |
| Perplexity           | 0.49446    |
| AveragePolicyStd     | 0.21875    |
| AveragePolicyStd[0]  | 0.25572    |
| AveragePolicyStd[1]  | 0.23868    |
| AveragePolicyStd[2]  | 0.17628    |
| AveragePolicyStd[3]  | 0.23263    |
| AveragePolicyStd[4]  | 0.15763    |
| AveragePolicyStd[5]  | 0.25157    |
| AverageReturn        | 1584       |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 344.46     |
| AverageEpisodeLength | 939.46     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.54     |
| TotalNEpisodes       | 19054      |
| TotalNSamples        | 4.3933e+06 |
| ExplainedVariance    | 0.10102    |
-------------------------------------
[2018-12-22 11:37:08.178543 UTC] Saving snapshot
[2018-12-22 11:37:08.178800 UTC] Starting iteration 879
[2018-12-22 11:37:08.178923 UTC] Start collecting samples
[2018-12-22 11:37:11.157820 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:11.239725 UTC] Performing policy update
[2018-12-22 11:37:11.240430 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:11.331351 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:12.396689 UTC] Performing line search
[2018-12-22 11:37:12.524293 UTC] Updating baseline
[2018-12-22 11:37:13.888844 UTC] Computing logging information
-------------------------------------
| Iteration            | 879        |
| ExpectedImprovement  | 0.017437   |
| ActualImprovement    | 0.016511   |
| ImprovementRatio     | 0.94692    |
| MeanKL               | 0.0076109  |
| Entropy              | -0.71086   |
| Perplexity           | 0.49122    |
| AveragePolicyStd     | 0.21845    |
| AveragePolicyStd[0]  | 0.25488    |
| AveragePolicyStd[1]  | 0.23867    |
| AveragePolicyStd[2]  | 0.17711    |
| AveragePolicyStd[3]  | 0.23265    |
| AveragePolicyStd[4]  | 0.15722    |
| AveragePolicyStd[5]  | 0.25019    |
| AverageReturn        | 1585.3     |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 344.8      |
| AverageEpisodeLength | 939.46     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.54     |
| TotalNEpisodes       | 19061      |
| TotalNSamples        | 4.4003e+06 |
| ExplainedVariance    | 0.012472   |
-------------------------------------
[2018-12-22 11:37:14.278220 UTC] Saving snapshot
[2018-12-22 11:37:14.278468 UTC] Starting iteration 880
[2018-12-22 11:37:14.278611 UTC] Start collecting samples
[2018-12-22 11:37:17.214123 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:17.291933 UTC] Performing policy update
[2018-12-22 11:37:17.292857 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:17.383073 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:18.462438 UTC] Performing line search
[2018-12-22 11:37:18.591347 UTC] Updating baseline
[2018-12-22 11:37:20.118703 UTC] Computing logging information
-------------------------------------
| Iteration            | 880        |
| ExpectedImprovement  | 0.017383   |
| ActualImprovement    | 0.016807   |
| ImprovementRatio     | 0.96689    |
| MeanKL               | 0.0078441  |
| Entropy              | -0.71221   |
| Perplexity           | 0.49056    |
| AveragePolicyStd     | 0.21844    |
| AveragePolicyStd[0]  | 0.25507    |
| AveragePolicyStd[1]  | 0.23807    |
| AveragePolicyStd[2]  | 0.17624    |
| AveragePolicyStd[3]  | 0.23269    |
| AveragePolicyStd[4]  | 0.15742    |
| AveragePolicyStd[5]  | 0.25116    |
| AverageReturn        | 1598       |
| MinReturn            | 97.932     |
| MaxReturn            | 1747.9     |
| StdReturn            | 326.74     |
| AverageEpisodeLength | 946.38     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.98     |
| TotalNEpisodes       | 19065      |
| TotalNSamples        | 4.4043e+06 |
| ExplainedVariance    | -0.0090995 |
-------------------------------------
[2018-12-22 11:37:20.505527 UTC] Saving snapshot
[2018-12-22 11:37:20.513466 UTC] Starting iteration 881
[2018-12-22 11:37:20.513664 UTC] Start collecting samples
[2018-12-22 11:37:23.429290 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:23.507299 UTC] Performing policy update
[2018-12-22 11:37:23.508111 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:23.597978 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:24.663131 UTC] Performing line search
[2018-12-22 11:37:24.790560 UTC] Updating baseline
[2018-12-22 11:37:26.143036 UTC] Computing logging information
-------------------------------------
| Iteration            | 881        |
| ExpectedImprovement  | 0.021696   |
| ActualImprovement    | 0.020368   |
| ImprovementRatio     | 0.93879    |
| MeanKL               | 0.0069627  |
| Entropy              | -0.72052   |
| Perplexity           | 0.4865     |
| AveragePolicyStd     | 0.2181     |
| AveragePolicyStd[0]  | 0.25471    |
| AveragePolicyStd[1]  | 0.2375     |
| AveragePolicyStd[2]  | 0.17653    |
| AveragePolicyStd[3]  | 0.23197    |
| AveragePolicyStd[4]  | 0.15727    |
| AveragePolicyStd[5]  | 0.25064    |
| AverageReturn        | 1588.4     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 341.62     |
| AverageEpisodeLength | 940.21     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.24     |
| TotalNEpisodes       | 19069      |
| TotalNSamples        | 4.4077e+06 |
| ExplainedVariance    | 0.15343    |
-------------------------------------
[2018-12-22 11:37:26.528926 UTC] Saving snapshot
[2018-12-22 11:37:26.529174 UTC] Starting iteration 882
[2018-12-22 11:37:26.529292 UTC] Start collecting samples
[2018-12-22 11:37:29.470116 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:29.549043 UTC] Performing policy update
[2018-12-22 11:37:29.549855 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:29.640464 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:30.715382 UTC] Performing line search
[2018-12-22 11:37:30.844254 UTC] Updating baseline
[2018-12-22 11:37:32.368538 UTC] Computing logging information
-------------------------------------
| Iteration            | 882        |
| ExpectedImprovement  | 0.018502   |
| ActualImprovement    | 0.017164   |
| ImprovementRatio     | 0.92767    |
| MeanKL               | 0.0073253  |
| Entropy              | -0.72264   |
| Perplexity           | 0.48547    |
| AveragePolicyStd     | 0.21807    |
| AveragePolicyStd[0]  | 0.25491    |
| AveragePolicyStd[1]  | 0.23724    |
| AveragePolicyStd[2]  | 0.17613    |
| AveragePolicyStd[3]  | 0.23222    |
| AveragePolicyStd[4]  | 0.15703    |
| AveragePolicyStd[5]  | 0.25087    |
| AverageReturn        | 1588.4     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 341.73     |
| AverageEpisodeLength | 940.21     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.24     |
| TotalNEpisodes       | 19075      |
| TotalNSamples        | 4.4137e+06 |
| ExplainedVariance    | 0.0093405  |
-------------------------------------
[2018-12-22 11:37:32.750934 UTC] Saving snapshot
[2018-12-22 11:37:32.751183 UTC] Starting iteration 883
[2018-12-22 11:37:32.751302 UTC] Start collecting samples
[2018-12-22 11:37:35.697646 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:35.779102 UTC] Performing policy update
[2018-12-22 11:37:35.779740 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:35.869038 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:36.937473 UTC] Performing line search
[2018-12-22 11:37:37.064083 UTC] Updating baseline
[2018-12-22 11:37:38.668548 UTC] Computing logging information
-------------------------------------
| Iteration            | 883        |
| ExpectedImprovement  | 0.016934   |
| ActualImprovement    | 0.016473   |
| ImprovementRatio     | 0.9728     |
| MeanKL               | 0.0078458  |
| Entropy              | -0.7283    |
| Perplexity           | 0.48273    |
| AveragePolicyStd     | 0.21787    |
| AveragePolicyStd[0]  | 0.25389    |
| AveragePolicyStd[1]  | 0.23759    |
| AveragePolicyStd[2]  | 0.17615    |
| AveragePolicyStd[3]  | 0.23217    |
| AveragePolicyStd[4]  | 0.15654    |
| AveragePolicyStd[5]  | 0.2509     |
| AverageReturn        | 1590.7     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 342.45     |
| AverageEpisodeLength | 940.21     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.24     |
| TotalNEpisodes       | 19081      |
| TotalNSamples        | 4.4197e+06 |
| ExplainedVariance    | -0.0083848 |
-------------------------------------
[2018-12-22 11:37:39.056552 UTC] Saving snapshot
[2018-12-22 11:37:39.056801 UTC] Starting iteration 884
[2018-12-22 11:37:39.056926 UTC] Start collecting samples
[2018-12-22 11:37:41.980953 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:42.057269 UTC] Performing policy update
[2018-12-22 11:37:42.057866 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:42.147741 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:43.210084 UTC] Performing line search
[2018-12-22 11:37:43.338779 UTC] Updating baseline
[2018-12-22 11:37:44.822358 UTC] Computing logging information
-------------------------------------
| Iteration            | 884        |
| ExpectedImprovement  | 0.019965   |
| ActualImprovement    | 0.01853    |
| ImprovementRatio     | 0.92814    |
| MeanKL               | 0.0077579  |
| Entropy              | -0.72524   |
| Perplexity           | 0.48421    |
| AveragePolicyStd     | 0.218      |
| AveragePolicyStd[0]  | 0.25419    |
| AveragePolicyStd[1]  | 0.23763    |
| AveragePolicyStd[2]  | 0.17592    |
| AveragePolicyStd[3]  | 0.23206    |
| AveragePolicyStd[4]  | 0.15673    |
| AveragePolicyStd[5]  | 0.25148    |
| AverageReturn        | 1590.8     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 342.51     |
| AverageEpisodeLength | 940.21     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.24     |
| TotalNEpisodes       | 19084      |
| TotalNSamples        | 4.4227e+06 |
| ExplainedVariance    | 0.0034733  |
-------------------------------------
[2018-12-22 11:37:45.214257 UTC] Saving snapshot
[2018-12-22 11:37:45.214638 UTC] Starting iteration 885
[2018-12-22 11:37:45.214761 UTC] Start collecting samples
[2018-12-22 11:37:48.222563 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:48.301854 UTC] Performing policy update
[2018-12-22 11:37:48.302422 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:48.392932 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:49.453932 UTC] Performing line search
[2018-12-22 11:37:49.581966 UTC] Updating baseline
[2018-12-22 11:37:51.508182 UTC] Computing logging information
-------------------------------------
| Iteration            | 885        |
| ExpectedImprovement  | 0.01918    |
| ActualImprovement    | 0.018059   |
| ImprovementRatio     | 0.94155    |
| MeanKL               | 0.007154   |
| Entropy              | -0.7295    |
| Perplexity           | 0.48215    |
| AveragePolicyStd     | 0.21782    |
| AveragePolicyStd[0]  | 0.2539     |
| AveragePolicyStd[1]  | 0.23721    |
| AveragePolicyStd[2]  | 0.17584    |
| AveragePolicyStd[3]  | 0.23192    |
| AveragePolicyStd[4]  | 0.15683    |
| AveragePolicyStd[5]  | 0.25125    |
| AverageReturn        | 1599.3     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 322.03     |
| AverageEpisodeLength | 945.7      |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.54     |
| TotalNEpisodes       | 19093      |
| TotalNSamples        | 4.4307e+06 |
| ExplainedVariance    | 0.11038    |
-------------------------------------
[2018-12-22 11:37:51.898551 UTC] Saving snapshot
[2018-12-22 11:37:51.898845 UTC] Starting iteration 886
[2018-12-22 11:37:51.898991 UTC] Start collecting samples
[2018-12-22 11:37:54.825865 UTC] Computing input variables for policy optimization
[2018-12-22 11:37:54.903675 UTC] Performing policy update
[2018-12-22 11:37:54.904355 UTC] Computing gradient in Euclidean space
[2018-12-22 11:37:54.993519 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:37:56.055444 UTC] Performing line search
[2018-12-22 11:37:56.183018 UTC] Updating baseline
[2018-12-22 11:37:57.357979 UTC] Computing logging information
-------------------------------------
| Iteration            | 886        |
| ExpectedImprovement  | 0.015977   |
| ActualImprovement    | 0.015664   |
| ImprovementRatio     | 0.98041    |
| MeanKL               | 0.0075727  |
| Entropy              | -0.72918   |
| Perplexity           | 0.4823     |
| AveragePolicyStd     | 0.21784    |
| AveragePolicyStd[0]  | 0.25471    |
| AveragePolicyStd[1]  | 0.23767    |
| AveragePolicyStd[2]  | 0.17599    |
| AveragePolicyStd[3]  | 0.23145    |
| AveragePolicyStd[4]  | 0.15676    |
| AveragePolicyStd[5]  | 0.25043    |
| AverageReturn        | 1606.3     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 316.86     |
| AverageEpisodeLength | 949.04     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.41     |
| TotalNEpisodes       | 19096      |
| TotalNSamples        | 4.4337e+06 |
| ExplainedVariance    | -0.10746   |
-------------------------------------
[2018-12-22 11:37:57.743515 UTC] Saving snapshot
[2018-12-22 11:37:57.743794 UTC] Starting iteration 887
[2018-12-22 11:37:57.743915 UTC] Start collecting samples
[2018-12-22 11:38:00.748423 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:00.829821 UTC] Performing policy update
[2018-12-22 11:38:00.830470 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:00.925587 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:02.054144 UTC] Performing line search
[2018-12-22 11:38:02.187853 UTC] Updating baseline
[2018-12-22 11:38:03.595827 UTC] Computing logging information
-------------------------------------
| Iteration            | 887        |
| ExpectedImprovement  | 0.01754    |
| ActualImprovement    | 0.017029   |
| ImprovementRatio     | 0.97087    |
| MeanKL               | 0.0076021  |
| Entropy              | -0.72872   |
| Perplexity           | 0.48253    |
| AveragePolicyStd     | 0.21784    |
| AveragePolicyStd[0]  | 0.25508    |
| AveragePolicyStd[1]  | 0.23823    |
| AveragePolicyStd[2]  | 0.17569    |
| AveragePolicyStd[3]  | 0.23054    |
| AveragePolicyStd[4]  | 0.15724    |
| AveragePolicyStd[5]  | 0.25025    |
| AverageReturn        | 1607.8     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 317.28     |
| AverageEpisodeLength | 949.04     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.41     |
| TotalNEpisodes       | 19100      |
| TotalNSamples        | 4.4377e+06 |
| ExplainedVariance    | -0.014044  |
-------------------------------------
[2018-12-22 11:38:04.012545 UTC] Saving snapshot
[2018-12-22 11:38:04.012802 UTC] Starting iteration 888
[2018-12-22 11:38:04.012923 UTC] Start collecting samples
[2018-12-22 11:38:07.202316 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:07.288030 UTC] Performing policy update
[2018-12-22 11:38:07.288763 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:07.382598 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:08.493898 UTC] Performing line search
[2018-12-22 11:38:08.627737 UTC] Updating baseline
[2018-12-22 11:38:10.318941 UTC] Computing logging information
-------------------------------------
| Iteration            | 888        |
| ExpectedImprovement  | 0.017564   |
| ActualImprovement    | 0.016494   |
| ImprovementRatio     | 0.93905    |
| MeanKL               | 0.0069156  |
| Entropy              | -0.73235   |
| Perplexity           | 0.48078    |
| AveragePolicyStd     | 0.21769    |
| AveragePolicyStd[0]  | 0.254      |
| AveragePolicyStd[1]  | 0.23785    |
| AveragePolicyStd[2]  | 0.17521    |
| AveragePolicyStd[3]  | 0.23114    |
| AveragePolicyStd[4]  | 0.15744    |
| AveragePolicyStd[5]  | 0.25052    |
| AverageReturn        | 1592.7     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 351.11     |
| AverageEpisodeLength | 939.96     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.65     |
| TotalNEpisodes       | 19107      |
| TotalNSamples        | 4.4438e+06 |
| ExplainedVariance    | 0.080728   |
-------------------------------------
[2018-12-22 11:38:10.712280 UTC] Saving snapshot
[2018-12-22 11:38:10.712552 UTC] Starting iteration 889
[2018-12-22 11:38:10.712675 UTC] Start collecting samples
[2018-12-22 11:38:13.695663 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:13.776505 UTC] Performing policy update
[2018-12-22 11:38:13.777392 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:13.868420 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:14.948471 UTC] Performing line search
[2018-12-22 11:38:15.082327 UTC] Updating baseline
[2018-12-22 11:38:16.417109 UTC] Computing logging information
-------------------------------------
| Iteration            | 889        |
| ExpectedImprovement  | 0.01829    |
| ActualImprovement    | 0.017897   |
| ImprovementRatio     | 0.97852    |
| MeanKL               | 0.0075013  |
| Entropy              | -0.73448   |
| Perplexity           | 0.47976    |
| AveragePolicyStd     | 0.21761    |
| AveragePolicyStd[0]  | 0.25455    |
| AveragePolicyStd[1]  | 0.23807    |
| AveragePolicyStd[2]  | 0.17474    |
| AveragePolicyStd[3]  | 0.23096    |
| AveragePolicyStd[4]  | 0.1578     |
| AveragePolicyStd[5]  | 0.24952    |
| AverageReturn        | 1599.1     |
| MinReturn            | 97.932     |
| MaxReturn            | 1759.4     |
| StdReturn            | 346.26     |
| AverageEpisodeLength | 943.43     |
| MinEpisodeLength     | 87         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.62     |
| TotalNEpisodes       | 19113      |
| TotalNSamples        | 4.4498e+06 |
| ExplainedVariance    | 0.041088   |
-------------------------------------
[2018-12-22 11:38:16.844106 UTC] Saving snapshot
[2018-12-22 11:38:16.844405 UTC] Starting iteration 890
[2018-12-22 11:38:16.844633 UTC] Start collecting samples
[2018-12-22 11:38:20.014847 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:20.099832 UTC] Performing policy update
[2018-12-22 11:38:20.100858 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:20.193361 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:21.242838 UTC] Performing line search
[2018-12-22 11:38:21.371331 UTC] Updating baseline
[2018-12-22 11:38:22.723242 UTC] Computing logging information
-------------------------------------
| Iteration            | 890        |
| ExpectedImprovement  | 0.016814   |
| ActualImprovement    | 0.015783   |
| ImprovementRatio     | 0.93871    |
| MeanKL               | 0.0076676  |
| Entropy              | -0.73695   |
| Perplexity           | 0.47857    |
| AveragePolicyStd     | 0.21747    |
| AveragePolicyStd[0]  | 0.25348    |
| AveragePolicyStd[1]  | 0.23812    |
| AveragePolicyStd[2]  | 0.17495    |
| AveragePolicyStd[3]  | 0.23056    |
| AveragePolicyStd[4]  | 0.15803    |
| AveragePolicyStd[5]  | 0.24965    |
| AverageReturn        | 1620.1     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 299.32     |
| AverageEpisodeLength | 954.7      |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.27     |
| TotalNEpisodes       | 19117      |
| TotalNSamples        | 4.4534e+06 |
| ExplainedVariance    | 0.14637    |
-------------------------------------
[2018-12-22 11:38:23.107861 UTC] Saving snapshot
[2018-12-22 11:38:23.115954 UTC] Starting iteration 891
[2018-12-22 11:38:23.116158 UTC] Start collecting samples
[2018-12-22 11:38:26.075317 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:26.152557 UTC] Performing policy update
[2018-12-22 11:38:26.153289 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:26.243766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:27.308359 UTC] Performing line search
[2018-12-22 11:38:27.435967 UTC] Updating baseline
[2018-12-22 11:38:28.781112 UTC] Computing logging information
------------------------------------
| Iteration            | 891       |
| ExpectedImprovement  | 0.018812  |
| ActualImprovement    | 0.017577  |
| ImprovementRatio     | 0.93436   |
| MeanKL               | 0.0070773 |
| Entropy              | -0.74239  |
| Perplexity           | 0.47597   |
| AveragePolicyStd     | 0.21727   |
| AveragePolicyStd[0]  | 0.2537    |
| AveragePolicyStd[1]  | 0.23861   |
| AveragePolicyStd[2]  | 0.17449   |
| AveragePolicyStd[3]  | 0.22951   |
| AveragePolicyStd[4]  | 0.15817   |
| AveragePolicyStd[5]  | 0.24915   |
| AverageReturn        | 1614      |
| MinReturn            | 98.305    |
| MaxReturn            | 1759.4    |
| StdReturn            | 303.44    |
| AverageEpisodeLength | 951.28    |
| MinEpisodeLength     | 92        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 171.76    |
| TotalNEpisodes       | 19124     |
| TotalNSamples        | 4.46e+06  |
| ExplainedVariance    | 0.07294   |
------------------------------------
[2018-12-22 11:38:29.164620 UTC] Saving snapshot
[2018-12-22 11:38:29.164870 UTC] Starting iteration 892
[2018-12-22 11:38:29.164993 UTC] Start collecting samples
[2018-12-22 11:38:32.113463 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:32.192061 UTC] Performing policy update
[2018-12-22 11:38:32.192732 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:32.281038 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:33.338614 UTC] Performing line search
[2018-12-22 11:38:33.466202 UTC] Updating baseline
[2018-12-22 11:38:34.905542 UTC] Computing logging information
-------------------------------------
| Iteration            | 892        |
| ExpectedImprovement  | 0.017211   |
| ActualImprovement    | 0.016376   |
| ImprovementRatio     | 0.95152    |
| MeanKL               | 0.0076567  |
| Entropy              | -0.74969   |
| Perplexity           | 0.47251    |
| AveragePolicyStd     | 0.21705    |
| AveragePolicyStd[0]  | 0.25367    |
| AveragePolicyStd[1]  | 0.23843    |
| AveragePolicyStd[2]  | 0.17466    |
| AveragePolicyStd[3]  | 0.22939    |
| AveragePolicyStd[4]  | 0.15724    |
| AveragePolicyStd[5]  | 0.2489     |
| AverageReturn        | 1620.9     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 297.53     |
| AverageEpisodeLength | 955.22     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.27     |
| TotalNEpisodes       | 19129      |
| TotalNSamples        | 4.465e+06  |
| ExplainedVariance    | -0.0039257 |
-------------------------------------
[2018-12-22 11:38:35.295741 UTC] Saving snapshot
[2018-12-22 11:38:35.295985 UTC] Starting iteration 893
[2018-12-22 11:38:35.296099 UTC] Start collecting samples
[2018-12-22 11:38:38.221449 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:38.298388 UTC] Performing policy update
[2018-12-22 11:38:38.299320 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:38.389133 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:39.436729 UTC] Performing line search
[2018-12-22 11:38:39.563247 UTC] Updating baseline
[2018-12-22 11:38:40.889886 UTC] Computing logging information
------------------------------------
| Iteration            | 893       |
| ExpectedImprovement  | 0.017925  |
| ActualImprovement    | 0.016887  |
| ImprovementRatio     | 0.94209   |
| MeanKL               | 0.0071323 |
| Entropy              | -0.75113  |
| Perplexity           | 0.47183   |
| AveragePolicyStd     | 0.21703   |
| AveragePolicyStd[0]  | 0.25304   |
| AveragePolicyStd[1]  | 0.23868   |
| AveragePolicyStd[2]  | 0.17451   |
| AveragePolicyStd[3]  | 0.22981   |
| AveragePolicyStd[4]  | 0.15687   |
| AveragePolicyStd[5]  | 0.24923   |
| AverageReturn        | 1623.3    |
| MinReturn            | 98.305    |
| MaxReturn            | 1759.4    |
| StdReturn            | 297.14    |
| AverageEpisodeLength | 956.08    |
| MinEpisodeLength     | 92        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 168.25    |
| TotalNEpisodes       | 19132     |
| TotalNSamples        | 4.468e+06 |
| ExplainedVariance    | 0.25267   |
------------------------------------
[2018-12-22 11:38:41.276558 UTC] Saving snapshot
[2018-12-22 11:38:41.276823 UTC] Starting iteration 894
[2018-12-22 11:38:41.276954 UTC] Start collecting samples
[2018-12-22 11:38:44.291969 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:44.372215 UTC] Performing policy update
[2018-12-22 11:38:44.372826 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:44.462923 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:45.521608 UTC] Performing line search
[2018-12-22 11:38:45.648923 UTC] Updating baseline
[2018-12-22 11:38:46.997887 UTC] Computing logging information
-------------------------------------
| Iteration            | 894        |
| ExpectedImprovement  | 0.01638    |
| ActualImprovement    | 0.015434   |
| ImprovementRatio     | 0.94227    |
| MeanKL               | 0.0078639  |
| Entropy              | -0.75513   |
| Perplexity           | 0.46995    |
| AveragePolicyStd     | 0.2168     |
| AveragePolicyStd[0]  | 0.25231    |
| AveragePolicyStd[1]  | 0.23792    |
| AveragePolicyStd[2]  | 0.17474    |
| AveragePolicyStd[3]  | 0.22855    |
| AveragePolicyStd[4]  | 0.15759    |
| AveragePolicyStd[5]  | 0.24967    |
| AverageReturn        | 1594.2     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 354.1      |
| AverageEpisodeLength | 939.61     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.56     |
| TotalNEpisodes       | 19141      |
| TotalNSamples        | 4.4754e+06 |
| ExplainedVariance    | 0.17496    |
-------------------------------------
[2018-12-22 11:38:47.381869 UTC] Saving snapshot
[2018-12-22 11:38:47.382112 UTC] Starting iteration 895
[2018-12-22 11:38:47.382228 UTC] Start collecting samples
[2018-12-22 11:38:50.326513 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:50.405986 UTC] Performing policy update
[2018-12-22 11:38:50.406676 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:50.496533 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:51.560207 UTC] Performing line search
[2018-12-22 11:38:51.687857 UTC] Updating baseline
[2018-12-22 11:38:53.661488 UTC] Computing logging information
-------------------------------------
| Iteration            | 895        |
| ExpectedImprovement  | 0.018032   |
| ActualImprovement    | 0.017375   |
| ImprovementRatio     | 0.96354    |
| MeanKL               | 0.0075997  |
| Entropy              | -0.75822   |
| Perplexity           | 0.4685     |
| AveragePolicyStd     | 0.21667    |
| AveragePolicyStd[0]  | 0.25297    |
| AveragePolicyStd[1]  | 0.23826    |
| AveragePolicyStd[2]  | 0.17471    |
| AveragePolicyStd[3]  | 0.22743    |
| AveragePolicyStd[4]  | 0.15773    |
| AveragePolicyStd[5]  | 0.24894    |
| AverageReturn        | 1594.7     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 354.31     |
| AverageEpisodeLength | 939.61     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.56     |
| TotalNEpisodes       | 19146      |
| TotalNSamples        | 4.4804e+06 |
| ExplainedVariance    | -0.017851  |
-------------------------------------
[2018-12-22 11:38:54.054982 UTC] Saving snapshot
[2018-12-22 11:38:54.055237 UTC] Starting iteration 896
[2018-12-22 11:38:54.055369 UTC] Start collecting samples
[2018-12-22 11:38:56.959302 UTC] Computing input variables for policy optimization
[2018-12-22 11:38:57.035700 UTC] Performing policy update
[2018-12-22 11:38:57.036535 UTC] Computing gradient in Euclidean space
[2018-12-22 11:38:57.126205 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:38:58.192932 UTC] Performing line search
[2018-12-22 11:38:58.320521 UTC] Updating baseline
[2018-12-22 11:39:00.012517 UTC] Computing logging information
-------------------------------------
| Iteration            | 896        |
| ExpectedImprovement  | 0.018353   |
| ActualImprovement    | 0.016269   |
| ImprovementRatio     | 0.88642    |
| MeanKL               | 0.0073562  |
| Entropy              | -0.76269   |
| Perplexity           | 0.46641    |
| AveragePolicyStd     | 0.21651    |
| AveragePolicyStd[0]  | 0.25279    |
| AveragePolicyStd[1]  | 0.23733    |
| AveragePolicyStd[2]  | 0.17451    |
| AveragePolicyStd[3]  | 0.22736    |
| AveragePolicyStd[4]  | 0.15773    |
| AveragePolicyStd[5]  | 0.24934    |
| AverageReturn        | 1599.6     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 352.81     |
| AverageEpisodeLength | 942.06     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.79     |
| TotalNEpisodes       | 19148      |
| TotalNSamples        | 4.4824e+06 |
| ExplainedVariance    | -0.021123  |
-------------------------------------
[2018-12-22 11:39:00.401723 UTC] Saving snapshot
[2018-12-22 11:39:00.401998 UTC] Starting iteration 897
[2018-12-22 11:39:00.402116 UTC] Start collecting samples
[2018-12-22 11:39:03.386019 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:03.467512 UTC] Performing policy update
[2018-12-22 11:39:03.468297 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:03.560060 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:04.626636 UTC] Performing line search
[2018-12-22 11:39:04.753809 UTC] Updating baseline
[2018-12-22 11:39:05.930880 UTC] Computing logging information
-------------------------------------
| Iteration            | 897        |
| ExpectedImprovement  | 0.016331   |
| ActualImprovement    | 0.015291   |
| ImprovementRatio     | 0.93632    |
| MeanKL               | 0.0076398  |
| Entropy              | -0.76829   |
| Perplexity           | 0.46381    |
| AveragePolicyStd     | 0.21632    |
| AveragePolicyStd[0]  | 0.25229    |
| AveragePolicyStd[1]  | 0.23756    |
| AveragePolicyStd[2]  | 0.17445    |
| AveragePolicyStd[3]  | 0.22738    |
| AveragePolicyStd[4]  | 0.15728    |
| AveragePolicyStd[5]  | 0.24895    |
| AverageReturn        | 1614.6     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 321.17     |
| AverageEpisodeLength | 951.01     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.28     |
| TotalNEpisodes       | 19155      |
| TotalNSamples        | 4.4894e+06 |
| ExplainedVariance    | -0.0034626 |
-------------------------------------
[2018-12-22 11:39:06.317977 UTC] Saving snapshot
[2018-12-22 11:39:06.318220 UTC] Starting iteration 898
[2018-12-22 11:39:06.318344 UTC] Start collecting samples
[2018-12-22 11:39:09.252579 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:09.331906 UTC] Performing policy update
[2018-12-22 11:39:09.332566 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:09.421636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:10.479680 UTC] Performing line search
[2018-12-22 11:39:10.606172 UTC] Updating baseline
[2018-12-22 11:39:12.122303 UTC] Computing logging information
-------------------------------------
| Iteration            | 898        |
| ExpectedImprovement  | 0.016886   |
| ActualImprovement    | 0.016191   |
| ImprovementRatio     | 0.95887    |
| MeanKL               | 0.0074728  |
| Entropy              | -0.7694    |
| Perplexity           | 0.46329    |
| AveragePolicyStd     | 0.21631    |
| AveragePolicyStd[0]  | 0.25141    |
| AveragePolicyStd[1]  | 0.23772    |
| AveragePolicyStd[2]  | 0.17452    |
| AveragePolicyStd[3]  | 0.22724    |
| AveragePolicyStd[4]  | 0.15689    |
| AveragePolicyStd[5]  | 0.25006    |
| AverageReturn        | 1607.7     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 325.3      |
| AverageEpisodeLength | 946.72     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.59     |
| TotalNEpisodes       | 19162      |
| TotalNSamples        | 4.4959e+06 |
| ExplainedVariance    | 0.20295    |
-------------------------------------
[2018-12-22 11:39:12.513034 UTC] Saving snapshot
[2018-12-22 11:39:12.513279 UTC] Starting iteration 899
[2018-12-22 11:39:12.513399 UTC] Start collecting samples
[2018-12-22 11:39:15.431638 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:15.508409 UTC] Performing policy update
[2018-12-22 11:39:15.509153 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:15.598779 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:16.659221 UTC] Performing line search
[2018-12-22 11:39:16.786686 UTC] Updating baseline
[2018-12-22 11:39:18.138249 UTC] Computing logging information
-------------------------------------
| Iteration            | 899        |
| ExpectedImprovement  | 0.019386   |
| ActualImprovement    | 0.018344   |
| ImprovementRatio     | 0.94625    |
| MeanKL               | 0.0077113  |
| Entropy              | -0.77607   |
| Perplexity           | 0.46021    |
| AveragePolicyStd     | 0.21612    |
| AveragePolicyStd[0]  | 0.25166    |
| AveragePolicyStd[1]  | 0.23758    |
| AveragePolicyStd[2]  | 0.17428    |
| AveragePolicyStd[3]  | 0.22697    |
| AveragePolicyStd[4]  | 0.15623    |
| AveragePolicyStd[5]  | 0.24999    |
| AverageReturn        | 1593.6     |
| MinReturn            | 98.305     |
| MaxReturn            | 1759.4     |
| StdReturn            | 354.69     |
| AverageEpisodeLength | 938.18     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.04     |
| TotalNEpisodes       | 19165      |
| TotalNSamples        | 4.4981e+06 |
| ExplainedVariance    | 0.2496     |
-------------------------------------
[2018-12-22 11:39:18.531227 UTC] Saving snapshot
[2018-12-22 11:39:18.531474 UTC] Starting iteration 900
[2018-12-22 11:39:18.531620 UTC] Start collecting samples
[2018-12-22 11:39:21.472104 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:21.550192 UTC] Performing policy update
[2018-12-22 11:39:21.550803 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:21.640553 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:22.704334 UTC] Performing line search
[2018-12-22 11:39:22.830686 UTC] Updating baseline
[2018-12-22 11:39:24.089184 UTC] Computing logging information
-------------------------------------
| Iteration            | 900        |
| ExpectedImprovement  | 0.018738   |
| ActualImprovement    | 0.018105   |
| ImprovementRatio     | 0.96625    |
| MeanKL               | 0.0081038  |
| Entropy              | -0.78102   |
| Perplexity           | 0.45794    |
| AveragePolicyStd     | 0.21593    |
| AveragePolicyStd[0]  | 0.25179    |
| AveragePolicyStd[1]  | 0.23728    |
| AveragePolicyStd[2]  | 0.17391    |
| AveragePolicyStd[3]  | 0.22615    |
| AveragePolicyStd[4]  | 0.15646    |
| AveragePolicyStd[5]  | 0.25       |
| AverageReturn        | 1596.4     |
| MinReturn            | 98.305     |
| MaxReturn            | 1756.5     |
| StdReturn            | 343.94     |
| AverageEpisodeLength | 940.7      |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.54     |
| TotalNEpisodes       | 19170      |
| TotalNSamples        | 4.5027e+06 |
| ExplainedVariance    | 0.08711    |
-------------------------------------
[2018-12-22 11:39:24.479820 UTC] Saving snapshot
[2018-12-22 11:39:24.487987 UTC] Starting iteration 901
[2018-12-22 11:39:24.488199 UTC] Start collecting samples
[2018-12-22 11:39:27.494702 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:27.575297 UTC] Performing policy update
[2018-12-22 11:39:27.576016 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:27.666645 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:28.727172 UTC] Performing line search
[2018-12-22 11:39:28.854428 UTC] Updating baseline
[2018-12-22 11:39:30.387353 UTC] Computing logging information
-------------------------------------
| Iteration            | 901        |
| ExpectedImprovement  | 0.01624    |
| ActualImprovement    | 0.015833   |
| ImprovementRatio     | 0.97494    |
| MeanKL               | 0.0078651  |
| Entropy              | -0.77536   |
| Perplexity           | 0.46054    |
| AveragePolicyStd     | 0.21618    |
| AveragePolicyStd[0]  | 0.25211    |
| AveragePolicyStd[1]  | 0.2374     |
| AveragePolicyStd[2]  | 0.17383    |
| AveragePolicyStd[3]  | 0.22711    |
| AveragePolicyStd[4]  | 0.15629    |
| AveragePolicyStd[5]  | 0.25031    |
| AverageReturn        | 1593.6     |
| MinReturn            | 98.305     |
| MaxReturn            | 1756.5     |
| StdReturn            | 343.08     |
| AverageEpisodeLength | 940.7      |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.54     |
| TotalNEpisodes       | 19179      |
| TotalNSamples        | 4.5117e+06 |
| ExplainedVariance    | -0.0016681 |
-------------------------------------
[2018-12-22 11:39:30.775903 UTC] Saving snapshot
[2018-12-22 11:39:30.776154 UTC] Starting iteration 902
[2018-12-22 11:39:30.776275 UTC] Start collecting samples
[2018-12-22 11:39:33.695009 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:33.772802 UTC] Performing policy update
[2018-12-22 11:39:33.773397 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:33.863359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:34.917424 UTC] Performing line search
[2018-12-22 11:39:35.044885 UTC] Updating baseline
[2018-12-22 11:39:36.774772 UTC] Computing logging information
-------------------------------------
| Iteration            | 902        |
| ExpectedImprovement  | 0.01685    |
| ActualImprovement    | 0.016078   |
| ImprovementRatio     | 0.95419    |
| MeanKL               | 0.0075491  |
| Entropy              | -0.77308   |
| Perplexity           | 0.46159    |
| AveragePolicyStd     | 0.21621    |
| AveragePolicyStd[0]  | 0.2513     |
| AveragePolicyStd[1]  | 0.23719    |
| AveragePolicyStd[2]  | 0.1738     |
| AveragePolicyStd[3]  | 0.22781    |
| AveragePolicyStd[4]  | 0.15678    |
| AveragePolicyStd[5]  | 0.25038    |
| AverageReturn        | 1583       |
| MinReturn            | 98.305     |
| MaxReturn            | 1756.5     |
| StdReturn            | 354.09     |
| AverageEpisodeLength | 934.93     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.14     |
| TotalNEpisodes       | 19182      |
| TotalNSamples        | 4.5142e+06 |
| ExplainedVariance    | 0.16902    |
-------------------------------------
[2018-12-22 11:39:37.157903 UTC] Saving snapshot
[2018-12-22 11:39:37.158154 UTC] Starting iteration 903
[2018-12-22 11:39:37.158270 UTC] Start collecting samples
[2018-12-22 11:39:40.106470 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:40.183753 UTC] Performing policy update
[2018-12-22 11:39:40.184382 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:40.273245 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:41.324865 UTC] Performing line search
[2018-12-22 11:39:41.451607 UTC] Updating baseline
[2018-12-22 11:39:42.847672 UTC] Computing logging information
-------------------------------------
| Iteration            | 903        |
| ExpectedImprovement  | 0.015109   |
| ActualImprovement    | 0.014824   |
| ImprovementRatio     | 0.98114    |
| MeanKL               | 0.0077005  |
| Entropy              | -0.77819   |
| Perplexity           | 0.45924    |
| AveragePolicyStd     | 0.21601    |
| AveragePolicyStd[0]  | 0.25091    |
| AveragePolicyStd[1]  | 0.23698    |
| AveragePolicyStd[2]  | 0.17365    |
| AveragePolicyStd[3]  | 0.22762    |
| AveragePolicyStd[4]  | 0.15684    |
| AveragePolicyStd[5]  | 0.25007    |
| AverageReturn        | 1584.1     |
| MinReturn            | 98.305     |
| MaxReturn            | 1756.5     |
| StdReturn            | 354.47     |
| AverageEpisodeLength | 934.93     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.14     |
| TotalNEpisodes       | 19187      |
| TotalNSamples        | 4.5192e+06 |
| ExplainedVariance    | -0.016802  |
-------------------------------------
[2018-12-22 11:39:43.226599 UTC] Saving snapshot
[2018-12-22 11:39:43.226864 UTC] Starting iteration 904
[2018-12-22 11:39:43.226986 UTC] Start collecting samples
[2018-12-22 11:39:46.166248 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:46.243795 UTC] Performing policy update
[2018-12-22 11:39:46.244564 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:46.335549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:47.392834 UTC] Performing line search
[2018-12-22 11:39:47.520136 UTC] Updating baseline
[2018-12-22 11:39:48.748568 UTC] Computing logging information
-------------------------------------
| Iteration            | 904        |
| ExpectedImprovement  | 0.017169   |
| ActualImprovement    | 0.016151   |
| ImprovementRatio     | 0.94072    |
| MeanKL               | 0.0075551  |
| Entropy              | -0.7721    |
| Perplexity           | 0.46204    |
| AveragePolicyStd     | 0.21626    |
| AveragePolicyStd[0]  | 0.25135    |
| AveragePolicyStd[1]  | 0.23652    |
| AveragePolicyStd[2]  | 0.17385    |
| AveragePolicyStd[3]  | 0.22853    |
| AveragePolicyStd[4]  | 0.15665    |
| AveragePolicyStd[5]  | 0.25064    |
| AverageReturn        | 1601.2     |
| MinReturn            | 98.305     |
| MaxReturn            | 1756.5     |
| StdReturn            | 331.15     |
| AverageEpisodeLength | 944.04     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.81     |
| TotalNEpisodes       | 19192      |
| TotalNSamples        | 4.5241e+06 |
| ExplainedVariance    | 0.10196    |
-------------------------------------
[2018-12-22 11:39:49.137464 UTC] Saving snapshot
[2018-12-22 11:39:49.137719 UTC] Starting iteration 905
[2018-12-22 11:39:49.137873 UTC] Start collecting samples
[2018-12-22 11:39:52.097766 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:52.175052 UTC] Performing policy update
[2018-12-22 11:39:52.175742 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:52.265602 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:53.325252 UTC] Performing line search
[2018-12-22 11:39:53.454825 UTC] Updating baseline
[2018-12-22 11:39:55.394375 UTC] Computing logging information
-------------------------------------
| Iteration            | 905        |
| ExpectedImprovement  | 0.01581    |
| ActualImprovement    | 0.014927   |
| ImprovementRatio     | 0.94413    |
| MeanKL               | 0.0081269  |
| Entropy              | -0.77219   |
| Perplexity           | 0.462      |
| AveragePolicyStd     | 0.21623    |
| AveragePolicyStd[0]  | 0.25165    |
| AveragePolicyStd[1]  | 0.23617    |
| AveragePolicyStd[2]  | 0.17369    |
| AveragePolicyStd[3]  | 0.2278     |
| AveragePolicyStd[4]  | 0.15716    |
| AveragePolicyStd[5]  | 0.25091    |
| AverageReturn        | 1599.9     |
| MinReturn            | 98.305     |
| MaxReturn            | 1763.9     |
| StdReturn            | 330.88     |
| AverageEpisodeLength | 944.04     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.81     |
| TotalNEpisodes       | 19198      |
| TotalNSamples        | 4.5301e+06 |
| ExplainedVariance    | 0.0013826  |
-------------------------------------
[2018-12-22 11:39:55.785414 UTC] Saving snapshot
[2018-12-22 11:39:55.785698 UTC] Starting iteration 906
[2018-12-22 11:39:55.785857 UTC] Start collecting samples
[2018-12-22 11:39:58.710682 UTC] Computing input variables for policy optimization
[2018-12-22 11:39:58.787732 UTC] Performing policy update
[2018-12-22 11:39:58.788660 UTC] Computing gradient in Euclidean space
[2018-12-22 11:39:58.878415 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:39:59.941899 UTC] Performing line search
[2018-12-22 11:40:00.070240 UTC] Updating baseline
[2018-12-22 11:40:03.023666 UTC] Computing logging information
-------------------------------------
| Iteration            | 906        |
| ExpectedImprovement  | 0.019789   |
| ActualImprovement    | 0.018305   |
| ImprovementRatio     | 0.925      |
| MeanKL               | 0.0075585  |
| Entropy              | -0.76805   |
| Perplexity           | 0.46392    |
| AveragePolicyStd     | 0.21634    |
| AveragePolicyStd[0]  | 0.25119    |
| AveragePolicyStd[1]  | 0.23655    |
| AveragePolicyStd[2]  | 0.17438    |
| AveragePolicyStd[3]  | 0.22851    |
| AveragePolicyStd[4]  | 0.15715    |
| AveragePolicyStd[5]  | 0.25025    |
| AverageReturn        | 1599.5     |
| MinReturn            | 98.305     |
| MaxReturn            | 1763.9     |
| StdReturn            | 330.82     |
| AverageEpisodeLength | 944.04     |
| MinEpisodeLength     | 92         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.81     |
| TotalNEpisodes       | 19202      |
| TotalNSamples        | 4.5341e+06 |
| ExplainedVariance    | 0.00059767 |
-------------------------------------
[2018-12-22 11:40:03.416533 UTC] Saving snapshot
[2018-12-22 11:40:03.416823 UTC] Starting iteration 907
[2018-12-22 11:40:03.416943 UTC] Start collecting samples
[2018-12-22 11:40:06.386319 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:06.463642 UTC] Performing policy update
[2018-12-22 11:40:06.464385 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:06.554547 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:07.618037 UTC] Performing line search
[2018-12-22 11:40:07.746690 UTC] Updating baseline
[2018-12-22 11:40:09.509664 UTC] Computing logging information
-------------------------------------
| Iteration            | 907        |
| ExpectedImprovement  | 0.017498   |
| ActualImprovement    | 0.016878   |
| ImprovementRatio     | 0.96458    |
| MeanKL               | 0.0070837  |
| Entropy              | -0.77377   |
| Perplexity           | 0.46127    |
| AveragePolicyStd     | 0.21615    |
| AveragePolicyStd[0]  | 0.25184    |
| AveragePolicyStd[1]  | 0.23593    |
| AveragePolicyStd[2]  | 0.17341    |
| AveragePolicyStd[3]  | 0.22873    |
| AveragePolicyStd[4]  | 0.15745    |
| AveragePolicyStd[5]  | 0.24952    |
| AverageReturn        | 1612.9     |
| MinReturn            | 135.48     |
| MaxReturn            | 1763.9     |
| StdReturn            | 294.01     |
| AverageEpisodeLength | 953.12     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.22     |
| TotalNEpisodes       | 19208      |
| TotalNSamples        | 4.5401e+06 |
| ExplainedVariance    | 0.00057061 |
-------------------------------------
[2018-12-22 11:40:09.902316 UTC] Saving snapshot
[2018-12-22 11:40:09.902608 UTC] Starting iteration 908
[2018-12-22 11:40:09.902729 UTC] Start collecting samples
[2018-12-22 11:40:12.837642 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:12.914155 UTC] Performing policy update
[2018-12-22 11:40:12.914811 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:13.005196 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:14.063756 UTC] Performing line search
[2018-12-22 11:40:14.192073 UTC] Updating baseline
[2018-12-22 11:40:16.183099 UTC] Computing logging information
-------------------------------------
| Iteration            | 908        |
| ExpectedImprovement  | 0.017325   |
| ActualImprovement    | 0.016494   |
| ImprovementRatio     | 0.95203    |
| MeanKL               | 0.0079902  |
| Entropy              | -0.76401   |
| Perplexity           | 0.46579    |
| AveragePolicyStd     | 0.21652    |
| AveragePolicyStd[0]  | 0.25185    |
| AveragePolicyStd[1]  | 0.23646    |
| AveragePolicyStd[2]  | 0.17357    |
| AveragePolicyStd[3]  | 0.22898    |
| AveragePolicyStd[4]  | 0.15757    |
| AveragePolicyStd[5]  | 0.25068    |
| AverageReturn        | 1613.8     |
| MinReturn            | 135.48     |
| MaxReturn            | 1763.9     |
| StdReturn            | 294.21     |
| AverageEpisodeLength | 953.12     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.22     |
| TotalNEpisodes       | 19213      |
| TotalNSamples        | 4.5451e+06 |
| ExplainedVariance    | 0.0020749  |
-------------------------------------
[2018-12-22 11:40:16.568626 UTC] Saving snapshot
[2018-12-22 11:40:16.568901 UTC] Starting iteration 909
[2018-12-22 11:40:16.569019 UTC] Start collecting samples
[2018-12-22 11:40:19.483927 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:19.560950 UTC] Performing policy update
[2018-12-22 11:40:19.561643 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:19.651257 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:20.718392 UTC] Performing line search
[2018-12-22 11:40:20.846903 UTC] Updating baseline
[2018-12-22 11:40:22.454249 UTC] Computing logging information
-------------------------------------
| Iteration            | 909        |
| ExpectedImprovement  | 0.018558   |
| ActualImprovement    | 0.016333   |
| ImprovementRatio     | 0.88014    |
| MeanKL               | 0.0073993  |
| Entropy              | -0.76789   |
| Perplexity           | 0.46399    |
| AveragePolicyStd     | 0.21633    |
| AveragePolicyStd[0]  | 0.25059    |
| AveragePolicyStd[1]  | 0.23607    |
| AveragePolicyStd[2]  | 0.17394    |
| AveragePolicyStd[3]  | 0.22949    |
| AveragePolicyStd[4]  | 0.15752    |
| AveragePolicyStd[5]  | 0.25036    |
| AverageReturn        | 1612.7     |
| MinReturn            | 135.48     |
| MaxReturn            | 1763.9     |
| StdReturn            | 293.89     |
| AverageEpisodeLength | 953.12     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.22     |
| TotalNEpisodes       | 19215      |
| TotalNSamples        | 4.5471e+06 |
| ExplainedVariance    | 0.0078362  |
-------------------------------------
[2018-12-22 11:40:22.846058 UTC] Saving snapshot
[2018-12-22 11:40:22.846334 UTC] Starting iteration 910
[2018-12-22 11:40:22.846454 UTC] Start collecting samples
[2018-12-22 11:40:25.802023 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:25.881446 UTC] Performing policy update
[2018-12-22 11:40:25.882237 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:25.976946 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:27.041975 UTC] Performing line search
[2018-12-22 11:40:27.170176 UTC] Updating baseline
[2018-12-22 11:40:28.760384 UTC] Computing logging information
-------------------------------------
| Iteration            | 910        |
| ExpectedImprovement  | 0.015947   |
| ActualImprovement    | 0.015007   |
| ImprovementRatio     | 0.94105    |
| MeanKL               | 0.0074924  |
| Entropy              | -0.76568   |
| Perplexity           | 0.46502    |
| AveragePolicyStd     | 0.21642    |
| AveragePolicyStd[0]  | 0.2508     |
| AveragePolicyStd[1]  | 0.2359     |
| AveragePolicyStd[2]  | 0.1741     |
| AveragePolicyStd[3]  | 0.22964    |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.2506     |
| AverageReturn        | 1624.3     |
| MinReturn            | 135.48     |
| MaxReturn            | 1763.9     |
| StdReturn            | 280.58     |
| AverageEpisodeLength | 961.02     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.24     |
| TotalNEpisodes       | 19222      |
| TotalNSamples        | 4.5541e+06 |
| ExplainedVariance    | 0.089184   |
-------------------------------------
[2018-12-22 11:40:29.144367 UTC] Saving snapshot
[2018-12-22 11:40:29.152646 UTC] Starting iteration 911
[2018-12-22 11:40:29.152849 UTC] Start collecting samples
[2018-12-22 11:40:32.076065 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:32.154538 UTC] Performing policy update
[2018-12-22 11:40:32.155296 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:32.245328 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:33.294742 UTC] Performing line search
[2018-12-22 11:40:33.422663 UTC] Updating baseline
[2018-12-22 11:40:34.864109 UTC] Computing logging information
-------------------------------------
| Iteration            | 911        |
| ExpectedImprovement  | 0.016223   |
| ActualImprovement    | 0.016025   |
| ImprovementRatio     | 0.9878     |
| MeanKL               | 0.0076573  |
| Entropy              | -0.75584   |
| Perplexity           | 0.46962    |
| AveragePolicyStd     | 0.21677    |
| AveragePolicyStd[0]  | 0.2506     |
| AveragePolicyStd[1]  | 0.23649    |
| AveragePolicyStd[2]  | 0.17454    |
| AveragePolicyStd[3]  | 0.23001    |
| AveragePolicyStd[4]  | 0.15762    |
| AveragePolicyStd[5]  | 0.25134    |
| AverageReturn        | 1623.6     |
| MinReturn            | 135.48     |
| MaxReturn            | 1763.9     |
| StdReturn            | 280.42     |
| AverageEpisodeLength | 961.02     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.24     |
| TotalNEpisodes       | 19227      |
| TotalNSamples        | 4.5591e+06 |
| ExplainedVariance    | 0.0044748  |
-------------------------------------
[2018-12-22 11:40:35.253993 UTC] Saving snapshot
[2018-12-22 11:40:35.254231 UTC] Starting iteration 912
[2018-12-22 11:40:35.254351 UTC] Start collecting samples
[2018-12-22 11:40:38.191082 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:38.268181 UTC] Performing policy update
[2018-12-22 11:40:38.269044 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:38.358367 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:39.418931 UTC] Performing line search
[2018-12-22 11:40:39.547454 UTC] Updating baseline
[2018-12-22 11:40:41.148978 UTC] Computing logging information
-------------------------------------
| Iteration            | 912        |
| ExpectedImprovement  | 0.017538   |
| ActualImprovement    | 0.016848   |
| ImprovementRatio     | 0.96067    |
| MeanKL               | 0.0075135  |
| Entropy              | -0.75021   |
| Perplexity           | 0.47227    |
| AveragePolicyStd     | 0.21697    |
| AveragePolicyStd[0]  | 0.2507     |
| AveragePolicyStd[1]  | 0.23704    |
| AveragePolicyStd[2]  | 0.17472    |
| AveragePolicyStd[3]  | 0.23072    |
| AveragePolicyStd[4]  | 0.15764    |
| AveragePolicyStd[5]  | 0.25101    |
| AverageReturn        | 1623.5     |
| MinReturn            | 135.48     |
| MaxReturn            | 1763.9     |
| StdReturn            | 280.28     |
| AverageEpisodeLength | 961.09     |
| MinEpisodeLength     | 116        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.25     |
| TotalNEpisodes       | 19231      |
| TotalNSamples        | 4.5631e+06 |
| ExplainedVariance    | -0.030982  |
-------------------------------------
[2018-12-22 11:40:41.542732 UTC] Saving snapshot
[2018-12-22 11:40:41.542986 UTC] Starting iteration 913
[2018-12-22 11:40:41.543101 UTC] Start collecting samples
[2018-12-22 11:40:44.554016 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:44.634422 UTC] Performing policy update
[2018-12-22 11:40:44.635055 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:44.725218 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:45.793073 UTC] Performing line search
[2018-12-22 11:40:45.925526 UTC] Updating baseline
[2018-12-22 11:40:47.287695 UTC] Computing logging information
-------------------------------------
| Iteration            | 913        |
| ExpectedImprovement  | 0.016639   |
| ActualImprovement    | 0.015713   |
| ImprovementRatio     | 0.94431    |
| MeanKL               | 0.0074777  |
| Entropy              | -0.75252   |
| Perplexity           | 0.47118    |
| AveragePolicyStd     | 0.21685    |
| AveragePolicyStd[0]  | 0.25019    |
| AveragePolicyStd[1]  | 0.23635    |
| AveragePolicyStd[2]  | 0.17471    |
| AveragePolicyStd[3]  | 0.23026    |
| AveragePolicyStd[4]  | 0.15807    |
| AveragePolicyStd[5]  | 0.25152    |
| AverageReturn        | 1629.7     |
| MinReturn            | 157.87     |
| MaxReturn            | 1763.9     |
| StdReturn            | 256.83     |
| AverageEpisodeLength | 964.72     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.42     |
| TotalNEpisodes       | 19239      |
| TotalNSamples        | 4.5698e+06 |
| ExplainedVariance    | 0.1213     |
-------------------------------------
[2018-12-22 11:40:47.684890 UTC] Saving snapshot
[2018-12-22 11:40:47.685140 UTC] Starting iteration 914
[2018-12-22 11:40:47.685256 UTC] Start collecting samples
[2018-12-22 11:40:50.635552 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:50.713890 UTC] Performing policy update
[2018-12-22 11:40:50.714653 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:50.804836 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:51.861826 UTC] Performing line search
[2018-12-22 11:40:51.990931 UTC] Updating baseline
[2018-12-22 11:40:53.471949 UTC] Computing logging information
-------------------------------------
| Iteration            | 914        |
| ExpectedImprovement  | 0.018313   |
| ActualImprovement    | 0.016807   |
| ImprovementRatio     | 0.91775    |
| MeanKL               | 0.0073305  |
| Entropy              | -0.76004   |
| Perplexity           | 0.46765    |
| AveragePolicyStd     | 0.21652    |
| AveragePolicyStd[0]  | 0.24948    |
| AveragePolicyStd[1]  | 0.23595    |
| AveragePolicyStd[2]  | 0.17444    |
| AveragePolicyStd[3]  | 0.22988    |
| AveragePolicyStd[4]  | 0.15853    |
| AveragePolicyStd[5]  | 0.25083    |
| AverageReturn        | 1629       |
| MinReturn            | 157.87     |
| MaxReturn            | 1763.9     |
| StdReturn            | 256.51     |
| AverageEpisodeLength | 964.72     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.42     |
| TotalNEpisodes       | 19244      |
| TotalNSamples        | 4.5748e+06 |
| ExplainedVariance    | -0.0006879 |
-------------------------------------
[2018-12-22 11:40:53.862858 UTC] Saving snapshot
[2018-12-22 11:40:53.863103 UTC] Starting iteration 915
[2018-12-22 11:40:53.863222 UTC] Start collecting samples
[2018-12-22 11:40:56.807745 UTC] Computing input variables for policy optimization
[2018-12-22 11:40:56.884249 UTC] Performing policy update
[2018-12-22 11:40:56.884897 UTC] Computing gradient in Euclidean space
[2018-12-22 11:40:56.974614 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:40:58.029843 UTC] Performing line search
[2018-12-22 11:40:58.157467 UTC] Updating baseline
[2018-12-22 11:41:00.206910 UTC] Computing logging information
-------------------------------------
| Iteration            | 915        |
| ExpectedImprovement  | 0.01668    |
| ActualImprovement    | 0.015502   |
| ImprovementRatio     | 0.92939    |
| MeanKL               | 0.00711    |
| Entropy              | -0.76741   |
| Perplexity           | 0.46421    |
| AveragePolicyStd     | 0.21627    |
| AveragePolicyStd[0]  | 0.24984    |
| AveragePolicyStd[1]  | 0.23587    |
| AveragePolicyStd[2]  | 0.17386    |
| AveragePolicyStd[3]  | 0.23054    |
| AveragePolicyStd[4]  | 0.15825    |
| AveragePolicyStd[5]  | 0.24928    |
| AverageReturn        | 1628.8     |
| MinReturn            | 157.87     |
| MaxReturn            | 1763.9     |
| StdReturn            | 256.45     |
| AverageEpisodeLength | 964.72     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.42     |
| TotalNEpisodes       | 19247      |
| TotalNSamples        | 4.5778e+06 |
| ExplainedVariance    | 0.0051756  |
-------------------------------------
[2018-12-22 11:41:00.595354 UTC] Saving snapshot
[2018-12-22 11:41:00.595643 UTC] Starting iteration 916
[2018-12-22 11:41:00.595785 UTC] Start collecting samples
[2018-12-22 11:41:03.555566 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:03.632468 UTC] Performing policy update
[2018-12-22 11:41:03.633229 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:03.722742 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:04.788658 UTC] Performing line search
[2018-12-22 11:41:04.916895 UTC] Updating baseline
[2018-12-22 11:41:08.350934 UTC] Computing logging information
-------------------------------------
| Iteration            | 916        |
| ExpectedImprovement  | 0.01799    |
| ActualImprovement    | 0.016734   |
| ImprovementRatio     | 0.93019    |
| MeanKL               | 0.0074806  |
| Entropy              | -0.76963   |
| Perplexity           | 0.46318    |
| AveragePolicyStd     | 0.21619    |
| AveragePolicyStd[0]  | 0.24997    |
| AveragePolicyStd[1]  | 0.23496    |
| AveragePolicyStd[2]  | 0.17377    |
| AveragePolicyStd[3]  | 0.23073    |
| AveragePolicyStd[4]  | 0.15823    |
| AveragePolicyStd[5]  | 0.24949    |
| AverageReturn        | 1628.3     |
| MinReturn            | 157.87     |
| MaxReturn            | 1763.9     |
| StdReturn            | 256.37     |
| AverageEpisodeLength | 964.72     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.42     |
| TotalNEpisodes       | 19252      |
| TotalNSamples        | 4.5828e+06 |
| ExplainedVariance    | -0.001091  |
-------------------------------------
[2018-12-22 11:41:08.742457 UTC] Saving snapshot
[2018-12-22 11:41:08.742730 UTC] Starting iteration 917
[2018-12-22 11:41:08.742866 UTC] Start collecting samples
[2018-12-22 11:41:11.745639 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:11.824860 UTC] Performing policy update
[2018-12-22 11:41:11.825638 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:11.915382 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:12.972927 UTC] Performing line search
[2018-12-22 11:41:13.100400 UTC] Updating baseline
[2018-12-22 11:41:14.690689 UTC] Computing logging information
-------------------------------------
| Iteration            | 917        |
| ExpectedImprovement  | 0.019418   |
| ActualImprovement    | 0.018152   |
| ImprovementRatio     | 0.93479    |
| MeanKL               | 0.0071944  |
| Entropy              | -0.77478   |
| Perplexity           | 0.4608     |
| AveragePolicyStd     | 0.21601    |
| AveragePolicyStd[0]  | 0.25002    |
| AveragePolicyStd[1]  | 0.23426    |
| AveragePolicyStd[2]  | 0.17358    |
| AveragePolicyStd[3]  | 0.23044    |
| AveragePolicyStd[4]  | 0.15817    |
| AveragePolicyStd[5]  | 0.24959    |
| AverageReturn        | 1630       |
| MinReturn            | 157.87     |
| MaxReturn            | 1763.9     |
| StdReturn            | 256.34     |
| AverageEpisodeLength | 965.52     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.39     |
| TotalNEpisodes       | 19260      |
| TotalNSamples        | 4.5908e+06 |
| ExplainedVariance    | 0.0014253  |
-------------------------------------
[2018-12-22 11:41:15.085517 UTC] Saving snapshot
[2018-12-22 11:41:15.085810 UTC] Starting iteration 918
[2018-12-22 11:41:15.085936 UTC] Start collecting samples
[2018-12-22 11:41:18.046749 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:18.124027 UTC] Performing policy update
[2018-12-22 11:41:18.124986 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:18.215164 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:19.274916 UTC] Performing line search
[2018-12-22 11:41:19.402292 UTC] Updating baseline
[2018-12-22 11:41:20.662511 UTC] Computing logging information
------------------------------------
| Iteration            | 918       |
| ExpectedImprovement  | 0.017984  |
| ActualImprovement    | 0.016577  |
| ImprovementRatio     | 0.92177   |
| MeanKL               | 0.0071629 |
| Entropy              | -0.77438  |
| Perplexity           | 0.46099   |
| AveragePolicyStd     | 0.21606   |
| AveragePolicyStd[0]  | 0.25081   |
| AveragePolicyStd[1]  | 0.2342    |
| AveragePolicyStd[2]  | 0.17333   |
| AveragePolicyStd[3]  | 0.23038   |
| AveragePolicyStd[4]  | 0.15805   |
| AveragePolicyStd[5]  | 0.24959   |
| AverageReturn        | 1632.4    |
| MinReturn            | 157.87    |
| MaxReturn            | 1763.9    |
| StdReturn            | 233.4     |
| AverageEpisodeLength | 968.61    |
| MinEpisodeLength     | 113       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 131.63    |
| TotalNEpisodes       | 19265     |
| TotalNSamples        | 4.595e+06 |
| ExplainedVariance    | 0.19114   |
------------------------------------
[2018-12-22 11:41:21.050935 UTC] Saving snapshot
[2018-12-22 11:41:21.051180 UTC] Starting iteration 919
[2018-12-22 11:41:21.051299 UTC] Start collecting samples
[2018-12-22 11:41:24.012585 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:24.090726 UTC] Performing policy update
[2018-12-22 11:41:24.091601 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:24.180908 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:25.244008 UTC] Performing line search
[2018-12-22 11:41:25.372418 UTC] Updating baseline
[2018-12-22 11:41:26.741407 UTC] Computing logging information
------------------------------------
| Iteration            | 919       |
| ExpectedImprovement  | 0.01612   |
| ActualImprovement    | 0.015516  |
| ImprovementRatio     | 0.96255   |
| MeanKL               | 0.0080016 |
| Entropy              | -0.7748   |
| Perplexity           | 0.4608    |
| AveragePolicyStd     | 0.21601   |
| AveragePolicyStd[0]  | 0.25051   |
| AveragePolicyStd[1]  | 0.23381   |
| AveragePolicyStd[2]  | 0.17362   |
| AveragePolicyStd[3]  | 0.2307    |
| AveragePolicyStd[4]  | 0.15811   |
| AveragePolicyStd[5]  | 0.24933   |
| AverageReturn        | 1639.7    |
| MinReturn            | 157.87    |
| MaxReturn            | 1763.9    |
| StdReturn            | 226.95    |
| AverageEpisodeLength | 972.26    |
| MinEpisodeLength     | 113       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 127.32    |
| TotalNEpisodes       | 19269     |
| TotalNSamples        | 4.599e+06 |
| ExplainedVariance    | 0.11649   |
------------------------------------
[2018-12-22 11:41:27.133414 UTC] Saving snapshot
[2018-12-22 11:41:27.133672 UTC] Starting iteration 920
[2018-12-22 11:41:27.133833 UTC] Start collecting samples
[2018-12-22 11:41:30.131710 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:30.212538 UTC] Performing policy update
[2018-12-22 11:41:30.213417 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:30.303331 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:31.368411 UTC] Performing line search
[2018-12-22 11:41:31.497180 UTC] Updating baseline
[2018-12-22 11:41:32.849061 UTC] Computing logging information
------------------------------------
| Iteration            | 920       |
| ExpectedImprovement  | 0.021885  |
| ActualImprovement    | 0.020745  |
| ImprovementRatio     | 0.94791   |
| MeanKL               | 0.0069292 |
| Entropy              | -0.77211  |
| Perplexity           | 0.46204   |
| AveragePolicyStd     | 0.2161    |
| AveragePolicyStd[0]  | 0.25024   |
| AveragePolicyStd[1]  | 0.23394   |
| AveragePolicyStd[2]  | 0.17369   |
| AveragePolicyStd[3]  | 0.2307    |
| AveragePolicyStd[4]  | 0.15829   |
| AveragePolicyStd[5]  | 0.24976   |
| AverageReturn        | 1622.9    |
| MinReturn            | 103.53    |
| MaxReturn            | 1763.9    |
| StdReturn            | 273.45    |
| AverageEpisodeLength | 963.16    |
| MinEpisodeLength     | 90        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 154.61    |
| TotalNEpisodes       | 19275     |
| TotalNSamples        | 4.604e+06 |
| ExplainedVariance    | 0.087146  |
------------------------------------
[2018-12-22 11:41:33.240020 UTC] Saving snapshot
[2018-12-22 11:41:33.248132 UTC] Starting iteration 921
[2018-12-22 11:41:33.248346 UTC] Start collecting samples
[2018-12-22 11:41:36.262726 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:36.344339 UTC] Performing policy update
[2018-12-22 11:41:36.345042 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:36.435141 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:37.504735 UTC] Performing line search
[2018-12-22 11:41:37.633003 UTC] Updating baseline
[2018-12-22 11:41:39.010428 UTC] Computing logging information
-------------------------------------
| Iteration            | 921        |
| ExpectedImprovement  | 0.016722   |
| ActualImprovement    | 0.01621    |
| ImprovementRatio     | 0.9694     |
| MeanKL               | 0.0075857  |
| Entropy              | -0.78529   |
| Perplexity           | 0.45599    |
| AveragePolicyStd     | 0.21567    |
| AveragePolicyStd[0]  | 0.25029    |
| AveragePolicyStd[1]  | 0.23303    |
| AveragePolicyStd[2]  | 0.17326    |
| AveragePolicyStd[3]  | 0.2305     |
| AveragePolicyStd[4]  | 0.1576     |
| AveragePolicyStd[5]  | 0.2493     |
| AverageReturn        | 1616       |
| MinReturn            | 103.53     |
| MaxReturn            | 1763.9     |
| StdReturn            | 286.07     |
| AverageEpisodeLength | 959.22     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.84     |
| TotalNEpisodes       | 19283      |
| TotalNSamples        | 4.6111e+06 |
| ExplainedVariance    | 0.16489    |
-------------------------------------
[2018-12-22 11:41:39.403352 UTC] Saving snapshot
[2018-12-22 11:41:39.403618 UTC] Starting iteration 922
[2018-12-22 11:41:39.403736 UTC] Start collecting samples
[2018-12-22 11:41:42.366235 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:42.444717 UTC] Performing policy update
[2018-12-22 11:41:42.445324 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:42.535166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:43.591113 UTC] Performing line search
[2018-12-22 11:41:43.718413 UTC] Updating baseline
[2018-12-22 11:41:45.168617 UTC] Computing logging information
-------------------------------------
| Iteration            | 922        |
| ExpectedImprovement  | 0.018772   |
| ActualImprovement    | 0.016939   |
| ImprovementRatio     | 0.90234    |
| MeanKL               | 0.0078822  |
| Entropy              | -0.78785   |
| Perplexity           | 0.45482    |
| AveragePolicyStd     | 0.21558    |
| AveragePolicyStd[0]  | 0.24945    |
| AveragePolicyStd[1]  | 0.23347    |
| AveragePolicyStd[2]  | 0.17256    |
| AveragePolicyStd[3]  | 0.23078    |
| AveragePolicyStd[4]  | 0.15778    |
| AveragePolicyStd[5]  | 0.24946    |
| AverageReturn        | 1615.3     |
| MinReturn            | 103.53     |
| MaxReturn            | 1763.9     |
| StdReturn            | 285.88     |
| AverageEpisodeLength | 959.22     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.84     |
| TotalNEpisodes       | 19286      |
| TotalNSamples        | 4.6141e+06 |
| ExplainedVariance    | -0.0035929 |
-------------------------------------
[2018-12-22 11:41:45.563096 UTC] Saving snapshot
[2018-12-22 11:41:45.563375 UTC] Starting iteration 923
[2018-12-22 11:41:45.563516 UTC] Start collecting samples
[2018-12-22 11:41:48.520465 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:48.598759 UTC] Performing policy update
[2018-12-22 11:41:48.599546 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:48.689712 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:49.751078 UTC] Performing line search
[2018-12-22 11:41:49.878575 UTC] Updating baseline
[2018-12-22 11:41:51.415123 UTC] Computing logging information
--------------------------------------
| Iteration            | 923         |
| ExpectedImprovement  | 0.016441    |
| ActualImprovement    | 0.015386    |
| ImprovementRatio     | 0.93585     |
| MeanKL               | 0.0077288   |
| Entropy              | -0.79221    |
| Perplexity           | 0.45284     |
| AveragePolicyStd     | 0.21539     |
| AveragePolicyStd[0]  | 0.24832     |
| AveragePolicyStd[1]  | 0.23262     |
| AveragePolicyStd[2]  | 0.17274     |
| AveragePolicyStd[3]  | 0.23091     |
| AveragePolicyStd[4]  | 0.1579      |
| AveragePolicyStd[5]  | 0.24983     |
| AverageReturn        | 1614.5      |
| MinReturn            | 103.53      |
| MaxReturn            | 1763.9      |
| StdReturn            | 285.56      |
| AverageEpisodeLength | 959.22      |
| MinEpisodeLength     | 90          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 160.84      |
| TotalNEpisodes       | 19291       |
| TotalNSamples        | 4.6191e+06  |
| ExplainedVariance    | -0.00022915 |
--------------------------------------
[2018-12-22 11:41:51.805082 UTC] Saving snapshot
[2018-12-22 11:41:51.805342 UTC] Starting iteration 924
[2018-12-22 11:41:51.805461 UTC] Start collecting samples
[2018-12-22 11:41:54.769080 UTC] Computing input variables for policy optimization
[2018-12-22 11:41:54.849447 UTC] Performing policy update
[2018-12-22 11:41:54.850446 UTC] Computing gradient in Euclidean space
[2018-12-22 11:41:54.941012 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:41:56.007794 UTC] Performing line search
[2018-12-22 11:41:56.136109 UTC] Updating baseline
[2018-12-22 11:41:57.399842 UTC] Computing logging information
-------------------------------------
| Iteration            | 924        |
| ExpectedImprovement  | 0.018475   |
| ActualImprovement    | 0.01754    |
| ImprovementRatio     | 0.94938    |
| MeanKL               | 0.0077734  |
| Entropy              | -0.79007   |
| Perplexity           | 0.45382    |
| AveragePolicyStd     | 0.2155     |
| AveragePolicyStd[0]  | 0.24907    |
| AveragePolicyStd[1]  | 0.23243    |
| AveragePolicyStd[2]  | 0.17266    |
| AveragePolicyStd[3]  | 0.23071    |
| AveragePolicyStd[4]  | 0.1578     |
| AveragePolicyStd[5]  | 0.25032    |
| AverageReturn        | 1615.5     |
| MinReturn            | 103.53     |
| MaxReturn            | 1740.4     |
| StdReturn            | 285.74     |
| AverageEpisodeLength | 959.29     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.85     |
| TotalNEpisodes       | 19298      |
| TotalNSamples        | 4.6261e+06 |
| ExplainedVariance    | 0.00063727 |
-------------------------------------
[2018-12-22 11:41:57.796398 UTC] Saving snapshot
[2018-12-22 11:41:57.796658 UTC] Starting iteration 925
[2018-12-22 11:41:57.796775 UTC] Start collecting samples
[2018-12-22 11:42:00.745949 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:00.823568 UTC] Performing policy update
[2018-12-22 11:42:00.824386 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:00.915139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:01.989713 UTC] Performing line search
[2018-12-22 11:42:02.117915 UTC] Updating baseline
[2018-12-22 11:42:03.646969 UTC] Computing logging information
-------------------------------------
| Iteration            | 925        |
| ExpectedImprovement  | 0.018063   |
| ActualImprovement    | 0.016712   |
| ImprovementRatio     | 0.92518    |
| MeanKL               | 0.007015   |
| Entropy              | -0.78285   |
| Perplexity           | 0.4571     |
| AveragePolicyStd     | 0.21573    |
| AveragePolicyStd[0]  | 0.24944    |
| AveragePolicyStd[1]  | 0.23267    |
| AveragePolicyStd[2]  | 0.17359    |
| AveragePolicyStd[3]  | 0.23065    |
| AveragePolicyStd[4]  | 0.15777    |
| AveragePolicyStd[5]  | 0.25026    |
| AverageReturn        | 1604.1     |
| MinReturn            | 103.53     |
| MaxReturn            | 1740.4     |
| StdReturn            | 308.5      |
| AverageEpisodeLength | 952.19     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.03     |
| TotalNEpisodes       | 19302      |
| TotalNSamples        | 4.6294e+06 |
| ExplainedVariance    | 0.16124    |
-------------------------------------
[2018-12-22 11:42:04.043193 UTC] Saving snapshot
[2018-12-22 11:42:04.043427 UTC] Starting iteration 926
[2018-12-22 11:42:04.043561 UTC] Start collecting samples
[2018-12-22 11:42:06.988960 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:07.068186 UTC] Performing policy update
[2018-12-22 11:42:07.068885 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:07.158471 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:08.221691 UTC] Performing line search
[2018-12-22 11:42:08.349204 UTC] Updating baseline
[2018-12-22 11:42:09.547246 UTC] Computing logging information
-------------------------------------
| Iteration            | 926        |
| ExpectedImprovement  | 0.016925   |
| ActualImprovement    | 0.016262   |
| ImprovementRatio     | 0.96084    |
| MeanKL               | 0.0072904  |
| Entropy              | -0.79088   |
| Perplexity           | 0.45345    |
| AveragePolicyStd     | 0.21547    |
| AveragePolicyStd[0]  | 0.24761    |
| AveragePolicyStd[1]  | 0.23294    |
| AveragePolicyStd[2]  | 0.17303    |
| AveragePolicyStd[3]  | 0.231      |
| AveragePolicyStd[4]  | 0.15739    |
| AveragePolicyStd[5]  | 0.25083    |
| AverageReturn        | 1606.9     |
| MinReturn            | 103.53     |
| MaxReturn            | 1740.4     |
| StdReturn            | 309.11     |
| AverageEpisodeLength | 952.19     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.03     |
| TotalNEpisodes       | 19306      |
| TotalNSamples        | 4.6334e+06 |
| ExplainedVariance    | -0.087999  |
-------------------------------------
[2018-12-22 11:42:09.940014 UTC] Saving snapshot
[2018-12-22 11:42:09.940309 UTC] Starting iteration 927
[2018-12-22 11:42:09.940432 UTC] Start collecting samples
[2018-12-22 11:42:12.929750 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:13.010882 UTC] Performing policy update
[2018-12-22 11:42:13.011769 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:13.100319 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:14.167373 UTC] Performing line search
[2018-12-22 11:42:14.295863 UTC] Updating baseline
[2018-12-22 11:42:15.730711 UTC] Computing logging information
-------------------------------------
| Iteration            | 927        |
| ExpectedImprovement  | 0.019048   |
| ActualImprovement    | 0.01759    |
| ImprovementRatio     | 0.92344    |
| MeanKL               | 0.0077327  |
| Entropy              | -0.8003    |
| Perplexity           | 0.4492     |
| AveragePolicyStd     | 0.2151     |
| AveragePolicyStd[0]  | 0.24772    |
| AveragePolicyStd[1]  | 0.23315    |
| AveragePolicyStd[2]  | 0.17255    |
| AveragePolicyStd[3]  | 0.23039    |
| AveragePolicyStd[4]  | 0.15756    |
| AveragePolicyStd[5]  | 0.24923    |
| AverageReturn        | 1610       |
| MinReturn            | 103.53     |
| MaxReturn            | 1763       |
| StdReturn            | 310.19     |
| AverageEpisodeLength | 952.19     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.03     |
| TotalNEpisodes       | 19313      |
| TotalNSamples        | 4.6404e+06 |
| ExplainedVariance    | -0.011355  |
-------------------------------------
[2018-12-22 11:42:16.126030 UTC] Saving snapshot
[2018-12-22 11:42:16.126288 UTC] Starting iteration 928
[2018-12-22 11:42:16.126429 UTC] Start collecting samples
[2018-12-22 11:42:19.054478 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:19.131484 UTC] Performing policy update
[2018-12-22 11:42:19.132228 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:19.220754 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:20.281251 UTC] Performing line search
[2018-12-22 11:42:20.408943 UTC] Updating baseline
[2018-12-22 11:42:21.870029 UTC] Computing logging information
-------------------------------------
| Iteration            | 928        |
| ExpectedImprovement  | 0.016192   |
| ActualImprovement    | 0.015445   |
| ImprovementRatio     | 0.95391    |
| MeanKL               | 0.0072684  |
| Entropy              | -0.81124   |
| Perplexity           | 0.44431    |
| AveragePolicyStd     | 0.21469    |
| AveragePolicyStd[0]  | 0.2467     |
| AveragePolicyStd[1]  | 0.23252    |
| AveragePolicyStd[2]  | 0.17172    |
| AveragePolicyStd[3]  | 0.23028    |
| AveragePolicyStd[4]  | 0.15779    |
| AveragePolicyStd[5]  | 0.24916    |
| AverageReturn        | 1610.7     |
| MinReturn            | 103.53     |
| MaxReturn            | 1763       |
| StdReturn            | 310.38     |
| AverageEpisodeLength | 952.19     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.03     |
| TotalNEpisodes       | 19317      |
| TotalNSamples        | 4.6444e+06 |
| ExplainedVariance    | -0.0083089 |
-------------------------------------
[2018-12-22 11:42:22.270391 UTC] Saving snapshot
[2018-12-22 11:42:22.270686 UTC] Starting iteration 929
[2018-12-22 11:42:22.270805 UTC] Start collecting samples
[2018-12-22 11:42:25.225690 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:25.303750 UTC] Performing policy update
[2018-12-22 11:42:25.304633 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:25.393970 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:26.456882 UTC] Performing line search
[2018-12-22 11:42:26.583896 UTC] Updating baseline
[2018-12-22 11:42:27.938355 UTC] Computing logging information
-------------------------------------
| Iteration            | 929        |
| ExpectedImprovement  | 0.015749   |
| ActualImprovement    | 0.014993   |
| ImprovementRatio     | 0.95199    |
| MeanKL               | 0.0082787  |
| Entropy              | -0.81164   |
| Perplexity           | 0.44413    |
| AveragePolicyStd     | 0.21464    |
| AveragePolicyStd[0]  | 0.24703    |
| AveragePolicyStd[1]  | 0.23099    |
| AveragePolicyStd[2]  | 0.17241    |
| AveragePolicyStd[3]  | 0.22985    |
| AveragePolicyStd[4]  | 0.1579     |
| AveragePolicyStd[5]  | 0.24966    |
| AverageReturn        | 1613.6     |
| MinReturn            | 103.53     |
| MaxReturn            | 1763       |
| StdReturn            | 310.77     |
| AverageEpisodeLength | 952.31     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.06     |
| TotalNEpisodes       | 19322      |
| TotalNSamples        | 4.6494e+06 |
| ExplainedVariance    | 0.015545   |
-------------------------------------
[2018-12-22 11:42:28.331270 UTC] Saving snapshot
[2018-12-22 11:42:28.331531 UTC] Starting iteration 930
[2018-12-22 11:42:28.331657 UTC] Start collecting samples
[2018-12-22 11:42:31.309583 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:31.389152 UTC] Performing policy update
[2018-12-22 11:42:31.390145 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:31.480917 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:32.547084 UTC] Performing line search
[2018-12-22 11:42:32.673039 UTC] Updating baseline
[2018-12-22 11:42:33.925378 UTC] Computing logging information
-------------------------------------
| Iteration            | 930        |
| ExpectedImprovement  | 0.017209   |
| ActualImprovement    | 0.016358   |
| ImprovementRatio     | 0.95055    |
| MeanKL               | 0.0076815  |
| Entropy              | -0.81363   |
| Perplexity           | 0.44324    |
| AveragePolicyStd     | 0.21458    |
| AveragePolicyStd[0]  | 0.24711    |
| AveragePolicyStd[1]  | 0.23124    |
| AveragePolicyStd[2]  | 0.17219    |
| AveragePolicyStd[3]  | 0.2302     |
| AveragePolicyStd[4]  | 0.1578     |
| AveragePolicyStd[5]  | 0.24891    |
| AverageReturn        | 1613.5     |
| MinReturn            | 103.53     |
| MaxReturn            | 1763       |
| StdReturn            | 310.75     |
| AverageEpisodeLength | 952.31     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.06     |
| TotalNEpisodes       | 19328      |
| TotalNSamples        | 4.6554e+06 |
| ExplainedVariance    | 0.0018612  |
-------------------------------------
[2018-12-22 11:42:34.315191 UTC] Saving snapshot
[2018-12-22 11:42:34.323389 UTC] Starting iteration 931
[2018-12-22 11:42:34.323604 UTC] Start collecting samples
[2018-12-22 11:42:37.252841 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:37.330992 UTC] Performing policy update
[2018-12-22 11:42:37.331742 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:37.421699 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:38.489805 UTC] Performing line search
[2018-12-22 11:42:38.618189 UTC] Updating baseline
[2018-12-22 11:42:40.276329 UTC] Computing logging information
--------------------------------------
| Iteration            | 931         |
| ExpectedImprovement  | 0.017555    |
| ActualImprovement    | 0.015787    |
| ImprovementRatio     | 0.89928     |
| MeanKL               | 0.0073653   |
| Entropy              | -0.81494    |
| Perplexity           | 0.44267     |
| AveragePolicyStd     | 0.21453     |
| AveragePolicyStd[0]  | 0.24751     |
| AveragePolicyStd[1]  | 0.23161     |
| AveragePolicyStd[2]  | 0.17188     |
| AveragePolicyStd[3]  | 0.22967     |
| AveragePolicyStd[4]  | 0.15797     |
| AveragePolicyStd[5]  | 0.24852     |
| AverageReturn        | 1614.7      |
| MinReturn            | 103.53      |
| MaxReturn            | 1763        |
| StdReturn            | 311.09      |
| AverageEpisodeLength | 952.31      |
| MinEpisodeLength     | 90          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 174.06      |
| TotalNEpisodes       | 19331       |
| TotalNSamples        | 4.6584e+06  |
| ExplainedVariance    | -0.00026626 |
--------------------------------------
[2018-12-22 11:42:40.667850 UTC] Saving snapshot
[2018-12-22 11:42:40.668229 UTC] Starting iteration 932
[2018-12-22 11:42:40.668462 UTC] Start collecting samples
[2018-12-22 11:42:43.619233 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:43.696072 UTC] Performing policy update
[2018-12-22 11:42:43.696734 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:43.789831 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:44.848033 UTC] Performing line search
[2018-12-22 11:42:44.974909 UTC] Updating baseline
[2018-12-22 11:42:46.674349 UTC] Computing logging information
-------------------------------------
| Iteration            | 932        |
| ExpectedImprovement  | 0.017619   |
| ActualImprovement    | 0.015967   |
| ImprovementRatio     | 0.90625    |
| MeanKL               | 0.0076756  |
| Entropy              | -0.8131    |
| Perplexity           | 0.44348    |
| AveragePolicyStd     | 0.21465    |
| AveragePolicyStd[0]  | 0.248      |
| AveragePolicyStd[1]  | 0.23166    |
| AveragePolicyStd[2]  | 0.17112    |
| AveragePolicyStd[3]  | 0.23055    |
| AveragePolicyStd[4]  | 0.158      |
| AveragePolicyStd[5]  | 0.24855    |
| AverageReturn        | 1606.5     |
| MinReturn            | 103.53     |
| MaxReturn            | 1764.8     |
| StdReturn            | 339.34     |
| AverageEpisodeLength | 947.21     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.88     |
| TotalNEpisodes       | 19337      |
| TotalNSamples        | 4.6635e+06 |
| ExplainedVariance    | 0.035717   |
-------------------------------------
[2018-12-22 11:42:47.067396 UTC] Saving snapshot
[2018-12-22 11:42:47.067662 UTC] Starting iteration 933
[2018-12-22 11:42:47.067784 UTC] Start collecting samples
[2018-12-22 11:42:50.024387 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:50.103971 UTC] Performing policy update
[2018-12-22 11:42:50.104732 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:50.194033 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:51.243183 UTC] Performing line search
[2018-12-22 11:42:51.370244 UTC] Updating baseline
[2018-12-22 11:42:52.627801 UTC] Computing logging information
-------------------------------------
| Iteration            | 933        |
| ExpectedImprovement  | 0.015395   |
| ActualImprovement    | 0.014986   |
| ImprovementRatio     | 0.97341    |
| MeanKL               | 0.0076956  |
| Entropy              | -0.81551   |
| Perplexity           | 0.44241    |
| AveragePolicyStd     | 0.21454    |
| AveragePolicyStd[0]  | 0.24744    |
| AveragePolicyStd[1]  | 0.23132    |
| AveragePolicyStd[2]  | 0.17073    |
| AveragePolicyStd[3]  | 0.23084    |
| AveragePolicyStd[4]  | 0.15835    |
| AveragePolicyStd[5]  | 0.24859    |
| AverageReturn        | 1624       |
| MinReturn            | 103.53     |
| MaxReturn            | 1764.8     |
| StdReturn            | 307.21     |
| AverageEpisodeLength | 956.08     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.53     |
| TotalNEpisodes       | 19345      |
| TotalNSamples        | 4.6715e+06 |
| ExplainedVariance    | -0.0024104 |
-------------------------------------
[2018-12-22 11:42:53.019483 UTC] Saving snapshot
[2018-12-22 11:42:53.019752 UTC] Starting iteration 934
[2018-12-22 11:42:53.019882 UTC] Start collecting samples
[2018-12-22 11:42:55.947252 UTC] Computing input variables for policy optimization
[2018-12-22 11:42:56.024644 UTC] Performing policy update
[2018-12-22 11:42:56.025238 UTC] Computing gradient in Euclidean space
[2018-12-22 11:42:56.113726 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:42:57.165832 UTC] Performing line search
[2018-12-22 11:42:57.293808 UTC] Updating baseline
[2018-12-22 11:42:58.547971 UTC] Computing logging information
-------------------------------------
| Iteration            | 934        |
| ExpectedImprovement  | 0.016949   |
| ActualImprovement    | 0.016246   |
| ImprovementRatio     | 0.95848    |
| MeanKL               | 0.007702   |
| Entropy              | -0.81305   |
| Perplexity           | 0.44351    |
| AveragePolicyStd     | 0.21464    |
| AveragePolicyStd[0]  | 0.24765    |
| AveragePolicyStd[1]  | 0.23121    |
| AveragePolicyStd[2]  | 0.17072    |
| AveragePolicyStd[3]  | 0.23182    |
| AveragePolicyStd[4]  | 0.15833    |
| AveragePolicyStd[5]  | 0.24809    |
| AverageReturn        | 1610.2     |
| MinReturn            | 103.53     |
| MaxReturn            | 1764.8     |
| StdReturn            | 334.78     |
| AverageEpisodeLength | 948.09     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.2      |
| TotalNEpisodes       | 19349      |
| TotalNSamples        | 4.6747e+06 |
| ExplainedVariance    | 0.2044     |
-------------------------------------
[2018-12-22 11:42:58.940049 UTC] Saving snapshot
[2018-12-22 11:42:58.940290 UTC] Starting iteration 935
[2018-12-22 11:42:58.940411 UTC] Start collecting samples
[2018-12-22 11:43:01.876947 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:01.955595 UTC] Performing policy update
[2018-12-22 11:43:01.956555 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:02.046634 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:03.100355 UTC] Performing line search
[2018-12-22 11:43:03.227939 UTC] Updating baseline
[2018-12-22 11:43:04.644184 UTC] Computing logging information
-------------------------------------
| Iteration            | 935        |
| ExpectedImprovement  | 0.01662    |
| ActualImprovement    | 0.016372   |
| ImprovementRatio     | 0.98509    |
| MeanKL               | 0.0078139  |
| Entropy              | -0.81032   |
| Perplexity           | 0.44472    |
| AveragePolicyStd     | 0.21474    |
| AveragePolicyStd[0]  | 0.24855    |
| AveragePolicyStd[1]  | 0.23124    |
| AveragePolicyStd[2]  | 0.17082    |
| AveragePolicyStd[3]  | 0.23092    |
| AveragePolicyStd[4]  | 0.15849    |
| AveragePolicyStd[5]  | 0.24841    |
| AverageReturn        | 1610.8     |
| MinReturn            | 103.53     |
| MaxReturn            | 1764.8     |
| StdReturn            | 334.95     |
| AverageEpisodeLength | 948.09     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.2      |
| TotalNEpisodes       | 19354      |
| TotalNSamples        | 4.6797e+06 |
| ExplainedVariance    | -0.012202  |
-------------------------------------
[2018-12-22 11:43:05.036667 UTC] Saving snapshot
[2018-12-22 11:43:05.036933 UTC] Starting iteration 936
[2018-12-22 11:43:05.037045 UTC] Start collecting samples
[2018-12-22 11:43:07.946463 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:08.024060 UTC] Performing policy update
[2018-12-22 11:43:08.024847 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:08.115348 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:09.176169 UTC] Performing line search
[2018-12-22 11:43:09.303916 UTC] Updating baseline
[2018-12-22 11:43:10.899344 UTC] Computing logging information
-------------------------------------
| Iteration            | 936        |
| ExpectedImprovement  | 0.017238   |
| ActualImprovement    | 0.016161   |
| ImprovementRatio     | 0.93752    |
| MeanKL               | 0.0074488  |
| Entropy              | -0.807     |
| Perplexity           | 0.44619    |
| AveragePolicyStd     | 0.21481    |
| AveragePolicyStd[0]  | 0.24801    |
| AveragePolicyStd[1]  | 0.23133    |
| AveragePolicyStd[2]  | 0.17104    |
| AveragePolicyStd[3]  | 0.2313     |
| AveragePolicyStd[4]  | 0.15891    |
| AveragePolicyStd[5]  | 0.24829    |
| AverageReturn        | 1610.7     |
| MinReturn            | 103.53     |
| MaxReturn            | 1764.8     |
| StdReturn            | 334.99     |
| AverageEpisodeLength | 948.09     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.2      |
| TotalNEpisodes       | 19359      |
| TotalNSamples        | 4.6847e+06 |
| ExplainedVariance    | -0.0016818 |
-------------------------------------
[2018-12-22 11:43:11.324141 UTC] Saving snapshot
[2018-12-22 11:43:11.324436 UTC] Starting iteration 937
[2018-12-22 11:43:11.324648 UTC] Start collecting samples
[2018-12-22 11:43:14.509101 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:14.591792 UTC] Performing policy update
[2018-12-22 11:43:14.592365 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:14.686724 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:15.788201 UTC] Performing line search
[2018-12-22 11:43:15.925910 UTC] Updating baseline
[2018-12-22 11:43:18.572098 UTC] Computing logging information
-------------------------------------
| Iteration            | 937        |
| ExpectedImprovement  | 0.016469   |
| ActualImprovement    | 0.01561    |
| ImprovementRatio     | 0.94782    |
| MeanKL               | 0.0074494  |
| Entropy              | -0.81115   |
| Perplexity           | 0.44435    |
| AveragePolicyStd     | 0.21461    |
| AveragePolicyStd[0]  | 0.24732    |
| AveragePolicyStd[1]  | 0.23018    |
| AveragePolicyStd[2]  | 0.17144    |
| AveragePolicyStd[3]  | 0.23128    |
| AveragePolicyStd[4]  | 0.15915    |
| AveragePolicyStd[5]  | 0.24826    |
| AverageReturn        | 1629.7     |
| MinReturn            | 103.53     |
| MaxReturn            | 1764.8     |
| StdReturn            | 316.7      |
| AverageEpisodeLength | 957.03     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.01     |
| TotalNEpisodes       | 19365      |
| TotalNSamples        | 4.6907e+06 |
| ExplainedVariance    | 0.0003177  |
-------------------------------------
[2018-12-22 11:43:18.985820 UTC] Saving snapshot
[2018-12-22 11:43:18.986077 UTC] Starting iteration 938
[2018-12-22 11:43:18.986194 UTC] Start collecting samples
[2018-12-22 11:43:22.218380 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:22.300345 UTC] Performing policy update
[2018-12-22 11:43:22.300970 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:22.396847 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:23.517825 UTC] Performing line search
[2018-12-22 11:43:23.650953 UTC] Updating baseline
[2018-12-22 11:43:25.226532 UTC] Computing logging information
-------------------------------------
| Iteration            | 938        |
| ExpectedImprovement  | 0.017649   |
| ActualImprovement    | 0.015803   |
| ImprovementRatio     | 0.89541    |
| MeanKL               | 0.007304   |
| Entropy              | -0.81221   |
| Perplexity           | 0.44388    |
| AveragePolicyStd     | 0.21452    |
| AveragePolicyStd[0]  | 0.24705    |
| AveragePolicyStd[1]  | 0.22921    |
| AveragePolicyStd[2]  | 0.17203    |
| AveragePolicyStd[3]  | 0.23143    |
| AveragePolicyStd[4]  | 0.15928    |
| AveragePolicyStd[5]  | 0.2481     |
| AverageReturn        | 1631.9     |
| MinReturn            | 103.53     |
| MaxReturn            | 1776.1     |
| StdReturn            | 317.23     |
| AverageEpisodeLength | 957.03     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.01     |
| TotalNEpisodes       | 19369      |
| TotalNSamples        | 4.6947e+06 |
| ExplainedVariance    | 0.0024304  |
-------------------------------------
[2018-12-22 11:43:25.650927 UTC] Saving snapshot
[2018-12-22 11:43:25.651167 UTC] Starting iteration 939
[2018-12-22 11:43:25.651298 UTC] Start collecting samples
[2018-12-22 11:43:28.687569 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:28.765102 UTC] Performing policy update
[2018-12-22 11:43:28.765927 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:28.856062 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:29.923599 UTC] Performing line search
[2018-12-22 11:43:30.050673 UTC] Updating baseline
[2018-12-22 11:43:31.440901 UTC] Computing logging information
-------------------------------------
| Iteration            | 939        |
| ExpectedImprovement  | 0.016944   |
| ActualImprovement    | 0.016269   |
| ImprovementRatio     | 0.96016    |
| MeanKL               | 0.0082964  |
| Entropy              | -0.81241   |
| Perplexity           | 0.44379    |
| AveragePolicyStd     | 0.21453    |
| AveragePolicyStd[0]  | 0.24711    |
| AveragePolicyStd[1]  | 0.2284     |
| AveragePolicyStd[2]  | 0.17237    |
| AveragePolicyStd[3]  | 0.23146    |
| AveragePolicyStd[4]  | 0.15889    |
| AveragePolicyStd[5]  | 0.24896    |
| AverageReturn        | 1648.5     |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 277.61     |
| AverageEpisodeLength | 966.13     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.26     |
| TotalNEpisodes       | 19374      |
| TotalNSamples        | 4.6997e+06 |
| ExplainedVariance    | 0.001289   |
-------------------------------------
[2018-12-22 11:43:31.837399 UTC] Saving snapshot
[2018-12-22 11:43:31.837659 UTC] Starting iteration 940
[2018-12-22 11:43:31.837809 UTC] Start collecting samples
[2018-12-22 11:43:34.818011 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:34.895679 UTC] Performing policy update
[2018-12-22 11:43:34.896374 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:34.986556 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:36.042655 UTC] Performing line search
[2018-12-22 11:43:36.169844 UTC] Updating baseline
[2018-12-22 11:43:38.066879 UTC] Computing logging information
--------------------------------------
| Iteration            | 940         |
| ExpectedImprovement  | 0.017519    |
| ActualImprovement    | 0.016459    |
| ImprovementRatio     | 0.93948     |
| MeanKL               | 0.0087066   |
| Entropy              | -0.80488    |
| Perplexity           | 0.44714     |
| AveragePolicyStd     | 0.21483     |
| AveragePolicyStd[0]  | 0.2473      |
| AveragePolicyStd[1]  | 0.22843     |
| AveragePolicyStd[2]  | 0.17265     |
| AveragePolicyStd[3]  | 0.2318      |
| AveragePolicyStd[4]  | 0.15881     |
| AveragePolicyStd[5]  | 0.24997     |
| AverageReturn        | 1665.9      |
| MinReturn            | 120.15      |
| MaxReturn            | 1776.1      |
| StdReturn            | 245.52      |
| AverageEpisodeLength | 975.84      |
| MinEpisodeLength     | 93          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 138.09      |
| TotalNEpisodes       | 19381       |
| TotalNSamples        | 4.7067e+06  |
| ExplainedVariance    | -0.00049595 |
--------------------------------------
[2018-12-22 11:43:38.456026 UTC] Saving snapshot
[2018-12-22 11:43:38.463987 UTC] Starting iteration 941
[2018-12-22 11:43:38.464187 UTC] Start collecting samples
[2018-12-22 11:43:41.370998 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:41.446718 UTC] Performing policy update
[2018-12-22 11:43:41.447304 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:41.536915 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:42.601881 UTC] Performing line search
[2018-12-22 11:43:42.730067 UTC] Updating baseline
[2018-12-22 11:43:44.927198 UTC] Computing logging information
-------------------------------------
| Iteration            | 941        |
| ExpectedImprovement  | 0.017324   |
| ActualImprovement    | 0.015717   |
| ImprovementRatio     | 0.90722    |
| MeanKL               | 0.0078641  |
| Entropy              | -0.81494   |
| Perplexity           | 0.44267    |
| AveragePolicyStd     | 0.21451    |
| AveragePolicyStd[0]  | 0.2475     |
| AveragePolicyStd[1]  | 0.22818    |
| AveragePolicyStd[2]  | 0.17217    |
| AveragePolicyStd[3]  | 0.23119    |
| AveragePolicyStd[4]  | 0.15823    |
| AveragePolicyStd[5]  | 0.24978    |
| AverageReturn        | 1666.4     |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 245.55     |
| AverageEpisodeLength | 975.84     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.09     |
| TotalNEpisodes       | 19383      |
| TotalNSamples        | 4.7087e+06 |
| ExplainedVariance    | 0.00088155 |
-------------------------------------
[2018-12-22 11:43:45.328050 UTC] Saving snapshot
[2018-12-22 11:43:45.328311 UTC] Starting iteration 942
[2018-12-22 11:43:45.328440 UTC] Start collecting samples
[2018-12-22 11:43:48.283636 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:48.361975 UTC] Performing policy update
[2018-12-22 11:43:48.362599 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:48.454002 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:49.511908 UTC] Performing line search
[2018-12-22 11:43:49.638897 UTC] Updating baseline
[2018-12-22 11:43:51.293057 UTC] Computing logging information
-------------------------------------
| Iteration            | 942        |
| ExpectedImprovement  | 0.015942   |
| ActualImprovement    | 0.015409   |
| ImprovementRatio     | 0.96657    |
| MeanKL               | 0.0080831  |
| Entropy              | -0.81673   |
| Perplexity           | 0.44187    |
| AveragePolicyStd     | 0.21443    |
| AveragePolicyStd[0]  | 0.24754    |
| AveragePolicyStd[1]  | 0.22769    |
| AveragePolicyStd[2]  | 0.17256    |
| AveragePolicyStd[3]  | 0.23045    |
| AveragePolicyStd[4]  | 0.15817    |
| AveragePolicyStd[5]  | 0.25017    |
| AverageReturn        | 1666       |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 245.68     |
| AverageEpisodeLength | 975.84     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.09     |
| TotalNEpisodes       | 19388      |
| TotalNSamples        | 4.7137e+06 |
| ExplainedVariance    | 0.0028805  |
-------------------------------------
[2018-12-22 11:43:51.679401 UTC] Saving snapshot
[2018-12-22 11:43:51.679683 UTC] Starting iteration 943
[2018-12-22 11:43:51.679815 UTC] Start collecting samples
[2018-12-22 11:43:54.645355 UTC] Computing input variables for policy optimization
[2018-12-22 11:43:54.722798 UTC] Performing policy update
[2018-12-22 11:43:54.723427 UTC] Computing gradient in Euclidean space
[2018-12-22 11:43:54.813871 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:43:55.877832 UTC] Performing line search
[2018-12-22 11:43:56.009205 UTC] Updating baseline
[2018-12-22 11:43:57.520327 UTC] Computing logging information
-------------------------------------
| Iteration            | 943        |
| ExpectedImprovement  | 0.016748   |
| ActualImprovement    | 0.0156     |
| ImprovementRatio     | 0.93144    |
| MeanKL               | 0.0079335  |
| Entropy              | -0.81083   |
| Perplexity           | 0.44449    |
| AveragePolicyStd     | 0.21467    |
| AveragePolicyStd[0]  | 0.24743    |
| AveragePolicyStd[1]  | 0.22836    |
| AveragePolicyStd[2]  | 0.17248    |
| AveragePolicyStd[3]  | 0.23147    |
| AveragePolicyStd[4]  | 0.15805    |
| AveragePolicyStd[5]  | 0.25023    |
| AverageReturn        | 1666.9     |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 245.91     |
| AverageEpisodeLength | 975.84     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.09     |
| TotalNEpisodes       | 19394      |
| TotalNSamples        | 4.7197e+06 |
| ExplainedVariance    | -0.004718  |
-------------------------------------
[2018-12-22 11:43:57.918943 UTC] Saving snapshot
[2018-12-22 11:43:57.919191 UTC] Starting iteration 944
[2018-12-22 11:43:57.919307 UTC] Start collecting samples
[2018-12-22 11:44:00.841441 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:00.920150 UTC] Performing policy update
[2018-12-22 11:44:00.920925 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:01.009935 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:02.077624 UTC] Performing line search
[2018-12-22 11:44:02.205324 UTC] Updating baseline
[2018-12-22 11:44:03.541714 UTC] Computing logging information
-------------------------------------
| Iteration            | 944        |
| ExpectedImprovement  | 0.019121   |
| ActualImprovement    | 0.017352   |
| ImprovementRatio     | 0.90747    |
| MeanKL               | 0.0078013  |
| Entropy              | -0.80497   |
| Perplexity           | 0.4471     |
| AveragePolicyStd     | 0.21493    |
| AveragePolicyStd[0]  | 0.248      |
| AveragePolicyStd[1]  | 0.22849    |
| AveragePolicyStd[2]  | 0.1729     |
| AveragePolicyStd[3]  | 0.23167    |
| AveragePolicyStd[4]  | 0.15755    |
| AveragePolicyStd[5]  | 0.25095    |
| AverageReturn        | 1666       |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 245.87     |
| AverageEpisodeLength | 975.84     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.09     |
| TotalNEpisodes       | 19398      |
| TotalNSamples        | 4.7237e+06 |
| ExplainedVariance    | 0.0044885  |
-------------------------------------
[2018-12-22 11:44:03.942367 UTC] Saving snapshot
[2018-12-22 11:44:03.942634 UTC] Starting iteration 945
[2018-12-22 11:44:03.942753 UTC] Start collecting samples
[2018-12-22 11:44:06.890564 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:06.968293 UTC] Performing policy update
[2018-12-22 11:44:06.968935 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:07.058295 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:08.108529 UTC] Performing line search
[2018-12-22 11:44:08.235732 UTC] Updating baseline
[2018-12-22 11:44:10.080525 UTC] Computing logging information
-------------------------------------
| Iteration            | 945        |
| ExpectedImprovement  | 0.019825   |
| ActualImprovement    | 0.018125   |
| ImprovementRatio     | 0.91426    |
| MeanKL               | 0.0073983  |
| Entropy              | -0.8049    |
| Perplexity           | 0.44713    |
| AveragePolicyStd     | 0.21499    |
| AveragePolicyStd[0]  | 0.24863    |
| AveragePolicyStd[1]  | 0.22855    |
| AveragePolicyStd[2]  | 0.17235    |
| AveragePolicyStd[3]  | 0.23159    |
| AveragePolicyStd[4]  | 0.1573     |
| AveragePolicyStd[5]  | 0.25155    |
| AverageReturn        | 1678.5     |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 213.29     |
| AverageEpisodeLength | 982.94     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.66     |
| TotalNEpisodes       | 19404      |
| TotalNSamples        | 4.7297e+06 |
| ExplainedVariance    | -0.0018115 |
-------------------------------------
[2018-12-22 11:44:10.466420 UTC] Saving snapshot
[2018-12-22 11:44:10.466704 UTC] Starting iteration 946
[2018-12-22 11:44:10.466833 UTC] Start collecting samples
[2018-12-22 11:44:13.382852 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:13.460593 UTC] Performing policy update
[2018-12-22 11:44:13.461328 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:13.551232 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:14.605535 UTC] Performing line search
[2018-12-22 11:44:14.733582 UTC] Updating baseline
[2018-12-22 11:44:16.344199 UTC] Computing logging information
-------------------------------------
| Iteration            | 946        |
| ExpectedImprovement  | 0.018382   |
| ActualImprovement    | 0.017458   |
| ImprovementRatio     | 0.94973    |
| MeanKL               | 0.0082312  |
| Entropy              | -0.80729   |
| Perplexity           | 0.44607    |
| AveragePolicyStd     | 0.21491    |
| AveragePolicyStd[0]  | 0.24882    |
| AveragePolicyStd[1]  | 0.22857    |
| AveragePolicyStd[2]  | 0.17237    |
| AveragePolicyStd[3]  | 0.23118    |
| AveragePolicyStd[4]  | 0.15723    |
| AveragePolicyStd[5]  | 0.25126    |
| AverageReturn        | 1675.6     |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 212.89     |
| AverageEpisodeLength | 982.83     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.65     |
| TotalNEpisodes       | 19409      |
| TotalNSamples        | 4.7346e+06 |
| ExplainedVariance    | 0.09791    |
-------------------------------------
[2018-12-22 11:44:16.739611 UTC] Saving snapshot
[2018-12-22 11:44:16.739864 UTC] Starting iteration 947
[2018-12-22 11:44:16.739984 UTC] Start collecting samples
[2018-12-22 11:44:19.705378 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:19.785812 UTC] Performing policy update
[2018-12-22 11:44:19.786569 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:19.876840 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:20.932327 UTC] Performing line search
[2018-12-22 11:44:21.060939 UTC] Updating baseline
[2018-12-22 11:44:22.412826 UTC] Computing logging information
-------------------------------------
| Iteration            | 947        |
| ExpectedImprovement  | 0.017441   |
| ActualImprovement    | 0.017318   |
| ImprovementRatio     | 0.99296    |
| MeanKL               | 0.0077291  |
| Entropy              | -0.80586   |
| Perplexity           | 0.4467     |
| AveragePolicyStd     | 0.21494    |
| AveragePolicyStd[0]  | 0.24924    |
| AveragePolicyStd[1]  | 0.22847    |
| AveragePolicyStd[2]  | 0.1725     |
| AveragePolicyStd[3]  | 0.23089    |
| AveragePolicyStd[4]  | 0.15738    |
| AveragePolicyStd[5]  | 0.25118    |
| AverageReturn        | 1653.1     |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 252.24     |
| AverageEpisodeLength | 972.12     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.89     |
| TotalNEpisodes       | 19415      |
| TotalNSamples        | 4.7396e+06 |
| ExplainedVariance    | 0.3523     |
-------------------------------------
[2018-12-22 11:44:22.803970 UTC] Saving snapshot
[2018-12-22 11:44:22.804215 UTC] Starting iteration 948
[2018-12-22 11:44:22.804350 UTC] Start collecting samples
[2018-12-22 11:44:25.758181 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:25.838154 UTC] Performing policy update
[2018-12-22 11:44:25.838751 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:25.934400 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:26.996072 UTC] Performing line search
[2018-12-22 11:44:27.123347 UTC] Updating baseline
[2018-12-22 11:44:28.564805 UTC] Computing logging information
------------------------------------
| Iteration            | 948       |
| ExpectedImprovement  | 0.018495  |
| ActualImprovement    | 0.017687  |
| ImprovementRatio     | 0.95631   |
| MeanKL               | 0.0072692 |
| Entropy              | -0.80869  |
| Perplexity           | 0.44544   |
| AveragePolicyStd     | 0.21486   |
| AveragePolicyStd[0]  | 0.2493    |
| AveragePolicyStd[1]  | 0.22828   |
| AveragePolicyStd[2]  | 0.17228   |
| AveragePolicyStd[3]  | 0.23121   |
| AveragePolicyStd[4]  | 0.15725   |
| AveragePolicyStd[5]  | 0.25082   |
| AverageReturn        | 1642.5    |
| MinReturn            | 120.15    |
| MaxReturn            | 1776.1    |
| StdReturn            | 268.71    |
| AverageEpisodeLength | 966.6     |
| MinEpisodeLength     | 93        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 149.25    |
| TotalNEpisodes       | 19421     |
| TotalNSamples        | 4.745e+06 |
| ExplainedVariance    | 0.14978   |
------------------------------------
[2018-12-22 11:44:28.961469 UTC] Saving snapshot
[2018-12-22 11:44:28.961768 UTC] Starting iteration 949
[2018-12-22 11:44:28.961900 UTC] Start collecting samples
[2018-12-22 11:44:31.911781 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:31.994054 UTC] Performing policy update
[2018-12-22 11:44:31.994787 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:32.084349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:33.140008 UTC] Performing line search
[2018-12-22 11:44:33.268131 UTC] Updating baseline
[2018-12-22 11:44:34.788814 UTC] Computing logging information
------------------------------------
| Iteration            | 949       |
| ExpectedImprovement  | 0.015681  |
| ActualImprovement    | 0.014952  |
| ImprovementRatio     | 0.95349   |
| MeanKL               | 0.0080847 |
| Entropy              | -0.8232   |
| Perplexity           | 0.43903   |
| AveragePolicyStd     | 0.2144    |
| AveragePolicyStd[0]  | 0.24939   |
| AveragePolicyStd[1]  | 0.22759   |
| AveragePolicyStd[2]  | 0.17133   |
| AveragePolicyStd[3]  | 0.23119   |
| AveragePolicyStd[4]  | 0.15659   |
| AveragePolicyStd[5]  | 0.2503    |
| AverageReturn        | 1642.8    |
| MinReturn            | 120.15    |
| MaxReturn            | 1776.1    |
| StdReturn            | 268.66    |
| AverageEpisodeLength | 966.6     |
| MinEpisodeLength     | 93        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 149.25    |
| TotalNEpisodes       | 19426     |
| TotalNSamples        | 4.75e+06  |
| ExplainedVariance    | -0.013919 |
------------------------------------
[2018-12-22 11:44:35.185632 UTC] Saving snapshot
[2018-12-22 11:44:35.185929 UTC] Starting iteration 950
[2018-12-22 11:44:35.186048 UTC] Start collecting samples
[2018-12-22 11:44:38.134276 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:38.212174 UTC] Performing policy update
[2018-12-22 11:44:38.212911 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:38.302995 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:39.355163 UTC] Performing line search
[2018-12-22 11:44:39.482174 UTC] Updating baseline
[2018-12-22 11:44:41.257180 UTC] Computing logging information
-------------------------------------
| Iteration            | 950        |
| ExpectedImprovement  | 0.016662   |
| ActualImprovement    | 0.015777   |
| ImprovementRatio     | 0.94685    |
| MeanKL               | 0.0076131  |
| Entropy              | -0.82674   |
| Perplexity           | 0.43748    |
| AveragePolicyStd     | 0.21425    |
| AveragePolicyStd[0]  | 0.24911    |
| AveragePolicyStd[1]  | 0.22751    |
| AveragePolicyStd[2]  | 0.1713     |
| AveragePolicyStd[3]  | 0.23117    |
| AveragePolicyStd[4]  | 0.15661    |
| AveragePolicyStd[5]  | 0.24982    |
| AverageReturn        | 1643.7     |
| MinReturn            | 120.15     |
| MaxReturn            | 1776.1     |
| StdReturn            | 268.84     |
| AverageEpisodeLength | 966.6      |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.25     |
| TotalNEpisodes       | 19430      |
| TotalNSamples        | 4.754e+06  |
| ExplainedVariance    | -0.0015736 |
-------------------------------------
[2018-12-22 11:44:41.645612 UTC] Saving snapshot
[2018-12-22 11:44:41.653696 UTC] Starting iteration 951
[2018-12-22 11:44:41.653931 UTC] Start collecting samples
[2018-12-22 11:44:44.632674 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:44.711899 UTC] Performing policy update
[2018-12-22 11:44:44.712668 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:44.802821 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:45.872952 UTC] Performing line search
[2018-12-22 11:44:46.007131 UTC] Updating baseline
[2018-12-22 11:44:47.790135 UTC] Computing logging information
-------------------------------------
| Iteration            | 951        |
| ExpectedImprovement  | 0.018956   |
| ActualImprovement    | 0.018247   |
| ImprovementRatio     | 0.96257    |
| MeanKL               | 0.0075254  |
| Entropy              | -0.82748   |
| Perplexity           | 0.43715    |
| AveragePolicyStd     | 0.21429    |
| AveragePolicyStd[0]  | 0.24868    |
| AveragePolicyStd[1]  | 0.2288     |
| AveragePolicyStd[2]  | 0.17071    |
| AveragePolicyStd[3]  | 0.23201    |
| AveragePolicyStd[4]  | 0.15604    |
| AveragePolicyStd[5]  | 0.24952    |
| AverageReturn        | 1657.1     |
| MinReturn            | 285.92     |
| MaxReturn            | 1776.1     |
| StdReturn            | 220.63     |
| AverageEpisodeLength | 975.67     |
| MinEpisodeLength     | 201        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.72     |
| TotalNEpisodes       | 19437      |
| TotalNSamples        | 4.761e+06  |
| ExplainedVariance    | -0.0086644 |
-------------------------------------
[2018-12-22 11:44:48.183643 UTC] Saving snapshot
[2018-12-22 11:44:48.183892 UTC] Starting iteration 952
[2018-12-22 11:44:48.184010 UTC] Start collecting samples
[2018-12-22 11:44:51.114936 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:51.193096 UTC] Performing policy update
[2018-12-22 11:44:51.193990 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:51.284680 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:52.350220 UTC] Performing line search
[2018-12-22 11:44:52.477197 UTC] Updating baseline
[2018-12-22 11:44:53.994777 UTC] Computing logging information
-------------------------------------
| Iteration            | 952        |
| ExpectedImprovement  | 0.018349   |
| ActualImprovement    | 0.017078   |
| ImprovementRatio     | 0.93073    |
| MeanKL               | 0.0078591  |
| Entropy              | -0.8244    |
| Perplexity           | 0.4385     |
| AveragePolicyStd     | 0.21439    |
| AveragePolicyStd[0]  | 0.24894    |
| AveragePolicyStd[1]  | 0.2285     |
| AveragePolicyStd[2]  | 0.17118    |
| AveragePolicyStd[3]  | 0.23224    |
| AveragePolicyStd[4]  | 0.15598    |
| AveragePolicyStd[5]  | 0.24954    |
| AverageReturn        | 1641.8     |
| MinReturn            | 194.21     |
| MaxReturn            | 1776.1     |
| StdReturn            | 264.14     |
| AverageEpisodeLength | 967        |
| MinEpisodeLength     | 133        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.95     |
| TotalNEpisodes       | 19442      |
| TotalNSamples        | 4.7652e+06 |
| ExplainedVariance    | 0.096324   |
-------------------------------------
[2018-12-22 11:44:54.387895 UTC] Saving snapshot
[2018-12-22 11:44:54.388154 UTC] Starting iteration 953
[2018-12-22 11:44:54.388284 UTC] Start collecting samples
[2018-12-22 11:44:57.350514 UTC] Computing input variables for policy optimization
[2018-12-22 11:44:57.428322 UTC] Performing policy update
[2018-12-22 11:44:57.429100 UTC] Computing gradient in Euclidean space
[2018-12-22 11:44:57.518586 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:44:58.578308 UTC] Performing line search
[2018-12-22 11:44:58.705272 UTC] Updating baseline
[2018-12-22 11:45:00.401478 UTC] Computing logging information
-------------------------------------
| Iteration            | 953        |
| ExpectedImprovement  | 0.016715   |
| ActualImprovement    | 0.015969   |
| ImprovementRatio     | 0.95538    |
| MeanKL               | 0.0073474  |
| Entropy              | -0.82936   |
| Perplexity           | 0.43633    |
| AveragePolicyStd     | 0.21416    |
| AveragePolicyStd[0]  | 0.24829    |
| AveragePolicyStd[1]  | 0.2279     |
| AveragePolicyStd[2]  | 0.17105    |
| AveragePolicyStd[3]  | 0.23157    |
| AveragePolicyStd[4]  | 0.15651    |
| AveragePolicyStd[5]  | 0.24965    |
| AverageReturn        | 1639.7     |
| MinReturn            | 194.21     |
| MaxReturn            | 1776.1     |
| StdReturn            | 263.76     |
| AverageEpisodeLength | 967        |
| MinEpisodeLength     | 133        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.95     |
| TotalNEpisodes       | 19447      |
| TotalNSamples        | 4.7702e+06 |
| ExplainedVariance    | -0.028435  |
-------------------------------------
[2018-12-22 11:45:00.793641 UTC] Saving snapshot
[2018-12-22 11:45:00.793932 UTC] Starting iteration 954
[2018-12-22 11:45:00.794051 UTC] Start collecting samples
[2018-12-22 11:45:03.767157 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:03.844623 UTC] Performing policy update
[2018-12-22 11:45:03.845243 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:03.935768 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:05.001253 UTC] Performing line search
[2018-12-22 11:45:05.130401 UTC] Updating baseline
[2018-12-22 11:45:06.385883 UTC] Computing logging information
--------------------------------------
| Iteration            | 954         |
| ExpectedImprovement  | 0.016814    |
| ActualImprovement    | 0.015987    |
| ImprovementRatio     | 0.95085     |
| MeanKL               | 0.0074215   |
| Entropy              | -0.82683    |
| Perplexity           | 0.43743     |
| AveragePolicyStd     | 0.21422     |
| AveragePolicyStd[0]  | 0.24796     |
| AveragePolicyStd[1]  | 0.22735     |
| AveragePolicyStd[2]  | 0.17124     |
| AveragePolicyStd[3]  | 0.23227     |
| AveragePolicyStd[4]  | 0.15685     |
| AveragePolicyStd[5]  | 0.24965     |
| AverageReturn        | 1653.8      |
| MinReturn            | 194.21      |
| MaxReturn            | 1776.1      |
| StdReturn            | 226.03      |
| AverageEpisodeLength | 974.99      |
| MinEpisodeLength     | 133         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 125.19      |
| TotalNEpisodes       | 19452       |
| TotalNSamples        | 4.7752e+06  |
| ExplainedVariance    | -6.4303e-05 |
--------------------------------------
[2018-12-22 11:45:06.780044 UTC] Saving snapshot
[2018-12-22 11:45:06.780302 UTC] Starting iteration 955
[2018-12-22 11:45:06.780438 UTC] Start collecting samples
[2018-12-22 11:45:09.729116 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:09.807893 UTC] Performing policy update
[2018-12-22 11:45:09.808785 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:09.899537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:10.961988 UTC] Performing line search
[2018-12-22 11:45:11.091237 UTC] Updating baseline
[2018-12-22 11:45:12.920009 UTC] Computing logging information
-------------------------------------
| Iteration            | 955        |
| ExpectedImprovement  | 0.016611   |
| ActualImprovement    | 0.015731   |
| ImprovementRatio     | 0.94704    |
| MeanKL               | 0.0080438  |
| Entropy              | -0.83105   |
| Perplexity           | 0.43559    |
| AveragePolicyStd     | 0.2141     |
| AveragePolicyStd[0]  | 0.24836    |
| AveragePolicyStd[1]  | 0.22746    |
| AveragePolicyStd[2]  | 0.17104    |
| AveragePolicyStd[3]  | 0.23237    |
| AveragePolicyStd[4]  | 0.1564     |
| AveragePolicyStd[5]  | 0.24897    |
| AverageReturn        | 1653.1     |
| MinReturn            | 194.21     |
| MaxReturn            | 1776.1     |
| StdReturn            | 225.85     |
| AverageEpisodeLength | 974.99     |
| MinEpisodeLength     | 133        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.19     |
| TotalNEpisodes       | 19456      |
| TotalNSamples        | 4.7792e+06 |
| ExplainedVariance    | 0.0014599  |
-------------------------------------
[2018-12-22 11:45:13.309354 UTC] Saving snapshot
[2018-12-22 11:45:13.309612 UTC] Starting iteration 956
[2018-12-22 11:45:13.309752 UTC] Start collecting samples
[2018-12-22 11:45:16.273154 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:16.352360 UTC] Performing policy update
[2018-12-22 11:45:16.353080 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:16.443372 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:17.505628 UTC] Performing line search
[2018-12-22 11:45:17.634546 UTC] Updating baseline
[2018-12-22 11:45:18.963865 UTC] Computing logging information
-------------------------------------
| Iteration            | 956        |
| ExpectedImprovement  | 0.017604   |
| ActualImprovement    | 0.016104   |
| ImprovementRatio     | 0.91477    |
| MeanKL               | 0.0080828  |
| Entropy              | -0.83602   |
| Perplexity           | 0.43343    |
| AveragePolicyStd     | 0.21394    |
| AveragePolicyStd[0]  | 0.24865    |
| AveragePolicyStd[1]  | 0.22755    |
| AveragePolicyStd[2]  | 0.17095    |
| AveragePolicyStd[3]  | 0.23164    |
| AveragePolicyStd[4]  | 0.15612    |
| AveragePolicyStd[5]  | 0.24871    |
| AverageReturn        | 1653.4     |
| MinReturn            | 194.21     |
| MaxReturn            | 1776.1     |
| StdReturn            | 225.95     |
| AverageEpisodeLength | 974.99     |
| MinEpisodeLength     | 133        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.19     |
| TotalNEpisodes       | 19463      |
| TotalNSamples        | 4.7862e+06 |
| ExplainedVariance    | 0.001598   |
-------------------------------------
[2018-12-22 11:45:19.355511 UTC] Saving snapshot
[2018-12-22 11:45:19.355800 UTC] Starting iteration 957
[2018-12-22 11:45:19.355921 UTC] Start collecting samples
[2018-12-22 11:45:22.269773 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:22.346322 UTC] Performing policy update
[2018-12-22 11:45:22.347277 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:22.436750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:23.497381 UTC] Performing line search
[2018-12-22 11:45:23.625524 UTC] Updating baseline
[2018-12-22 11:45:24.857930 UTC] Computing logging information
--------------------------------------
| Iteration            | 957         |
| ExpectedImprovement  | 0.017694    |
| ActualImprovement    | 0.016145    |
| ImprovementRatio     | 0.91243     |
| MeanKL               | 0.0077895   |
| Entropy              | -0.84302    |
| Perplexity           | 0.43041     |
| AveragePolicyStd     | 0.21367     |
| AveragePolicyStd[0]  | 0.2481      |
| AveragePolicyStd[1]  | 0.228       |
| AveragePolicyStd[2]  | 0.17069     |
| AveragePolicyStd[3]  | 0.23132     |
| AveragePolicyStd[4]  | 0.15601     |
| AveragePolicyStd[5]  | 0.24793     |
| AverageReturn        | 1652.4      |
| MinReturn            | 194.21      |
| MaxReturn            | 1768.7      |
| StdReturn            | 225.6       |
| AverageEpisodeLength | 974.99      |
| MinEpisodeLength     | 133         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 125.19      |
| TotalNEpisodes       | 19466       |
| TotalNSamples        | 4.7892e+06  |
| ExplainedVariance    | -0.00071006 |
--------------------------------------
[2018-12-22 11:45:25.252473 UTC] Saving snapshot
[2018-12-22 11:45:25.252741 UTC] Starting iteration 958
[2018-12-22 11:45:25.252862 UTC] Start collecting samples
[2018-12-22 11:45:28.193387 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:28.269480 UTC] Performing policy update
[2018-12-22 11:45:28.270179 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:28.360636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:29.419298 UTC] Performing line search
[2018-12-22 11:45:29.546209 UTC] Updating baseline
[2018-12-22 11:45:31.151847 UTC] Computing logging information
-------------------------------------
| Iteration            | 958        |
| ExpectedImprovement  | 0.018647   |
| ActualImprovement    | 0.016539   |
| ImprovementRatio     | 0.88693    |
| MeanKL               | 0.0071805  |
| Entropy              | -0.83793   |
| Perplexity           | 0.4326     |
| AveragePolicyStd     | 0.21388    |
| AveragePolicyStd[0]  | 0.2477     |
| AveragePolicyStd[1]  | 0.22823    |
| AveragePolicyStd[2]  | 0.17045    |
| AveragePolicyStd[3]  | 0.23192    |
| AveragePolicyStd[4]  | 0.15617    |
| AveragePolicyStd[5]  | 0.24879    |
| AverageReturn        | 1652.4     |
| MinReturn            | 194.21     |
| MaxReturn            | 1768.7     |
| StdReturn            | 225.64     |
| AverageEpisodeLength | 974.99     |
| MinEpisodeLength     | 133        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.19     |
| TotalNEpisodes       | 19471      |
| TotalNSamples        | 4.7942e+06 |
| ExplainedVariance    | 0.0021332  |
-------------------------------------
[2018-12-22 11:45:31.543770 UTC] Saving snapshot
[2018-12-22 11:45:31.544041 UTC] Starting iteration 959
[2018-12-22 11:45:31.544156 UTC] Start collecting samples
[2018-12-22 11:45:34.473959 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:34.549984 UTC] Performing policy update
[2018-12-22 11:45:34.550825 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:34.639521 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:35.691109 UTC] Performing line search
[2018-12-22 11:45:35.821685 UTC] Updating baseline
[2018-12-22 11:45:37.682249 UTC] Computing logging information
-------------------------------------
| Iteration            | 959        |
| ExpectedImprovement  | 0.018204   |
| ActualImprovement    | 0.016921   |
| ImprovementRatio     | 0.9295     |
| MeanKL               | 0.0069855  |
| Entropy              | -0.84183   |
| Perplexity           | 0.43092    |
| AveragePolicyStd     | 0.21373    |
| AveragePolicyStd[0]  | 0.2474     |
| AveragePolicyStd[1]  | 0.22755    |
| AveragePolicyStd[2]  | 0.17042    |
| AveragePolicyStd[3]  | 0.23134    |
| AveragePolicyStd[4]  | 0.15627    |
| AveragePolicyStd[5]  | 0.24938    |
| AverageReturn        | 1654.2     |
| MinReturn            | 194.21     |
| MaxReturn            | 1768.7     |
| StdReturn            | 226.1      |
| AverageEpisodeLength | 974.99     |
| MinEpisodeLength     | 133        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.19     |
| TotalNEpisodes       | 19476      |
| TotalNSamples        | 4.7992e+06 |
| ExplainedVariance    | 0.0026078  |
-------------------------------------
[2018-12-22 11:45:38.080309 UTC] Saving snapshot
[2018-12-22 11:45:38.080583 UTC] Starting iteration 960
[2018-12-22 11:45:38.080705 UTC] Start collecting samples
[2018-12-22 11:45:41.038116 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:41.116511 UTC] Performing policy update
[2018-12-22 11:45:41.117132 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:41.207605 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:42.273068 UTC] Performing line search
[2018-12-22 11:45:42.402297 UTC] Updating baseline
[2018-12-22 11:45:43.738609 UTC] Computing logging information
------------------------------------
| Iteration            | 960       |
| ExpectedImprovement  | 0.020059  |
| ActualImprovement    | 0.019654  |
| ImprovementRatio     | 0.97983   |
| MeanKL               | 0.0070799 |
| Entropy              | -0.83695  |
| Perplexity           | 0.43303   |
| AveragePolicyStd     | 0.21386   |
| AveragePolicyStd[0]  | 0.24763   |
| AveragePolicyStd[1]  | 0.22864   |
| AveragePolicyStd[2]  | 0.17055   |
| AveragePolicyStd[3]  | 0.23145   |
| AveragePolicyStd[4]  | 0.15671   |
| AveragePolicyStd[5]  | 0.24816   |
| AverageReturn        | 1651.3    |
| MinReturn            | 194.21    |
| MaxReturn            | 1768.7    |
| StdReturn            | 227.97    |
| AverageEpisodeLength | 973.17    |
| MinEpisodeLength     | 133       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 126.14    |
| TotalNEpisodes       | 19482     |
| TotalNSamples        | 4.805e+06 |
| ExplainedVariance    | 0.18609   |
------------------------------------
[2018-12-22 11:45:44.135658 UTC] Saving snapshot
[2018-12-22 11:45:44.143782 UTC] Starting iteration 961
[2018-12-22 11:45:44.143979 UTC] Start collecting samples
[2018-12-22 11:45:47.088228 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:47.165236 UTC] Performing policy update
[2018-12-22 11:45:47.166228 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:47.254818 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:48.309084 UTC] Performing line search
[2018-12-22 11:45:48.437090 UTC] Updating baseline
[2018-12-22 11:45:50.048662 UTC] Computing logging information
------------------------------------
| Iteration            | 961       |
| ExpectedImprovement  | 0.016529  |
| ActualImprovement    | 0.015695  |
| ImprovementRatio     | 0.94956   |
| MeanKL               | 0.0075654 |
| Entropy              | -0.83485  |
| Perplexity           | 0.43394   |
| AveragePolicyStd     | 0.21391   |
| AveragePolicyStd[0]  | 0.24796   |
| AveragePolicyStd[1]  | 0.2286    |
| AveragePolicyStd[2]  | 0.17038   |
| AveragePolicyStd[3]  | 0.23105   |
| AveragePolicyStd[4]  | 0.15728   |
| AveragePolicyStd[5]  | 0.24817   |
| AverageReturn        | 1652.7    |
| MinReturn            | 194.21    |
| MaxReturn            | 1768.7    |
| StdReturn            | 228.12    |
| AverageEpisodeLength | 973.17    |
| MinEpisodeLength     | 133       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 126.14    |
| TotalNEpisodes       | 19487     |
| TotalNSamples        | 4.81e+06  |
| ExplainedVariance    | 0.018895  |
------------------------------------
[2018-12-22 11:45:50.442424 UTC] Saving snapshot
[2018-12-22 11:45:50.442697 UTC] Starting iteration 962
[2018-12-22 11:45:50.442815 UTC] Start collecting samples
[2018-12-22 11:45:53.415036 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:53.493970 UTC] Performing policy update
[2018-12-22 11:45:53.494622 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:53.585932 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:45:54.648511 UTC] Performing line search
[2018-12-22 11:45:54.774949 UTC] Updating baseline
[2018-12-22 11:45:56.106595 UTC] Computing logging information
-------------------------------------
| Iteration            | 962        |
| ExpectedImprovement  | 0.018228   |
| ActualImprovement    | 0.017056   |
| ImprovementRatio     | 0.93574    |
| MeanKL               | 0.0079512  |
| Entropy              | -0.82748   |
| Perplexity           | 0.43715    |
| AveragePolicyStd     | 0.21419    |
| AveragePolicyStd[0]  | 0.24879    |
| AveragePolicyStd[1]  | 0.2294     |
| AveragePolicyStd[2]  | 0.16951    |
| AveragePolicyStd[3]  | 0.23139    |
| AveragePolicyStd[4]  | 0.15801    |
| AveragePolicyStd[5]  | 0.24806    |
| AverageReturn        | 1617.2     |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 297.49     |
| AverageEpisodeLength | 954.3      |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.58     |
| TotalNEpisodes       | 19494      |
| TotalNSamples        | 4.8151e+06 |
| ExplainedVariance    | 0.35928    |
-------------------------------------
[2018-12-22 11:45:56.501024 UTC] Saving snapshot
[2018-12-22 11:45:56.501296 UTC] Starting iteration 963
[2018-12-22 11:45:56.501415 UTC] Start collecting samples
[2018-12-22 11:45:59.448926 UTC] Computing input variables for policy optimization
[2018-12-22 11:45:59.526601 UTC] Performing policy update
[2018-12-22 11:45:59.527465 UTC] Computing gradient in Euclidean space
[2018-12-22 11:45:59.617660 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:00.672675 UTC] Performing line search
[2018-12-22 11:46:00.798964 UTC] Updating baseline
[2018-12-22 11:46:02.238832 UTC] Computing logging information
-------------------------------------
| Iteration            | 963        |
| ExpectedImprovement  | 0.017415   |
| ActualImprovement    | 0.016903   |
| ImprovementRatio     | 0.97058    |
| MeanKL               | 0.0076409  |
| Entropy              | -0.8317    |
| Perplexity           | 0.43531    |
| AveragePolicyStd     | 0.21404    |
| AveragePolicyStd[0]  | 0.24861    |
| AveragePolicyStd[1]  | 0.22857    |
| AveragePolicyStd[2]  | 0.16958    |
| AveragePolicyStd[3]  | 0.23126    |
| AveragePolicyStd[4]  | 0.15786    |
| AveragePolicyStd[5]  | 0.24836    |
| AverageReturn        | 1607.8     |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 310.9      |
| AverageEpisodeLength | 948.77     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.03     |
| TotalNEpisodes       | 19499      |
| TotalNSamples        | 4.8195e+06 |
| ExplainedVariance    | 0.11924    |
-------------------------------------
[2018-12-22 11:46:02.637841 UTC] Saving snapshot
[2018-12-22 11:46:02.638117 UTC] Starting iteration 964
[2018-12-22 11:46:02.638251 UTC] Start collecting samples
[2018-12-22 11:46:05.598332 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:05.673824 UTC] Performing policy update
[2018-12-22 11:46:05.674657 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:05.763877 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:06.823000 UTC] Performing line search
[2018-12-22 11:46:06.950026 UTC] Updating baseline
[2018-12-22 11:46:08.550998 UTC] Computing logging information
-------------------------------------
| Iteration            | 964        |
| ExpectedImprovement  | 0.015202   |
| ActualImprovement    | 0.01406    |
| ImprovementRatio     | 0.92484    |
| MeanKL               | 0.0073604  |
| Entropy              | -0.83453   |
| Perplexity           | 0.43408    |
| AveragePolicyStd     | 0.21396    |
| AveragePolicyStd[0]  | 0.24861    |
| AveragePolicyStd[1]  | 0.22836    |
| AveragePolicyStd[2]  | 0.16963    |
| AveragePolicyStd[3]  | 0.23103    |
| AveragePolicyStd[4]  | 0.15752    |
| AveragePolicyStd[5]  | 0.24861    |
| AverageReturn        | 1609.4     |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 311.43     |
| AverageEpisodeLength | 948.77     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.03     |
| TotalNEpisodes       | 19503      |
| TotalNSamples        | 4.8235e+06 |
| ExplainedVariance    | -0.0033936 |
-------------------------------------
[2018-12-22 11:46:08.945136 UTC] Saving snapshot
[2018-12-22 11:46:08.945384 UTC] Starting iteration 965
[2018-12-22 11:46:08.945516 UTC] Start collecting samples
[2018-12-22 11:46:11.931477 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:12.011652 UTC] Performing policy update
[2018-12-22 11:46:12.012315 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:12.101602 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:13.149815 UTC] Performing line search
[2018-12-22 11:46:13.277231 UTC] Updating baseline
[2018-12-22 11:46:14.502069 UTC] Computing logging information
-------------------------------------
| Iteration            | 965        |
| ExpectedImprovement  | 0.015837   |
| ActualImprovement    | 0.015255   |
| ImprovementRatio     | 0.96327    |
| MeanKL               | 0.0083184  |
| Entropy              | -0.83845   |
| Perplexity           | 0.43238    |
| AveragePolicyStd     | 0.21381    |
| AveragePolicyStd[0]  | 0.24863    |
| AveragePolicyStd[1]  | 0.22787    |
| AveragePolicyStd[2]  | 0.16972    |
| AveragePolicyStd[3]  | 0.23118    |
| AveragePolicyStd[4]  | 0.15732    |
| AveragePolicyStd[5]  | 0.24815    |
| AverageReturn        | 1610.5     |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 311.73     |
| AverageEpisodeLength | 948.88     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.06     |
| TotalNEpisodes       | 19510      |
| TotalNSamples        | 4.8305e+06 |
| ExplainedVariance    | -0.0011236 |
-------------------------------------
[2018-12-22 11:46:14.888696 UTC] Saving snapshot
[2018-12-22 11:46:14.888957 UTC] Starting iteration 966
[2018-12-22 11:46:14.889074 UTC] Start collecting samples
[2018-12-22 11:46:17.851092 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:17.933001 UTC] Performing policy update
[2018-12-22 11:46:17.933725 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:18.024940 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:19.083975 UTC] Performing line search
[2018-12-22 11:46:19.212176 UTC] Updating baseline
[2018-12-22 11:46:20.729467 UTC] Computing logging information
-------------------------------------
| Iteration            | 966        |
| ExpectedImprovement  | 0.017722   |
| ActualImprovement    | 0.016688   |
| ImprovementRatio     | 0.94165    |
| MeanKL               | 0.0074441  |
| Entropy              | -0.83429   |
| Perplexity           | 0.43418    |
| AveragePolicyStd     | 0.21391    |
| AveragePolicyStd[0]  | 0.24845    |
| AveragePolicyStd[1]  | 0.22868    |
| AveragePolicyStd[2]  | 0.17006    |
| AveragePolicyStd[3]  | 0.23126    |
| AveragePolicyStd[4]  | 0.15768    |
| AveragePolicyStd[5]  | 0.24737    |
| AverageReturn        | 1621       |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 305.36     |
| AverageEpisodeLength | 952.89     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.58     |
| TotalNEpisodes       | 19516      |
| TotalNSamples        | 4.8359e+06 |
| ExplainedVariance    | 0.093649   |
-------------------------------------
[2018-12-22 11:46:21.122570 UTC] Saving snapshot
[2018-12-22 11:46:21.122818 UTC] Starting iteration 967
[2018-12-22 11:46:21.122937 UTC] Start collecting samples
[2018-12-22 11:46:24.067345 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:24.144404 UTC] Performing policy update
[2018-12-22 11:46:24.145184 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:24.234667 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:25.286101 UTC] Performing line search
[2018-12-22 11:46:25.413935 UTC] Updating baseline
[2018-12-22 11:46:26.968544 UTC] Computing logging information
-------------------------------------
| Iteration            | 967        |
| ExpectedImprovement  | 0.017747   |
| ActualImprovement    | 0.017118   |
| ImprovementRatio     | 0.96458    |
| MeanKL               | 0.0076985  |
| Entropy              | -0.83654   |
| Perplexity           | 0.43321    |
| AveragePolicyStd     | 0.21384    |
| AveragePolicyStd[0]  | 0.24805    |
| AveragePolicyStd[1]  | 0.22892    |
| AveragePolicyStd[2]  | 0.16993    |
| AveragePolicyStd[3]  | 0.23111    |
| AveragePolicyStd[4]  | 0.15754    |
| AveragePolicyStd[5]  | 0.2475     |
| AverageReturn        | 1621.8     |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 305.57     |
| AverageEpisodeLength | 952.89     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.58     |
| TotalNEpisodes       | 19520      |
| TotalNSamples        | 4.8399e+06 |
| ExplainedVariance    | -0.055233  |
-------------------------------------
[2018-12-22 11:46:27.360799 UTC] Saving snapshot
[2018-12-22 11:46:27.361047 UTC] Starting iteration 968
[2018-12-22 11:46:27.361163 UTC] Start collecting samples
[2018-12-22 11:46:30.317668 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:30.394825 UTC] Performing policy update
[2018-12-22 11:46:30.395573 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:30.484550 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:31.539260 UTC] Performing line search
[2018-12-22 11:46:31.666281 UTC] Updating baseline
[2018-12-22 11:46:32.895900 UTC] Computing logging information
-------------------------------------
| Iteration            | 968        |
| ExpectedImprovement  | 0.016222   |
| ActualImprovement    | 0.015745   |
| ImprovementRatio     | 0.97056    |
| MeanKL               | 0.0076211  |
| Entropy              | -0.83873   |
| Perplexity           | 0.43226    |
| AveragePolicyStd     | 0.21381    |
| AveragePolicyStd[0]  | 0.24862    |
| AveragePolicyStd[1]  | 0.22837    |
| AveragePolicyStd[2]  | 0.16958    |
| AveragePolicyStd[3]  | 0.23088    |
| AveragePolicyStd[4]  | 0.15731    |
| AveragePolicyStd[5]  | 0.24809    |
| AverageReturn        | 1632.1     |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 292.02     |
| AverageEpisodeLength | 958.41     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.91     |
| TotalNEpisodes       | 19525      |
| TotalNSamples        | 4.8449e+06 |
| ExplainedVariance    | 0.019814   |
-------------------------------------
[2018-12-22 11:46:33.292906 UTC] Saving snapshot
[2018-12-22 11:46:33.293162 UTC] Starting iteration 969
[2018-12-22 11:46:33.293283 UTC] Start collecting samples
[2018-12-22 11:46:36.308227 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:36.387274 UTC] Performing policy update
[2018-12-22 11:46:36.388035 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:36.477274 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:37.524819 UTC] Performing line search
[2018-12-22 11:46:37.651634 UTC] Updating baseline
[2018-12-22 11:46:39.074591 UTC] Computing logging information
------------------------------------
| Iteration            | 969       |
| ExpectedImprovement  | 0.017874  |
| ActualImprovement    | 0.017027  |
| ImprovementRatio     | 0.95257   |
| MeanKL               | 0.007632  |
| Entropy              | -0.8364   |
| Perplexity           | 0.43327   |
| AveragePolicyStd     | 0.2139    |
| AveragePolicyStd[0]  | 0.24851   |
| AveragePolicyStd[1]  | 0.22874   |
| AveragePolicyStd[2]  | 0.17      |
| AveragePolicyStd[3]  | 0.23088   |
| AveragePolicyStd[4]  | 0.15701   |
| AveragePolicyStd[5]  | 0.24824   |
| AverageReturn        | 1616.8    |
| MinReturn            | 117.98    |
| MaxReturn            | 1768.7    |
| StdReturn            | 327.94    |
| AverageEpisodeLength | 949.53    |
| MinEpisodeLength     | 112       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 183.33    |
| TotalNEpisodes       | 19532     |
| TotalNSamples        | 4.851e+06 |
| ExplainedVariance    | 0.071698  |
------------------------------------
[2018-12-22 11:46:39.470008 UTC] Saving snapshot
[2018-12-22 11:46:39.470253 UTC] Starting iteration 970
[2018-12-22 11:46:39.470372 UTC] Start collecting samples
[2018-12-22 11:46:42.443435 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:42.519620 UTC] Performing policy update
[2018-12-22 11:46:42.520392 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:42.610075 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:43.663505 UTC] Performing line search
[2018-12-22 11:46:43.794473 UTC] Updating baseline
[2018-12-22 11:46:45.302214 UTC] Computing logging information
-------------------------------------
| Iteration            | 970        |
| ExpectedImprovement  | 0.017929   |
| ActualImprovement    | 0.01704    |
| ImprovementRatio     | 0.95039    |
| MeanKL               | 0.0075128  |
| Entropy              | -0.83565   |
| Perplexity           | 0.43359    |
| AveragePolicyStd     | 0.21388    |
| AveragePolicyStd[0]  | 0.24801    |
| AveragePolicyStd[1]  | 0.22805    |
| AveragePolicyStd[2]  | 0.17014    |
| AveragePolicyStd[3]  | 0.2319     |
| AveragePolicyStd[4]  | 0.15737    |
| AveragePolicyStd[5]  | 0.2478     |
| AverageReturn        | 1606       |
| MinReturn            | 117.98     |
| MaxReturn            | 1768.7     |
| StdReturn            | 347.55     |
| AverageEpisodeLength | 942.46     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.53     |
| TotalNEpisodes       | 19537      |
| TotalNSamples        | 4.8553e+06 |
| ExplainedVariance    | 0.07875    |
-------------------------------------
[2018-12-22 11:46:45.695054 UTC] Saving snapshot
[2018-12-22 11:46:45.703308 UTC] Starting iteration 971
[2018-12-22 11:46:45.703538 UTC] Start collecting samples
[2018-12-22 11:46:48.610681 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:48.685571 UTC] Performing policy update
[2018-12-22 11:46:48.686322 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:48.776765 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:49.836712 UTC] Performing line search
[2018-12-22 11:46:49.969245 UTC] Updating baseline
[2018-12-22 11:46:52.341184 UTC] Computing logging information
-------------------------------------
| Iteration            | 971        |
| ExpectedImprovement  | 0.018592   |
| ActualImprovement    | 0.016729   |
| ImprovementRatio     | 0.89978    |
| MeanKL               | 0.007181   |
| Entropy              | -0.8356    |
| Perplexity           | 0.43361    |
| AveragePolicyStd     | 0.21387    |
| AveragePolicyStd[0]  | 0.2482     |
| AveragePolicyStd[1]  | 0.22819    |
| AveragePolicyStd[2]  | 0.17042    |
| AveragePolicyStd[3]  | 0.23112    |
| AveragePolicyStd[4]  | 0.15731    |
| AveragePolicyStd[5]  | 0.24801    |
| AverageReturn        | 1622.8     |
| MinReturn            | 117.98     |
| MaxReturn            | 1791       |
| StdReturn            | 317.98     |
| AverageEpisodeLength | 951.13     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.77     |
| TotalNEpisodes       | 19540      |
| TotalNSamples        | 4.8583e+06 |
| ExplainedVariance    | 0.0089415  |
-------------------------------------
[2018-12-22 11:46:52.737888 UTC] Saving snapshot
[2018-12-22 11:46:52.738138 UTC] Starting iteration 972
[2018-12-22 11:46:52.738292 UTC] Start collecting samples
[2018-12-22 11:46:55.728066 UTC] Computing input variables for policy optimization
[2018-12-22 11:46:55.806640 UTC] Performing policy update
[2018-12-22 11:46:55.807211 UTC] Computing gradient in Euclidean space
[2018-12-22 11:46:55.897590 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:46:56.953598 UTC] Performing line search
[2018-12-22 11:46:57.080522 UTC] Updating baseline
[2018-12-22 11:46:58.999231 UTC] Computing logging information
-------------------------------------
| Iteration            | 972        |
| ExpectedImprovement  | 0.017483   |
| ActualImprovement    | 0.017031   |
| ImprovementRatio     | 0.97416    |
| MeanKL               | 0.0080526  |
| Entropy              | -0.84011   |
| Perplexity           | 0.43166    |
| AveragePolicyStd     | 0.2137     |
| AveragePolicyStd[0]  | 0.24774    |
| AveragePolicyStd[1]  | 0.22792    |
| AveragePolicyStd[2]  | 0.1699     |
| AveragePolicyStd[3]  | 0.23118    |
| AveragePolicyStd[4]  | 0.15768    |
| AveragePolicyStd[5]  | 0.24776    |
| AverageReturn        | 1609.3     |
| MinReturn            | 117.98     |
| MaxReturn            | 1791       |
| StdReturn            | 350.91     |
| AverageEpisodeLength | 942.16     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.8      |
| TotalNEpisodes       | 19549      |
| TotalNSamples        | 4.8664e+06 |
| ExplainedVariance    | 0.041859   |
-------------------------------------
[2018-12-22 11:46:59.387989 UTC] Saving snapshot
[2018-12-22 11:46:59.388226 UTC] Starting iteration 973
[2018-12-22 11:46:59.388365 UTC] Start collecting samples
[2018-12-22 11:47:02.353769 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:02.432927 UTC] Performing policy update
[2018-12-22 11:47:02.433574 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:02.525021 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:03.585929 UTC] Performing line search
[2018-12-22 11:47:03.713245 UTC] Updating baseline
[2018-12-22 11:47:05.505588 UTC] Computing logging information
-------------------------------------
| Iteration            | 973        |
| ExpectedImprovement  | 0.016292   |
| ActualImprovement    | 0.015832   |
| ImprovementRatio     | 0.97179    |
| MeanKL               | 0.0077655  |
| Entropy              | -0.83857   |
| Perplexity           | 0.43233    |
| AveragePolicyStd     | 0.21379    |
| AveragePolicyStd[0]  | 0.2474     |
| AveragePolicyStd[1]  | 0.22859    |
| AveragePolicyStd[2]  | 0.16981    |
| AveragePolicyStd[3]  | 0.23156    |
| AveragePolicyStd[4]  | 0.15731    |
| AveragePolicyStd[5]  | 0.24806    |
| AverageReturn        | 1607.9     |
| MinReturn            | 117.98     |
| MaxReturn            | 1791       |
| StdReturn            | 351.39     |
| AverageEpisodeLength | 940.9      |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.83     |
| TotalNEpisodes       | 19553      |
| TotalNSamples        | 4.8702e+06 |
| ExplainedVariance    | 0.15053    |
-------------------------------------
[2018-12-22 11:47:05.899972 UTC] Saving snapshot
[2018-12-22 11:47:05.900301 UTC] Starting iteration 974
[2018-12-22 11:47:05.900423 UTC] Start collecting samples
[2018-12-22 11:47:08.855900 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:08.933784 UTC] Performing policy update
[2018-12-22 11:47:08.934439 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:09.024593 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:10.090704 UTC] Performing line search
[2018-12-22 11:47:10.218645 UTC] Updating baseline
[2018-12-22 11:47:11.654501 UTC] Computing logging information
-------------------------------------
| Iteration            | 974        |
| ExpectedImprovement  | 0.021853   |
| ActualImprovement    | 0.020398   |
| ImprovementRatio     | 0.93341    |
| MeanKL               | 0.00759    |
| Entropy              | -0.84327   |
| Perplexity           | 0.4303     |
| AveragePolicyStd     | 0.21359    |
| AveragePolicyStd[0]  | 0.24626    |
| AveragePolicyStd[1]  | 0.2284     |
| AveragePolicyStd[2]  | 0.16934    |
| AveragePolicyStd[3]  | 0.23142    |
| AveragePolicyStd[4]  | 0.15781    |
| AveragePolicyStd[5]  | 0.24832    |
| AverageReturn        | 1608.7     |
| MinReturn            | 117.98     |
| MaxReturn            | 1791       |
| StdReturn            | 351.66     |
| AverageEpisodeLength | 940.9      |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.83     |
| TotalNEpisodes       | 19557      |
| TotalNSamples        | 4.8742e+06 |
| ExplainedVariance    | 0.027843   |
-------------------------------------
[2018-12-22 11:47:12.055618 UTC] Saving snapshot
[2018-12-22 11:47:12.055875 UTC] Starting iteration 975
[2018-12-22 11:47:12.055992 UTC] Start collecting samples
[2018-12-22 11:47:15.017936 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:15.097233 UTC] Performing policy update
[2018-12-22 11:47:15.097928 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:15.189611 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:16.255212 UTC] Performing line search
[2018-12-22 11:47:16.383239 UTC] Updating baseline
[2018-12-22 11:47:17.720984 UTC] Computing logging information
-------------------------------------
| Iteration            | 975        |
| ExpectedImprovement  | 0.018154   |
| ActualImprovement    | 0.016806   |
| ImprovementRatio     | 0.92578    |
| MeanKL               | 0.0080391  |
| Entropy              | -0.84538   |
| Perplexity           | 0.42939    |
| AveragePolicyStd     | 0.21348    |
| AveragePolicyStd[0]  | 0.24628    |
| AveragePolicyStd[1]  | 0.2277     |
| AveragePolicyStd[2]  | 0.16968    |
| AveragePolicyStd[3]  | 0.23167    |
| AveragePolicyStd[4]  | 0.15784    |
| AveragePolicyStd[5]  | 0.2477     |
| AverageReturn        | 1603.7     |
| MinReturn            | 117.98     |
| MaxReturn            | 1797.1     |
| StdReturn            | 357.31     |
| AverageEpisodeLength | 936.94     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.58     |
| TotalNEpisodes       | 19563      |
| TotalNSamples        | 4.8799e+06 |
| ExplainedVariance    | 0.093902   |
-------------------------------------
[2018-12-22 11:47:18.115149 UTC] Saving snapshot
[2018-12-22 11:47:18.115432 UTC] Starting iteration 976
[2018-12-22 11:47:18.115581 UTC] Start collecting samples
[2018-12-22 11:47:21.079341 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:21.158765 UTC] Performing policy update
[2018-12-22 11:47:21.159384 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:21.249125 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:22.306321 UTC] Performing line search
[2018-12-22 11:47:22.434506 UTC] Updating baseline
[2018-12-22 11:47:23.694213 UTC] Computing logging information
-------------------------------------
| Iteration            | 976        |
| ExpectedImprovement  | 0.016895   |
| ActualImprovement    | 0.015434   |
| ImprovementRatio     | 0.91353    |
| MeanKL               | 0.007962   |
| Entropy              | -0.84452   |
| Perplexity           | 0.42976    |
| AveragePolicyStd     | 0.21352    |
| AveragePolicyStd[0]  | 0.24641    |
| AveragePolicyStd[1]  | 0.22788    |
| AveragePolicyStd[2]  | 0.16994    |
| AveragePolicyStd[3]  | 0.23135    |
| AveragePolicyStd[4]  | 0.15761    |
| AveragePolicyStd[5]  | 0.24791    |
| AverageReturn        | 1604.1     |
| MinReturn            | 117.98     |
| MaxReturn            | 1797.1     |
| StdReturn            | 358.35     |
| AverageEpisodeLength | 935.48     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.65     |
| TotalNEpisodes       | 19569      |
| TotalNSamples        | 4.8857e+06 |
| ExplainedVariance    | 0.14474    |
-------------------------------------
[2018-12-22 11:47:24.100753 UTC] Saving snapshot
[2018-12-22 11:47:24.100999 UTC] Starting iteration 977
[2018-12-22 11:47:24.101114 UTC] Start collecting samples
[2018-12-22 11:47:27.023548 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:27.101188 UTC] Performing policy update
[2018-12-22 11:47:27.101843 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:27.190905 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:28.251196 UTC] Performing line search
[2018-12-22 11:47:28.378718 UTC] Updating baseline
[2018-12-22 11:47:29.887969 UTC] Computing logging information
-------------------------------------
| Iteration            | 977        |
| ExpectedImprovement  | 0.016541   |
| ActualImprovement    | 0.016152   |
| ImprovementRatio     | 0.97644    |
| MeanKL               | 0.0072805  |
| Entropy              | -0.85507   |
| Perplexity           | 0.42525    |
| AveragePolicyStd     | 0.21314    |
| AveragePolicyStd[0]  | 0.24617    |
| AveragePolicyStd[1]  | 0.22741    |
| AveragePolicyStd[2]  | 0.16972    |
| AveragePolicyStd[3]  | 0.23047    |
| AveragePolicyStd[4]  | 0.15735    |
| AveragePolicyStd[5]  | 0.24771    |
| AverageReturn        | 1605.1     |
| MinReturn            | 117.98     |
| MaxReturn            | 1797.1     |
| StdReturn            | 358.72     |
| AverageEpisodeLength | 935.48     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.65     |
| TotalNEpisodes       | 19572      |
| TotalNSamples        | 4.8887e+06 |
| ExplainedVariance    | -0.0050741 |
-------------------------------------
[2018-12-22 11:47:30.289170 UTC] Saving snapshot
[2018-12-22 11:47:30.289424 UTC] Starting iteration 978
[2018-12-22 11:47:30.289559 UTC] Start collecting samples
[2018-12-22 11:47:33.267558 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:33.347856 UTC] Performing policy update
[2018-12-22 11:47:33.348632 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:33.438377 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:34.496114 UTC] Performing line search
[2018-12-22 11:47:34.623675 UTC] Updating baseline
[2018-12-22 11:47:35.972007 UTC] Computing logging information
-------------------------------------
| Iteration            | 978        |
| ExpectedImprovement  | 0.016833   |
| ActualImprovement    | 0.016033   |
| ImprovementRatio     | 0.95248    |
| MeanKL               | 0.0076395  |
| Entropy              | -0.85825   |
| Perplexity           | 0.4239     |
| AveragePolicyStd     | 0.21302    |
| AveragePolicyStd[0]  | 0.24556    |
| AveragePolicyStd[1]  | 0.22739    |
| AveragePolicyStd[2]  | 0.16885    |
| AveragePolicyStd[3]  | 0.23051    |
| AveragePolicyStd[4]  | 0.15791    |
| AveragePolicyStd[5]  | 0.24792    |
| AverageReturn        | 1600.7     |
| MinReturn            | 117.98     |
| MaxReturn            | 1798.8     |
| StdReturn            | 364.95     |
| AverageEpisodeLength | 931.67     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.01     |
| TotalNEpisodes       | 19579      |
| TotalNSamples        | 4.8953e+06 |
| ExplainedVariance    | 0.14708    |
-------------------------------------
[2018-12-22 11:47:36.370642 UTC] Saving snapshot
[2018-12-22 11:47:36.370908 UTC] Starting iteration 979
[2018-12-22 11:47:36.371029 UTC] Start collecting samples
[2018-12-22 11:47:39.331214 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:39.410701 UTC] Performing policy update
[2018-12-22 11:47:39.411347 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:39.499963 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:40.566027 UTC] Performing line search
[2018-12-22 11:47:40.692007 UTC] Updating baseline
[2018-12-22 11:47:42.047075 UTC] Computing logging information
-------------------------------------
| Iteration            | 979        |
| ExpectedImprovement  | 0.015993   |
| ActualImprovement    | 0.014756   |
| ImprovementRatio     | 0.92263    |
| MeanKL               | 0.0079286  |
| Entropy              | -0.86407   |
| Perplexity           | 0.42144    |
| AveragePolicyStd     | 0.21281    |
| AveragePolicyStd[0]  | 0.24487    |
| AveragePolicyStd[1]  | 0.22757    |
| AveragePolicyStd[2]  | 0.16861    |
| AveragePolicyStd[3]  | 0.23005    |
| AveragePolicyStd[4]  | 0.15785    |
| AveragePolicyStd[5]  | 0.24793    |
| AverageReturn        | 1591       |
| MinReturn            | 117.98     |
| MaxReturn            | 1798.8     |
| StdReturn            | 393.98     |
| AverageEpisodeLength | 924.4      |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.47     |
| TotalNEpisodes       | 19586      |
| TotalNSamples        | 4.9014e+06 |
| ExplainedVariance    | 0.0080851  |
-------------------------------------
[2018-12-22 11:47:42.445326 UTC] Saving snapshot
[2018-12-22 11:47:42.445588 UTC] Starting iteration 980
[2018-12-22 11:47:42.445707 UTC] Start collecting samples
[2018-12-22 11:47:45.339613 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:45.415476 UTC] Performing policy update
[2018-12-22 11:47:45.416297 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:45.506383 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:46.571922 UTC] Performing line search
[2018-12-22 11:47:46.701390 UTC] Updating baseline
[2018-12-22 11:47:48.107078 UTC] Computing logging information
-------------------------------------
| Iteration            | 980        |
| ExpectedImprovement  | 0.020609   |
| ActualImprovement    | 0.017051   |
| ImprovementRatio     | 0.82734    |
| MeanKL               | 0.0074992  |
| Entropy              | -0.86861   |
| Perplexity           | 0.41954    |
| AveragePolicyStd     | 0.21265    |
| AveragePolicyStd[0]  | 0.24518    |
| AveragePolicyStd[1]  | 0.22773    |
| AveragePolicyStd[2]  | 0.16868    |
| AveragePolicyStd[3]  | 0.22934    |
| AveragePolicyStd[4]  | 0.15764    |
| AveragePolicyStd[5]  | 0.24732    |
| AverageReturn        | 1601.6     |
| MinReturn            | 117.98     |
| MaxReturn            | 1798.8     |
| StdReturn            | 385.16     |
| AverageEpisodeLength | 929.59     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.97     |
| TotalNEpisodes       | 19588      |
| TotalNSamples        | 4.9034e+06 |
| ExplainedVariance    | -0.0016702 |
-------------------------------------
[2018-12-22 11:47:48.503204 UTC] Saving snapshot
[2018-12-22 11:47:48.511405 UTC] Starting iteration 981
[2018-12-22 11:47:48.511620 UTC] Start collecting samples
[2018-12-22 11:47:51.454782 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:51.532805 UTC] Performing policy update
[2018-12-22 11:47:51.533433 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:51.623746 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:52.683147 UTC] Performing line search
[2018-12-22 11:47:52.811579 UTC] Updating baseline
[2018-12-22 11:47:54.483038 UTC] Computing logging information
-------------------------------------
| Iteration            | 981        |
| ExpectedImprovement  | 0.015837   |
| ActualImprovement    | 0.01501    |
| ImprovementRatio     | 0.94775    |
| MeanKL               | 0.0073194  |
| Entropy              | -0.87791   |
| Perplexity           | 0.41565    |
| AveragePolicyStd     | 0.21232    |
| AveragePolicyStd[0]  | 0.24463    |
| AveragePolicyStd[1]  | 0.22751    |
| AveragePolicyStd[2]  | 0.16784    |
| AveragePolicyStd[3]  | 0.22923    |
| AveragePolicyStd[4]  | 0.15774    |
| AveragePolicyStd[5]  | 0.24699    |
| AverageReturn        | 1630       |
| MinReturn            | 120.09     |
| MaxReturn            | 1815.1     |
| StdReturn            | 348.47     |
| AverageEpisodeLength | 943.27     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.13     |
| TotalNEpisodes       | 19593      |
| TotalNSamples        | 4.9084e+06 |
| ExplainedVariance    | -0.0017384 |
-------------------------------------
[2018-12-22 11:47:54.876125 UTC] Saving snapshot
[2018-12-22 11:47:54.876401 UTC] Starting iteration 982
[2018-12-22 11:47:54.876535 UTC] Start collecting samples
[2018-12-22 11:47:57.870567 UTC] Computing input variables for policy optimization
[2018-12-22 11:47:57.950122 UTC] Performing policy update
[2018-12-22 11:47:57.951024 UTC] Computing gradient in Euclidean space
[2018-12-22 11:47:58.040334 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:47:59.104079 UTC] Performing line search
[2018-12-22 11:47:59.232312 UTC] Updating baseline
[2018-12-22 11:48:00.756682 UTC] Computing logging information
-------------------------------------
| Iteration            | 982        |
| ExpectedImprovement  | 0.015672   |
| ActualImprovement    | 0.015089   |
| ImprovementRatio     | 0.96283    |
| MeanKL               | 0.0083786  |
| Entropy              | -0.88291   |
| Perplexity           | 0.41358    |
| AveragePolicyStd     | 0.21212    |
| AveragePolicyStd[0]  | 0.24288    |
| AveragePolicyStd[1]  | 0.22817    |
| AveragePolicyStd[2]  | 0.16758    |
| AveragePolicyStd[3]  | 0.22977    |
| AveragePolicyStd[4]  | 0.15781    |
| AveragePolicyStd[5]  | 0.24651    |
| AverageReturn        | 1644.8     |
| MinReturn            | 120.09     |
| MaxReturn            | 1822.5     |
| StdReturn            | 337.49     |
| AverageEpisodeLength | 948.8      |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.61     |
| TotalNEpisodes       | 19601      |
| TotalNSamples        | 4.9164e+06 |
| ExplainedVariance    | 0.00032062 |
-------------------------------------
[2018-12-22 11:48:01.154146 UTC] Saving snapshot
[2018-12-22 11:48:01.154420 UTC] Starting iteration 983
[2018-12-22 11:48:01.154560 UTC] Start collecting samples
[2018-12-22 11:48:04.081716 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:04.156120 UTC] Performing policy update
[2018-12-22 11:48:04.156853 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:04.247094 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:05.307107 UTC] Performing line search
[2018-12-22 11:48:05.434452 UTC] Updating baseline
[2018-12-22 11:48:07.222742 UTC] Computing logging information
-------------------------------------
| Iteration            | 983        |
| ExpectedImprovement  | 0.014923   |
| ActualImprovement    | 0.013994   |
| ImprovementRatio     | 0.93771    |
| MeanKL               | 0.0075972  |
| Entropy              | -0.88158   |
| Perplexity           | 0.41413    |
| AveragePolicyStd     | 0.21221    |
| AveragePolicyStd[0]  | 0.24224    |
| AveragePolicyStd[1]  | 0.22828    |
| AveragePolicyStd[2]  | 0.16736    |
| AveragePolicyStd[3]  | 0.22998    |
| AveragePolicyStd[4]  | 0.1576     |
| AveragePolicyStd[5]  | 0.24779    |
| AverageReturn        | 1646.1     |
| MinReturn            | 120.09     |
| MaxReturn            | 1822.5     |
| StdReturn            | 337.85     |
| AverageEpisodeLength | 948.8      |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.61     |
| TotalNEpisodes       | 19604      |
| TotalNSamples        | 4.9194e+06 |
| ExplainedVariance    | 0.0015106  |
-------------------------------------
[2018-12-22 11:48:07.623129 UTC] Saving snapshot
[2018-12-22 11:48:07.623429 UTC] Starting iteration 984
[2018-12-22 11:48:07.623566 UTC] Start collecting samples
[2018-12-22 11:48:10.566003 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:10.644462 UTC] Performing policy update
[2018-12-22 11:48:10.645208 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:10.733143 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:11.796954 UTC] Performing line search
[2018-12-22 11:48:11.924672 UTC] Updating baseline
[2018-12-22 11:48:13.188108 UTC] Computing logging information
-------------------------------------
| Iteration            | 984        |
| ExpectedImprovement  | 0.016194   |
| ActualImprovement    | 0.015506   |
| ImprovementRatio     | 0.95751    |
| MeanKL               | 0.0081004  |
| Entropy              | -0.88232   |
| Perplexity           | 0.41382    |
| AveragePolicyStd     | 0.21216    |
| AveragePolicyStd[0]  | 0.24256    |
| AveragePolicyStd[1]  | 0.22753    |
| AveragePolicyStd[2]  | 0.16757    |
| AveragePolicyStd[3]  | 0.22964    |
| AveragePolicyStd[4]  | 0.15774    |
| AveragePolicyStd[5]  | 0.24795    |
| AverageReturn        | 1647.3     |
| MinReturn            | 120.09     |
| MaxReturn            | 1822.5     |
| StdReturn            | 338.78     |
| AverageEpisodeLength | 947.71     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.63     |
| TotalNEpisodes       | 19609      |
| TotalNSamples        | 4.9243e+06 |
| ExplainedVariance    | 0.11215    |
-------------------------------------
[2018-12-22 11:48:13.588910 UTC] Saving snapshot
[2018-12-22 11:48:13.589157 UTC] Starting iteration 985
[2018-12-22 11:48:13.589284 UTC] Start collecting samples
[2018-12-22 11:48:16.567690 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:16.646657 UTC] Performing policy update
[2018-12-22 11:48:16.647472 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:16.736750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:17.791654 UTC] Performing line search
[2018-12-22 11:48:17.923004 UTC] Updating baseline
[2018-12-22 11:48:19.431099 UTC] Computing logging information
-------------------------------------
| Iteration            | 985        |
| ExpectedImprovement  | 0.01599    |
| ActualImprovement    | 0.015687   |
| ImprovementRatio     | 0.98106    |
| MeanKL               | 0.0081661  |
| Entropy              | -0.8824    |
| Perplexity           | 0.41379    |
| AveragePolicyStd     | 0.21214    |
| AveragePolicyStd[0]  | 0.24252    |
| AveragePolicyStd[1]  | 0.22806    |
| AveragePolicyStd[2]  | 0.16792    |
| AveragePolicyStd[3]  | 0.2292     |
| AveragePolicyStd[4]  | 0.15763    |
| AveragePolicyStd[5]  | 0.24751    |
| AverageReturn        | 1663       |
| MinReturn            | 120.09     |
| MaxReturn            | 1822.5     |
| StdReturn            | 319.66     |
| AverageEpisodeLength | 954.41     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175        |
| TotalNEpisodes       | 19616      |
| TotalNSamples        | 4.9313e+06 |
| ExplainedVariance    | -0.025444  |
-------------------------------------
[2018-12-22 11:48:19.829977 UTC] Saving snapshot
[2018-12-22 11:48:19.830224 UTC] Starting iteration 986
[2018-12-22 11:48:19.830350 UTC] Start collecting samples
[2018-12-22 11:48:22.814391 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:22.895308 UTC] Performing policy update
[2018-12-22 11:48:22.895940 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:22.990435 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:24.107839 UTC] Performing line search
[2018-12-22 11:48:24.241486 UTC] Updating baseline
[2018-12-22 11:48:26.281458 UTC] Computing logging information
-------------------------------------
| Iteration            | 986        |
| ExpectedImprovement  | 0.017171   |
| ActualImprovement    | 0.015954   |
| ImprovementRatio     | 0.92912    |
| MeanKL               | 0.008039   |
| Entropy              | -0.88362   |
| Perplexity           | 0.41328    |
| AveragePolicyStd     | 0.21207    |
| AveragePolicyStd[0]  | 0.24262    |
| AveragePolicyStd[1]  | 0.22779    |
| AveragePolicyStd[2]  | 0.16813    |
| AveragePolicyStd[3]  | 0.22851    |
| AveragePolicyStd[4]  | 0.15776    |
| AveragePolicyStd[5]  | 0.24764    |
| AverageReturn        | 1664.4     |
| MinReturn            | 120.09     |
| MaxReturn            | 1822.5     |
| StdReturn            | 320.01     |
| AverageEpisodeLength | 954.41     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175        |
| TotalNEpisodes       | 19619      |
| TotalNSamples        | 4.9343e+06 |
| ExplainedVariance    | 0.0027201  |
-------------------------------------
[2018-12-22 11:48:26.715230 UTC] Saving snapshot
[2018-12-22 11:48:26.715473 UTC] Starting iteration 987
[2018-12-22 11:48:26.715662 UTC] Start collecting samples
[2018-12-22 11:48:30.345161 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:30.439441 UTC] Performing policy update
[2018-12-22 11:48:30.440446 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:30.545423 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:31.835863 UTC] Performing line search
[2018-12-22 11:48:31.991023 UTC] Updating baseline
[2018-12-22 11:48:33.661306 UTC] Computing logging information
-------------------------------------
| Iteration            | 987        |
| ExpectedImprovement  | 0.017124   |
| ActualImprovement    | 0.015725   |
| ImprovementRatio     | 0.91831    |
| MeanKL               | 0.0078522  |
| Entropy              | -0.88085   |
| Perplexity           | 0.41443    |
| AveragePolicyStd     | 0.2122     |
| AveragePolicyStd[0]  | 0.2429     |
| AveragePolicyStd[1]  | 0.22811    |
| AveragePolicyStd[2]  | 0.16778    |
| AveragePolicyStd[3]  | 0.22855    |
| AveragePolicyStd[4]  | 0.15786    |
| AveragePolicyStd[5]  | 0.24801    |
| AverageReturn        | 1667.5     |
| MinReturn            | 120.09     |
| MaxReturn            | 1822.5     |
| StdReturn            | 320.79     |
| AverageEpisodeLength | 954.41     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175        |
| TotalNEpisodes       | 19623      |
| TotalNSamples        | 4.9383e+06 |
| ExplainedVariance    | 0.0012793  |
-------------------------------------
[2018-12-22 11:48:34.094585 UTC] Saving snapshot
[2018-12-22 11:48:34.094829 UTC] Starting iteration 988
[2018-12-22 11:48:34.094966 UTC] Start collecting samples
[2018-12-22 11:48:37.097083 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:37.175860 UTC] Performing policy update
[2018-12-22 11:48:37.176606 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:37.266444 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:38.322042 UTC] Performing line search
[2018-12-22 11:48:38.449110 UTC] Updating baseline
[2018-12-22 11:48:40.098056 UTC] Computing logging information
-------------------------------------
| Iteration            | 988        |
| ExpectedImprovement  | 0.016693   |
| ActualImprovement    | 0.016261   |
| ImprovementRatio     | 0.97413    |
| MeanKL               | 0.0071431  |
| Entropy              | -0.87947   |
| Perplexity           | 0.415      |
| AveragePolicyStd     | 0.21233    |
| AveragePolicyStd[0]  | 0.2427     |
| AveragePolicyStd[1]  | 0.22869    |
| AveragePolicyStd[2]  | 0.16739    |
| AveragePolicyStd[3]  | 0.22791    |
| AveragePolicyStd[4]  | 0.15751    |
| AveragePolicyStd[5]  | 0.24976    |
| AverageReturn        | 1686.8     |
| MinReturn            | 120.09     |
| MaxReturn            | 1828.5     |
| StdReturn            | 282.09     |
| AverageEpisodeLength | 963.29     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.2      |
| TotalNEpisodes       | 19632      |
| TotalNSamples        | 4.9473e+06 |
| ExplainedVariance    | -0.0013072 |
-------------------------------------
[2018-12-22 11:48:40.486231 UTC] Saving snapshot
[2018-12-22 11:48:40.486472 UTC] Starting iteration 989
[2018-12-22 11:48:40.486609 UTC] Start collecting samples
[2018-12-22 11:48:43.381673 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:43.456660 UTC] Performing policy update
[2018-12-22 11:48:43.457336 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:43.546632 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:44.606469 UTC] Performing line search
[2018-12-22 11:48:44.734708 UTC] Updating baseline
[2018-12-22 11:48:46.077977 UTC] Computing logging information
-------------------------------------
| Iteration            | 989        |
| ExpectedImprovement  | 0.01681    |
| ActualImprovement    | 0.015977   |
| ImprovementRatio     | 0.95043    |
| MeanKL               | 0.0077744  |
| Entropy              | -0.86908   |
| Perplexity           | 0.41934    |
| AveragePolicyStd     | 0.21264    |
| AveragePolicyStd[0]  | 0.24228    |
| AveragePolicyStd[1]  | 0.22926    |
| AveragePolicyStd[2]  | 0.16793    |
| AveragePolicyStd[3]  | 0.22827    |
| AveragePolicyStd[4]  | 0.15813    |
| AveragePolicyStd[5]  | 0.24998    |
| AverageReturn        | 1702.3     |
| MinReturn            | 120.09     |
| MaxReturn            | 1828.5     |
| StdReturn            | 254.32     |
| AverageEpisodeLength | 970.36     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.63     |
| TotalNEpisodes       | 19635      |
| TotalNSamples        | 4.9503e+06 |
| ExplainedVariance    | 0.00049767 |
-------------------------------------
[2018-12-22 11:48:46.477421 UTC] Saving snapshot
[2018-12-22 11:48:46.477686 UTC] Starting iteration 990
[2018-12-22 11:48:46.477836 UTC] Start collecting samples
[2018-12-22 11:48:49.387197 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:49.464222 UTC] Performing policy update
[2018-12-22 11:48:49.464984 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:49.555615 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:50.614718 UTC] Performing line search
[2018-12-22 11:48:50.742710 UTC] Updating baseline
[2018-12-22 11:48:52.514783 UTC] Computing logging information
-------------------------------------
| Iteration            | 990        |
| ExpectedImprovement  | 0.015679   |
| ActualImprovement    | 0.014844   |
| ImprovementRatio     | 0.94674    |
| MeanKL               | 0.0074613  |
| Entropy              | -0.86726   |
| Perplexity           | 0.4201     |
| AveragePolicyStd     | 0.21271    |
| AveragePolicyStd[0]  | 0.24247    |
| AveragePolicyStd[1]  | 0.22873    |
| AveragePolicyStd[2]  | 0.1677     |
| AveragePolicyStd[3]  | 0.22856    |
| AveragePolicyStd[4]  | 0.15844    |
| AveragePolicyStd[5]  | 0.25035    |
| AverageReturn        | 1703.8     |
| MinReturn            | 120.09     |
| MaxReturn            | 1828.5     |
| StdReturn            | 254.7      |
| AverageEpisodeLength | 970.36     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.63     |
| TotalNEpisodes       | 19638      |
| TotalNSamples        | 4.9533e+06 |
| ExplainedVariance    | 0.0021882  |
-------------------------------------
[2018-12-22 11:48:52.908599 UTC] Saving snapshot
[2018-12-22 11:48:52.916870 UTC] Starting iteration 991
[2018-12-22 11:48:52.917066 UTC] Start collecting samples
[2018-12-22 11:48:55.897462 UTC] Computing input variables for policy optimization
[2018-12-22 11:48:55.979547 UTC] Performing policy update
[2018-12-22 11:48:55.980309 UTC] Computing gradient in Euclidean space
[2018-12-22 11:48:56.070508 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:48:57.126716 UTC] Performing line search
[2018-12-22 11:48:57.254100 UTC] Updating baseline
[2018-12-22 11:48:58.854119 UTC] Computing logging information
--------------------------------------
| Iteration            | 991         |
| ExpectedImprovement  | 0.017598    |
| ActualImprovement    | 0.016936    |
| ImprovementRatio     | 0.96242     |
| MeanKL               | 0.0080383   |
| Entropy              | -0.86892    |
| Perplexity           | 0.4194      |
| AveragePolicyStd     | 0.2127      |
| AveragePolicyStd[0]  | 0.24213     |
| AveragePolicyStd[1]  | 0.22878     |
| AveragePolicyStd[2]  | 0.16776     |
| AveragePolicyStd[3]  | 0.22838     |
| AveragePolicyStd[4]  | 0.15777     |
| AveragePolicyStd[5]  | 0.25142     |
| AverageReturn        | 1724.3      |
| MinReturn            | 120.09      |
| MaxReturn            | 1844.2      |
| StdReturn            | 201.67      |
| AverageEpisodeLength | 979.33      |
| MinEpisodeLength     | 91          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 106.52      |
| TotalNEpisodes       | 19647       |
| TotalNSamples        | 4.9623e+06  |
| ExplainedVariance    | -0.00054904 |
--------------------------------------
[2018-12-22 11:48:59.254046 UTC] Saving snapshot
[2018-12-22 11:48:59.254367 UTC] Starting iteration 992
[2018-12-22 11:48:59.254504 UTC] Start collecting samples
[2018-12-22 11:49:02.191463 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:02.267912 UTC] Performing policy update
[2018-12-22 11:49:02.268840 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:02.359061 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:03.410419 UTC] Performing line search
[2018-12-22 11:49:03.537036 UTC] Updating baseline
[2018-12-22 11:49:05.117563 UTC] Computing logging information
--------------------------------------
| Iteration            | 992         |
| ExpectedImprovement  | 0.019493    |
| ActualImprovement    | 0.01795     |
| ImprovementRatio     | 0.92086     |
| MeanKL               | 0.0086001   |
| Entropy              | -0.87715    |
| Perplexity           | 0.41597     |
| AveragePolicyStd     | 0.21236     |
| AveragePolicyStd[0]  | 0.24252     |
| AveragePolicyStd[1]  | 0.22724     |
| AveragePolicyStd[2]  | 0.16796     |
| AveragePolicyStd[3]  | 0.22783     |
| AveragePolicyStd[4]  | 0.15789     |
| AveragePolicyStd[5]  | 0.25075     |
| AverageReturn        | 1729.3      |
| MinReturn            | 120.09      |
| MaxReturn            | 1844.2      |
| StdReturn            | 200         |
| AverageEpisodeLength | 980.59      |
| MinEpisodeLength     | 91          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 106.01      |
| TotalNEpisodes       | 19650       |
| TotalNSamples        | 4.9653e+06  |
| ExplainedVariance    | -0.00029939 |
--------------------------------------
[2018-12-22 11:49:05.511909 UTC] Saving snapshot
[2018-12-22 11:49:05.512155 UTC] Starting iteration 993
[2018-12-22 11:49:05.512271 UTC] Start collecting samples
[2018-12-22 11:49:08.465693 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:08.543178 UTC] Performing policy update
[2018-12-22 11:49:08.544040 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:08.634365 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:09.693913 UTC] Performing line search
[2018-12-22 11:49:09.821413 UTC] Updating baseline
[2018-12-22 11:49:11.236903 UTC] Computing logging information
-------------------------------------
| Iteration            | 993        |
| ExpectedImprovement  | 0.020134   |
| ActualImprovement    | 0.018461   |
| ImprovementRatio     | 0.91688    |
| MeanKL               | 0.0072383  |
| Entropy              | -0.88167   |
| Perplexity           | 0.41409    |
| AveragePolicyStd     | 0.21219    |
| AveragePolicyStd[0]  | 0.24209    |
| AveragePolicyStd[1]  | 0.22735    |
| AveragePolicyStd[2]  | 0.16769    |
| AveragePolicyStd[3]  | 0.2275     |
| AveragePolicyStd[4]  | 0.15804    |
| AveragePolicyStd[5]  | 0.25048    |
| AverageReturn        | 1722.5     |
| MinReturn            | 120.09     |
| MaxReturn            | 1844.2     |
| StdReturn            | 216.11     |
| AverageEpisodeLength | 976.06     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.43     |
| TotalNEpisodes       | 19654      |
| TotalNSamples        | 4.9689e+06 |
| ExplainedVariance    | 0.17998    |
-------------------------------------
[2018-12-22 11:49:11.634287 UTC] Saving snapshot
[2018-12-22 11:49:11.634563 UTC] Starting iteration 994
[2018-12-22 11:49:11.634692 UTC] Start collecting samples
[2018-12-22 11:49:14.598082 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:14.678219 UTC] Performing policy update
[2018-12-22 11:49:14.679119 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:14.770737 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:15.838795 UTC] Performing line search
[2018-12-22 11:49:15.970675 UTC] Updating baseline
[2018-12-22 11:49:17.200728 UTC] Computing logging information
-------------------------------------
| Iteration            | 994        |
| ExpectedImprovement  | 0.016837   |
| ActualImprovement    | 0.015715   |
| ImprovementRatio     | 0.93334    |
| MeanKL               | 0.0077256  |
| Entropy              | -0.88492   |
| Perplexity           | 0.41275    |
| AveragePolicyStd     | 0.2121     |
| AveragePolicyStd[0]  | 0.24206    |
| AveragePolicyStd[1]  | 0.22757    |
| AveragePolicyStd[2]  | 0.16751    |
| AveragePolicyStd[3]  | 0.22691    |
| AveragePolicyStd[4]  | 0.15773    |
| AveragePolicyStd[5]  | 0.25085    |
| AverageReturn        | 1727       |
| MinReturn            | 120.09     |
| MaxReturn            | 1844.2     |
| StdReturn            | 216.84     |
| AverageEpisodeLength | 976.06     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.43     |
| TotalNEpisodes       | 19661      |
| TotalNSamples        | 4.9759e+06 |
| ExplainedVariance    | -0.0012186 |
-------------------------------------
[2018-12-22 11:49:17.601833 UTC] Saving snapshot
[2018-12-22 11:49:17.602079 UTC] Starting iteration 995
[2018-12-22 11:49:17.602194 UTC] Start collecting samples
[2018-12-22 11:49:20.565282 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:20.641669 UTC] Performing policy update
[2018-12-22 11:49:20.642449 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:20.732369 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:21.797848 UTC] Performing line search
[2018-12-22 11:49:21.929307 UTC] Updating baseline
[2018-12-22 11:49:23.174073 UTC] Computing logging information
-------------------------------------
| Iteration            | 995        |
| ExpectedImprovement  | 0.017491   |
| ActualImprovement    | 0.016282   |
| ImprovementRatio     | 0.93086    |
| MeanKL               | 0.007784   |
| Entropy              | -0.89093   |
| Perplexity           | 0.41027    |
| AveragePolicyStd     | 0.21188    |
| AveragePolicyStd[0]  | 0.2422     |
| AveragePolicyStd[1]  | 0.22716    |
| AveragePolicyStd[2]  | 0.16739    |
| AveragePolicyStd[3]  | 0.22648    |
| AveragePolicyStd[4]  | 0.15767    |
| AveragePolicyStd[5]  | 0.2504     |
| AverageReturn        | 1736.6     |
| MinReturn            | 120.09     |
| MaxReturn            | 1872.1     |
| StdReturn            | 204.99     |
| AverageEpisodeLength | 980.02     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.16     |
| TotalNEpisodes       | 19666      |
| TotalNSamples        | 4.9809e+06 |
| ExplainedVariance    | -0.0054662 |
-------------------------------------
[2018-12-22 11:49:23.568512 UTC] Saving snapshot
[2018-12-22 11:49:23.568807 UTC] Starting iteration 996
[2018-12-22 11:49:23.568926 UTC] Start collecting samples
[2018-12-22 11:49:26.545332 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:26.621763 UTC] Performing policy update
[2018-12-22 11:49:26.622461 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:26.712089 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:27.772199 UTC] Performing line search
[2018-12-22 11:49:27.898867 UTC] Updating baseline
[2018-12-22 11:49:29.485515 UTC] Computing logging information
-------------------------------------
| Iteration            | 996        |
| ExpectedImprovement  | 0.01802    |
| ActualImprovement    | 0.016605   |
| ImprovementRatio     | 0.92146    |
| MeanKL               | 0.0070955  |
| Entropy              | -0.89486   |
| Perplexity           | 0.40867    |
| AveragePolicyStd     | 0.21173    |
| AveragePolicyStd[0]  | 0.24213    |
| AveragePolicyStd[1]  | 0.22705    |
| AveragePolicyStd[2]  | 0.16721    |
| AveragePolicyStd[3]  | 0.2265     |
| AveragePolicyStd[4]  | 0.15773    |
| AveragePolicyStd[5]  | 0.24975    |
| AverageReturn        | 1733.5     |
| MinReturn            | 120.09     |
| MaxReturn            | 1872.1     |
| StdReturn            | 218.31     |
| AverageEpisodeLength | 976.96     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 115.75     |
| TotalNEpisodes       | 19670      |
| TotalNSamples        | 4.9844e+06 |
| ExplainedVariance    | 0.10955    |
-------------------------------------
[2018-12-22 11:49:29.888820 UTC] Saving snapshot
[2018-12-22 11:49:29.889072 UTC] Starting iteration 997
[2018-12-22 11:49:29.889193 UTC] Start collecting samples
[2018-12-22 11:49:32.859850 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:32.942743 UTC] Performing policy update
[2018-12-22 11:49:32.943322 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:33.033552 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:34.102589 UTC] Performing line search
[2018-12-22 11:49:34.230607 UTC] Updating baseline
[2018-12-22 11:49:35.825117 UTC] Computing logging information
-------------------------------------
| Iteration            | 997        |
| ExpectedImprovement  | 0.01656    |
| ActualImprovement    | 0.015637   |
| ImprovementRatio     | 0.94425    |
| MeanKL               | 0.0078225  |
| Entropy              | -0.89924   |
| Perplexity           | 0.40688    |
| AveragePolicyStd     | 0.21157    |
| AveragePolicyStd[0]  | 0.24241    |
| AveragePolicyStd[1]  | 0.2267     |
| AveragePolicyStd[2]  | 0.1671     |
| AveragePolicyStd[3]  | 0.22692    |
| AveragePolicyStd[4]  | 0.15753    |
| AveragePolicyStd[5]  | 0.24878    |
| AverageReturn        | 1744.2     |
| MinReturn            | 120.09     |
| MaxReturn            | 1872.1     |
| StdReturn            | 205.48     |
| AverageEpisodeLength | 980.77     |
| MinEpisodeLength     | 91         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 110.03     |
| TotalNEpisodes       | 19676      |
| TotalNSamples        | 4.9904e+06 |
| ExplainedVariance    | 0.035115   |
-------------------------------------
[2018-12-22 11:49:36.226689 UTC] Saving snapshot
[2018-12-22 11:49:36.226961 UTC] Starting iteration 998
[2018-12-22 11:49:36.227082 UTC] Start collecting samples
[2018-12-22 11:49:39.170989 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:39.248430 UTC] Performing policy update
[2018-12-22 11:49:39.249235 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:39.338165 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:40.395022 UTC] Performing line search
[2018-12-22 11:49:40.522939 UTC] Updating baseline
[2018-12-22 11:49:41.780460 UTC] Computing logging information
-------------------------------------
| Iteration            | 998        |
| ExpectedImprovement  | 0.01857    |
| ActualImprovement    | 0.018536   |
| ImprovementRatio     | 0.99819    |
| MeanKL               | 0.0072217  |
| Entropy              | -0.89979   |
| Perplexity           | 0.40665    |
| AveragePolicyStd     | 0.21156    |
| AveragePolicyStd[0]  | 0.24258    |
| AveragePolicyStd[1]  | 0.22656    |
| AveragePolicyStd[2]  | 0.16679    |
| AveragePolicyStd[3]  | 0.22654    |
| AveragePolicyStd[4]  | 0.15781    |
| AveragePolicyStd[5]  | 0.24907    |
| AverageReturn        | 1762.6     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 125.34     |
| AverageEpisodeLength | 989.86     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 64.118     |
| TotalNEpisodes       | 19681      |
| TotalNSamples        | 4.9954e+06 |
| ExplainedVariance    | 0.075168   |
-------------------------------------
[2018-12-22 11:49:42.185985 UTC] Saving snapshot
[2018-12-22 11:49:42.186233 UTC] Starting iteration 999
[2018-12-22 11:49:42.186364 UTC] Start collecting samples
[2018-12-22 11:49:45.123843 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:45.202221 UTC] Performing policy update
[2018-12-22 11:49:45.202825 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:45.294596 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:46.355729 UTC] Performing line search
[2018-12-22 11:49:46.483849 UTC] Updating baseline
[2018-12-22 11:49:47.830253 UTC] Computing logging information
-------------------------------------
| Iteration            | 999        |
| ExpectedImprovement  | 0.017217   |
| ActualImprovement    | 0.016324   |
| ImprovementRatio     | 0.94813    |
| MeanKL               | 0.0076161  |
| Entropy              | -0.89992   |
| Perplexity           | 0.4066     |
| AveragePolicyStd     | 0.21158    |
| AveragePolicyStd[0]  | 0.24235    |
| AveragePolicyStd[1]  | 0.22662    |
| AveragePolicyStd[2]  | 0.16651    |
| AveragePolicyStd[3]  | 0.22691    |
| AveragePolicyStd[4]  | 0.15766    |
| AveragePolicyStd[5]  | 0.24945    |
| AverageReturn        | 1762.8     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 126.29     |
| AverageEpisodeLength | 988.98     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 64.575     |
| TotalNEpisodes       | 19685      |
| TotalNSamples        | 4.9993e+06 |
| ExplainedVariance    | 0.13581    |
-------------------------------------
[2018-12-22 11:49:48.229356 UTC] Saving snapshot
[2018-12-22 11:49:48.229616 UTC] Starting iteration 1000
[2018-12-22 11:49:48.229755 UTC] Start collecting samples
[2018-12-22 11:49:51.172270 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:51.251363 UTC] Performing policy update
[2018-12-22 11:49:51.251959 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:51.342141 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:52.397551 UTC] Performing line search
[2018-12-22 11:49:52.525461 UTC] Updating baseline
[2018-12-22 11:49:53.922120 UTC] Computing logging information
-------------------------------------
| Iteration            | 1000       |
| ExpectedImprovement  | 0.016373   |
| ActualImprovement    | 0.015937   |
| ImprovementRatio     | 0.97339    |
| MeanKL               | 0.0079069  |
| Entropy              | -0.9098    |
| Perplexity           | 0.4026     |
| AveragePolicyStd     | 0.21125    |
| AveragePolicyStd[0]  | 0.24158    |
| AveragePolicyStd[1]  | 0.22688    |
| AveragePolicyStd[2]  | 0.16595    |
| AveragePolicyStd[3]  | 0.22659    |
| AveragePolicyStd[4]  | 0.15738    |
| AveragePolicyStd[5]  | 0.24914    |
| AverageReturn        | 1765.2     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 126.79     |
| AverageEpisodeLength | 988.98     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 64.575     |
| TotalNEpisodes       | 19691      |
| TotalNSamples        | 5.0053e+06 |
| ExplainedVariance    | 0.0013168  |
-------------------------------------
[2018-12-22 11:49:54.319217 UTC] Saving snapshot
[2018-12-22 11:49:54.327292 UTC] Starting iteration 1001
[2018-12-22 11:49:54.327505 UTC] Start collecting samples
[2018-12-22 11:49:57.276394 UTC] Computing input variables for policy optimization
[2018-12-22 11:49:57.353129 UTC] Performing policy update
[2018-12-22 11:49:57.354105 UTC] Computing gradient in Euclidean space
[2018-12-22 11:49:57.444791 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:49:58.515211 UTC] Performing line search
[2018-12-22 11:49:58.644042 UTC] Updating baseline
[2018-12-22 11:50:00.033696 UTC] Computing logging information
-------------------------------------
| Iteration            | 1001       |
| ExpectedImprovement  | 0.015267   |
| ActualImprovement    | 0.014628   |
| ImprovementRatio     | 0.95815    |
| MeanKL               | 0.0087144  |
| Entropy              | -0.90321   |
| Perplexity           | 0.40527    |
| AveragePolicyStd     | 0.21145    |
| AveragePolicyStd[0]  | 0.24123    |
| AveragePolicyStd[1]  | 0.2274     |
| AveragePolicyStd[2]  | 0.16658    |
| AveragePolicyStd[3]  | 0.22627    |
| AveragePolicyStd[4]  | 0.1576     |
| AveragePolicyStd[5]  | 0.24962    |
| AverageReturn        | 1766.1     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 127.01     |
| AverageEpisodeLength | 988.98     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 64.575     |
| TotalNEpisodes       | 19696      |
| TotalNSamples        | 5.0103e+06 |
| ExplainedVariance    | 0.0019916  |
-------------------------------------
[2018-12-22 11:50:00.432299 UTC] Saving snapshot
[2018-12-22 11:50:00.432558 UTC] Starting iteration 1002
[2018-12-22 11:50:00.432680 UTC] Start collecting samples
[2018-12-22 11:50:03.369659 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:03.446716 UTC] Performing policy update
[2018-12-22 11:50:03.447600 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:03.537593 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:04.607992 UTC] Performing line search
[2018-12-22 11:50:04.737666 UTC] Updating baseline
[2018-12-22 11:50:06.340807 UTC] Computing logging information
-------------------------------------
| Iteration            | 1002       |
| ExpectedImprovement  | 0.017877   |
| ActualImprovement    | 0.016413   |
| ImprovementRatio     | 0.9181     |
| MeanKL               | 0.0077995  |
| Entropy              | -0.89758   |
| Perplexity           | 0.40755    |
| AveragePolicyStd     | 0.21167    |
| AveragePolicyStd[0]  | 0.24132    |
| AveragePolicyStd[1]  | 0.22749    |
| AveragePolicyStd[2]  | 0.16683    |
| AveragePolicyStd[3]  | 0.22694    |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.24996    |
| AverageReturn        | 1768.3     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 127.45     |
| AverageEpisodeLength | 988.98     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 64.575     |
| TotalNEpisodes       | 19699      |
| TotalNSamples        | 5.0133e+06 |
| ExplainedVariance    | 0.0062353  |
-------------------------------------
[2018-12-22 11:50:06.744206 UTC] Saving snapshot
[2018-12-22 11:50:06.744482 UTC] Starting iteration 1003
[2018-12-22 11:50:06.744620 UTC] Start collecting samples
[2018-12-22 11:50:09.725624 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:09.804188 UTC] Performing policy update
[2018-12-22 11:50:09.804909 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:09.896286 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:10.959146 UTC] Performing line search
[2018-12-22 11:50:11.086570 UTC] Updating baseline
[2018-12-22 11:50:13.627076 UTC] Computing logging information
------------------------------------
| Iteration            | 1003      |
| ExpectedImprovement  | 0.0185    |
| ActualImprovement    | 0.016836  |
| ImprovementRatio     | 0.91001   |
| MeanKL               | 0.0069498 |
| Entropy              | -0.89804  |
| Perplexity           | 0.40737   |
| AveragePolicyStd     | 0.21161   |
| AveragePolicyStd[0]  | 0.2412    |
| AveragePolicyStd[1]  | 0.22746   |
| AveragePolicyStd[2]  | 0.16706   |
| AveragePolicyStd[3]  | 0.22685   |
| AveragePolicyStd[4]  | 0.15766   |
| AveragePolicyStd[5]  | 0.24942   |
| AverageReturn        | 1765.7    |
| MinReturn            | 914.14    |
| MaxReturn            | 1872.1    |
| StdReturn            | 139.27    |
| AverageEpisodeLength | 985.95    |
| MinEpisodeLength     | 547       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 70.796    |
| TotalNEpisodes       | 19706     |
| TotalNSamples        | 5.02e+06  |
| ExplainedVariance    | 0.067696  |
------------------------------------
[2018-12-22 11:50:14.033962 UTC] Saving snapshot
[2018-12-22 11:50:14.034282 UTC] Starting iteration 1004
[2018-12-22 11:50:14.034411 UTC] Start collecting samples
[2018-12-22 11:50:16.991156 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:17.068446 UTC] Performing policy update
[2018-12-22 11:50:17.069130 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:17.158394 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:18.223291 UTC] Performing line search
[2018-12-22 11:50:18.351797 UTC] Updating baseline
[2018-12-22 11:50:20.041520 UTC] Computing logging information
------------------------------------
| Iteration            | 1004      |
| ExpectedImprovement  | 0.016659  |
| ActualImprovement    | 0.015738  |
| ImprovementRatio     | 0.94474   |
| MeanKL               | 0.0075642 |
| Entropy              | -0.89925  |
| Perplexity           | 0.40688   |
| AveragePolicyStd     | 0.21157   |
| AveragePolicyStd[0]  | 0.24073   |
| AveragePolicyStd[1]  | 0.22678   |
| AveragePolicyStd[2]  | 0.16687   |
| AveragePolicyStd[3]  | 0.22698   |
| AveragePolicyStd[4]  | 0.15784   |
| AveragePolicyStd[5]  | 0.25022   |
| AverageReturn        | 1770.7    |
| MinReturn            | 914.14    |
| MaxReturn            | 1872.1    |
| StdReturn            | 137.83    |
| AverageEpisodeLength | 987.04    |
| MinEpisodeLength     | 547       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 70.162    |
| TotalNEpisodes       | 19712     |
| TotalNSamples        | 5.026e+06 |
| ExplainedVariance    | 0.0046861 |
------------------------------------
[2018-12-22 11:50:20.439947 UTC] Saving snapshot
[2018-12-22 11:50:20.440188 UTC] Starting iteration 1005
[2018-12-22 11:50:20.440316 UTC] Start collecting samples
[2018-12-22 11:50:23.346593 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:23.422953 UTC] Performing policy update
[2018-12-22 11:50:23.423589 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:23.513037 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:24.572239 UTC] Performing line search
[2018-12-22 11:50:24.699704 UTC] Updating baseline
[2018-12-22 11:50:26.286607 UTC] Computing logging information
------------------------------------
| Iteration            | 1005      |
| ExpectedImprovement  | 0.016839  |
| ActualImprovement    | 0.016778  |
| ImprovementRatio     | 0.99637   |
| MeanKL               | 0.0082496 |
| Entropy              | -0.90026  |
| Perplexity           | 0.40646   |
| AveragePolicyStd     | 0.21153   |
| AveragePolicyStd[0]  | 0.24046   |
| AveragePolicyStd[1]  | 0.2268    |
| AveragePolicyStd[2]  | 0.16689   |
| AveragePolicyStd[3]  | 0.22655   |
| AveragePolicyStd[4]  | 0.15786   |
| AveragePolicyStd[5]  | 0.25063   |
| AverageReturn        | 1772.6    |
| MinReturn            | 914.14    |
| MaxReturn            | 1872.1    |
| StdReturn            | 138.2     |
| AverageEpisodeLength | 987.04    |
| MinEpisodeLength     | 547       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 70.162    |
| TotalNEpisodes       | 19715     |
| TotalNSamples        | 5.029e+06 |
| ExplainedVariance    | -0.024925 |
------------------------------------
[2018-12-22 11:50:26.684175 UTC] Saving snapshot
[2018-12-22 11:50:26.684452 UTC] Starting iteration 1006
[2018-12-22 11:50:26.684588 UTC] Start collecting samples
[2018-12-22 11:50:29.644670 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:29.723428 UTC] Performing policy update
[2018-12-22 11:50:29.724199 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:29.818622 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:30.878239 UTC] Performing line search
[2018-12-22 11:50:31.005652 UTC] Updating baseline
[2018-12-22 11:50:32.350194 UTC] Computing logging information
-------------------------------------
| Iteration            | 1006       |
| ExpectedImprovement  | 0.016514   |
| ActualImprovement    | 0.015149   |
| ImprovementRatio     | 0.9173     |
| MeanKL               | 0.007676   |
| Entropy              | -0.89625   |
| Perplexity           | 0.4081     |
| AveragePolicyStd     | 0.21166    |
| AveragePolicyStd[0]  | 0.24008    |
| AveragePolicyStd[1]  | 0.22694    |
| AveragePolicyStd[2]  | 0.16704    |
| AveragePolicyStd[3]  | 0.22689    |
| AveragePolicyStd[4]  | 0.15808    |
| AveragePolicyStd[5]  | 0.25093    |
| AverageReturn        | 1773.9     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 139.4      |
| AverageEpisodeLength | 986.22     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 70.484     |
| TotalNEpisodes       | 19721      |
| TotalNSamples        | 5.0349e+06 |
| ExplainedVariance    | 0.11778    |
-------------------------------------
[2018-12-22 11:50:32.742075 UTC] Saving snapshot
[2018-12-22 11:50:32.742386 UTC] Starting iteration 1007
[2018-12-22 11:50:32.742522 UTC] Start collecting samples
[2018-12-22 11:50:35.718427 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:35.799039 UTC] Performing policy update
[2018-12-22 11:50:35.799671 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:35.889027 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:36.848030 UTC] Performing line search
[2018-12-22 11:50:36.963667 UTC] Updating baseline
[2018-12-22 11:50:38.972825 UTC] Computing logging information
-------------------------------------
| Iteration            | 1007       |
| ExpectedImprovement  | 0.017015   |
| ActualImprovement    | 0.015871   |
| ImprovementRatio     | 0.93277    |
| MeanKL               | 0.0074199  |
| Entropy              | -0.90077   |
| Perplexity           | 0.40626    |
| AveragePolicyStd     | 0.21153    |
| AveragePolicyStd[0]  | 0.23972    |
| AveragePolicyStd[1]  | 0.2267     |
| AveragePolicyStd[2]  | 0.16719    |
| AveragePolicyStd[3]  | 0.22641    |
| AveragePolicyStd[4]  | 0.15752    |
| AveragePolicyStd[5]  | 0.25164    |
| AverageReturn        | 1774.1     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 139.35     |
| AverageEpisodeLength | 986.22     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 70.484     |
| TotalNEpisodes       | 19727      |
| TotalNSamples        | 5.0409e+06 |
| ExplainedVariance    | 0.0021529  |
-------------------------------------
[2018-12-22 11:50:39.368763 UTC] Saving snapshot
[2018-12-22 11:50:39.369048 UTC] Starting iteration 1008
[2018-12-22 11:50:39.369164 UTC] Start collecting samples
[2018-12-22 11:50:42.307484 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:42.384271 UTC] Performing policy update
[2018-12-22 11:50:42.385863 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:42.476719 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:43.534292 UTC] Performing line search
[2018-12-22 11:50:43.662396 UTC] Updating baseline
[2018-12-22 11:50:45.006553 UTC] Computing logging information
-------------------------------------
| Iteration            | 1008       |
| ExpectedImprovement  | 0.017707   |
| ActualImprovement    | 0.016179   |
| ImprovementRatio     | 0.9137     |
| MeanKL               | 0.0073568  |
| Entropy              | -0.90122   |
| Perplexity           | 0.40607    |
| AveragePolicyStd     | 0.21153    |
| AveragePolicyStd[0]  | 0.23956    |
| AveragePolicyStd[1]  | 0.22689    |
| AveragePolicyStd[2]  | 0.16652    |
| AveragePolicyStd[3]  | 0.22687    |
| AveragePolicyStd[4]  | 0.15782    |
| AveragePolicyStd[5]  | 0.25151    |
| AverageReturn        | 1770.3     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 146.12     |
| AverageEpisodeLength | 983.64     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 74.538     |
| TotalNEpisodes       | 19731      |
| TotalNSamples        | 5.0447e+06 |
| ExplainedVariance    | 0.13337    |
-------------------------------------
[2018-12-22 11:50:45.409136 UTC] Saving snapshot
[2018-12-22 11:50:45.409425 UTC] Starting iteration 1009
[2018-12-22 11:50:45.409559 UTC] Start collecting samples
[2018-12-22 11:50:48.379121 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:48.456898 UTC] Performing policy update
[2018-12-22 11:50:48.457638 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:48.547434 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:49.605455 UTC] Performing line search
[2018-12-22 11:50:49.732983 UTC] Updating baseline
[2018-12-22 11:50:50.980422 UTC] Computing logging information
-------------------------------------
| Iteration            | 1009       |
| ExpectedImprovement  | 0.016825   |
| ActualImprovement    | 0.01584    |
| ImprovementRatio     | 0.94149    |
| MeanKL               | 0.0079187  |
| Entropy              | -0.9011    |
| Perplexity           | 0.40612    |
| AveragePolicyStd     | 0.21154    |
| AveragePolicyStd[0]  | 0.23874    |
| AveragePolicyStd[1]  | 0.22602    |
| AveragePolicyStd[2]  | 0.16667    |
| AveragePolicyStd[3]  | 0.2275     |
| AveragePolicyStd[4]  | 0.15773    |
| AveragePolicyStd[5]  | 0.25258    |
| AverageReturn        | 1770.8     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 146.14     |
| AverageEpisodeLength | 983.64     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 74.538     |
| TotalNEpisodes       | 19737      |
| TotalNSamples        | 5.0507e+06 |
| ExplainedVariance    | -0.0062351 |
-------------------------------------
[2018-12-22 11:50:51.382439 UTC] Saving snapshot
[2018-12-22 11:50:51.382732 UTC] Starting iteration 1010
[2018-12-22 11:50:51.382998 UTC] Start collecting samples
[2018-12-22 11:50:54.332832 UTC] Computing input variables for policy optimization
[2018-12-22 11:50:54.407997 UTC] Performing policy update
[2018-12-22 11:50:54.408781 UTC] Computing gradient in Euclidean space
[2018-12-22 11:50:54.497812 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:50:55.561196 UTC] Performing line search
[2018-12-22 11:50:55.688143 UTC] Updating baseline
[2018-12-22 11:50:57.712022 UTC] Computing logging information
-------------------------------------
| Iteration            | 1010       |
| ExpectedImprovement  | 0.016614   |
| ActualImprovement    | 0.01535    |
| ImprovementRatio     | 0.92393    |
| MeanKL               | 0.007984   |
| Entropy              | -0.90379   |
| Perplexity           | 0.40503    |
| AveragePolicyStd     | 0.21146    |
| AveragePolicyStd[0]  | 0.2386     |
| AveragePolicyStd[1]  | 0.22522    |
| AveragePolicyStd[2]  | 0.16614    |
| AveragePolicyStd[3]  | 0.22791    |
| AveragePolicyStd[4]  | 0.15801    |
| AveragePolicyStd[5]  | 0.25285    |
| AverageReturn        | 1772.5     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 146.03     |
| AverageEpisodeLength | 983.64     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 74.538     |
| TotalNEpisodes       | 19742      |
| TotalNSamples        | 5.0557e+06 |
| ExplainedVariance    | 0.003217   |
-------------------------------------
[2018-12-22 11:50:58.118646 UTC] Saving snapshot
[2018-12-22 11:50:58.126554 UTC] Starting iteration 1011
[2018-12-22 11:50:58.126745 UTC] Start collecting samples
[2018-12-22 11:51:01.103337 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:01.180985 UTC] Performing policy update
[2018-12-22 11:51:01.181846 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:01.272748 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:02.344647 UTC] Performing line search
[2018-12-22 11:51:02.472560 UTC] Updating baseline
[2018-12-22 11:51:03.813952 UTC] Computing logging information
-------------------------------------
| Iteration            | 1011       |
| ExpectedImprovement  | 0.017686   |
| ActualImprovement    | 0.017249   |
| ImprovementRatio     | 0.97529    |
| MeanKL               | 0.0074114  |
| Entropy              | -0.90651   |
| Perplexity           | 0.40393    |
| AveragePolicyStd     | 0.21137    |
| AveragePolicyStd[0]  | 0.23811    |
| AveragePolicyStd[1]  | 0.22502    |
| AveragePolicyStd[2]  | 0.1663     |
| AveragePolicyStd[3]  | 0.22777    |
| AveragePolicyStd[4]  | 0.15767    |
| AveragePolicyStd[5]  | 0.25337    |
| AverageReturn        | 1771.7     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 145.79     |
| AverageEpisodeLength | 983.64     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 74.538     |
| TotalNEpisodes       | 19747      |
| TotalNSamples        | 5.0607e+06 |
| ExplainedVariance    | -0.0081771 |
-------------------------------------
[2018-12-22 11:51:04.214997 UTC] Saving snapshot
[2018-12-22 11:51:04.215383 UTC] Starting iteration 1012
[2018-12-22 11:51:04.215521 UTC] Start collecting samples
[2018-12-22 11:51:07.143609 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:07.219317 UTC] Performing policy update
[2018-12-22 11:51:07.220001 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:07.310105 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:08.375233 UTC] Performing line search
[2018-12-22 11:51:08.502564 UTC] Updating baseline
[2018-12-22 11:51:10.038485 UTC] Computing logging information
-------------------------------------
| Iteration            | 1012       |
| ExpectedImprovement  | 0.017247   |
| ActualImprovement    | 0.015984   |
| ImprovementRatio     | 0.92679    |
| MeanKL               | 0.0076914  |
| Entropy              | -0.90875   |
| Perplexity           | 0.40303    |
| AveragePolicyStd     | 0.21126    |
| AveragePolicyStd[0]  | 0.23714    |
| AveragePolicyStd[1]  | 0.22465    |
| AveragePolicyStd[2]  | 0.16622    |
| AveragePolicyStd[3]  | 0.22788    |
| AveragePolicyStd[4]  | 0.15809    |
| AveragePolicyStd[5]  | 0.25357    |
| AverageReturn        | 1772.2     |
| MinReturn            | 914.14     |
| MaxReturn            | 1872.1     |
| StdReturn            | 145.85     |
| AverageEpisodeLength | 983.64     |
| MinEpisodeLength     | 547        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 74.538     |
| TotalNEpisodes       | 19750      |
| TotalNSamples        | 5.0637e+06 |
| ExplainedVariance    | 0.0040335  |
-------------------------------------
[2018-12-22 11:51:10.436919 UTC] Saving snapshot
[2018-12-22 11:51:10.437165 UTC] Starting iteration 1013
[2018-12-22 11:51:10.437296 UTC] Start collecting samples
[2018-12-22 11:51:13.397903 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:13.474173 UTC] Performing policy update
[2018-12-22 11:51:13.474873 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:13.564325 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:14.623966 UTC] Performing line search
[2018-12-22 11:51:14.752515 UTC] Updating baseline
[2018-12-22 11:51:16.259156 UTC] Computing logging information
-------------------------------------
| Iteration            | 1013       |
| ExpectedImprovement  | 0.017418   |
| ActualImprovement    | 0.016519   |
| ImprovementRatio     | 0.94841    |
| MeanKL               | 0.0077054  |
| Entropy              | -0.91411   |
| Perplexity           | 0.40087    |
| AveragePolicyStd     | 0.21112    |
| AveragePolicyStd[0]  | 0.23736    |
| AveragePolicyStd[1]  | 0.22335    |
| AveragePolicyStd[2]  | 0.16593    |
| AveragePolicyStd[3]  | 0.22836    |
| AveragePolicyStd[4]  | 0.15762    |
| AveragePolicyStd[5]  | 0.25413    |
| AverageReturn        | 1780       |
| MinReturn            | 954.74     |
| MaxReturn            | 1872.1     |
| StdReturn            | 117.53     |
| AverageEpisodeLength | 988.17     |
| MinEpisodeLength     | 548        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 60.262     |
| TotalNEpisodes       | 19757      |
| TotalNSamples        | 5.0707e+06 |
| ExplainedVariance    | 0.0013627  |
-------------------------------------
[2018-12-22 11:51:16.659447 UTC] Saving snapshot
[2018-12-22 11:51:16.659741 UTC] Starting iteration 1014
[2018-12-22 11:51:16.659864 UTC] Start collecting samples
[2018-12-22 11:51:19.597376 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:19.674907 UTC] Performing policy update
[2018-12-22 11:51:19.675791 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:19.768634 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:20.825929 UTC] Performing line search
[2018-12-22 11:51:20.954424 UTC] Updating baseline
[2018-12-22 11:51:23.161917 UTC] Computing logging information
-------------------------------------
| Iteration            | 1014       |
| ExpectedImprovement  | 0.016918   |
| ActualImprovement    | 0.015781   |
| ImprovementRatio     | 0.93282    |
| MeanKL               | 0.0075749  |
| Entropy              | -0.91517   |
| Perplexity           | 0.40045    |
| AveragePolicyStd     | 0.21106    |
| AveragePolicyStd[0]  | 0.23747    |
| AveragePolicyStd[1]  | 0.22294    |
| AveragePolicyStd[2]  | 0.16592    |
| AveragePolicyStd[3]  | 0.22832    |
| AveragePolicyStd[4]  | 0.1579     |
| AveragePolicyStd[5]  | 0.25382    |
| AverageReturn        | 1779       |
| MinReturn            | 954.74     |
| MaxReturn            | 1872.1     |
| StdReturn            | 117.52     |
| AverageEpisodeLength | 988.17     |
| MinEpisodeLength     | 548        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 60.262     |
| TotalNEpisodes       | 19761      |
| TotalNSamples        | 5.0747e+06 |
| ExplainedVariance    | 0.0036231  |
-------------------------------------
[2018-12-22 11:51:23.564043 UTC] Saving snapshot
[2018-12-22 11:51:23.564300 UTC] Starting iteration 1015
[2018-12-22 11:51:23.564430 UTC] Start collecting samples
[2018-12-22 11:51:26.501335 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:26.578414 UTC] Performing policy update
[2018-12-22 11:51:26.579236 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:26.670727 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:27.728585 UTC] Performing line search
[2018-12-22 11:51:27.857062 UTC] Updating baseline
[2018-12-22 11:51:29.712163 UTC] Computing logging information
-------------------------------------
| Iteration            | 1015       |
| ExpectedImprovement  | 0.015547   |
| ActualImprovement    | 0.014576   |
| ImprovementRatio     | 0.93751    |
| MeanKL               | 0.0069489  |
| Entropy              | -0.92218   |
| Perplexity           | 0.39765    |
| AveragePolicyStd     | 0.21081    |
| AveragePolicyStd[0]  | 0.23751    |
| AveragePolicyStd[1]  | 0.22245    |
| AveragePolicyStd[2]  | 0.16609    |
| AveragePolicyStd[3]  | 0.22739    |
| AveragePolicyStd[4]  | 0.15758    |
| AveragePolicyStd[5]  | 0.25383    |
| AverageReturn        | 1779.4     |
| MinReturn            | 954.74     |
| MaxReturn            | 1872.1     |
| StdReturn            | 117.52     |
| AverageEpisodeLength | 988.17     |
| MinEpisodeLength     | 548        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 60.262     |
| TotalNEpisodes       | 19765      |
| TotalNSamples        | 5.0787e+06 |
| ExplainedVariance    | 0.0060685  |
-------------------------------------
[2018-12-22 11:51:30.115201 UTC] Saving snapshot
[2018-12-22 11:51:30.115461 UTC] Starting iteration 1016
[2018-12-22 11:51:30.115602 UTC] Start collecting samples
[2018-12-22 11:51:33.101438 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:33.179486 UTC] Performing policy update
[2018-12-22 11:51:33.180361 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:33.271601 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:34.333299 UTC] Performing line search
[2018-12-22 11:51:34.459868 UTC] Updating baseline
[2018-12-22 11:51:36.223695 UTC] Computing logging information
--------------------------------------
| Iteration            | 1016        |
| ExpectedImprovement  | 0.017623    |
| ActualImprovement    | 0.016644    |
| ImprovementRatio     | 0.94445     |
| MeanKL               | 0.0072928   |
| Entropy              | -0.91622    |
| Perplexity           | 0.40003     |
| AveragePolicyStd     | 0.21104     |
| AveragePolicyStd[0]  | 0.23671     |
| AveragePolicyStd[1]  | 0.22303     |
| AveragePolicyStd[2]  | 0.16594     |
| AveragePolicyStd[3]  | 0.22843     |
| AveragePolicyStd[4]  | 0.15773     |
| AveragePolicyStd[5]  | 0.25437     |
| AverageReturn        | 1785.3      |
| MinReturn            | 1224.8      |
| MaxReturn            | 1868.4      |
| StdReturn            | 82.597      |
| AverageEpisodeLength | 992.69      |
| MinEpisodeLength     | 697         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 40.926      |
| TotalNEpisodes       | 19772       |
| TotalNSamples        | 5.0857e+06  |
| ExplainedVariance    | -0.00098726 |
--------------------------------------
[2018-12-22 11:51:36.616607 UTC] Saving snapshot
[2018-12-22 11:51:36.616867 UTC] Starting iteration 1017
[2018-12-22 11:51:36.616995 UTC] Start collecting samples
[2018-12-22 11:51:39.553524 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:39.630142 UTC] Performing policy update
[2018-12-22 11:51:39.630723 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:39.719169 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:40.772843 UTC] Performing line search
[2018-12-22 11:51:40.899285 UTC] Updating baseline
[2018-12-22 11:51:42.141420 UTC] Computing logging information
-------------------------------------
| Iteration            | 1017       |
| ExpectedImprovement  | 0.017191   |
| ActualImprovement    | 0.016419   |
| ImprovementRatio     | 0.95508    |
| MeanKL               | 0.0075168  |
| Entropy              | -0.90854   |
| Perplexity           | 0.40311    |
| AveragePolicyStd     | 0.21129    |
| AveragePolicyStd[0]  | 0.23712    |
| AveragePolicyStd[1]  | 0.22352    |
| AveragePolicyStd[2]  | 0.16602    |
| AveragePolicyStd[3]  | 0.22749    |
| AveragePolicyStd[4]  | 0.15841    |
| AveragePolicyStd[5]  | 0.25515    |
| AverageReturn        | 1785.6     |
| MinReturn            | 1224.8     |
| MaxReturn            | 1868.4     |
| StdReturn            | 82.72      |
| AverageEpisodeLength | 992.69     |
| MinEpisodeLength     | 697        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 40.926     |
| TotalNEpisodes       | 19777      |
| TotalNSamples        | 5.0907e+06 |
| ExplainedVariance    | 0.00069842 |
-------------------------------------
[2018-12-22 11:51:42.538720 UTC] Saving snapshot
[2018-12-22 11:51:42.538980 UTC] Starting iteration 1018
[2018-12-22 11:51:42.539098 UTC] Start collecting samples
[2018-12-22 11:51:45.471124 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:45.545471 UTC] Performing policy update
[2018-12-22 11:51:45.546095 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:45.635420 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:46.693107 UTC] Performing line search
[2018-12-22 11:51:46.819731 UTC] Updating baseline
[2018-12-22 11:51:48.117252 UTC] Computing logging information
-------------------------------------
| Iteration            | 1018       |
| ExpectedImprovement  | 0.016805   |
| ActualImprovement    | 0.015656   |
| ImprovementRatio     | 0.9316     |
| MeanKL               | 0.0073961  |
| Entropy              | -0.90893   |
| Perplexity           | 0.40296    |
| AveragePolicyStd     | 0.21133    |
| AveragePolicyStd[0]  | 0.23778    |
| AveragePolicyStd[1]  | 0.22264    |
| AveragePolicyStd[2]  | 0.16561    |
| AveragePolicyStd[3]  | 0.2274     |
| AveragePolicyStd[4]  | 0.15834    |
| AveragePolicyStd[5]  | 0.25619    |
| AverageReturn        | 1785.1     |
| MinReturn            | 1224.8     |
| MaxReturn            | 1864.6     |
| StdReturn            | 82.373     |
| AverageEpisodeLength | 992.69     |
| MinEpisodeLength     | 697        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 40.926     |
| TotalNEpisodes       | 19780      |
| TotalNSamples        | 5.0937e+06 |
| ExplainedVariance    | 0.0084168  |
-------------------------------------
[2018-12-22 11:51:48.509432 UTC] Saving snapshot
[2018-12-22 11:51:48.509707 UTC] Starting iteration 1019
[2018-12-22 11:51:48.509870 UTC] Start collecting samples
[2018-12-22 11:51:51.456347 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:51.535107 UTC] Performing policy update
[2018-12-22 11:51:51.535932 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:51.624934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:52.687706 UTC] Performing line search
[2018-12-22 11:51:52.814760 UTC] Updating baseline
[2018-12-22 11:51:54.240848 UTC] Computing logging information
--------------------------------------
| Iteration            | 1019        |
| ExpectedImprovement  | 0.018353    |
| ActualImprovement    | 0.016912    |
| ImprovementRatio     | 0.92148     |
| MeanKL               | 0.0073737   |
| Entropy              | -0.91067    |
| Perplexity           | 0.40225     |
| AveragePolicyStd     | 0.2113      |
| AveragePolicyStd[0]  | 0.23812     |
| AveragePolicyStd[1]  | 0.22211     |
| AveragePolicyStd[2]  | 0.16534     |
| AveragePolicyStd[3]  | 0.22702     |
| AveragePolicyStd[4]  | 0.1583      |
| AveragePolicyStd[5]  | 0.25692     |
| AverageReturn        | 1785.8      |
| MinReturn            | 1224.8      |
| MaxReturn            | 1864.6      |
| StdReturn            | 81.28       |
| AverageEpisodeLength | 993.57      |
| MinEpisodeLength     | 697         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 40.12       |
| TotalNEpisodes       | 19786       |
| TotalNSamples        | 5.0997e+06  |
| ExplainedVariance    | -0.00054467 |
--------------------------------------
[2018-12-22 11:51:54.639001 UTC] Saving snapshot
[2018-12-22 11:51:54.639239 UTC] Starting iteration 1020
[2018-12-22 11:51:54.639371 UTC] Start collecting samples
[2018-12-22 11:51:57.575807 UTC] Computing input variables for policy optimization
[2018-12-22 11:51:57.652241 UTC] Performing policy update
[2018-12-22 11:51:57.652937 UTC] Computing gradient in Euclidean space
[2018-12-22 11:51:57.742929 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:51:58.815356 UTC] Performing line search
[2018-12-22 11:51:58.943705 UTC] Updating baseline
[2018-12-22 11:52:00.803578 UTC] Computing logging information
-------------------------------------
| Iteration            | 1020       |
| ExpectedImprovement  | 0.019037   |
| ActualImprovement    | 0.017846   |
| ImprovementRatio     | 0.93741    |
| MeanKL               | 0.0078536  |
| Entropy              | -0.91236   |
| Perplexity           | 0.40158    |
| AveragePolicyStd     | 0.21124    |
| AveragePolicyStd[0]  | 0.2384     |
| AveragePolicyStd[1]  | 0.22218    |
| AveragePolicyStd[2]  | 0.16555    |
| AveragePolicyStd[3]  | 0.22664    |
| AveragePolicyStd[4]  | 0.15799    |
| AveragePolicyStd[5]  | 0.25671    |
| AverageReturn        | 1785.4     |
| MinReturn            | 1224.8     |
| MaxReturn            | 1864.6     |
| StdReturn            | 81.162     |
| AverageEpisodeLength | 993.57     |
| MinEpisodeLength     | 697        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 40.12      |
| TotalNEpisodes       | 19792      |
| TotalNSamples        | 5.1057e+06 |
| ExplainedVariance    | 0.0032151  |
-------------------------------------
[2018-12-22 11:52:01.207913 UTC] Saving snapshot
[2018-12-22 11:52:01.216027 UTC] Starting iteration 1021
[2018-12-22 11:52:01.216217 UTC] Start collecting samples
[2018-12-22 11:52:04.159275 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:04.235935 UTC] Performing policy update
[2018-12-22 11:52:04.236655 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:04.325874 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:05.394077 UTC] Performing line search
[2018-12-22 11:52:05.523205 UTC] Updating baseline
[2018-12-22 11:52:06.857770 UTC] Computing logging information
-------------------------------------
| Iteration            | 1021       |
| ExpectedImprovement  | 0.017649   |
| ActualImprovement    | 0.016778   |
| ImprovementRatio     | 0.95061    |
| MeanKL               | 0.0084447  |
| Entropy              | -0.91782   |
| Perplexity           | 0.39939    |
| AveragePolicyStd     | 0.21107    |
| AveragePolicyStd[0]  | 0.23795    |
| AveragePolicyStd[1]  | 0.22188    |
| AveragePolicyStd[2]  | 0.16549    |
| AveragePolicyStd[3]  | 0.22662    |
| AveragePolicyStd[4]  | 0.15762    |
| AveragePolicyStd[5]  | 0.25684    |
| AverageReturn        | 1785.6     |
| MinReturn            | 1224.8     |
| MaxReturn            | 1864.6     |
| StdReturn            | 81.222     |
| AverageEpisodeLength | 993.57     |
| MinEpisodeLength     | 697        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 40.12      |
| TotalNEpisodes       | 19796      |
| TotalNSamples        | 5.1097e+06 |
| ExplainedVariance    | 0.0018254  |
-------------------------------------
[2018-12-22 11:52:07.255853 UTC] Saving snapshot
[2018-12-22 11:52:07.256156 UTC] Starting iteration 1022
[2018-12-22 11:52:07.256359 UTC] Start collecting samples
[2018-12-22 11:52:10.203587 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:10.279785 UTC] Performing policy update
[2018-12-22 11:52:10.280415 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:10.369349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:11.422617 UTC] Performing line search
[2018-12-22 11:52:11.550405 UTC] Updating baseline
[2018-12-22 11:52:13.294277 UTC] Computing logging information
-------------------------------------
| Iteration            | 1022       |
| ExpectedImprovement  | 0.016067   |
| ActualImprovement    | 0.015184   |
| ImprovementRatio     | 0.94501    |
| MeanKL               | 0.007632   |
| Entropy              | -0.91067   |
| Perplexity           | 0.40225    |
| AveragePolicyStd     | 0.21133    |
| AveragePolicyStd[0]  | 0.23755    |
| AveragePolicyStd[1]  | 0.22289    |
| AveragePolicyStd[2]  | 0.16565    |
| AveragePolicyStd[3]  | 0.22706    |
| AveragePolicyStd[4]  | 0.15766    |
| AveragePolicyStd[5]  | 0.25713    |
| AverageReturn        | 1784.6     |
| MinReturn            | 1224.8     |
| MaxReturn            | 1864.6     |
| StdReturn            | 80.845     |
| AverageEpisodeLength | 993.57     |
| MinEpisodeLength     | 697        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 40.12      |
| TotalNEpisodes       | 19801      |
| TotalNSamples        | 5.1147e+06 |
| ExplainedVariance    | 0.0030385  |
-------------------------------------
[2018-12-22 11:52:13.695670 UTC] Saving snapshot
[2018-12-22 11:52:13.695923 UTC] Starting iteration 1023
[2018-12-22 11:52:13.696040 UTC] Start collecting samples
[2018-12-22 11:52:16.677350 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:16.754889 UTC] Performing policy update
[2018-12-22 11:52:16.755889 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:16.845787 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:17.921775 UTC] Performing line search
[2018-12-22 11:52:18.049766 UTC] Updating baseline
[2018-12-22 11:52:19.384962 UTC] Computing logging information
-------------------------------------
| Iteration            | 1023       |
| ExpectedImprovement  | 0.01734    |
| ActualImprovement    | 0.016461   |
| ImprovementRatio     | 0.94932    |
| MeanKL               | 0.0076375  |
| Entropy              | -0.91408   |
| Perplexity           | 0.40089    |
| AveragePolicyStd     | 0.21123    |
| AveragePolicyStd[0]  | 0.23723    |
| AveragePolicyStd[1]  | 0.2225     |
| AveragePolicyStd[2]  | 0.16537    |
| AveragePolicyStd[3]  | 0.22677    |
| AveragePolicyStd[4]  | 0.15764    |
| AveragePolicyStd[5]  | 0.25785    |
| AverageReturn        | 1788.2     |
| MinReturn            | 1328.8     |
| MaxReturn            | 1864.6     |
| StdReturn            | 57.173     |
| AverageEpisodeLength | 996.6      |
| MinEpisodeLength     | 742        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 26.857     |
| TotalNEpisodes       | 19807      |
| TotalNSamples        | 5.1207e+06 |
| ExplainedVariance    | 0.0031403  |
-------------------------------------
[2018-12-22 11:52:19.790318 UTC] Saving snapshot
[2018-12-22 11:52:19.790593 UTC] Starting iteration 1024
[2018-12-22 11:52:19.790716 UTC] Start collecting samples
[2018-12-22 11:52:22.745165 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:22.820128 UTC] Performing policy update
[2018-12-22 11:52:22.820902 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:22.911140 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:23.975849 UTC] Performing line search
[2018-12-22 11:52:24.102890 UTC] Updating baseline
[2018-12-22 11:52:25.504277 UTC] Computing logging information
-------------------------------------
| Iteration            | 1024       |
| ExpectedImprovement  | 0.016905   |
| ActualImprovement    | 0.015804   |
| ImprovementRatio     | 0.93486    |
| MeanKL               | 0.0079555  |
| Entropy              | -0.91203   |
| Perplexity           | 0.40171    |
| AveragePolicyStd     | 0.2113     |
| AveragePolicyStd[0]  | 0.23843    |
| AveragePolicyStd[1]  | 0.22297    |
| AveragePolicyStd[2]  | 0.16511    |
| AveragePolicyStd[3]  | 0.22659    |
| AveragePolicyStd[4]  | 0.15776    |
| AveragePolicyStd[5]  | 0.25697    |
| AverageReturn        | 1786.8     |
| MinReturn            | 1328.8     |
| MaxReturn            | 1864       |
| StdReturn            | 56.846     |
| AverageEpisodeLength | 996.6      |
| MinEpisodeLength     | 742        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 26.857     |
| TotalNEpisodes       | 19811      |
| TotalNSamples        | 5.1247e+06 |
| ExplainedVariance    | 0.0020091  |
-------------------------------------
[2018-12-22 11:52:25.904590 UTC] Saving snapshot
[2018-12-22 11:52:25.904848 UTC] Starting iteration 1025
[2018-12-22 11:52:25.904967 UTC] Start collecting samples
[2018-12-22 11:52:28.871913 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:28.948864 UTC] Performing policy update
[2018-12-22 11:52:28.949514 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:29.038914 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:30.100596 UTC] Performing line search
[2018-12-22 11:52:30.229236 UTC] Updating baseline
[2018-12-22 11:52:31.907663 UTC] Computing logging information
-------------------------------------
| Iteration            | 1025       |
| ExpectedImprovement  | 0.01727    |
| ActualImprovement    | 0.016493   |
| ImprovementRatio     | 0.955      |
| MeanKL               | 0.0076063  |
| Entropy              | -0.91512   |
| Perplexity           | 0.40047    |
| AveragePolicyStd     | 0.21119    |
| AveragePolicyStd[0]  | 0.23694    |
| AveragePolicyStd[1]  | 0.2231     |
| AveragePolicyStd[2]  | 0.16508    |
| AveragePolicyStd[3]  | 0.2272     |
| AveragePolicyStd[4]  | 0.15763    |
| AveragePolicyStd[5]  | 0.25721    |
| AverageReturn        | 1785       |
| MinReturn            | 1328.8     |
| MaxReturn            | 1857.7     |
| StdReturn            | 56.459     |
| AverageEpisodeLength | 996.6      |
| MinEpisodeLength     | 742        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 26.857     |
| TotalNEpisodes       | 19817      |
| TotalNSamples        | 5.1307e+06 |
| ExplainedVariance    | 0.0023625  |
-------------------------------------
[2018-12-22 11:52:32.309642 UTC] Saving snapshot
[2018-12-22 11:52:32.309923 UTC] Starting iteration 1026
[2018-12-22 11:52:32.310043 UTC] Start collecting samples
[2018-12-22 11:52:35.256092 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:35.332458 UTC] Performing policy update
[2018-12-22 11:52:35.333337 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:35.424408 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:36.504345 UTC] Performing line search
[2018-12-22 11:52:36.631744 UTC] Updating baseline
[2018-12-22 11:52:37.976969 UTC] Computing logging information
-------------------------------------
| Iteration            | 1026       |
| ExpectedImprovement  | 0.016197   |
| ActualImprovement    | 0.015211   |
| ImprovementRatio     | 0.93909    |
| MeanKL               | 0.0075085  |
| Entropy              | -0.90609   |
| Perplexity           | 0.4041     |
| AveragePolicyStd     | 0.21143    |
| AveragePolicyStd[0]  | 0.23647    |
| AveragePolicyStd[1]  | 0.22336    |
| AveragePolicyStd[2]  | 0.16564    |
| AveragePolicyStd[3]  | 0.22747    |
| AveragePolicyStd[4]  | 0.15847    |
| AveragePolicyStd[5]  | 0.25718    |
| AverageReturn        | 1784.7     |
| MinReturn            | 1328.8     |
| MaxReturn            | 1850.3     |
| StdReturn            | 53.469     |
| AverageEpisodeLength | 997.42     |
| MinEpisodeLength     | 742        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 25.671     |
| TotalNEpisodes       | 19822      |
| TotalNSamples        | 5.1357e+06 |
| ExplainedVariance    | 0.0025849  |
-------------------------------------
[2018-12-22 11:52:38.374169 UTC] Saving snapshot
[2018-12-22 11:52:38.374431 UTC] Starting iteration 1027
[2018-12-22 11:52:38.374569 UTC] Start collecting samples
[2018-12-22 11:52:41.321698 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:41.400127 UTC] Performing policy update
[2018-12-22 11:52:41.401115 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:41.491220 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:42.554726 UTC] Performing line search
[2018-12-22 11:52:42.682033 UTC] Updating baseline
[2018-12-22 11:52:44.380597 UTC] Computing logging information
-------------------------------------
| Iteration            | 1027       |
| ExpectedImprovement  | 0.019028   |
| ActualImprovement    | 0.018087   |
| ImprovementRatio     | 0.95051    |
| MeanKL               | 0.0076805  |
| Entropy              | -0.90415   |
| Perplexity           | 0.40488    |
| AveragePolicyStd     | 0.21152    |
| AveragePolicyStd[0]  | 0.23674    |
| AveragePolicyStd[1]  | 0.22339    |
| AveragePolicyStd[2]  | 0.16592    |
| AveragePolicyStd[3]  | 0.22717    |
| AveragePolicyStd[4]  | 0.15823    |
| AveragePolicyStd[5]  | 0.25765    |
| AverageReturn        | 1784.8     |
| MinReturn            | 1328.8     |
| MaxReturn            | 1850.3     |
| StdReturn            | 53.743     |
| AverageEpisodeLength | 996.93     |
| MinEpisodeLength     | 742        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 26.081     |
| TotalNEpisodes       | 19827      |
| TotalNSamples        | 5.1406e+06 |
| ExplainedVariance    | 0.088847   |
-------------------------------------
[2018-12-22 11:52:44.782345 UTC] Saving snapshot
[2018-12-22 11:52:44.782622 UTC] Starting iteration 1028
[2018-12-22 11:52:44.782745 UTC] Start collecting samples
[2018-12-22 11:52:47.774170 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:47.854135 UTC] Performing policy update
[2018-12-22 11:52:47.854727 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:47.947358 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:49.008657 UTC] Performing line search
[2018-12-22 11:52:49.137163 UTC] Updating baseline
[2018-12-22 11:52:50.405208 UTC] Computing logging information
-------------------------------------
| Iteration            | 1028       |
| ExpectedImprovement  | 0.01796    |
| ActualImprovement    | 0.016632   |
| ImprovementRatio     | 0.92606    |
| MeanKL               | 0.0076546  |
| Entropy              | -0.90619   |
| Perplexity           | 0.40406    |
| AveragePolicyStd     | 0.21143    |
| AveragePolicyStd[0]  | 0.23661    |
| AveragePolicyStd[1]  | 0.22359    |
| AveragePolicyStd[2]  | 0.16606    |
| AveragePolicyStd[3]  | 0.22624    |
| AveragePolicyStd[4]  | 0.15832    |
| AveragePolicyStd[5]  | 0.25774    |
| AverageReturn        | 1774.5     |
| MinReturn            | 828.21     |
| MaxReturn            | 1853.2     |
| StdReturn            | 120.2      |
| AverageEpisodeLength | 990.55     |
| MinEpisodeLength     | 480        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 63.659     |
| TotalNEpisodes       | 19833      |
| TotalNSamples        | 5.1457e+06 |
| ExplainedVariance    | 0.18939    |
-------------------------------------
[2018-12-22 11:52:50.808152 UTC] Saving snapshot
[2018-12-22 11:52:50.808397 UTC] Starting iteration 1029
[2018-12-22 11:52:50.808548 UTC] Start collecting samples
[2018-12-22 11:52:53.761960 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:53.841415 UTC] Performing policy update
[2018-12-22 11:52:53.842088 UTC] Computing gradient in Euclidean space
[2018-12-22 11:52:53.931728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:52:54.991833 UTC] Performing line search
[2018-12-22 11:52:55.119666 UTC] Updating baseline
[2018-12-22 11:52:56.570368 UTC] Computing logging information
-------------------------------------
| Iteration            | 1029       |
| ExpectedImprovement  | 0.018965   |
| ActualImprovement    | 0.018296   |
| ImprovementRatio     | 0.96472    |
| MeanKL               | 0.0075167  |
| Entropy              | -0.91137   |
| Perplexity           | 0.40197    |
| AveragePolicyStd     | 0.21123    |
| AveragePolicyStd[0]  | 0.23658    |
| AveragePolicyStd[1]  | 0.22375    |
| AveragePolicyStd[2]  | 0.16636    |
| AveragePolicyStd[3]  | 0.22605    |
| AveragePolicyStd[4]  | 0.15781    |
| AveragePolicyStd[5]  | 0.25684    |
| AverageReturn        | 1775.4     |
| MinReturn            | 828.21     |
| MaxReturn            | 1853.2     |
| StdReturn            | 120.4      |
| AverageEpisodeLength | 990.55     |
| MinEpisodeLength     | 480        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 63.659     |
| TotalNEpisodes       | 19838      |
| TotalNSamples        | 5.1507e+06 |
| ExplainedVariance    | 0.086849   |
-------------------------------------
[2018-12-22 11:52:56.972929 UTC] Saving snapshot
[2018-12-22 11:52:56.973174 UTC] Starting iteration 1030
[2018-12-22 11:52:56.973305 UTC] Start collecting samples
[2018-12-22 11:52:59.905830 UTC] Computing input variables for policy optimization
[2018-12-22 11:52:59.986536 UTC] Performing policy update
[2018-12-22 11:52:59.987301 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:00.077560 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:01.142512 UTC] Performing line search
[2018-12-22 11:53:01.270816 UTC] Updating baseline
[2018-12-22 11:53:02.977771 UTC] Computing logging information
-------------------------------------
| Iteration            | 1030       |
| ExpectedImprovement  | 0.016841   |
| ActualImprovement    | 0.015357   |
| ImprovementRatio     | 0.91188    |
| MeanKL               | 0.0078103  |
| Entropy              | -0.9186    |
| Perplexity           | 0.39908    |
| AveragePolicyStd     | 0.21097    |
| AveragePolicyStd[0]  | 0.23537    |
| AveragePolicyStd[1]  | 0.22397    |
| AveragePolicyStd[2]  | 0.16636    |
| AveragePolicyStd[3]  | 0.22571    |
| AveragePolicyStd[4]  | 0.15752    |
| AveragePolicyStd[5]  | 0.2569     |
| AverageReturn        | 1761.5     |
| MinReturn            | 313.61     |
| MaxReturn            | 1853.2     |
| StdReturn            | 189        |
| AverageEpisodeLength | 982.68     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.18     |
| TotalNEpisodes       | 19842      |
| TotalNSamples        | 5.1539e+06 |
| ExplainedVariance    | 0.18518    |
-------------------------------------
[2018-12-22 11:53:03.379808 UTC] Saving snapshot
[2018-12-22 11:53:03.388049 UTC] Starting iteration 1031
[2018-12-22 11:53:03.388252 UTC] Start collecting samples
[2018-12-22 11:53:06.394834 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:06.475911 UTC] Performing policy update
[2018-12-22 11:53:06.476653 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:06.566653 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:07.635868 UTC] Performing line search
[2018-12-22 11:53:07.767110 UTC] Updating baseline
[2018-12-22 11:53:08.949112 UTC] Computing logging information
-------------------------------------
| Iteration            | 1031       |
| ExpectedImprovement  | 0.016851   |
| ActualImprovement    | 0.016382   |
| ImprovementRatio     | 0.97216    |
| MeanKL               | 0.0075653  |
| Entropy              | -0.92834   |
| Perplexity           | 0.39521    |
| AveragePolicyStd     | 0.21069    |
| AveragePolicyStd[0]  | 0.23547    |
| AveragePolicyStd[1]  | 0.22316    |
| AveragePolicyStd[2]  | 0.16564    |
| AveragePolicyStd[3]  | 0.22633    |
| AveragePolicyStd[4]  | 0.15692    |
| AveragePolicyStd[5]  | 0.2566     |
| AverageReturn        | 1747.2     |
| MinReturn            | 313.61     |
| MaxReturn            | 1853.2     |
| StdReturn            | 217.26     |
| AverageEpisodeLength | 974.81     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 115.55     |
| TotalNEpisodes       | 19850      |
| TotalNSamples        | 5.1612e+06 |
| ExplainedVariance    | 0.18608    |
-------------------------------------
[2018-12-22 11:53:09.353379 UTC] Saving snapshot
[2018-12-22 11:53:09.353637 UTC] Starting iteration 1032
[2018-12-22 11:53:09.353780 UTC] Start collecting samples
[2018-12-22 11:53:12.344806 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:12.424313 UTC] Performing policy update
[2018-12-22 11:53:12.425014 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:12.514722 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:13.580685 UTC] Performing line search
[2018-12-22 11:53:13.709519 UTC] Updating baseline
[2018-12-22 11:53:15.232680 UTC] Computing logging information
-------------------------------------
| Iteration            | 1032       |
| ExpectedImprovement  | 0.017613   |
| ActualImprovement    | 0.017025   |
| ImprovementRatio     | 0.96665    |
| MeanKL               | 0.0085443  |
| Entropy              | -0.92399   |
| Perplexity           | 0.39693    |
| AveragePolicyStd     | 0.21081    |
| AveragePolicyStd[0]  | 0.23579    |
| AveragePolicyStd[1]  | 0.22325    |
| AveragePolicyStd[2]  | 0.16599    |
| AveragePolicyStd[3]  | 0.22596    |
| AveragePolicyStd[4]  | 0.15725    |
| AveragePolicyStd[5]  | 0.25662    |
| AverageReturn        | 1723.6     |
| MinReturn            | 313.61     |
| MaxReturn            | 1853.2     |
| StdReturn            | 283        |
| AverageEpisodeLength | 960.62     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.28     |
| TotalNEpisodes       | 19858      |
| TotalNSamples        | 5.1677e+06 |
| ExplainedVariance    | 0.19412    |
-------------------------------------
[2018-12-22 11:53:15.634223 UTC] Saving snapshot
[2018-12-22 11:53:15.634480 UTC] Starting iteration 1033
[2018-12-22 11:53:15.634619 UTC] Start collecting samples
[2018-12-22 11:53:18.532294 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:18.609374 UTC] Performing policy update
[2018-12-22 11:53:18.610224 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:18.699580 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:19.753870 UTC] Performing line search
[2018-12-22 11:53:19.881509 UTC] Updating baseline
[2018-12-22 11:53:21.730974 UTC] Computing logging information
--------------------------------------
| Iteration            | 1033        |
| ExpectedImprovement  | 0.018194    |
| ActualImprovement    | 0.016759    |
| ImprovementRatio     | 0.92112     |
| MeanKL               | 0.0082598   |
| Entropy              | -0.92153    |
| Perplexity           | 0.39791     |
| AveragePolicyStd     | 0.21096     |
| AveragePolicyStd[0]  | 0.23653     |
| AveragePolicyStd[1]  | 0.22296     |
| AveragePolicyStd[2]  | 0.16598     |
| AveragePolicyStd[3]  | 0.22588     |
| AveragePolicyStd[4]  | 0.15684     |
| AveragePolicyStd[5]  | 0.25755     |
| AverageReturn        | 1723.8      |
| MinReturn            | 313.61      |
| MaxReturn            | 1853.2      |
| StdReturn            | 283.05      |
| AverageEpisodeLength | 960.62      |
| MinEpisodeLength     | 213         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 150.28      |
| TotalNEpisodes       | 19859       |
| TotalNSamples        | 5.1687e+06  |
| ExplainedVariance    | -0.00083612 |
--------------------------------------
[2018-12-22 11:53:22.129054 UTC] Saving snapshot
[2018-12-22 11:53:22.129312 UTC] Starting iteration 1034
[2018-12-22 11:53:22.129435 UTC] Start collecting samples
[2018-12-22 11:53:25.090037 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:25.168283 UTC] Performing policy update
[2018-12-22 11:53:25.169047 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:25.258572 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:26.305909 UTC] Performing line search
[2018-12-22 11:53:26.433124 UTC] Updating baseline
[2018-12-22 11:53:27.951228 UTC] Computing logging information
-------------------------------------
| Iteration            | 1034       |
| ExpectedImprovement  | 0.017319   |
| ActualImprovement    | 0.016261   |
| ImprovementRatio     | 0.93891    |
| MeanKL               | 0.0078741  |
| Entropy              | -0.92388   |
| Perplexity           | 0.39697    |
| AveragePolicyStd     | 0.21091    |
| AveragePolicyStd[0]  | 0.23681    |
| AveragePolicyStd[1]  | 0.22226    |
| AveragePolicyStd[2]  | 0.16562    |
| AveragePolicyStd[3]  | 0.22575    |
| AveragePolicyStd[4]  | 0.15675    |
| AveragePolicyStd[5]  | 0.25829    |
| AverageReturn        | 1725.5     |
| MinReturn            | 313.61     |
| MaxReturn            | 1853.2     |
| StdReturn            | 283.48     |
| AverageEpisodeLength | 960.62     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.28     |
| TotalNEpisodes       | 19866      |
| TotalNSamples        | 5.1757e+06 |
| ExplainedVariance    | 0.006231   |
-------------------------------------
[2018-12-22 11:53:28.350434 UTC] Saving snapshot
[2018-12-22 11:53:28.350704 UTC] Starting iteration 1035
[2018-12-22 11:53:28.350836 UTC] Start collecting samples
[2018-12-22 11:53:31.302524 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:31.382991 UTC] Performing policy update
[2018-12-22 11:53:31.383706 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:31.475311 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:32.525425 UTC] Performing line search
[2018-12-22 11:53:32.652172 UTC] Updating baseline
[2018-12-22 11:53:34.346800 UTC] Computing logging information
-------------------------------------
| Iteration            | 1035       |
| ExpectedImprovement  | 0.017156   |
| ActualImprovement    | 0.016354   |
| ImprovementRatio     | 0.95325    |
| MeanKL               | 0.0080271  |
| Entropy              | -0.92993   |
| Perplexity           | 0.39458    |
| AveragePolicyStd     | 0.21071    |
| AveragePolicyStd[0]  | 0.23613    |
| AveragePolicyStd[1]  | 0.22232    |
| AveragePolicyStd[2]  | 0.16527    |
| AveragePolicyStd[3]  | 0.22593    |
| AveragePolicyStd[4]  | 0.15655    |
| AveragePolicyStd[5]  | 0.25809    |
| AverageReturn        | 1727.9     |
| MinReturn            | 313.61     |
| MaxReturn            | 1882.6     |
| StdReturn            | 284.24     |
| AverageEpisodeLength | 960.62     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.28     |
| TotalNEpisodes       | 19872      |
| TotalNSamples        | 5.1817e+06 |
| ExplainedVariance    | -0.045097  |
-------------------------------------
[2018-12-22 11:53:34.795272 UTC] Saving snapshot
[2018-12-22 11:53:34.795697 UTC] Starting iteration 1036
[2018-12-22 11:53:34.795848 UTC] Start collecting samples
[2018-12-22 11:53:38.676216 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:38.777439 UTC] Performing policy update
[2018-12-22 11:53:38.778250 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:38.890881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:40.198249 UTC] Performing line search
[2018-12-22 11:53:40.352485 UTC] Updating baseline
[2018-12-22 11:53:41.763623 UTC] Computing logging information
-------------------------------------
| Iteration            | 1036       |
| ExpectedImprovement  | 0.017482   |
| ActualImprovement    | 0.016268   |
| ImprovementRatio     | 0.93053    |
| MeanKL               | 0.0074952  |
| Entropy              | -0.93228   |
| Perplexity           | 0.39366    |
| AveragePolicyStd     | 0.21063    |
| AveragePolicyStd[0]  | 0.23614    |
| AveragePolicyStd[1]  | 0.22188    |
| AveragePolicyStd[2]  | 0.16546    |
| AveragePolicyStd[3]  | 0.22603    |
| AveragePolicyStd[4]  | 0.1563     |
| AveragePolicyStd[5]  | 0.25799    |
| AverageReturn        | 1717.7     |
| MinReturn            | 313.61     |
| MaxReturn            | 1882.6     |
| StdReturn            | 299.81     |
| AverageEpisodeLength | 955.17     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.42     |
| TotalNEpisodes       | 19875      |
| TotalNSamples        | 5.1842e+06 |
| ExplainedVariance    | 0.22057    |
-------------------------------------
[2018-12-22 11:53:42.191992 UTC] Saving snapshot
[2018-12-22 11:53:42.192234 UTC] Starting iteration 1037
[2018-12-22 11:53:42.192371 UTC] Start collecting samples
[2018-12-22 11:53:45.194740 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:45.275972 UTC] Performing policy update
[2018-12-22 11:53:45.276564 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:45.365362 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:46.426390 UTC] Performing line search
[2018-12-22 11:53:46.552163 UTC] Updating baseline
[2018-12-22 11:53:47.727821 UTC] Computing logging information
-------------------------------------
| Iteration            | 1037       |
| ExpectedImprovement  | 0.01743    |
| ActualImprovement    | 0.01692    |
| ImprovementRatio     | 0.97077    |
| MeanKL               | 0.0081921  |
| Entropy              | -0.92932   |
| Perplexity           | 0.39482    |
| AveragePolicyStd     | 0.21073    |
| AveragePolicyStd[0]  | 0.23588    |
| AveragePolicyStd[1]  | 0.22176    |
| AveragePolicyStd[2]  | 0.16527    |
| AveragePolicyStd[3]  | 0.22627    |
| AveragePolicyStd[4]  | 0.15675    |
| AveragePolicyStd[5]  | 0.25844    |
| AverageReturn        | 1706.9     |
| MinReturn            | 313.61     |
| MaxReturn            | 1882.6     |
| StdReturn            | 313.73     |
| AverageEpisodeLength | 949.83     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.65     |
| TotalNEpisodes       | 19880      |
| TotalNSamples        | 5.1887e+06 |
| ExplainedVariance    | 0.13677    |
-------------------------------------
[2018-12-22 11:53:48.132128 UTC] Saving snapshot
[2018-12-22 11:53:48.132370 UTC] Starting iteration 1038
[2018-12-22 11:53:48.132486 UTC] Start collecting samples
[2018-12-22 11:53:51.105154 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:51.186515 UTC] Performing policy update
[2018-12-22 11:53:51.187406 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:51.277019 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:52.331421 UTC] Performing line search
[2018-12-22 11:53:52.457580 UTC] Updating baseline
[2018-12-22 11:53:53.620185 UTC] Computing logging information
-------------------------------------
| Iteration            | 1038       |
| ExpectedImprovement  | 0.017951   |
| ActualImprovement    | 0.01702    |
| ImprovementRatio     | 0.94815    |
| MeanKL               | 0.0076229  |
| Entropy              | -0.92786   |
| Perplexity           | 0.3954     |
| AveragePolicyStd     | 0.21076    |
| AveragePolicyStd[0]  | 0.23647    |
| AveragePolicyStd[1]  | 0.22177    |
| AveragePolicyStd[2]  | 0.16585    |
| AveragePolicyStd[3]  | 0.22588    |
| AveragePolicyStd[4]  | 0.15651    |
| AveragePolicyStd[5]  | 0.25811    |
| AverageReturn        | 1697.3     |
| MinReturn            | 313.61     |
| MaxReturn            | 1882.6     |
| StdReturn            | 327.81     |
| AverageEpisodeLength | 944.38     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.72     |
| TotalNEpisodes       | 19889      |
| TotalNSamples        | 5.1971e+06 |
| ExplainedVariance    | 0.10519    |
-------------------------------------
[2018-12-22 11:53:54.024130 UTC] Saving snapshot
[2018-12-22 11:53:54.024378 UTC] Starting iteration 1039
[2018-12-22 11:53:54.024497 UTC] Start collecting samples
[2018-12-22 11:53:56.931424 UTC] Computing input variables for policy optimization
[2018-12-22 11:53:57.008938 UTC] Performing policy update
[2018-12-22 11:53:57.009795 UTC] Computing gradient in Euclidean space
[2018-12-22 11:53:57.098476 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:53:58.171127 UTC] Performing line search
[2018-12-22 11:53:58.300396 UTC] Updating baseline
[2018-12-22 11:53:59.917417 UTC] Computing logging information
-------------------------------------
| Iteration            | 1039       |
| ExpectedImprovement  | 0.015313   |
| ActualImprovement    | 0.014587   |
| ImprovementRatio     | 0.95262    |
| MeanKL               | 0.0077039  |
| Entropy              | -0.92484   |
| Perplexity           | 0.3966     |
| AveragePolicyStd     | 0.21085    |
| AveragePolicyStd[0]  | 0.23619    |
| AveragePolicyStd[1]  | 0.22158    |
| AveragePolicyStd[2]  | 0.16598    |
| AveragePolicyStd[3]  | 0.2265     |
| AveragePolicyStd[4]  | 0.15672    |
| AveragePolicyStd[5]  | 0.25815    |
| AverageReturn        | 1696.7     |
| MinReturn            | 313.61     |
| MaxReturn            | 1882.6     |
| StdReturn            | 327.61     |
| AverageEpisodeLength | 944.38     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.72     |
| TotalNEpisodes       | 19892      |
| TotalNSamples        | 5.2001e+06 |
| ExplainedVariance    | -0.0072491 |
-------------------------------------
[2018-12-22 11:54:00.324197 UTC] Saving snapshot
[2018-12-22 11:54:00.324482 UTC] Starting iteration 1040
[2018-12-22 11:54:00.324664 UTC] Start collecting samples
[2018-12-22 11:54:03.233290 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:03.310850 UTC] Performing policy update
[2018-12-22 11:54:03.311546 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:03.401803 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:04.463566 UTC] Performing line search
[2018-12-22 11:54:04.590946 UTC] Updating baseline
[2018-12-22 11:54:05.849103 UTC] Computing logging information
-------------------------------------
| Iteration            | 1040       |
| ExpectedImprovement  | 0.017031   |
| ActualImprovement    | 0.015523   |
| ImprovementRatio     | 0.91146    |
| MeanKL               | 0.0076006  |
| Entropy              | -0.93038   |
| Perplexity           | 0.3944     |
| AveragePolicyStd     | 0.21068    |
| AveragePolicyStd[0]  | 0.23472    |
| AveragePolicyStd[1]  | 0.2214     |
| AveragePolicyStd[2]  | 0.16562    |
| AveragePolicyStd[3]  | 0.22688    |
| AveragePolicyStd[4]  | 0.15664    |
| AveragePolicyStd[5]  | 0.2588     |
| AverageReturn        | 1697.6     |
| MinReturn            | 313.61     |
| MaxReturn            | 1882.6     |
| StdReturn            | 327.96     |
| AverageEpisodeLength | 944.38     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.72     |
| TotalNEpisodes       | 19895      |
| TotalNSamples        | 5.2031e+06 |
| ExplainedVariance    | 0.0033775  |
-------------------------------------
[2018-12-22 11:54:06.250447 UTC] Saving snapshot
[2018-12-22 11:54:06.258534 UTC] Starting iteration 1041
[2018-12-22 11:54:06.258759 UTC] Start collecting samples
[2018-12-22 11:54:09.232280 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:09.312705 UTC] Performing policy update
[2018-12-22 11:54:09.313455 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:09.403511 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:10.468532 UTC] Performing line search
[2018-12-22 11:54:10.595662 UTC] Updating baseline
[2018-12-22 11:54:12.550022 UTC] Computing logging information
-------------------------------------
| Iteration            | 1041       |
| ExpectedImprovement  | 0.017711   |
| ActualImprovement    | 0.016901   |
| ImprovementRatio     | 0.95429    |
| MeanKL               | 0.0079645  |
| Entropy              | -0.92855   |
| Perplexity           | 0.39513    |
| AveragePolicyStd     | 0.21075    |
| AveragePolicyStd[0]  | 0.23473    |
| AveragePolicyStd[1]  | 0.22142    |
| AveragePolicyStd[2]  | 0.16575    |
| AveragePolicyStd[3]  | 0.22694    |
| AveragePolicyStd[4]  | 0.15655    |
| AveragePolicyStd[5]  | 0.25913    |
| AverageReturn        | 1698.7     |
| MinReturn            | 313.61     |
| MaxReturn            | 1882.6     |
| StdReturn            | 328.33     |
| AverageEpisodeLength | 944.38     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.72     |
| TotalNEpisodes       | 19905      |
| TotalNSamples        | 5.2131e+06 |
| ExplainedVariance    | -0.0030455 |
-------------------------------------
[2018-12-22 11:54:12.952329 UTC] Saving snapshot
[2018-12-22 11:54:12.952588 UTC] Starting iteration 1042
[2018-12-22 11:54:12.952712 UTC] Start collecting samples
[2018-12-22 11:54:15.867067 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:15.943649 UTC] Performing policy update
[2018-12-22 11:54:15.944328 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:16.034170 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:17.094958 UTC] Performing line search
[2018-12-22 11:54:17.222431 UTC] Updating baseline
[2018-12-22 11:54:19.092442 UTC] Computing logging information
-------------------------------------
| Iteration            | 1042       |
| ExpectedImprovement  | 0.021683   |
| ActualImprovement    | 0.020431   |
| ImprovementRatio     | 0.94226    |
| MeanKL               | 0.0072065  |
| Entropy              | -0.93165   |
| Perplexity           | 0.3939     |
| AveragePolicyStd     | 0.2106     |
| AveragePolicyStd[0]  | 0.23425    |
| AveragePolicyStd[1]  | 0.22173    |
| AveragePolicyStd[2]  | 0.16571    |
| AveragePolicyStd[3]  | 0.22657    |
| AveragePolicyStd[4]  | 0.15684    |
| AveragePolicyStd[5]  | 0.25851    |
| AverageReturn        | 1693.4     |
| MinReturn            | 313.61     |
| MaxReturn            | 1884.7     |
| StdReturn            | 334.48     |
| AverageEpisodeLength | 940.66     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.47     |
| TotalNEpisodes       | 19909      |
| TotalNSamples        | 5.2167e+06 |
| ExplainedVariance    | 0.11109    |
-------------------------------------
[2018-12-22 11:54:19.494452 UTC] Saving snapshot
[2018-12-22 11:54:19.494717 UTC] Starting iteration 1043
[2018-12-22 11:54:19.494853 UTC] Start collecting samples
[2018-12-22 11:54:22.381019 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:22.456332 UTC] Performing policy update
[2018-12-22 11:54:22.457003 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:22.547033 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:23.610058 UTC] Performing line search
[2018-12-22 11:54:23.738251 UTC] Updating baseline
[2018-12-22 11:54:24.996634 UTC] Computing logging information
-------------------------------------
| Iteration            | 1043       |
| ExpectedImprovement  | 0.01844    |
| ActualImprovement    | 0.016965   |
| ImprovementRatio     | 0.91999    |
| MeanKL               | 0.0076082  |
| Entropy              | -0.93598   |
| Perplexity           | 0.3922     |
| AveragePolicyStd     | 0.21046    |
| AveragePolicyStd[0]  | 0.2338     |
| AveragePolicyStd[1]  | 0.22167    |
| AveragePolicyStd[2]  | 0.16547    |
| AveragePolicyStd[3]  | 0.22655    |
| AveragePolicyStd[4]  | 0.15671    |
| AveragePolicyStd[5]  | 0.25856    |
| AverageReturn        | 1695.1     |
| MinReturn            | 313.61     |
| MaxReturn            | 1884.7     |
| StdReturn            | 335.05     |
| AverageEpisodeLength | 940.66     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.47     |
| TotalNEpisodes       | 19911      |
| TotalNSamples        | 5.2187e+06 |
| ExplainedVariance    | 0.037277   |
-------------------------------------
[2018-12-22 11:54:25.391947 UTC] Saving snapshot
[2018-12-22 11:54:25.392220 UTC] Starting iteration 1044
[2018-12-22 11:54:25.392351 UTC] Start collecting samples
[2018-12-22 11:54:28.376854 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:28.455659 UTC] Performing policy update
[2018-12-22 11:54:28.456309 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:28.546717 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:29.610122 UTC] Performing line search
[2018-12-22 11:54:29.737465 UTC] Updating baseline
[2018-12-22 11:54:31.083450 UTC] Computing logging information
-------------------------------------
| Iteration            | 1044       |
| ExpectedImprovement  | 0.017051   |
| ActualImprovement    | 0.016658   |
| ImprovementRatio     | 0.97697    |
| MeanKL               | 0.0073989  |
| Entropy              | -0.94178   |
| Perplexity           | 0.38993    |
| AveragePolicyStd     | 0.21026    |
| AveragePolicyStd[0]  | 0.23311    |
| AveragePolicyStd[1]  | 0.22118    |
| AveragePolicyStd[2]  | 0.16505    |
| AveragePolicyStd[3]  | 0.22691    |
| AveragePolicyStd[4]  | 0.15673    |
| AveragePolicyStd[5]  | 0.25861    |
| AverageReturn        | 1698.8     |
| MinReturn            | 313.61     |
| MaxReturn            | 1884.7     |
| StdReturn            | 336.12     |
| AverageEpisodeLength | 940.66     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.47     |
| TotalNEpisodes       | 19919      |
| TotalNSamples        | 5.2267e+06 |
| ExplainedVariance    | -0.003627  |
-------------------------------------
[2018-12-22 11:54:31.484664 UTC] Saving snapshot
[2018-12-22 11:54:31.484914 UTC] Starting iteration 1045
[2018-12-22 11:54:31.485032 UTC] Start collecting samples
[2018-12-22 11:54:34.426781 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:34.504382 UTC] Performing policy update
[2018-12-22 11:54:34.505052 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:34.594342 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:35.641269 UTC] Performing line search
[2018-12-22 11:54:35.774968 UTC] Updating baseline
[2018-12-22 11:54:37.104031 UTC] Computing logging information
------------------------------------
| Iteration            | 1045      |
| ExpectedImprovement  | 0.018655  |
| ActualImprovement    | 0.017604  |
| ImprovementRatio     | 0.94363   |
| MeanKL               | 0.0074781 |
| Entropy              | -0.9301   |
| Perplexity           | 0.39451   |
| AveragePolicyStd     | 0.21065   |
| AveragePolicyStd[0]  | 0.23381   |
| AveragePolicyStd[1]  | 0.22183   |
| AveragePolicyStd[2]  | 0.16584   |
| AveragePolicyStd[3]  | 0.22717   |
| AveragePolicyStd[4]  | 0.15675   |
| AveragePolicyStd[5]  | 0.25853   |
| AverageReturn        | 1686.9    |
| MinReturn            | 313.61    |
| MaxReturn            | 1884.7    |
| StdReturn            | 359.43    |
| AverageEpisodeLength | 933.92    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.48    |
| TotalNEpisodes       | 19925     |
| TotalNSamples        | 5.232e+06 |
| ExplainedVariance    | 0.10671   |
------------------------------------
[2018-12-22 11:54:37.499175 UTC] Saving snapshot
[2018-12-22 11:54:37.499479 UTC] Starting iteration 1046
[2018-12-22 11:54:37.499634 UTC] Start collecting samples
[2018-12-22 11:54:40.393899 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:40.469703 UTC] Performing policy update
[2018-12-22 11:54:40.470411 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:40.558102 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:41.607688 UTC] Performing line search
[2018-12-22 11:54:41.735457 UTC] Updating baseline
[2018-12-22 11:54:42.880597 UTC] Computing logging information
------------------------------------
| Iteration            | 1046      |
| ExpectedImprovement  | 0.017068  |
| ActualImprovement    | 0.015724  |
| ImprovementRatio     | 0.92127   |
| MeanKL               | 0.007445  |
| Entropy              | -0.93143  |
| Perplexity           | 0.39399   |
| AveragePolicyStd     | 0.21061   |
| AveragePolicyStd[0]  | 0.23389   |
| AveragePolicyStd[1]  | 0.2217    |
| AveragePolicyStd[2]  | 0.16564   |
| AveragePolicyStd[3]  | 0.22687   |
| AveragePolicyStd[4]  | 0.1569    |
| AveragePolicyStd[5]  | 0.25866   |
| AverageReturn        | 1687.1    |
| MinReturn            | 313.61    |
| MaxReturn            | 1884.7    |
| StdReturn            | 359.47    |
| AverageEpisodeLength | 933.92    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.48    |
| TotalNEpisodes       | 19926     |
| TotalNSamples        | 5.233e+06 |
| ExplainedVariance    | 0.13253   |
------------------------------------
[2018-12-22 11:54:43.283059 UTC] Saving snapshot
[2018-12-22 11:54:43.283335 UTC] Starting iteration 1047
[2018-12-22 11:54:43.283458 UTC] Start collecting samples
[2018-12-22 11:54:46.278900 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:46.358674 UTC] Performing policy update
[2018-12-22 11:54:46.359370 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:46.450735 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:47.507812 UTC] Performing line search
[2018-12-22 11:54:47.636766 UTC] Updating baseline
[2018-12-22 11:54:48.870906 UTC] Computing logging information
------------------------------------
| Iteration            | 1047      |
| ExpectedImprovement  | 0.018397  |
| ActualImprovement    | 0.017731  |
| ImprovementRatio     | 0.96378   |
| MeanKL               | 0.0078721 |
| Entropy              | -0.93099  |
| Perplexity           | 0.39416   |
| AveragePolicyStd     | 0.21062   |
| AveragePolicyStd[0]  | 0.23305   |
| AveragePolicyStd[1]  | 0.22213   |
| AveragePolicyStd[2]  | 0.16644   |
| AveragePolicyStd[3]  | 0.22738   |
| AveragePolicyStd[4]  | 0.15625   |
| AveragePolicyStd[5]  | 0.25843   |
| AverageReturn        | 1685.2    |
| MinReturn            | 313.61    |
| MaxReturn            | 1884.7    |
| StdReturn            | 353.44    |
| AverageEpisodeLength | 933.15    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 184.53    |
| TotalNEpisodes       | 19935     |
| TotalNSamples        | 5.241e+06 |
| ExplainedVariance    | 0.24462   |
------------------------------------
[2018-12-22 11:54:49.270892 UTC] Saving snapshot
[2018-12-22 11:54:49.271166 UTC] Starting iteration 1048
[2018-12-22 11:54:49.271296 UTC] Start collecting samples
[2018-12-22 11:54:52.225406 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:52.301858 UTC] Performing policy update
[2018-12-22 11:54:52.302538 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:52.394829 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:53.459605 UTC] Performing line search
[2018-12-22 11:54:53.588700 UTC] Updating baseline
[2018-12-22 11:54:54.936747 UTC] Computing logging information
------------------------------------
| Iteration            | 1048      |
| ExpectedImprovement  | 0.016072  |
| ActualImprovement    | 0.015722  |
| ImprovementRatio     | 0.9782    |
| MeanKL               | 0.0078714 |
| Entropy              | -0.93248  |
| Perplexity           | 0.39358   |
| AveragePolicyStd     | 0.21054   |
| AveragePolicyStd[0]  | 0.23267   |
| AveragePolicyStd[1]  | 0.22262   |
| AveragePolicyStd[2]  | 0.16663   |
| AveragePolicyStd[3]  | 0.22791   |
| AveragePolicyStd[4]  | 0.15603   |
| AveragePolicyStd[5]  | 0.25736   |
| AverageReturn        | 1685.3    |
| MinReturn            | 313.61    |
| MaxReturn            | 1884.7    |
| StdReturn            | 353.55    |
| AverageEpisodeLength | 933.15    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 184.53    |
| TotalNEpisodes       | 19941     |
| TotalNSamples        | 5.247e+06 |
| ExplainedVariance    | -0.01272  |
------------------------------------
[2018-12-22 11:54:55.336919 UTC] Saving snapshot
[2018-12-22 11:54:55.337165 UTC] Starting iteration 1049
[2018-12-22 11:54:55.337294 UTC] Start collecting samples
[2018-12-22 11:54:58.230261 UTC] Computing input variables for policy optimization
[2018-12-22 11:54:58.304696 UTC] Performing policy update
[2018-12-22 11:54:58.305429 UTC] Computing gradient in Euclidean space
[2018-12-22 11:54:58.395970 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:54:59.445318 UTC] Performing line search
[2018-12-22 11:54:59.573156 UTC] Updating baseline
[2018-12-22 11:55:01.225628 UTC] Computing logging information
------------------------------------
| Iteration            | 1049      |
| ExpectedImprovement  | 0.019201  |
| ActualImprovement    | 0.017564  |
| ImprovementRatio     | 0.91479   |
| MeanKL               | 0.007365  |
| Entropy              | -0.91818  |
| Perplexity           | 0.39925   |
| AveragePolicyStd     | 0.21107   |
| AveragePolicyStd[0]  | 0.23322   |
| AveragePolicyStd[1]  | 0.22283   |
| AveragePolicyStd[2]  | 0.16627   |
| AveragePolicyStd[3]  | 0.22875   |
| AveragePolicyStd[4]  | 0.15682   |
| AveragePolicyStd[5]  | 0.25853   |
| AverageReturn        | 1700.8    |
| MinReturn            | 351.34    |
| MaxReturn            | 1884.7    |
| StdReturn            | 325.94    |
| AverageEpisodeLength | 941.02    |
| MinEpisodeLength     | 227       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 169.84    |
| TotalNEpisodes       | 19942     |
| TotalNSamples        | 5.248e+06 |
| ExplainedVariance    | 0.0068526 |
------------------------------------
[2018-12-22 11:55:01.626142 UTC] Saving snapshot
[2018-12-22 11:55:01.626427 UTC] Starting iteration 1050
[2018-12-22 11:55:01.626569 UTC] Start collecting samples
[2018-12-22 11:55:04.640889 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:04.722427 UTC] Performing policy update
[2018-12-22 11:55:04.723366 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:04.814688 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:05.874162 UTC] Performing line search
[2018-12-22 11:55:06.005833 UTC] Updating baseline
[2018-12-22 11:55:07.421548 UTC] Computing logging information
-------------------------------------
| Iteration            | 1050       |
| ExpectedImprovement  | 0.017701   |
| ActualImprovement    | 0.016957   |
| ImprovementRatio     | 0.958      |
| MeanKL               | 0.0076983  |
| Entropy              | -0.92362   |
| Perplexity           | 0.39708    |
| AveragePolicyStd     | 0.21085    |
| AveragePolicyStd[0]  | 0.23168    |
| AveragePolicyStd[1]  | 0.22288    |
| AveragePolicyStd[2]  | 0.16623    |
| AveragePolicyStd[3]  | 0.22854    |
| AveragePolicyStd[4]  | 0.15701    |
| AveragePolicyStd[5]  | 0.25878    |
| AverageReturn        | 1721.4     |
| MinReturn            | 431.03     |
| MaxReturn            | 1884.7     |
| StdReturn            | 295.87     |
| AverageEpisodeLength | 950.91     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.46     |
| TotalNEpisodes       | 19952      |
| TotalNSamples        | 5.2575e+06 |
| ExplainedVariance    | 0.054419   |
-------------------------------------
[2018-12-22 11:55:07.822776 UTC] Saving snapshot
[2018-12-22 11:55:07.830851 UTC] Starting iteration 1051
[2018-12-22 11:55:07.831045 UTC] Start collecting samples
[2018-12-22 11:55:10.758696 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:10.835109 UTC] Performing policy update
[2018-12-22 11:55:10.836076 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:10.924990 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:11.995524 UTC] Performing line search
[2018-12-22 11:55:12.123248 UTC] Updating baseline
[2018-12-22 11:55:13.452712 UTC] Computing logging information
-------------------------------------
| Iteration            | 1051       |
| ExpectedImprovement  | 0.017712   |
| ActualImprovement    | 0.017058   |
| ImprovementRatio     | 0.96312    |
| MeanKL               | 0.0075924  |
| Entropy              | -0.92173   |
| Perplexity           | 0.39783    |
| AveragePolicyStd     | 0.21091    |
| AveragePolicyStd[0]  | 0.23132    |
| AveragePolicyStd[1]  | 0.22251    |
| AveragePolicyStd[2]  | 0.16632    |
| AveragePolicyStd[3]  | 0.22855    |
| AveragePolicyStd[4]  | 0.15729    |
| AveragePolicyStd[5]  | 0.25948    |
| AverageReturn        | 1734.2     |
| MinReturn            | 431.03     |
| MaxReturn            | 1884.7     |
| StdReturn            | 272.87     |
| AverageEpisodeLength | 957.37     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.31     |
| TotalNEpisodes       | 19956      |
| TotalNSamples        | 5.2615e+06 |
| ExplainedVariance    | 0.01076    |
-------------------------------------
[2018-12-22 11:55:13.859478 UTC] Saving snapshot
[2018-12-22 11:55:13.859748 UTC] Starting iteration 1052
[2018-12-22 11:55:13.859886 UTC] Start collecting samples
[2018-12-22 11:55:16.774855 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:16.853206 UTC] Performing policy update
[2018-12-22 11:55:16.853842 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:16.943818 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:18.008073 UTC] Performing line search
[2018-12-22 11:55:18.136486 UTC] Updating baseline
[2018-12-22 11:55:19.802276 UTC] Computing logging information
-------------------------------------
| Iteration            | 1052       |
| ExpectedImprovement  | 0.019114   |
| ActualImprovement    | 0.01812    |
| ImprovementRatio     | 0.94798    |
| MeanKL               | 0.0081117  |
| Entropy              | -0.92147   |
| Perplexity           | 0.39793    |
| AveragePolicyStd     | 0.21087    |
| AveragePolicyStd[0]  | 0.23131    |
| AveragePolicyStd[1]  | 0.22218    |
| AveragePolicyStd[2]  | 0.16642    |
| AveragePolicyStd[3]  | 0.22841    |
| AveragePolicyStd[4]  | 0.15781    |
| AveragePolicyStd[5]  | 0.25909    |
| AverageReturn        | 1733.5     |
| MinReturn            | 431.03     |
| MaxReturn            | 1884.7     |
| StdReturn            | 272.69     |
| AverageEpisodeLength | 957.37     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.31     |
| TotalNEpisodes       | 19958      |
| TotalNSamples        | 5.2635e+06 |
| ExplainedVariance    | -0.032776  |
-------------------------------------
[2018-12-22 11:55:20.202295 UTC] Saving snapshot
[2018-12-22 11:55:20.202562 UTC] Starting iteration 1053
[2018-12-22 11:55:20.202686 UTC] Start collecting samples
[2018-12-22 11:55:23.204728 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:23.285008 UTC] Performing policy update
[2018-12-22 11:55:23.285609 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:23.375186 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:24.430890 UTC] Performing line search
[2018-12-22 11:55:24.558797 UTC] Updating baseline
[2018-12-22 11:55:25.899128 UTC] Computing logging information
-------------------------------------
| Iteration            | 1053       |
| ExpectedImprovement  | 0.018524   |
| ActualImprovement    | 0.017528   |
| ImprovementRatio     | 0.94624    |
| MeanKL               | 0.0077998  |
| Entropy              | -0.92939   |
| Perplexity           | 0.39479    |
| AveragePolicyStd     | 0.21059    |
| AveragePolicyStd[0]  | 0.23122    |
| AveragePolicyStd[1]  | 0.22131    |
| AveragePolicyStd[2]  | 0.16647    |
| AveragePolicyStd[3]  | 0.22839    |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.25867    |
| AverageReturn        | 1727.1     |
| MinReturn            | 431.03     |
| MaxReturn            | 1884.7     |
| StdReturn            | 274.64     |
| AverageEpisodeLength | 954.81     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.53     |
| TotalNEpisodes       | 19968      |
| TotalNSamples        | 5.2732e+06 |
| ExplainedVariance    | 0.10848    |
-------------------------------------
[2018-12-22 11:55:26.301602 UTC] Saving snapshot
[2018-12-22 11:55:26.301890 UTC] Starting iteration 1054
[2018-12-22 11:55:26.302015 UTC] Start collecting samples
[2018-12-22 11:55:29.218885 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:29.295890 UTC] Performing policy update
[2018-12-22 11:55:29.296555 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:29.385467 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:30.450062 UTC] Performing line search
[2018-12-22 11:55:30.576871 UTC] Updating baseline
[2018-12-22 11:55:31.927005 UTC] Computing logging information
-------------------------------------
| Iteration            | 1054       |
| ExpectedImprovement  | 0.017121   |
| ActualImprovement    | 0.01586    |
| ImprovementRatio     | 0.92636    |
| MeanKL               | 0.0079011  |
| Entropy              | -0.93385   |
| Perplexity           | 0.39304    |
| AveragePolicyStd     | 0.2105     |
| AveragePolicyStd[0]  | 0.23131    |
| AveragePolicyStd[1]  | 0.22157    |
| AveragePolicyStd[2]  | 0.16549    |
| AveragePolicyStd[3]  | 0.22833    |
| AveragePolicyStd[4]  | 0.15729    |
| AveragePolicyStd[5]  | 0.25899    |
| AverageReturn        | 1726.3     |
| MinReturn            | 431.03     |
| MaxReturn            | 1884.7     |
| StdReturn            | 274.48     |
| AverageEpisodeLength | 954.81     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.53     |
| TotalNEpisodes       | 19971      |
| TotalNSamples        | 5.2762e+06 |
| ExplainedVariance    | -0.0086848 |
-------------------------------------
[2018-12-22 11:55:32.331743 UTC] Saving snapshot
[2018-12-22 11:55:32.331990 UTC] Starting iteration 1055
[2018-12-22 11:55:32.332111 UTC] Start collecting samples
[2018-12-22 11:55:35.241240 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:35.316977 UTC] Performing policy update
[2018-12-22 11:55:35.317632 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:35.407072 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:36.468751 UTC] Performing line search
[2018-12-22 11:55:36.597302 UTC] Updating baseline
[2018-12-22 11:55:38.028603 UTC] Computing logging information
-------------------------------------
| Iteration            | 1055       |
| ExpectedImprovement  | 0.017233   |
| ActualImprovement    | 0.015989   |
| ImprovementRatio     | 0.92785    |
| MeanKL               | 0.0072197  |
| Entropy              | -0.92922   |
| Perplexity           | 0.39486    |
| AveragePolicyStd     | 0.21069    |
| AveragePolicyStd[0]  | 0.23202    |
| AveragePolicyStd[1]  | 0.22169    |
| AveragePolicyStd[2]  | 0.16511    |
| AveragePolicyStd[3]  | 0.22865    |
| AveragePolicyStd[4]  | 0.15748    |
| AveragePolicyStd[5]  | 0.25917    |
| AverageReturn        | 1725.8     |
| MinReturn            | 431.03     |
| MaxReturn            | 1884.7     |
| StdReturn            | 274.39     |
| AverageEpisodeLength | 954.81     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.53     |
| TotalNEpisodes       | 19974      |
| TotalNSamples        | 5.2792e+06 |
| ExplainedVariance    | 0.065529   |
-------------------------------------
[2018-12-22 11:55:38.431584 UTC] Saving snapshot
[2018-12-22 11:55:38.431865 UTC] Starting iteration 1056
[2018-12-22 11:55:38.432009 UTC] Start collecting samples
[2018-12-22 11:55:41.385103 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:41.465494 UTC] Performing policy update
[2018-12-22 11:55:41.466241 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:41.559053 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:42.624891 UTC] Performing line search
[2018-12-22 11:55:42.752438 UTC] Updating baseline
[2018-12-22 11:55:44.115931 UTC] Computing logging information
-------------------------------------
| Iteration            | 1056       |
| ExpectedImprovement  | 0.017212   |
| ActualImprovement    | 0.016403   |
| ImprovementRatio     | 0.95295    |
| MeanKL               | 0.0076007  |
| Entropy              | -0.92969   |
| Perplexity           | 0.39468    |
| AveragePolicyStd     | 0.21069    |
| AveragePolicyStd[0]  | 0.23159    |
| AveragePolicyStd[1]  | 0.22173    |
| AveragePolicyStd[2]  | 0.16491    |
| AveragePolicyStd[3]  | 0.22932    |
| AveragePolicyStd[4]  | 0.15738    |
| AveragePolicyStd[5]  | 0.25921    |
| AverageReturn        | 1729.7     |
| MinReturn            | 431.03     |
| MaxReturn            | 1884.7     |
| StdReturn            | 265.63     |
| AverageEpisodeLength | 956.37     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.59     |
| TotalNEpisodes       | 19980      |
| TotalNSamples        | 5.2843e+06 |
| ExplainedVariance    | 0.19108    |
-------------------------------------
[2018-12-22 11:55:44.520089 UTC] Saving snapshot
[2018-12-22 11:55:44.520387 UTC] Starting iteration 1057
[2018-12-22 11:55:44.520523 UTC] Start collecting samples
[2018-12-22 11:55:47.459785 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:47.539449 UTC] Performing policy update
[2018-12-22 11:55:47.540100 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:47.627656 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:48.684380 UTC] Performing line search
[2018-12-22 11:55:48.811183 UTC] Updating baseline
[2018-12-22 11:55:50.295021 UTC] Computing logging information
-------------------------------------
| Iteration            | 1057       |
| ExpectedImprovement  | 0.01664    |
| ActualImprovement    | 0.015684   |
| ImprovementRatio     | 0.94258    |
| MeanKL               | 0.0078144  |
| Entropy              | -0.93504   |
| Perplexity           | 0.39257    |
| AveragePolicyStd     | 0.21051    |
| AveragePolicyStd[0]  | 0.23175    |
| AveragePolicyStd[1]  | 0.2217     |
| AveragePolicyStd[2]  | 0.16461    |
| AveragePolicyStd[3]  | 0.22845    |
| AveragePolicyStd[4]  | 0.15728    |
| AveragePolicyStd[5]  | 0.25928    |
| AverageReturn        | 1730.3     |
| MinReturn            | 431.03     |
| MaxReturn            | 1891.3     |
| StdReturn            | 265.89     |
| AverageEpisodeLength | 956.37     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.59     |
| TotalNEpisodes       | 19987      |
| TotalNSamples        | 5.2913e+06 |
| ExplainedVariance    | -0.11365   |
-------------------------------------
[2018-12-22 11:55:50.693885 UTC] Saving snapshot
[2018-12-22 11:55:50.694129 UTC] Starting iteration 1058
[2018-12-22 11:55:50.694248 UTC] Start collecting samples
[2018-12-22 11:55:53.607986 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:53.686565 UTC] Performing policy update
[2018-12-22 11:55:53.687417 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:53.779713 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:55:54.840869 UTC] Performing line search
[2018-12-22 11:55:54.968756 UTC] Updating baseline
[2018-12-22 11:55:56.296544 UTC] Computing logging information
-------------------------------------
| Iteration            | 1058       |
| ExpectedImprovement  | 0.017162   |
| ActualImprovement    | 0.016253   |
| ImprovementRatio     | 0.94706    |
| MeanKL               | 0.0079903  |
| Entropy              | -0.93572   |
| Perplexity           | 0.3923     |
| AveragePolicyStd     | 0.2105     |
| AveragePolicyStd[0]  | 0.23165    |
| AveragePolicyStd[1]  | 0.22099    |
| AveragePolicyStd[2]  | 0.1649     |
| AveragePolicyStd[3]  | 0.22839    |
| AveragePolicyStd[4]  | 0.1571     |
| AveragePolicyStd[5]  | 0.25996    |
| AverageReturn        | 1731.4     |
| MinReturn            | 431.03     |
| MaxReturn            | 1891.3     |
| StdReturn            | 258.93     |
| AverageEpisodeLength | 957.39     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.2      |
| TotalNEpisodes       | 19991      |
| TotalNSamples        | 5.2948e+06 |
| ExplainedVariance    | 0.12987    |
-------------------------------------
[2018-12-22 11:55:56.695358 UTC] Saving snapshot
[2018-12-22 11:55:56.695622 UTC] Starting iteration 1059
[2018-12-22 11:55:56.695743 UTC] Start collecting samples
[2018-12-22 11:55:59.630461 UTC] Computing input variables for policy optimization
[2018-12-22 11:55:59.709817 UTC] Performing policy update
[2018-12-22 11:55:59.710543 UTC] Computing gradient in Euclidean space
[2018-12-22 11:55:59.803567 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:00.860624 UTC] Performing line search
[2018-12-22 11:56:00.987834 UTC] Updating baseline
[2018-12-22 11:56:02.430717 UTC] Computing logging information
------------------------------------
| Iteration            | 1059      |
| ExpectedImprovement  | 0.017416  |
| ActualImprovement    | 0.016764  |
| ImprovementRatio     | 0.96257   |
| MeanKL               | 0.0077193 |
| Entropy              | -0.93952  |
| Perplexity           | 0.39081   |
| AveragePolicyStd     | 0.21033   |
| AveragePolicyStd[0]  | 0.23179   |
| AveragePolicyStd[1]  | 0.22062   |
| AveragePolicyStd[2]  | 0.16477   |
| AveragePolicyStd[3]  | 0.22832   |
| AveragePolicyStd[4]  | 0.15737   |
| AveragePolicyStd[5]  | 0.25911   |
| AverageReturn        | 1714.9    |
| MinReturn            | 431.03    |
| MaxReturn            | 1891.3    |
| StdReturn            | 279.76    |
| AverageEpisodeLength | 949.14    |
| MinEpisodeLength     | 277       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 145.62    |
| TotalNEpisodes       | 19997     |
| TotalNSamples        | 5.3e+06   |
| ExplainedVariance    | 0.31549   |
------------------------------------
[2018-12-22 11:56:02.829906 UTC] Saving snapshot
[2018-12-22 11:56:02.830182 UTC] Starting iteration 1060
[2018-12-22 11:56:02.830299 UTC] Start collecting samples
[2018-12-22 11:56:05.798833 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:05.879600 UTC] Performing policy update
[2018-12-22 11:56:05.883017 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:05.975535 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:07.024339 UTC] Performing line search
[2018-12-22 11:56:07.150559 UTC] Updating baseline
[2018-12-22 11:56:08.548715 UTC] Computing logging information
-------------------------------------
| Iteration            | 1060       |
| ExpectedImprovement  | 0.016429   |
| ActualImprovement    | 0.01608    |
| ImprovementRatio     | 0.97879    |
| MeanKL               | 0.007747   |
| Entropy              | -0.9326    |
| Perplexity           | 0.39353    |
| AveragePolicyStd     | 0.21057    |
| AveragePolicyStd[0]  | 0.23186    |
| AveragePolicyStd[1]  | 0.22137    |
| AveragePolicyStd[2]  | 0.16496    |
| AveragePolicyStd[3]  | 0.2285     |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.25928    |
| AverageReturn        | 1692.2     |
| MinReturn            | 409.3      |
| MaxReturn            | 1891.3     |
| StdReturn            | 319.76     |
| AverageEpisodeLength | 936.63     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.86     |
| TotalNEpisodes       | 20006      |
| TotalNSamples        | 5.3078e+06 |
| ExplainedVariance    | 0.10098    |
-------------------------------------
[2018-12-22 11:56:08.950468 UTC] Saving snapshot
[2018-12-22 11:56:08.958108 UTC] Starting iteration 1061
[2018-12-22 11:56:08.958319 UTC] Start collecting samples
[2018-12-22 11:56:11.855702 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:11.932144 UTC] Performing policy update
[2018-12-22 11:56:11.933191 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:12.022871 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:13.072322 UTC] Performing line search
[2018-12-22 11:56:13.198431 UTC] Updating baseline
[2018-12-22 11:56:14.453882 UTC] Computing logging information
-------------------------------------
| Iteration            | 1061       |
| ExpectedImprovement  | 0.015193   |
| ActualImprovement    | 0.014402   |
| ImprovementRatio     | 0.94794    |
| MeanKL               | 0.0081244  |
| Entropy              | -0.93407   |
| Perplexity           | 0.39295    |
| AveragePolicyStd     | 0.21051    |
| AveragePolicyStd[0]  | 0.23192    |
| AveragePolicyStd[1]  | 0.22097    |
| AveragePolicyStd[2]  | 0.16517    |
| AveragePolicyStd[3]  | 0.22893    |
| AveragePolicyStd[4]  | 0.15726    |
| AveragePolicyStd[5]  | 0.25882    |
| AverageReturn        | 1699.8     |
| MinReturn            | 409.3      |
| MaxReturn            | 1891.3     |
| StdReturn            | 314.3      |
| AverageEpisodeLength | 940.35     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.06     |
| TotalNEpisodes       | 20007      |
| TotalNSamples        | 5.3088e+06 |
| ExplainedVariance    | -0.16429   |
-------------------------------------
[2018-12-22 11:56:14.850047 UTC] Saving snapshot
[2018-12-22 11:56:14.850296 UTC] Starting iteration 1062
[2018-12-22 11:56:14.850419 UTC] Start collecting samples
[2018-12-22 11:56:17.769933 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:17.848242 UTC] Performing policy update
[2018-12-22 11:56:17.848985 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:17.942560 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:19.000004 UTC] Performing line search
[2018-12-22 11:56:19.127733 UTC] Updating baseline
[2018-12-22 11:56:20.722094 UTC] Computing logging information
-------------------------------------
| Iteration            | 1062       |
| ExpectedImprovement  | 0.018632   |
| ActualImprovement    | 0.017477   |
| ImprovementRatio     | 0.93801    |
| MeanKL               | 0.0076964  |
| Entropy              | -0.93158   |
| Perplexity           | 0.39393    |
| AveragePolicyStd     | 0.21059    |
| AveragePolicyStd[0]  | 0.23156    |
| AveragePolicyStd[1]  | 0.221      |
| AveragePolicyStd[2]  | 0.16559    |
| AveragePolicyStd[3]  | 0.22938    |
| AveragePolicyStd[4]  | 0.15709    |
| AveragePolicyStd[5]  | 0.25894    |
| AverageReturn        | 1693       |
| MinReturn            | 409.3      |
| MaxReturn            | 1891.3     |
| StdReturn            | 314.6      |
| AverageEpisodeLength | 938.36     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.53     |
| TotalNEpisodes       | 20012      |
| TotalNSamples        | 5.3136e+06 |
| ExplainedVariance    | 0.16296    |
-------------------------------------
[2018-12-22 11:56:21.123962 UTC] Saving snapshot
[2018-12-22 11:56:21.124211 UTC] Starting iteration 1063
[2018-12-22 11:56:21.124348 UTC] Start collecting samples
[2018-12-22 11:56:24.127257 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:24.210569 UTC] Performing policy update
[2018-12-22 11:56:24.211173 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:24.299419 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:25.364837 UTC] Performing line search
[2018-12-22 11:56:25.492380 UTC] Updating baseline
[2018-12-22 11:56:26.946569 UTC] Computing logging information
-------------------------------------
| Iteration            | 1063       |
| ExpectedImprovement  | 0.017275   |
| ActualImprovement    | 0.016148   |
| ImprovementRatio     | 0.93476    |
| MeanKL               | 0.0079342  |
| Entropy              | -0.93335   |
| Perplexity           | 0.39323    |
| AveragePolicyStd     | 0.21055    |
| AveragePolicyStd[0]  | 0.23147    |
| AveragePolicyStd[1]  | 0.22146    |
| AveragePolicyStd[2]  | 0.16559    |
| AveragePolicyStd[3]  | 0.22883    |
| AveragePolicyStd[4]  | 0.15682    |
| AveragePolicyStd[5]  | 0.25911    |
| AverageReturn        | 1692.8     |
| MinReturn            | 409.3      |
| MaxReturn            | 1891.3     |
| StdReturn            | 314.43     |
| AverageEpisodeLength | 938.36     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.53     |
| TotalNEpisodes       | 20022      |
| TotalNSamples        | 5.3236e+06 |
| ExplainedVariance    | -0.039069  |
-------------------------------------
[2018-12-22 11:56:27.350055 UTC] Saving snapshot
[2018-12-22 11:56:27.350344 UTC] Starting iteration 1064
[2018-12-22 11:56:27.350466 UTC] Start collecting samples
[2018-12-22 11:56:30.221518 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:30.298814 UTC] Performing policy update
[2018-12-22 11:56:30.299587 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:30.388488 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:31.455323 UTC] Performing line search
[2018-12-22 11:56:31.582677 UTC] Updating baseline
[2018-12-22 11:56:32.793954 UTC] Computing logging information
-------------------------------------
| Iteration            | 1064       |
| ExpectedImprovement  | 0.01719    |
| ActualImprovement    | 0.016701   |
| ImprovementRatio     | 0.97151    |
| MeanKL               | 0.0081333  |
| Entropy              | -0.93969   |
| Perplexity           | 0.39075    |
| AveragePolicyStd     | 0.21032    |
| AveragePolicyStd[0]  | 0.23005    |
| AveragePolicyStd[1]  | 0.22205    |
| AveragePolicyStd[2]  | 0.16513    |
| AveragePolicyStd[3]  | 0.2281     |
| AveragePolicyStd[4]  | 0.15701    |
| AveragePolicyStd[5]  | 0.25961    |
| AverageReturn        | 1707.1     |
| MinReturn            | 409.3      |
| MaxReturn            | 1891.3     |
| StdReturn            | 288.11     |
| AverageEpisodeLength | 945.59     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.6      |
| TotalNEpisodes       | 20023      |
| TotalNSamples        | 5.3246e+06 |
| ExplainedVariance    | -0.27451   |
-------------------------------------
[2018-12-22 11:56:33.198482 UTC] Saving snapshot
[2018-12-22 11:56:33.198757 UTC] Starting iteration 1065
[2018-12-22 11:56:33.198880 UTC] Start collecting samples
[2018-12-22 11:56:36.115045 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:36.192676 UTC] Performing policy update
[2018-12-22 11:56:36.193263 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:36.282446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:37.337561 UTC] Performing line search
[2018-12-22 11:56:37.466161 UTC] Updating baseline
[2018-12-22 11:56:39.088827 UTC] Computing logging information
-------------------------------------
| Iteration            | 1065       |
| ExpectedImprovement  | 0.017061   |
| ActualImprovement    | 0.015822   |
| ImprovementRatio     | 0.92736    |
| MeanKL               | 0.0089693  |
| Entropy              | -0.94494   |
| Perplexity           | 0.3887     |
| AveragePolicyStd     | 0.2101     |
| AveragePolicyStd[0]  | 0.23042    |
| AveragePolicyStd[1]  | 0.22122    |
| AveragePolicyStd[2]  | 0.16493    |
| AveragePolicyStd[3]  | 0.22826    |
| AveragePolicyStd[4]  | 0.15726    |
| AveragePolicyStd[5]  | 0.25851    |
| AverageReturn        | 1707.5     |
| MinReturn            | 409.3      |
| MaxReturn            | 1891.3     |
| StdReturn            | 288.19     |
| AverageEpisodeLength | 945.59     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.6      |
| TotalNEpisodes       | 20026      |
| TotalNSamples        | 5.3276e+06 |
| ExplainedVariance    | -0.014371  |
-------------------------------------
[2018-12-22 11:56:39.492396 UTC] Saving snapshot
[2018-12-22 11:56:39.492671 UTC] Starting iteration 1066
[2018-12-22 11:56:39.492790 UTC] Start collecting samples
[2018-12-22 11:56:42.517703 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:42.600886 UTC] Performing policy update
[2018-12-22 11:56:42.601551 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:42.690138 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:43.754241 UTC] Performing line search
[2018-12-22 11:56:43.881184 UTC] Updating baseline
[2018-12-22 11:56:45.236935 UTC] Computing logging information
-------------------------------------
| Iteration            | 1066       |
| ExpectedImprovement  | 0.015992   |
| ActualImprovement    | 0.015053   |
| ImprovementRatio     | 0.94127    |
| MeanKL               | 0.0078917  |
| Entropy              | -0.94782   |
| Perplexity           | 0.38758    |
| AveragePolicyStd     | 0.20998    |
| AveragePolicyStd[0]  | 0.23076    |
| AveragePolicyStd[1]  | 0.22013    |
| AveragePolicyStd[2]  | 0.16482    |
| AveragePolicyStd[3]  | 0.22795    |
| AveragePolicyStd[4]  | 0.15763    |
| AveragePolicyStd[5]  | 0.25859    |
| AverageReturn        | 1725.6     |
| MinReturn            | 409.3      |
| MaxReturn            | 1891.3     |
| StdReturn            | 275.27     |
| AverageEpisodeLength | 955.32     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.26     |
| TotalNEpisodes       | 20038      |
| TotalNSamples        | 5.3396e+06 |
| ExplainedVariance    | -0.0084784 |
-------------------------------------
[2018-12-22 11:56:45.639597 UTC] Saving snapshot
[2018-12-22 11:56:45.639860 UTC] Starting iteration 1067
[2018-12-22 11:56:45.639977 UTC] Start collecting samples
[2018-12-22 11:56:48.515444 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:48.593026 UTC] Performing policy update
[2018-12-22 11:56:48.593635 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:48.683680 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:49.744420 UTC] Performing line search
[2018-12-22 11:56:49.875253 UTC] Updating baseline
[2018-12-22 11:56:51.676122 UTC] Computing logging information
-------------------------------------
| Iteration            | 1067       |
| ExpectedImprovement  | 0.018615   |
| ActualImprovement    | 0.017421   |
| ImprovementRatio     | 0.93584    |
| MeanKL               | 0.0073645  |
| Entropy              | -0.94995   |
| Perplexity           | 0.38676    |
| AveragePolicyStd     | 0.2099     |
| AveragePolicyStd[0]  | 0.2299     |
| AveragePolicyStd[1]  | 0.2202     |
| AveragePolicyStd[2]  | 0.16486    |
| AveragePolicyStd[3]  | 0.22765    |
| AveragePolicyStd[4]  | 0.15773    |
| AveragePolicyStd[5]  | 0.25905    |
| AverageReturn        | 1710.5     |
| MinReturn            | 369.47     |
| MaxReturn            | 1891.3     |
| StdReturn            | 306.14     |
| AverageEpisodeLength | 947.76     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.7      |
| TotalNEpisodes       | 20040      |
| TotalNSamples        | 5.3408e+06 |
| ExplainedVariance    | 0.26325    |
-------------------------------------
[2018-12-22 11:56:52.082828 UTC] Saving snapshot
[2018-12-22 11:56:52.083067 UTC] Starting iteration 1068
[2018-12-22 11:56:52.083185 UTC] Start collecting samples
[2018-12-22 11:56:54.997099 UTC] Computing input variables for policy optimization
[2018-12-22 11:56:55.076366 UTC] Performing policy update
[2018-12-22 11:56:55.077020 UTC] Computing gradient in Euclidean space
[2018-12-22 11:56:55.166158 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:56:56.229771 UTC] Performing line search
[2018-12-22 11:56:56.358440 UTC] Updating baseline
[2018-12-22 11:56:57.628497 UTC] Computing logging information
-------------------------------------
| Iteration            | 1068       |
| ExpectedImprovement  | 0.017592   |
| ActualImprovement    | 0.016111   |
| ImprovementRatio     | 0.91582    |
| MeanKL               | 0.0081877  |
| Entropy              | -0.9486    |
| Perplexity           | 0.38728    |
| AveragePolicyStd     | 0.20996    |
| AveragePolicyStd[0]  | 0.22988    |
| AveragePolicyStd[1]  | 0.22048    |
| AveragePolicyStd[2]  | 0.1648     |
| AveragePolicyStd[3]  | 0.22743    |
| AveragePolicyStd[4]  | 0.15775    |
| AveragePolicyStd[5]  | 0.2594     |
| AverageReturn        | 1697.7     |
| MinReturn            | 369.47     |
| MaxReturn            | 1891.3     |
| StdReturn            | 325.28     |
| AverageEpisodeLength | 941.32     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.11     |
| TotalNEpisodes       | 20044      |
| TotalNSamples        | 5.3442e+06 |
| ExplainedVariance    | 0.13553    |
-------------------------------------
[2018-12-22 11:56:58.039165 UTC] Saving snapshot
[2018-12-22 11:56:58.039421 UTC] Starting iteration 1069
[2018-12-22 11:56:58.039560 UTC] Start collecting samples
[2018-12-22 11:57:01.033947 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:01.116744 UTC] Performing policy update
[2018-12-22 11:57:01.117320 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:01.207816 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:02.272283 UTC] Performing line search
[2018-12-22 11:57:02.399638 UTC] Updating baseline
[2018-12-22 11:57:03.754794 UTC] Computing logging information
-------------------------------------
| Iteration            | 1069       |
| ExpectedImprovement  | 0.016642   |
| ActualImprovement    | 0.016109   |
| ImprovementRatio     | 0.96797    |
| MeanKL               | 0.0084707  |
| Entropy              | -0.95126   |
| Perplexity           | 0.38625    |
| AveragePolicyStd     | 0.20985    |
| AveragePolicyStd[0]  | 0.22957    |
| AveragePolicyStd[1]  | 0.22076    |
| AveragePolicyStd[2]  | 0.16482    |
| AveragePolicyStd[3]  | 0.22713    |
| AveragePolicyStd[4]  | 0.15768    |
| AveragePolicyStd[5]  | 0.25915    |
| AverageReturn        | 1706.2     |
| MinReturn            | 369.47     |
| MaxReturn            | 1891.3     |
| StdReturn            | 310        |
| AverageEpisodeLength | 947.03     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.22     |
| TotalNEpisodes       | 20054      |
| TotalNSamples        | 5.3542e+06 |
| ExplainedVariance    | 0.035704   |
-------------------------------------
[2018-12-22 11:57:04.160534 UTC] Saving snapshot
[2018-12-22 11:57:04.160784 UTC] Starting iteration 1070
[2018-12-22 11:57:04.160904 UTC] Start collecting samples
[2018-12-22 11:57:07.043869 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:07.121252 UTC] Performing policy update
[2018-12-22 11:57:07.121914 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:07.211401 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:08.281894 UTC] Performing line search
[2018-12-22 11:57:08.410571 UTC] Updating baseline
[2018-12-22 11:57:09.864550 UTC] Computing logging information
------------------------------------
| Iteration            | 1070      |
| ExpectedImprovement  | 0.017277  |
| ActualImprovement    | 0.015839  |
| ImprovementRatio     | 0.91678   |
| MeanKL               | 0.0073624 |
| Entropy              | -0.95177  |
| Perplexity           | 0.38606   |
| AveragePolicyStd     | 0.20986   |
| AveragePolicyStd[0]  | 0.22934   |
| AveragePolicyStd[1]  | 0.22118   |
| AveragePolicyStd[2]  | 0.16408   |
| AveragePolicyStd[3]  | 0.22789   |
| AveragePolicyStd[4]  | 0.15786   |
| AveragePolicyStd[5]  | 0.25879   |
| AverageReturn        | 1702.3    |
| MinReturn            | 369.47    |
| MaxReturn            | 1891.3    |
| StdReturn            | 310.6     |
| AverageEpisodeLength | 945.16    |
| MinEpisodeLength     | 244       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.68    |
| TotalNEpisodes       | 20056     |
| TotalNSamples        | 5.356e+06 |
| ExplainedVariance    | 0.20512   |
------------------------------------
[2018-12-22 11:57:10.274436 UTC] Saving snapshot
[2018-12-22 11:57:10.282622 UTC] Starting iteration 1071
[2018-12-22 11:57:10.282826 UTC] Start collecting samples
[2018-12-22 11:57:13.185838 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:13.264287 UTC] Performing policy update
[2018-12-22 11:57:13.265117 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:13.355362 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:14.417890 UTC] Performing line search
[2018-12-22 11:57:14.546095 UTC] Updating baseline
[2018-12-22 11:57:16.221891 UTC] Computing logging information
------------------------------------
| Iteration            | 1071      |
| ExpectedImprovement  | 0.015553  |
| ActualImprovement    | 0.014377  |
| ImprovementRatio     | 0.92436   |
| MeanKL               | 0.0079082 |
| Entropy              | -0.9504   |
| Perplexity           | 0.38659   |
| AveragePolicyStd     | 0.20991   |
| AveragePolicyStd[0]  | 0.23002   |
| AveragePolicyStd[1]  | 0.22121   |
| AveragePolicyStd[2]  | 0.16357   |
| AveragePolicyStd[3]  | 0.22741   |
| AveragePolicyStd[4]  | 0.15831   |
| AveragePolicyStd[5]  | 0.25896   |
| AverageReturn        | 1702      |
| MinReturn            | 369.47    |
| MaxReturn            | 1891.3    |
| StdReturn            | 310.5     |
| AverageEpisodeLength | 945.16    |
| MinEpisodeLength     | 244       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.68    |
| TotalNEpisodes       | 20058     |
| TotalNSamples        | 5.358e+06 |
| ExplainedVariance    | 0.046398  |
------------------------------------
[2018-12-22 11:57:16.626148 UTC] Saving snapshot
[2018-12-22 11:57:16.626395 UTC] Starting iteration 1072
[2018-12-22 11:57:16.626529 UTC] Start collecting samples
[2018-12-22 11:57:19.619540 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:19.699884 UTC] Performing policy update
[2018-12-22 11:57:19.700448 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:19.792276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:20.848057 UTC] Performing line search
[2018-12-22 11:57:20.973956 UTC] Updating baseline
[2018-12-22 11:57:22.681907 UTC] Computing logging information
-------------------------------------
| Iteration            | 1072       |
| ExpectedImprovement  | 0.015804   |
| ActualImprovement    | 0.01538    |
| ImprovementRatio     | 0.97317    |
| MeanKL               | 0.0073616  |
| Entropy              | -0.95578   |
| Perplexity           | 0.38451    |
| AveragePolicyStd     | 0.20981    |
| AveragePolicyStd[0]  | 0.22994    |
| AveragePolicyStd[1]  | 0.22162    |
| AveragePolicyStd[2]  | 0.16303    |
| AveragePolicyStd[3]  | 0.22761    |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.2592     |
| AverageReturn        | 1695.8     |
| MinReturn            | 369.47     |
| MaxReturn            | 1891.3     |
| StdReturn            | 316.71     |
| AverageEpisodeLength | 942.83     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.34     |
| TotalNEpisodes       | 20067      |
| TotalNSamples        | 5.3665e+06 |
| ExplainedVariance    | 0.15892    |
-------------------------------------
[2018-12-22 11:57:23.083485 UTC] Saving snapshot
[2018-12-22 11:57:23.083756 UTC] Starting iteration 1073
[2018-12-22 11:57:23.084005 UTC] Start collecting samples
[2018-12-22 11:57:26.024150 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:26.103177 UTC] Performing policy update
[2018-12-22 11:57:26.103854 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:26.194509 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:27.256198 UTC] Performing line search
[2018-12-22 11:57:27.383391 UTC] Updating baseline
[2018-12-22 11:57:28.928371 UTC] Computing logging information
-------------------------------------
| Iteration            | 1073       |
| ExpectedImprovement  | 0.018808   |
| ActualImprovement    | 0.018177   |
| ImprovementRatio     | 0.96642    |
| MeanKL               | 0.0074219  |
| Entropy              | -0.94581   |
| Perplexity           | 0.38836    |
| AveragePolicyStd     | 0.21017    |
| AveragePolicyStd[0]  | 0.23062    |
| AveragePolicyStd[1]  | 0.22144    |
| AveragePolicyStd[2]  | 0.16363    |
| AveragePolicyStd[3]  | 0.22766    |
| AveragePolicyStd[4]  | 0.15747    |
| AveragePolicyStd[5]  | 0.26019    |
| AverageReturn        | 1697.6     |
| MinReturn            | 369.47     |
| MaxReturn            | 1891.3     |
| StdReturn            | 317.25     |
| AverageEpisodeLength | 942.83     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.34     |
| TotalNEpisodes       | 20072      |
| TotalNSamples        | 5.3715e+06 |
| ExplainedVariance    | 0.078831   |
-------------------------------------
[2018-12-22 11:57:29.333242 UTC] Saving snapshot
[2018-12-22 11:57:29.333536 UTC] Starting iteration 1074
[2018-12-22 11:57:29.333659 UTC] Start collecting samples
[2018-12-22 11:57:32.267282 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:32.345211 UTC] Performing policy update
[2018-12-22 11:57:32.345980 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:32.436974 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:33.490651 UTC] Performing line search
[2018-12-22 11:57:33.618303 UTC] Updating baseline
[2018-12-22 11:57:34.887758 UTC] Computing logging information
-------------------------------------
| Iteration            | 1074       |
| ExpectedImprovement  | 0.018243   |
| ActualImprovement    | 0.016283   |
| ImprovementRatio     | 0.89259    |
| MeanKL               | 0.0077114  |
| Entropy              | -0.94691   |
| Perplexity           | 0.38794    |
| AveragePolicyStd     | 0.21008    |
| AveragePolicyStd[0]  | 0.23088    |
| AveragePolicyStd[1]  | 0.22126    |
| AveragePolicyStd[2]  | 0.16375    |
| AveragePolicyStd[3]  | 0.22718    |
| AveragePolicyStd[4]  | 0.15785    |
| AveragePolicyStd[5]  | 0.25958    |
| AverageReturn        | 1693.8     |
| MinReturn            | 369.47     |
| MaxReturn            | 1891.3     |
| StdReturn            | 324.11     |
| AverageEpisodeLength | 941.5      |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.58     |
| TotalNEpisodes       | 20076      |
| TotalNSamples        | 5.3744e+06 |
| ExplainedVariance    | 0.37678    |
-------------------------------------
[2018-12-22 11:57:35.288692 UTC] Saving snapshot
[2018-12-22 11:57:35.288952 UTC] Starting iteration 1075
[2018-12-22 11:57:35.289072 UTC] Start collecting samples
[2018-12-22 11:57:38.269169 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:38.352472 UTC] Performing policy update
[2018-12-22 11:57:38.353122 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:38.443064 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:39.501620 UTC] Performing line search
[2018-12-22 11:57:39.628213 UTC] Updating baseline
[2018-12-22 11:57:41.048884 UTC] Computing logging information
-------------------------------------
| Iteration            | 1075       |
| ExpectedImprovement  | 0.018862   |
| ActualImprovement    | 0.017808   |
| ImprovementRatio     | 0.94414    |
| MeanKL               | 0.0078566  |
| Entropy              | -0.95267   |
| Perplexity           | 0.38571    |
| AveragePolicyStd     | 0.20985    |
| AveragePolicyStd[0]  | 0.23046    |
| AveragePolicyStd[1]  | 0.22036    |
| AveragePolicyStd[2]  | 0.16389    |
| AveragePolicyStd[3]  | 0.22762    |
| AveragePolicyStd[4]  | 0.15777    |
| AveragePolicyStd[5]  | 0.259      |
| AverageReturn        | 1676       |
| MinReturn            | 369.47     |
| MaxReturn            | 1867.9     |
| StdReturn            | 341.44     |
| AverageEpisodeLength | 933.1      |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.67     |
| TotalNEpisodes       | 20084      |
| TotalNSamples        | 5.3816e+06 |
| ExplainedVariance    | 0.15302    |
-------------------------------------
[2018-12-22 11:57:41.453031 UTC] Saving snapshot
[2018-12-22 11:57:41.453271 UTC] Starting iteration 1076
[2018-12-22 11:57:41.453389 UTC] Start collecting samples
[2018-12-22 11:57:44.392171 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:44.472248 UTC] Performing policy update
[2018-12-22 11:57:44.472973 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:44.562578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:45.618040 UTC] Performing line search
[2018-12-22 11:57:45.746706 UTC] Updating baseline
[2018-12-22 11:57:47.115524 UTC] Computing logging information
------------------------------------
| Iteration            | 1076      |
| ExpectedImprovement  | 0.019727  |
| ActualImprovement    | 0.018301  |
| ImprovementRatio     | 0.92769   |
| MeanKL               | 0.0075021 |
| Entropy              | -0.9559   |
| Perplexity           | 0.38447   |
| AveragePolicyStd     | 0.20971   |
| AveragePolicyStd[0]  | 0.22982   |
| AveragePolicyStd[1]  | 0.22049   |
| AveragePolicyStd[2]  | 0.16388   |
| AveragePolicyStd[3]  | 0.22728   |
| AveragePolicyStd[4]  | 0.15795   |
| AveragePolicyStd[5]  | 0.25887   |
| AverageReturn        | 1675.2    |
| MinReturn            | 369.47    |
| MaxReturn            | 1867.9    |
| StdReturn            | 352.37    |
| AverageEpisodeLength | 931.03    |
| MinEpisodeLength     | 244       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 184.13    |
| TotalNEpisodes       | 20090     |
| TotalNSamples        | 5.387e+06 |
| ExplainedVariance    | 0.12111   |
------------------------------------
[2018-12-22 11:57:47.521486 UTC] Saving snapshot
[2018-12-22 11:57:47.521780 UTC] Starting iteration 1077
[2018-12-22 11:57:47.521911 UTC] Start collecting samples
[2018-12-22 11:57:50.472889 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:50.552780 UTC] Performing policy update
[2018-12-22 11:57:50.553420 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:50.642248 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:51.704699 UTC] Performing line search
[2018-12-22 11:57:51.835885 UTC] Updating baseline
[2018-12-22 11:57:53.163116 UTC] Computing logging information
-------------------------------------
| Iteration            | 1077       |
| ExpectedImprovement  | 0.020839   |
| ActualImprovement    | 0.019694   |
| ImprovementRatio     | 0.94508    |
| MeanKL               | 0.0074351  |
| Entropy              | -0.96706   |
| Perplexity           | 0.3802     |
| AveragePolicyStd     | 0.2094     |
| AveragePolicyStd[0]  | 0.22966    |
| AveragePolicyStd[1]  | 0.22015    |
| AveragePolicyStd[2]  | 0.16288    |
| AveragePolicyStd[3]  | 0.22703    |
| AveragePolicyStd[4]  | 0.15747    |
| AveragePolicyStd[5]  | 0.25921    |
| AverageReturn        | 1659.9     |
| MinReturn            | 369.47     |
| MaxReturn            | 1867.9     |
| StdReturn            | 371.52     |
| AverageEpisodeLength | 923.8      |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.84     |
| TotalNEpisodes       | 20096      |
| TotalNSamples        | 5.3914e+06 |
| ExplainedVariance    | 0.38345    |
-------------------------------------
[2018-12-22 11:57:53.569563 UTC] Saving snapshot
[2018-12-22 11:57:53.569867 UTC] Starting iteration 1078
[2018-12-22 11:57:53.569994 UTC] Start collecting samples
[2018-12-22 11:57:56.483157 UTC] Computing input variables for policy optimization
[2018-12-22 11:57:56.561574 UTC] Performing policy update
[2018-12-22 11:57:56.562282 UTC] Computing gradient in Euclidean space
[2018-12-22 11:57:56.651984 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:57:57.693464 UTC] Performing line search
[2018-12-22 11:57:57.823523 UTC] Updating baseline
[2018-12-22 11:57:59.254039 UTC] Computing logging information
-------------------------------------
| Iteration            | 1078       |
| ExpectedImprovement  | 0.018704   |
| ActualImprovement    | 0.01908    |
| ImprovementRatio     | 1.0201     |
| MeanKL               | 0.0075204  |
| Entropy              | -0.96711   |
| Perplexity           | 0.38018    |
| AveragePolicyStd     | 0.20942    |
| AveragePolicyStd[0]  | 0.23       |
| AveragePolicyStd[1]  | 0.22004    |
| AveragePolicyStd[2]  | 0.16308    |
| AveragePolicyStd[3]  | 0.22711    |
| AveragePolicyStd[4]  | 0.15698    |
| AveragePolicyStd[5]  | 0.25934    |
| AverageReturn        | 1655.5     |
| MinReturn            | 369.47     |
| MaxReturn            | 1867.9     |
| StdReturn            | 379.47     |
| AverageEpisodeLength | 922.1      |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.39     |
| TotalNEpisodes       | 20100      |
| TotalNSamples        | 5.3947e+06 |
| ExplainedVariance    | 0.098562   |
-------------------------------------
[2018-12-22 11:57:59.654607 UTC] Saving snapshot
[2018-12-22 11:57:59.654887 UTC] Starting iteration 1079
[2018-12-22 11:57:59.655004 UTC] Start collecting samples
[2018-12-22 11:58:02.628646 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:02.707858 UTC] Performing policy update
[2018-12-22 11:58:02.708604 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:02.797531 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:03.856021 UTC] Performing line search
[2018-12-22 11:58:03.988018 UTC] Updating baseline
[2018-12-22 11:58:05.692249 UTC] Computing logging information
-------------------------------------
| Iteration            | 1079       |
| ExpectedImprovement  | 0.017553   |
| ActualImprovement    | 0.016574   |
| ImprovementRatio     | 0.94422    |
| MeanKL               | 0.0076249  |
| Entropy              | -0.965     |
| Perplexity           | 0.38098    |
| AveragePolicyStd     | 0.20952    |
| AveragePolicyStd[0]  | 0.23002    |
| AveragePolicyStd[1]  | 0.21991    |
| AveragePolicyStd[2]  | 0.16339    |
| AveragePolicyStd[3]  | 0.227      |
| AveragePolicyStd[4]  | 0.15671    |
| AveragePolicyStd[5]  | 0.2601     |
| AverageReturn        | 1656       |
| MinReturn            | 369.47     |
| MaxReturn            | 1867.9     |
| StdReturn            | 370.1      |
| AverageEpisodeLength | 923.06     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.52     |
| TotalNEpisodes       | 20106      |
| TotalNSamples        | 5.4001e+06 |
| ExplainedVariance    | 0.26466    |
-------------------------------------
[2018-12-22 11:58:06.099686 UTC] Saving snapshot
[2018-12-22 11:58:06.102436 UTC] Starting iteration 1080
[2018-12-22 11:58:06.102590 UTC] Start collecting samples
[2018-12-22 11:58:09.056444 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:09.135848 UTC] Performing policy update
[2018-12-22 11:58:09.136494 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:09.227559 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:10.293178 UTC] Performing line search
[2018-12-22 11:58:10.421178 UTC] Updating baseline
[2018-12-22 11:58:11.697253 UTC] Computing logging information
-------------------------------------
| Iteration            | 1080       |
| ExpectedImprovement  | 0.018836   |
| ActualImprovement    | 0.01743    |
| ImprovementRatio     | 0.92534    |
| MeanKL               | 0.0074189  |
| Entropy              | -0.95749   |
| Perplexity           | 0.38386    |
| AveragePolicyStd     | 0.20975    |
| AveragePolicyStd[0]  | 0.23007    |
| AveragePolicyStd[1]  | 0.21985    |
| AveragePolicyStd[2]  | 0.16375    |
| AveragePolicyStd[3]  | 0.22669    |
| AveragePolicyStd[4]  | 0.15731    |
| AveragePolicyStd[5]  | 0.26084    |
| AverageReturn        | 1662.3     |
| MinReturn            | 369.47     |
| MaxReturn            | 1889.9     |
| StdReturn            | 370.32     |
| AverageEpisodeLength | 925.05     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.28     |
| TotalNEpisodes       | 20112      |
| TotalNSamples        | 5.4061e+06 |
| ExplainedVariance    | -0.0083697 |
-------------------------------------
[2018-12-22 11:58:12.099425 UTC] Saving snapshot
[2018-12-22 11:58:12.107597 UTC] Starting iteration 1081
[2018-12-22 11:58:12.107796 UTC] Start collecting samples
[2018-12-22 11:58:15.035778 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:15.114941 UTC] Performing policy update
[2018-12-22 11:58:15.115541 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:15.205166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:16.269605 UTC] Performing line search
[2018-12-22 11:58:16.397693 UTC] Updating baseline
[2018-12-22 11:58:18.090284 UTC] Computing logging information
-------------------------------------
| Iteration            | 1081       |
| ExpectedImprovement  | 0.017774   |
| ActualImprovement    | 0.016396   |
| ImprovementRatio     | 0.92244    |
| MeanKL               | 0.0075222  |
| Entropy              | -0.96387   |
| Perplexity           | 0.38142    |
| AveragePolicyStd     | 0.20954    |
| AveragePolicyStd[0]  | 0.23034    |
| AveragePolicyStd[1]  | 0.21974    |
| AveragePolicyStd[2]  | 0.16295    |
| AveragePolicyStd[3]  | 0.22668    |
| AveragePolicyStd[4]  | 0.15745    |
| AveragePolicyStd[5]  | 0.26008    |
| AverageReturn        | 1648.9     |
| MinReturn            | 369.47     |
| MaxReturn            | 1889.9     |
| StdReturn            | 384.45     |
| AverageEpisodeLength | 918.75     |
| MinEpisodeLength     | 244        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.85     |
| TotalNEpisodes       | 20117      |
| TotalNSamples        | 5.4104e+06 |
| ExplainedVariance    | 0.10918    |
-------------------------------------
[2018-12-22 11:58:18.496182 UTC] Saving snapshot
[2018-12-22 11:58:18.496442 UTC] Starting iteration 1082
[2018-12-22 11:58:18.496585 UTC] Start collecting samples
[2018-12-22 11:58:21.459632 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:21.541247 UTC] Performing policy update
[2018-12-22 11:58:21.541860 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:21.631219 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:22.681830 UTC] Performing line search
[2018-12-22 11:58:22.809049 UTC] Updating baseline
[2018-12-22 11:58:24.429567 UTC] Computing logging information
-------------------------------------
| Iteration            | 1082       |
| ExpectedImprovement  | 0.016774   |
| ActualImprovement    | 0.016487   |
| ImprovementRatio     | 0.98288    |
| MeanKL               | 0.0076954  |
| Entropy              | -0.97063   |
| Perplexity           | 0.37884    |
| AveragePolicyStd     | 0.20931    |
| AveragePolicyStd[0]  | 0.23031    |
| AveragePolicyStd[1]  | 0.21938    |
| AveragePolicyStd[2]  | 0.16233    |
| AveragePolicyStd[3]  | 0.2269     |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.25949    |
| AverageReturn        | 1614.7     |
| MinReturn            | 151.91     |
| MaxReturn            | 1889.9     |
| StdReturn            | 432.62     |
| AverageEpisodeLength | 900.32     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.02     |
| TotalNEpisodes       | 20125      |
| TotalNSamples        | 5.4166e+06 |
| ExplainedVariance    | 0.36709    |
-------------------------------------
[2018-12-22 11:58:24.834848 UTC] Saving snapshot
[2018-12-22 11:58:24.835095 UTC] Starting iteration 1083
[2018-12-22 11:58:24.835211 UTC] Start collecting samples
[2018-12-22 11:58:27.755984 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:27.835181 UTC] Performing policy update
[2018-12-22 11:58:27.836050 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:27.928466 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:28.989522 UTC] Performing line search
[2018-12-22 11:58:29.118677 UTC] Updating baseline
[2018-12-22 11:58:30.474358 UTC] Computing logging information
-------------------------------------
| Iteration            | 1083       |
| ExpectedImprovement  | 0.017579   |
| ActualImprovement    | 0.016854   |
| ImprovementRatio     | 0.95872    |
| MeanKL               | 0.0076808  |
| Entropy              | -0.97224   |
| Perplexity           | 0.37823    |
| AveragePolicyStd     | 0.20922    |
| AveragePolicyStd[0]  | 0.22939    |
| AveragePolicyStd[1]  | 0.21901    |
| AveragePolicyStd[2]  | 0.16236    |
| AveragePolicyStd[3]  | 0.22713    |
| AveragePolicyStd[4]  | 0.15785    |
| AveragePolicyStd[5]  | 0.2596     |
| AverageReturn        | 1609.8     |
| MinReturn            | 151.91     |
| MaxReturn            | 1889.9     |
| StdReturn            | 432.35     |
| AverageEpisodeLength | 898.04     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.15     |
| TotalNEpisodes       | 20130      |
| TotalNSamples        | 5.4214e+06 |
| ExplainedVariance    | 0.084963   |
-------------------------------------
[2018-12-22 11:58:30.881246 UTC] Saving snapshot
[2018-12-22 11:58:30.881548 UTC] Starting iteration 1084
[2018-12-22 11:58:30.881681 UTC] Start collecting samples
[2018-12-22 11:58:33.792850 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:33.870605 UTC] Performing policy update
[2018-12-22 11:58:33.871376 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:33.964285 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:35.022674 UTC] Performing line search
[2018-12-22 11:58:35.149213 UTC] Updating baseline
[2018-12-22 11:58:36.499487 UTC] Computing logging information
-------------------------------------
| Iteration            | 1084       |
| ExpectedImprovement  | 0.017931   |
| ActualImprovement    | 0.017043   |
| ImprovementRatio     | 0.95048    |
| MeanKL               | 0.0077344  |
| Entropy              | -0.97341   |
| Perplexity           | 0.37779    |
| AveragePolicyStd     | 0.20917    |
| AveragePolicyStd[0]  | 0.22963    |
| AveragePolicyStd[1]  | 0.21841    |
| AveragePolicyStd[2]  | 0.16236    |
| AveragePolicyStd[3]  | 0.22706    |
| AveragePolicyStd[4]  | 0.15798    |
| AveragePolicyStd[5]  | 0.2596     |
| AverageReturn        | 1611.2     |
| MinReturn            | 151.91     |
| MaxReturn            | 1889.9     |
| StdReturn            | 433.01     |
| AverageEpisodeLength | 898.04     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.15     |
| TotalNEpisodes       | 20134      |
| TotalNSamples        | 5.4254e+06 |
| ExplainedVariance    | -0.04764   |
-------------------------------------
[2018-12-22 11:58:36.904986 UTC] Saving snapshot
[2018-12-22 11:58:36.905236 UTC] Starting iteration 1085
[2018-12-22 11:58:36.905371 UTC] Start collecting samples
[2018-12-22 11:58:39.829381 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:39.908615 UTC] Performing policy update
[2018-12-22 11:58:39.909238 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:39.999407 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:41.042752 UTC] Performing line search
[2018-12-22 11:58:41.170251 UTC] Updating baseline
[2018-12-22 11:58:42.905315 UTC] Computing logging information
-------------------------------------
| Iteration            | 1085       |
| ExpectedImprovement  | 0.018791   |
| ActualImprovement    | 0.017726   |
| ImprovementRatio     | 0.94337    |
| MeanKL               | 0.0072672  |
| Entropy              | -0.97916   |
| Perplexity           | 0.37563    |
| AveragePolicyStd     | 0.20895    |
| AveragePolicyStd[0]  | 0.23018    |
| AveragePolicyStd[1]  | 0.21783    |
| AveragePolicyStd[2]  | 0.16229    |
| AveragePolicyStd[3]  | 0.22635    |
| AveragePolicyStd[4]  | 0.158      |
| AveragePolicyStd[5]  | 0.25909    |
| AverageReturn        | 1622.7     |
| MinReturn            | 151.91     |
| MaxReturn            | 1889.9     |
| StdReturn            | 416.43     |
| AverageEpisodeLength | 902.99     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.02     |
| TotalNEpisodes       | 20140      |
| TotalNSamples        | 5.4311e+06 |
| ExplainedVariance    | 0.077322   |
-------------------------------------
[2018-12-22 11:58:43.355885 UTC] Saving snapshot
[2018-12-22 11:58:43.356138 UTC] Starting iteration 1086
[2018-12-22 11:58:43.356270 UTC] Start collecting samples
[2018-12-22 11:58:47.079021 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:47.171286 UTC] Performing policy update
[2018-12-22 11:58:47.172028 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:47.267618 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:48.367917 UTC] Performing line search
[2018-12-22 11:58:48.500495 UTC] Updating baseline
[2018-12-22 11:58:49.965489 UTC] Computing logging information
-------------------------------------
| Iteration            | 1086       |
| ExpectedImprovement  | 0.016883   |
| ActualImprovement    | 0.016002   |
| ImprovementRatio     | 0.94779    |
| MeanKL               | 0.0076046  |
| Entropy              | -0.98192   |
| Perplexity           | 0.37459    |
| AveragePolicyStd     | 0.20887    |
| AveragePolicyStd[0]  | 0.23       |
| AveragePolicyStd[1]  | 0.21772    |
| AveragePolicyStd[2]  | 0.16224    |
| AveragePolicyStd[3]  | 0.22624    |
| AveragePolicyStd[4]  | 0.15782    |
| AveragePolicyStd[5]  | 0.25919    |
| AverageReturn        | 1619.3     |
| MinReturn            | 151.91     |
| MaxReturn            | 1889.9     |
| StdReturn            | 426.27     |
| AverageEpisodeLength | 901.12     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.46     |
| TotalNEpisodes       | 20147      |
| TotalNSamples        | 5.4373e+06 |
| ExplainedVariance    | 0.059561   |
-------------------------------------
[2018-12-22 11:58:50.390758 UTC] Saving snapshot
[2018-12-22 11:58:50.391044 UTC] Starting iteration 1087
[2018-12-22 11:58:50.391159 UTC] Start collecting samples
[2018-12-22 11:58:53.491478 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:53.572942 UTC] Performing policy update
[2018-12-22 11:58:53.573725 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:53.667703 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:58:54.770646 UTC] Performing line search
[2018-12-22 11:58:54.897726 UTC] Updating baseline
[2018-12-22 11:58:56.242440 UTC] Computing logging information
-------------------------------------
| Iteration            | 1087       |
| ExpectedImprovement  | 0.017392   |
| ActualImprovement    | 0.015845   |
| ImprovementRatio     | 0.91105    |
| MeanKL               | 0.0076903  |
| Entropy              | -0.98754   |
| Perplexity           | 0.37249    |
| AveragePolicyStd     | 0.20864    |
| AveragePolicyStd[0]  | 0.22998    |
| AveragePolicyStd[1]  | 0.21786    |
| AveragePolicyStd[2]  | 0.16207    |
| AveragePolicyStd[3]  | 0.22527    |
| AveragePolicyStd[4]  | 0.15799    |
| AveragePolicyStd[5]  | 0.25869    |
| AverageReturn        | 1620.1     |
| MinReturn            | 151.91     |
| MaxReturn            | 1889.9     |
| StdReturn            | 426.65     |
| AverageEpisodeLength | 901.12     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.46     |
| TotalNEpisodes       | 20149      |
| TotalNSamples        | 5.4393e+06 |
| ExplainedVariance    | -0.0076107 |
-------------------------------------
[2018-12-22 11:58:56.646580 UTC] Saving snapshot
[2018-12-22 11:58:56.646831 UTC] Starting iteration 1088
[2018-12-22 11:58:56.646947 UTC] Start collecting samples
[2018-12-22 11:58:59.594890 UTC] Computing input variables for policy optimization
[2018-12-22 11:58:59.674643 UTC] Performing policy update
[2018-12-22 11:58:59.675585 UTC] Computing gradient in Euclidean space
[2018-12-22 11:58:59.764816 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:00.819676 UTC] Performing line search
[2018-12-22 11:59:00.948393 UTC] Updating baseline
[2018-12-22 11:59:02.205917 UTC] Computing logging information
-------------------------------------
| Iteration            | 1088       |
| ExpectedImprovement  | 0.018021   |
| ActualImprovement    | 0.017062   |
| ImprovementRatio     | 0.94681    |
| MeanKL               | 0.0077975  |
| Entropy              | -0.98948   |
| Perplexity           | 0.37177    |
| AveragePolicyStd     | 0.20861    |
| AveragePolicyStd[0]  | 0.22981    |
| AveragePolicyStd[1]  | 0.21824    |
| AveragePolicyStd[2]  | 0.16186    |
| AveragePolicyStd[3]  | 0.22508    |
| AveragePolicyStd[4]  | 0.15771    |
| AveragePolicyStd[5]  | 0.25895    |
| AverageReturn        | 1624.7     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 427.29     |
| AverageEpisodeLength | 902.99     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.5      |
| TotalNEpisodes       | 20156      |
| TotalNSamples        | 5.4463e+06 |
| ExplainedVariance    | 0.00055132 |
-------------------------------------
[2018-12-22 11:59:02.608119 UTC] Saving snapshot
[2018-12-22 11:59:02.608363 UTC] Starting iteration 1089
[2018-12-22 11:59:02.608479 UTC] Start collecting samples
[2018-12-22 11:59:05.544433 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:05.623217 UTC] Performing policy update
[2018-12-22 11:59:05.623871 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:05.713695 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:06.787028 UTC] Performing line search
[2018-12-22 11:59:06.914521 UTC] Updating baseline
[2018-12-22 11:59:08.262138 UTC] Computing logging information
-------------------------------------
| Iteration            | 1089       |
| ExpectedImprovement  | 0.017802   |
| ActualImprovement    | 0.016873   |
| ImprovementRatio     | 0.94782    |
| MeanKL               | 0.0072736  |
| Entropy              | -0.99202   |
| Perplexity           | 0.37083    |
| AveragePolicyStd     | 0.20852    |
| AveragePolicyStd[0]  | 0.22954    |
| AveragePolicyStd[1]  | 0.21771    |
| AveragePolicyStd[2]  | 0.16196    |
| AveragePolicyStd[3]  | 0.22526    |
| AveragePolicyStd[4]  | 0.15755    |
| AveragePolicyStd[5]  | 0.25913    |
| AverageReturn        | 1634.2     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 423.05     |
| AverageEpisodeLength | 907.03     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.55     |
| TotalNEpisodes       | 20162      |
| TotalNSamples        | 5.4523e+06 |
| ExplainedVariance    | 0.072601   |
-------------------------------------
[2018-12-22 11:59:08.673797 UTC] Saving snapshot
[2018-12-22 11:59:08.674051 UTC] Starting iteration 1090
[2018-12-22 11:59:08.674189 UTC] Start collecting samples
[2018-12-22 11:59:11.588652 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:11.668321 UTC] Performing policy update
[2018-12-22 11:59:11.669161 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:11.763225 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:12.826668 UTC] Performing line search
[2018-12-22 11:59:12.954650 UTC] Updating baseline
[2018-12-22 11:59:14.410031 UTC] Computing logging information
-------------------------------------
| Iteration            | 1090       |
| ExpectedImprovement  | 0.017889   |
| ActualImprovement    | 0.016887   |
| ImprovementRatio     | 0.94399    |
| MeanKL               | 0.0077113  |
| Entropy              | -0.98957   |
| Perplexity           | 0.37174    |
| AveragePolicyStd     | 0.20862    |
| AveragePolicyStd[0]  | 0.22914    |
| AveragePolicyStd[1]  | 0.21746    |
| AveragePolicyStd[2]  | 0.16174    |
| AveragePolicyStd[3]  | 0.22576    |
| AveragePolicyStd[4]  | 0.15785    |
| AveragePolicyStd[5]  | 0.25979    |
| AverageReturn        | 1635.4     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 423.59     |
| AverageEpisodeLength | 907.03     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.55     |
| TotalNEpisodes       | 20164      |
| TotalNSamples        | 5.4543e+06 |
| ExplainedVariance    | 0.16332    |
-------------------------------------
[2018-12-22 11:59:14.819296 UTC] Saving snapshot
[2018-12-22 11:59:14.827434 UTC] Starting iteration 1091
[2018-12-22 11:59:14.827651 UTC] Start collecting samples
[2018-12-22 11:59:17.818669 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:17.903067 UTC] Performing policy update
[2018-12-22 11:59:17.903963 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:17.996097 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:19.067032 UTC] Performing line search
[2018-12-22 11:59:19.197451 UTC] Updating baseline
[2018-12-22 11:59:20.728132 UTC] Computing logging information
------------------------------------
| Iteration            | 1091      |
| ExpectedImprovement  | 0.018393  |
| ActualImprovement    | 0.017597  |
| ImprovementRatio     | 0.95672   |
| MeanKL               | 0.007244  |
| Entropy              | -0.98219  |
| Perplexity           | 0.37449   |
| AveragePolicyStd     | 0.20891   |
| AveragePolicyStd[0]  | 0.22951   |
| AveragePolicyStd[1]  | 0.21751   |
| AveragePolicyStd[2]  | 0.16181   |
| AveragePolicyStd[3]  | 0.22612   |
| AveragePolicyStd[4]  | 0.15791   |
| AveragePolicyStd[5]  | 0.26062   |
| AverageReturn        | 1633      |
| MinReturn            | 151.91    |
| MaxReturn            | 1903.3    |
| StdReturn            | 425.61    |
| AverageEpisodeLength | 905.33    |
| MinEpisodeLength     | 136       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 221.13    |
| TotalNEpisodes       | 20172     |
| TotalNSamples        | 5.462e+06 |
| ExplainedVariance    | 0.14592   |
------------------------------------
[2018-12-22 11:59:21.133696 UTC] Saving snapshot
[2018-12-22 11:59:21.133997 UTC] Starting iteration 1092
[2018-12-22 11:59:21.134118 UTC] Start collecting samples
[2018-12-22 11:59:24.109595 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:24.188275 UTC] Performing policy update
[2018-12-22 11:59:24.189015 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:24.278938 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:25.344161 UTC] Performing line search
[2018-12-22 11:59:25.471901 UTC] Updating baseline
[2018-12-22 11:59:26.746640 UTC] Computing logging information
------------------------------------
| Iteration            | 1092      |
| ExpectedImprovement  | 0.017934  |
| ActualImprovement    | 0.01715   |
| ImprovementRatio     | 0.95627   |
| MeanKL               | 0.0084264 |
| Entropy              | -0.98265  |
| Perplexity           | 0.37432   |
| AveragePolicyStd     | 0.20888   |
| AveragePolicyStd[0]  | 0.22929   |
| AveragePolicyStd[1]  | 0.21763   |
| AveragePolicyStd[2]  | 0.16239   |
| AveragePolicyStd[3]  | 0.2256    |
| AveragePolicyStd[4]  | 0.15765   |
| AveragePolicyStd[5]  | 0.2607    |
| AverageReturn        | 1660.7    |
| MinReturn            | 151.91    |
| MaxReturn            | 1903.3    |
| StdReturn            | 407.9     |
| AverageEpisodeLength | 918.23    |
| MinEpisodeLength     | 136       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 211.55    |
| TotalNEpisodes       | 20177     |
| TotalNSamples        | 5.467e+06 |
| ExplainedVariance    | 0.019656  |
------------------------------------
[2018-12-22 11:59:27.155660 UTC] Saving snapshot
[2018-12-22 11:59:27.155907 UTC] Starting iteration 1093
[2018-12-22 11:59:27.156024 UTC] Start collecting samples
[2018-12-22 11:59:30.044806 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:30.122803 UTC] Performing policy update
[2018-12-22 11:59:30.123399 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:30.212193 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:31.266658 UTC] Performing line search
[2018-12-22 11:59:31.393806 UTC] Updating baseline
[2018-12-22 11:59:32.840620 UTC] Computing logging information
-------------------------------------
| Iteration            | 1093       |
| ExpectedImprovement  | 0.016847   |
| ActualImprovement    | 0.016062   |
| ImprovementRatio     | 0.95341    |
| MeanKL               | 0.007487   |
| Entropy              | -0.98233   |
| Perplexity           | 0.37444    |
| AveragePolicyStd     | 0.20888    |
| AveragePolicyStd[0]  | 0.22926    |
| AveragePolicyStd[1]  | 0.21732    |
| AveragePolicyStd[2]  | 0.16275    |
| AveragePolicyStd[3]  | 0.22517    |
| AveragePolicyStd[4]  | 0.15764    |
| AveragePolicyStd[5]  | 0.2611     |
| AverageReturn        | 1668.4     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 395.29     |
| AverageEpisodeLength | 922.29     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.26     |
| TotalNEpisodes       | 20180      |
| TotalNSamples        | 5.4698e+06 |
| ExplainedVariance    | 0.34143    |
-------------------------------------
[2018-12-22 11:59:33.246787 UTC] Saving snapshot
[2018-12-22 11:59:33.247065 UTC] Starting iteration 1094
[2018-12-22 11:59:33.247182 UTC] Start collecting samples
[2018-12-22 11:59:36.220648 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:36.300372 UTC] Performing policy update
[2018-12-22 11:59:36.301158 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:36.391234 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:37.451195 UTC] Performing line search
[2018-12-22 11:59:37.580467 UTC] Updating baseline
[2018-12-22 11:59:38.855938 UTC] Computing logging information
-------------------------------------
| Iteration            | 1094       |
| ExpectedImprovement  | 0.021096   |
| ActualImprovement    | 0.020167   |
| ImprovementRatio     | 0.95593    |
| MeanKL               | 0.0070854  |
| Entropy              | -0.98421   |
| Perplexity           | 0.37373    |
| AveragePolicyStd     | 0.20876    |
| AveragePolicyStd[0]  | 0.22935    |
| AveragePolicyStd[1]  | 0.21761    |
| AveragePolicyStd[2]  | 0.16285    |
| AveragePolicyStd[3]  | 0.22453    |
| AveragePolicyStd[4]  | 0.15788    |
| AveragePolicyStd[5]  | 0.26036    |
| AverageReturn        | 1658.3     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 397.44     |
| AverageEpisodeLength | 917.63     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.27     |
| TotalNEpisodes       | 20187      |
| TotalNSamples        | 5.4764e+06 |
| ExplainedVariance    | 0.090671   |
-------------------------------------
[2018-12-22 11:59:39.263965 UTC] Saving snapshot
[2018-12-22 11:59:39.264213 UTC] Starting iteration 1095
[2018-12-22 11:59:39.264348 UTC] Start collecting samples
[2018-12-22 11:59:42.182837 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:42.262613 UTC] Performing policy update
[2018-12-22 11:59:42.263235 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:42.353705 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:43.415935 UTC] Performing line search
[2018-12-22 11:59:43.544478 UTC] Updating baseline
[2018-12-22 11:59:44.907974 UTC] Computing logging information
-------------------------------------
| Iteration            | 1095       |
| ExpectedImprovement  | 0.016996   |
| ActualImprovement    | 0.01624    |
| ImprovementRatio     | 0.95549    |
| MeanKL               | 0.0075549  |
| Entropy              | -0.98654   |
| Perplexity           | 0.37287    |
| AveragePolicyStd     | 0.2087     |
| AveragePolicyStd[0]  | 0.22919    |
| AveragePolicyStd[1]  | 0.21765    |
| AveragePolicyStd[2]  | 0.1628     |
| AveragePolicyStd[3]  | 0.2244     |
| AveragePolicyStd[4]  | 0.15768    |
| AveragePolicyStd[5]  | 0.26046    |
| AverageReturn        | 1680.7     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 370.79     |
| AverageEpisodeLength | 929.7      |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.52     |
| TotalNEpisodes       | 20191      |
| TotalNSamples        | 5.4804e+06 |
| ExplainedVariance    | 0.0089526  |
-------------------------------------
[2018-12-22 11:59:45.314580 UTC] Saving snapshot
[2018-12-22 11:59:45.314842 UTC] Starting iteration 1096
[2018-12-22 11:59:45.314964 UTC] Start collecting samples
[2018-12-22 11:59:48.222930 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:48.302811 UTC] Performing policy update
[2018-12-22 11:59:48.303480 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:48.393976 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:49.449686 UTC] Performing line search
[2018-12-22 11:59:49.578345 UTC] Updating baseline
[2018-12-22 11:59:50.754859 UTC] Computing logging information
-------------------------------------
| Iteration            | 1096       |
| ExpectedImprovement  | 0.018256   |
| ActualImprovement    | 0.017166   |
| ImprovementRatio     | 0.94029    |
| MeanKL               | 0.0081511  |
| Entropy              | -0.99035   |
| Perplexity           | 0.37145    |
| AveragePolicyStd     | 0.20858    |
| AveragePolicyStd[0]  | 0.22874    |
| AveragePolicyStd[1]  | 0.21723    |
| AveragePolicyStd[2]  | 0.16251    |
| AveragePolicyStd[3]  | 0.22451    |
| AveragePolicyStd[4]  | 0.15772    |
| AveragePolicyStd[5]  | 0.26075    |
| AverageReturn        | 1700.7     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 348.71     |
| AverageEpisodeLength | 939.61     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.04     |
| TotalNEpisodes       | 20196      |
| TotalNSamples        | 5.4854e+06 |
| ExplainedVariance    | -0.049898  |
-------------------------------------
[2018-12-22 11:59:51.157399 UTC] Saving snapshot
[2018-12-22 11:59:51.157706 UTC] Starting iteration 1097
[2018-12-22 11:59:51.157858 UTC] Start collecting samples
[2018-12-22 11:59:54.091474 UTC] Computing input variables for policy optimization
[2018-12-22 11:59:54.170635 UTC] Performing policy update
[2018-12-22 11:59:54.171401 UTC] Computing gradient in Euclidean space
[2018-12-22 11:59:54.261435 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 11:59:55.322936 UTC] Performing line search
[2018-12-22 11:59:55.450718 UTC] Updating baseline
[2018-12-22 11:59:56.792469 UTC] Computing logging information
-------------------------------------
| Iteration            | 1097       |
| ExpectedImprovement  | 0.015973   |
| ActualImprovement    | 0.014808   |
| ImprovementRatio     | 0.92705    |
| MeanKL               | 0.0074426  |
| Entropy              | -0.98617   |
| Perplexity           | 0.373      |
| AveragePolicyStd     | 0.20876    |
| AveragePolicyStd[0]  | 0.22905    |
| AveragePolicyStd[1]  | 0.21827    |
| AveragePolicyStd[2]  | 0.16228    |
| AveragePolicyStd[3]  | 0.22485    |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.26063    |
| AverageReturn        | 1712.9     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 328.11     |
| AverageEpisodeLength | 946.47     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.86     |
| TotalNEpisodes       | 20202      |
| TotalNSamples        | 5.4914e+06 |
| ExplainedVariance    | -0.0044043 |
-------------------------------------
[2018-12-22 11:59:57.197078 UTC] Saving snapshot
[2018-12-22 11:59:57.197366 UTC] Starting iteration 1098
[2018-12-22 11:59:57.197483 UTC] Start collecting samples
[2018-12-22 12:00:00.110212 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:00.186884 UTC] Performing policy update
[2018-12-22 12:00:00.187831 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:00.278737 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:01.335775 UTC] Performing line search
[2018-12-22 12:00:01.465455 UTC] Updating baseline
[2018-12-22 12:00:03.181336 UTC] Computing logging information
-------------------------------------
| Iteration            | 1098       |
| ExpectedImprovement  | 0.016359   |
| ActualImprovement    | 0.01551    |
| ImprovementRatio     | 0.94811    |
| MeanKL               | 0.0073852  |
| Entropy              | -0.98771   |
| Perplexity           | 0.37243    |
| AveragePolicyStd     | 0.20873    |
| AveragePolicyStd[0]  | 0.22958    |
| AveragePolicyStd[1]  | 0.21839    |
| AveragePolicyStd[2]  | 0.16214    |
| AveragePolicyStd[3]  | 0.22391    |
| AveragePolicyStd[4]  | 0.15729    |
| AveragePolicyStd[5]  | 0.2611     |
| AverageReturn        | 1725.6     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 312.31     |
| AverageEpisodeLength | 952.86     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.7      |
| TotalNEpisodes       | 20206      |
| TotalNSamples        | 5.4954e+06 |
| ExplainedVariance    | -0.015111  |
-------------------------------------
[2018-12-22 12:00:03.592186 UTC] Saving snapshot
[2018-12-22 12:00:03.592482 UTC] Starting iteration 1099
[2018-12-22 12:00:03.592621 UTC] Start collecting samples
[2018-12-22 12:00:06.563335 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:06.642513 UTC] Performing policy update
[2018-12-22 12:00:06.643547 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:06.734071 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:07.797904 UTC] Performing line search
[2018-12-22 12:00:07.929901 UTC] Updating baseline
[2018-12-22 12:00:09.372312 UTC] Computing logging information
-------------------------------------
| Iteration            | 1099       |
| ExpectedImprovement  | 0.017943   |
| ActualImprovement    | 0.016548   |
| ImprovementRatio     | 0.92227    |
| MeanKL               | 0.0079825  |
| Entropy              | -0.98996   |
| Perplexity           | 0.37159    |
| AveragePolicyStd     | 0.20865    |
| AveragePolicyStd[0]  | 0.22954    |
| AveragePolicyStd[1]  | 0.21813    |
| AveragePolicyStd[2]  | 0.16226    |
| AveragePolicyStd[3]  | 0.22387    |
| AveragePolicyStd[4]  | 0.15713    |
| AveragePolicyStd[5]  | 0.26097    |
| AverageReturn        | 1723.3     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 311.6      |
| AverageEpisodeLength | 952.86     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.7      |
| TotalNEpisodes       | 20212      |
| TotalNSamples        | 5.5014e+06 |
| ExplainedVariance    | -0.021933  |
-------------------------------------
[2018-12-22 12:00:09.781512 UTC] Saving snapshot
[2018-12-22 12:00:09.781873 UTC] Starting iteration 1100
[2018-12-22 12:00:09.782014 UTC] Start collecting samples
[2018-12-22 12:00:12.731885 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:12.810729 UTC] Performing policy update
[2018-12-22 12:00:12.811594 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:12.902536 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:13.958585 UTC] Performing line search
[2018-12-22 12:00:14.086003 UTC] Updating baseline
[2018-12-22 12:00:15.506919 UTC] Computing logging information
-------------------------------------
| Iteration            | 1100       |
| ExpectedImprovement  | 0.016459   |
| ActualImprovement    | 0.015558   |
| ImprovementRatio     | 0.94522    |
| MeanKL               | 0.0079971  |
| Entropy              | -0.99199   |
| Perplexity           | 0.37084    |
| AveragePolicyStd     | 0.20863    |
| AveragePolicyStd[0]  | 0.2301     |
| AveragePolicyStd[1]  | 0.2177     |
| AveragePolicyStd[2]  | 0.16231    |
| AveragePolicyStd[3]  | 0.22429    |
| AveragePolicyStd[4]  | 0.15643    |
| AveragePolicyStd[5]  | 0.26092    |
| AverageReturn        | 1735.8     |
| MinReturn            | 151.91     |
| MaxReturn            | 1903.3     |
| StdReturn            | 290.38     |
| AverageEpisodeLength | 959.16     |
| MinEpisodeLength     | 136        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.77     |
| TotalNEpisodes       | 20218      |
| TotalNSamples        | 5.5074e+06 |
| ExplainedVariance    | 0.0085258  |
-------------------------------------
[2018-12-22 12:00:15.914480 UTC] Saving snapshot
[2018-12-22 12:00:15.925707 UTC] Starting iteration 1101
[2018-12-22 12:00:15.925946 UTC] Start collecting samples
[2018-12-22 12:00:18.814794 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:18.892393 UTC] Performing policy update
[2018-12-22 12:00:18.893204 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:18.981933 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:20.041676 UTC] Performing line search
[2018-12-22 12:00:20.169224 UTC] Updating baseline
[2018-12-22 12:00:21.904190 UTC] Computing logging information
------------------------------------
| Iteration            | 1101      |
| ExpectedImprovement  | 0.019285  |
| ActualImprovement    | 0.017676  |
| ImprovementRatio     | 0.91653   |
| MeanKL               | 0.0072213 |
| Entropy              | -0.99903  |
| Perplexity           | 0.36824   |
| AveragePolicyStd     | 0.20838   |
| AveragePolicyStd[0]  | 0.2305    |
| AveragePolicyStd[1]  | 0.21696   |
| AveragePolicyStd[2]  | 0.16237   |
| AveragePolicyStd[3]  | 0.2241    |
| AveragePolicyStd[4]  | 0.15598   |
| AveragePolicyStd[5]  | 0.2604    |
| AverageReturn        | 1731      |
| MinReturn            | 151.91    |
| MaxReturn            | 1903.3    |
| StdReturn            | 296.25    |
| AverageEpisodeLength | 956.78    |
| MinEpisodeLength     | 136       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 154.28    |
| TotalNEpisodes       | 20221     |
| TotalNSamples        | 5.51e+06  |
| ExplainedVariance    | 0.11914   |
------------------------------------
[2018-12-22 12:00:22.310127 UTC] Saving snapshot
[2018-12-22 12:00:22.310378 UTC] Starting iteration 1102
[2018-12-22 12:00:22.310514 UTC] Start collecting samples
[2018-12-22 12:00:25.269676 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:25.349513 UTC] Performing policy update
[2018-12-22 12:00:25.350344 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:25.439473 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:26.501536 UTC] Performing line search
[2018-12-22 12:00:26.630689 UTC] Updating baseline
[2018-12-22 12:00:27.911487 UTC] Computing logging information
-------------------------------------
| Iteration            | 1102       |
| ExpectedImprovement  | 0.015371   |
| ActualImprovement    | 0.014302   |
| ImprovementRatio     | 0.93047    |
| MeanKL               | 0.007504   |
| Entropy              | -0.99747   |
| Perplexity           | 0.36881    |
| AveragePolicyStd     | 0.20844    |
| AveragePolicyStd[0]  | 0.23056    |
| AveragePolicyStd[1]  | 0.21642    |
| AveragePolicyStd[2]  | 0.16289    |
| AveragePolicyStd[3]  | 0.22357    |
| AveragePolicyStd[4]  | 0.15593    |
| AveragePolicyStd[5]  | 0.26126    |
| AverageReturn        | 1757.6     |
| MinReturn            | 249.12     |
| MaxReturn            | 1903.3     |
| StdReturn            | 215.85     |
| AverageEpisodeLength | 971.42     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.17     |
| TotalNEpisodes       | 20229      |
| TotalNSamples        | 5.5175e+06 |
| ExplainedVariance    | 0.082935   |
-------------------------------------
[2018-12-22 12:00:28.324812 UTC] Saving snapshot
[2018-12-22 12:00:28.325058 UTC] Starting iteration 1103
[2018-12-22 12:00:28.325174 UTC] Start collecting samples
[2018-12-22 12:00:31.219381 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:31.296142 UTC] Performing policy update
[2018-12-22 12:00:31.296757 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:31.387104 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:32.454811 UTC] Performing line search
[2018-12-22 12:00:32.583731 UTC] Updating baseline
[2018-12-22 12:00:34.031154 UTC] Computing logging information
-------------------------------------
| Iteration            | 1103       |
| ExpectedImprovement  | 0.016575   |
| ActualImprovement    | 0.015904   |
| ImprovementRatio     | 0.95951    |
| MeanKL               | 0.0086372  |
| Entropy              | -1.0047    |
| Perplexity           | 0.36617    |
| AveragePolicyStd     | 0.20812    |
| AveragePolicyStd[0]  | 0.22972    |
| AveragePolicyStd[1]  | 0.2157     |
| AveragePolicyStd[2]  | 0.16337    |
| AveragePolicyStd[3]  | 0.22279    |
| AveragePolicyStd[4]  | 0.15611    |
| AveragePolicyStd[5]  | 0.26104    |
| AverageReturn        | 1757.2     |
| MinReturn            | 249.12     |
| MaxReturn            | 1903.3     |
| StdReturn            | 215.66     |
| AverageEpisodeLength | 971.42     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.17     |
| TotalNEpisodes       | 20231      |
| TotalNSamples        | 5.5195e+06 |
| ExplainedVariance    | 0.065059   |
-------------------------------------
[2018-12-22 12:00:34.438482 UTC] Saving snapshot
[2018-12-22 12:00:34.438738 UTC] Starting iteration 1104
[2018-12-22 12:00:34.438857 UTC] Start collecting samples
[2018-12-22 12:00:37.399452 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:37.479964 UTC] Performing policy update
[2018-12-22 12:00:37.480705 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:37.571300 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:38.629931 UTC] Performing line search
[2018-12-22 12:00:38.757380 UTC] Updating baseline
[2018-12-22 12:00:40.117173 UTC] Computing logging information
-------------------------------------
| Iteration            | 1104       |
| ExpectedImprovement  | 0.018293   |
| ActualImprovement    | 0.01748    |
| ImprovementRatio     | 0.95554    |
| MeanKL               | 0.0083247  |
| Entropy              | -0.99991   |
| Perplexity           | 0.36791    |
| AveragePolicyStd     | 0.20825    |
| AveragePolicyStd[0]  | 0.2297     |
| AveragePolicyStd[1]  | 0.2161     |
| AveragePolicyStd[2]  | 0.16341    |
| AveragePolicyStd[3]  | 0.22218    |
| AveragePolicyStd[4]  | 0.15685    |
| AveragePolicyStd[5]  | 0.26126    |
| AverageReturn        | 1748.2     |
| MinReturn            | 249.12     |
| MaxReturn            | 1903.3     |
| StdReturn            | 226.48     |
| AverageEpisodeLength | 967.12     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.44     |
| TotalNEpisodes       | 20239      |
| TotalNSamples        | 5.5268e+06 |
| ExplainedVariance    | 0.18164    |
-------------------------------------
[2018-12-22 12:00:40.522924 UTC] Saving snapshot
[2018-12-22 12:00:40.523173 UTC] Starting iteration 1105
[2018-12-22 12:00:40.523302 UTC] Start collecting samples
[2018-12-22 12:00:43.439463 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:43.517925 UTC] Performing policy update
[2018-12-22 12:00:43.518514 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:43.608605 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:44.679581 UTC] Performing line search
[2018-12-22 12:00:44.807329 UTC] Updating baseline
[2018-12-22 12:00:46.337025 UTC] Computing logging information
-------------------------------------
| Iteration            | 1105       |
| ExpectedImprovement  | 0.019312   |
| ActualImprovement    | 0.018734   |
| ImprovementRatio     | 0.9701     |
| MeanKL               | 0.0073752  |
| Entropy              | -0.99509   |
| Perplexity           | 0.36969    |
| AveragePolicyStd     | 0.20842    |
| AveragePolicyStd[0]  | 0.22991    |
| AveragePolicyStd[1]  | 0.21589    |
| AveragePolicyStd[2]  | 0.16366    |
| AveragePolicyStd[3]  | 0.22296    |
| AveragePolicyStd[4]  | 0.15678    |
| AveragePolicyStd[5]  | 0.2613     |
| AverageReturn        | 1764.4     |
| MinReturn            | 927.79     |
| MaxReturn            | 1903.3     |
| StdReturn            | 169.42     |
| AverageEpisodeLength | 975.43     |
| MinEpisodeLength     | 541        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.172     |
| TotalNEpisodes       | 20243      |
| TotalNSamples        | 5.5308e+06 |
| ExplainedVariance    | -0.057484  |
-------------------------------------
[2018-12-22 12:00:46.745957 UTC] Saving snapshot
[2018-12-22 12:00:46.746214 UTC] Starting iteration 1106
[2018-12-22 12:00:46.746349 UTC] Start collecting samples
[2018-12-22 12:00:49.628188 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:49.706258 UTC] Performing policy update
[2018-12-22 12:00:49.706872 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:49.799926 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:50.860261 UTC] Performing line search
[2018-12-22 12:00:50.986198 UTC] Updating baseline
[2018-12-22 12:00:52.597320 UTC] Computing logging information
-------------------------------------
| Iteration            | 1106       |
| ExpectedImprovement  | 0.017299   |
| ActualImprovement    | 0.016437   |
| ImprovementRatio     | 0.95018    |
| MeanKL               | 0.0075156  |
| Entropy              | -1.0015    |
| Perplexity           | 0.36732    |
| AveragePolicyStd     | 0.20819    |
| AveragePolicyStd[0]  | 0.22937    |
| AveragePolicyStd[1]  | 0.21605    |
| AveragePolicyStd[2]  | 0.16358    |
| AveragePolicyStd[3]  | 0.22266    |
| AveragePolicyStd[4]  | 0.15655    |
| AveragePolicyStd[5]  | 0.26092    |
| AverageReturn        | 1766.3     |
| MinReturn            | 927.79     |
| MaxReturn            | 1903.3     |
| StdReturn            | 170.12     |
| AverageEpisodeLength | 975.43     |
| MinEpisodeLength     | 541        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.172     |
| TotalNEpisodes       | 20246      |
| TotalNSamples        | 5.5338e+06 |
| ExplainedVariance    | 0.11982    |
-------------------------------------
[2018-12-22 12:00:53.000023 UTC] Saving snapshot
[2018-12-22 12:00:53.000271 UTC] Starting iteration 1107
[2018-12-22 12:00:53.000399 UTC] Start collecting samples
[2018-12-22 12:00:56.008713 UTC] Computing input variables for policy optimization
[2018-12-22 12:00:56.091699 UTC] Performing policy update
[2018-12-22 12:00:56.092295 UTC] Computing gradient in Euclidean space
[2018-12-22 12:00:56.183066 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:00:57.239183 UTC] Performing line search
[2018-12-22 12:00:57.366662 UTC] Updating baseline
[2018-12-22 12:00:58.700637 UTC] Computing logging information
-------------------------------------
| Iteration            | 1107       |
| ExpectedImprovement  | 0.018162   |
| ActualImprovement    | 0.017311   |
| ImprovementRatio     | 0.95312    |
| MeanKL               | 0.0076957  |
| Entropy              | -0.99694   |
| Perplexity           | 0.36901    |
| AveragePolicyStd     | 0.20835    |
| AveragePolicyStd[0]  | 0.22912    |
| AveragePolicyStd[1]  | 0.21631    |
| AveragePolicyStd[2]  | 0.16366    |
| AveragePolicyStd[3]  | 0.2223     |
| AveragePolicyStd[4]  | 0.15685    |
| AveragePolicyStd[5]  | 0.26188    |
| AverageReturn        | 1741       |
| MinReturn            | 582.26     |
| MaxReturn            | 1901.2     |
| StdReturn            | 233.04     |
| AverageEpisodeLength | 962.77     |
| MinEpisodeLength     | 350        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.06     |
| TotalNEpisodes       | 20257      |
| TotalNSamples        | 5.5436e+06 |
| ExplainedVariance    | 0.22617    |
-------------------------------------
[2018-12-22 12:00:59.101393 UTC] Saving snapshot
[2018-12-22 12:00:59.101670 UTC] Starting iteration 1108
[2018-12-22 12:00:59.101820 UTC] Start collecting samples
[2018-12-22 12:01:02.020908 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:02.100332 UTC] Performing policy update
[2018-12-22 12:01:02.101059 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:02.190011 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:03.235821 UTC] Performing line search
[2018-12-22 12:01:03.363938 UTC] Updating baseline
[2018-12-22 12:01:04.630166 UTC] Computing logging information
-------------------------------------
| Iteration            | 1108       |
| ExpectedImprovement  | 0.020002   |
| ActualImprovement    | 0.018802   |
| ImprovementRatio     | 0.94003    |
| MeanKL               | 0.0076752  |
| Entropy              | -1.0045    |
| Perplexity           | 0.36621    |
| AveragePolicyStd     | 0.20809    |
| AveragePolicyStd[0]  | 0.22906    |
| AveragePolicyStd[1]  | 0.21557    |
| AveragePolicyStd[2]  | 0.16368    |
| AveragePolicyStd[3]  | 0.22184    |
| AveragePolicyStd[4]  | 0.15661    |
| AveragePolicyStd[5]  | 0.26176    |
| AverageReturn        | 1734.2     |
| MinReturn            | 582.26     |
| MaxReturn            | 1901.2     |
| StdReturn            | 240.85     |
| AverageEpisodeLength | 959.05     |
| MinEpisodeLength     | 350        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.49     |
| TotalNEpisodes       | 20260      |
| TotalNSamples        | 5.5462e+06 |
| ExplainedVariance    | 0.165      |
-------------------------------------
[2018-12-22 12:01:05.038348 UTC] Saving snapshot
[2018-12-22 12:01:05.038618 UTC] Starting iteration 1109
[2018-12-22 12:01:05.038749 UTC] Start collecting samples
[2018-12-22 12:01:07.956550 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:08.034372 UTC] Performing policy update
[2018-12-22 12:01:08.034966 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:08.124134 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:09.182826 UTC] Performing line search
[2018-12-22 12:01:09.309407 UTC] Updating baseline
[2018-12-22 12:01:10.548170 UTC] Computing logging information
-------------------------------------
| Iteration            | 1109       |
| ExpectedImprovement  | 0.018588   |
| ActualImprovement    | 0.017263   |
| ImprovementRatio     | 0.92874    |
| MeanKL               | 0.0075885  |
| Entropy              | -1.0078    |
| Perplexity           | 0.36503    |
| AveragePolicyStd     | 0.20796    |
| AveragePolicyStd[0]  | 0.22953    |
| AveragePolicyStd[1]  | 0.2152     |
| AveragePolicyStd[2]  | 0.16374    |
| AveragePolicyStd[3]  | 0.22137    |
| AveragePolicyStd[4]  | 0.15658    |
| AveragePolicyStd[5]  | 0.26134    |
| AverageReturn        | 1713.7     |
| MinReturn            | 370.09     |
| MaxReturn            | 1901.2     |
| StdReturn            | 278.05     |
| AverageEpisodeLength | 948.92     |
| MinEpisodeLength     | 230        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.07     |
| TotalNEpisodes       | 20264      |
| TotalNSamples        | 5.5492e+06 |
| ExplainedVariance    | 0.30436    |
-------------------------------------
[2018-12-22 12:01:10.945396 UTC] Saving snapshot
[2018-12-22 12:01:10.945667 UTC] Starting iteration 1110
[2018-12-22 12:01:10.945830 UTC] Start collecting samples
[2018-12-22 12:01:13.920920 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:14.003868 UTC] Performing policy update
[2018-12-22 12:01:14.004559 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:14.094661 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:15.159029 UTC] Performing line search
[2018-12-22 12:01:15.286711 UTC] Updating baseline
[2018-12-22 12:01:16.630108 UTC] Computing logging information
-------------------------------------
| Iteration            | 1110       |
| ExpectedImprovement  | 0.019124   |
| ActualImprovement    | 0.018348   |
| ImprovementRatio     | 0.95943    |
| MeanKL               | 0.0074832  |
| Entropy              | -1.0098    |
| Perplexity           | 0.3643     |
| AveragePolicyStd     | 0.20791    |
| AveragePolicyStd[0]  | 0.22959    |
| AveragePolicyStd[1]  | 0.21565    |
| AveragePolicyStd[2]  | 0.16392    |
| AveragePolicyStd[3]  | 0.22085    |
| AveragePolicyStd[4]  | 0.1561     |
| AveragePolicyStd[5]  | 0.26134    |
| AverageReturn        | 1719       |
| MinReturn            | 370.09     |
| MaxReturn            | 1901.2     |
| StdReturn            | 274.74     |
| AverageEpisodeLength | 951.47     |
| MinEpisodeLength     | 230        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.71     |
| TotalNEpisodes       | 20272      |
| TotalNSamples        | 5.5572e+06 |
| ExplainedVariance    | -0.0024587 |
-------------------------------------
[2018-12-22 12:01:17.036184 UTC] Saving snapshot
[2018-12-22 12:01:17.044304 UTC] Starting iteration 1111
[2018-12-22 12:01:17.044507 UTC] Start collecting samples
[2018-12-22 12:01:19.970810 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:20.048063 UTC] Performing policy update
[2018-12-22 12:01:20.048734 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:20.139371 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:21.205650 UTC] Performing line search
[2018-12-22 12:01:21.333232 UTC] Updating baseline
[2018-12-22 12:01:22.770083 UTC] Computing logging information
-------------------------------------
| Iteration            | 1111       |
| ExpectedImprovement  | 0.017271   |
| ActualImprovement    | 0.015918   |
| ImprovementRatio     | 0.92164    |
| MeanKL               | 0.008112   |
| Entropy              | -1.02      |
| Perplexity           | 0.36058    |
| AveragePolicyStd     | 0.20752    |
| AveragePolicyStd[0]  | 0.22941    |
| AveragePolicyStd[1]  | 0.21536    |
| AveragePolicyStd[2]  | 0.16381    |
| AveragePolicyStd[3]  | 0.22046    |
| AveragePolicyStd[4]  | 0.15589    |
| AveragePolicyStd[5]  | 0.26021    |
| AverageReturn        | 1718.3     |
| MinReturn            | 370.09     |
| MaxReturn            | 1898.9     |
| StdReturn            | 274.35     |
| AverageEpisodeLength | 951.47     |
| MinEpisodeLength     | 230        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.71     |
| TotalNEpisodes       | 20276      |
| TotalNSamples        | 5.5612e+06 |
| ExplainedVariance    | 0.0012718  |
-------------------------------------
[2018-12-22 12:01:23.178937 UTC] Saving snapshot
[2018-12-22 12:01:23.179223 UTC] Starting iteration 1112
[2018-12-22 12:01:23.179360 UTC] Start collecting samples
[2018-12-22 12:01:26.147492 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:26.226065 UTC] Performing policy update
[2018-12-22 12:01:26.226970 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:26.316078 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:27.369851 UTC] Performing line search
[2018-12-22 12:01:27.496556 UTC] Updating baseline
[2018-12-22 12:01:28.839954 UTC] Computing logging information
------------------------------------
| Iteration            | 1112      |
| ExpectedImprovement  | 0.018467  |
| ActualImprovement    | 0.017998  |
| ImprovementRatio     | 0.9746    |
| MeanKL               | 0.0080768 |
| Entropy              | -1.0212   |
| Perplexity           | 0.36017   |
| AveragePolicyStd     | 0.20749   |
| AveragePolicyStd[0]  | 0.22972   |
| AveragePolicyStd[1]  | 0.21541   |
| AveragePolicyStd[2]  | 0.16327   |
| AveragePolicyStd[3]  | 0.22019   |
| AveragePolicyStd[4]  | 0.15616   |
| AveragePolicyStd[5]  | 0.26021   |
| AverageReturn        | 1725.5    |
| MinReturn            | 370.09    |
| MaxReturn            | 1898.9    |
| StdReturn            | 270.15    |
| AverageEpisodeLength | 954.72    |
| MinEpisodeLength     | 230       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 142.34    |
| TotalNEpisodes       | 20281     |
| TotalNSamples        | 5.566e+06 |
| ExplainedVariance    | 0.11846   |
------------------------------------
[2018-12-22 12:01:29.245167 UTC] Saving snapshot
[2018-12-22 12:01:29.245413 UTC] Starting iteration 1113
[2018-12-22 12:01:29.245549 UTC] Start collecting samples
[2018-12-22 12:01:32.203872 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:32.283808 UTC] Performing policy update
[2018-12-22 12:01:32.284567 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:32.373447 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:33.425114 UTC] Performing line search
[2018-12-22 12:01:33.552244 UTC] Updating baseline
[2018-12-22 12:01:34.734597 UTC] Computing logging information
-------------------------------------
| Iteration            | 1113       |
| ExpectedImprovement  | 0.01647    |
| ActualImprovement    | 0.015592   |
| ImprovementRatio     | 0.94674    |
| MeanKL               | 0.0084987  |
| Entropy              | -1.0197    |
| Perplexity           | 0.36072    |
| AveragePolicyStd     | 0.20755    |
| AveragePolicyStd[0]  | 0.22973    |
| AveragePolicyStd[1]  | 0.21577    |
| AveragePolicyStd[2]  | 0.16329    |
| AveragePolicyStd[3]  | 0.22024    |
| AveragePolicyStd[4]  | 0.15615    |
| AveragePolicyStd[5]  | 0.2601     |
| AverageReturn        | 1712.4     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 313.34     |
| AverageEpisodeLength | 947.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.25     |
| TotalNEpisodes       | 20288      |
| TotalNSamples        | 5.5721e+06 |
| ExplainedVariance    | 0.10286    |
-------------------------------------
[2018-12-22 12:01:35.134792 UTC] Saving snapshot
[2018-12-22 12:01:35.135044 UTC] Starting iteration 1114
[2018-12-22 12:01:35.135161 UTC] Start collecting samples
[2018-12-22 12:01:38.039821 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:38.117201 UTC] Performing policy update
[2018-12-22 12:01:38.117900 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:38.205293 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:39.263918 UTC] Performing line search
[2018-12-22 12:01:39.394355 UTC] Updating baseline
[2018-12-22 12:01:41.000984 UTC] Computing logging information
-------------------------------------
| Iteration            | 1114       |
| ExpectedImprovement  | 0.018558   |
| ActualImprovement    | 0.017268   |
| ImprovementRatio     | 0.93047    |
| MeanKL               | 0.0080178  |
| Entropy              | -1.022     |
| Perplexity           | 0.35989    |
| AveragePolicyStd     | 0.20743    |
| AveragePolicyStd[0]  | 0.22961    |
| AveragePolicyStd[1]  | 0.21596    |
| AveragePolicyStd[2]  | 0.16351    |
| AveragePolicyStd[3]  | 0.21963    |
| AveragePolicyStd[4]  | 0.1562     |
| AveragePolicyStd[5]  | 0.25969    |
| AverageReturn        | 1712.4     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 313.48     |
| AverageEpisodeLength | 947.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.25     |
| TotalNEpisodes       | 20292      |
| TotalNSamples        | 5.5761e+06 |
| ExplainedVariance    | 0.00026586 |
-------------------------------------
[2018-12-22 12:01:41.411324 UTC] Saving snapshot
[2018-12-22 12:01:41.411584 UTC] Starting iteration 1115
[2018-12-22 12:01:41.411724 UTC] Start collecting samples
[2018-12-22 12:01:44.337617 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:44.414318 UTC] Performing policy update
[2018-12-22 12:01:44.414946 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:44.505130 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:45.555665 UTC] Performing line search
[2018-12-22 12:01:45.682376 UTC] Updating baseline
[2018-12-22 12:01:46.916381 UTC] Computing logging information
-------------------------------------
| Iteration            | 1115       |
| ExpectedImprovement  | 0.017016   |
| ActualImprovement    | 0.016221   |
| ImprovementRatio     | 0.95332    |
| MeanKL               | 0.0078057  |
| Entropy              | -1.0322    |
| Perplexity           | 0.35622    |
| AveragePolicyStd     | 0.20712    |
| AveragePolicyStd[0]  | 0.22924    |
| AveragePolicyStd[1]  | 0.21538    |
| AveragePolicyStd[2]  | 0.16315    |
| AveragePolicyStd[3]  | 0.21913    |
| AveragePolicyStd[4]  | 0.15574    |
| AveragePolicyStd[5]  | 0.2601     |
| AverageReturn        | 1712       |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 313.35     |
| AverageEpisodeLength | 947.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.25     |
| TotalNEpisodes       | 20296      |
| TotalNSamples        | 5.5801e+06 |
| ExplainedVariance    | -0.020063  |
-------------------------------------
[2018-12-22 12:01:47.315793 UTC] Saving snapshot
[2018-12-22 12:01:47.316063 UTC] Starting iteration 1116
[2018-12-22 12:01:47.316177 UTC] Start collecting samples
[2018-12-22 12:01:50.259298 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:50.339948 UTC] Performing policy update
[2018-12-22 12:01:50.340761 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:50.430287 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:51.489003 UTC] Performing line search
[2018-12-22 12:01:51.615902 UTC] Updating baseline
[2018-12-22 12:01:53.448654 UTC] Computing logging information
-------------------------------------
| Iteration            | 1116       |
| ExpectedImprovement  | 0.018138   |
| ActualImprovement    | 0.017225   |
| ImprovementRatio     | 0.94964    |
| MeanKL               | 0.0082411  |
| Entropy              | -1.0367    |
| Perplexity           | 0.35462    |
| AveragePolicyStd     | 0.20696    |
| AveragePolicyStd[0]  | 0.22866    |
| AveragePolicyStd[1]  | 0.21512    |
| AveragePolicyStd[2]  | 0.16322    |
| AveragePolicyStd[3]  | 0.21875    |
| AveragePolicyStd[4]  | 0.15566    |
| AveragePolicyStd[5]  | 0.26036    |
| AverageReturn        | 1712.2     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 313.28     |
| AverageEpisodeLength | 947.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.25     |
| TotalNEpisodes       | 20303      |
| TotalNSamples        | 5.5871e+06 |
| ExplainedVariance    | 0.00022081 |
-------------------------------------
[2018-12-22 12:01:53.860720 UTC] Saving snapshot
[2018-12-22 12:01:53.860973 UTC] Starting iteration 1117
[2018-12-22 12:01:53.861106 UTC] Start collecting samples
[2018-12-22 12:01:56.816727 UTC] Computing input variables for policy optimization
[2018-12-22 12:01:56.895809 UTC] Performing policy update
[2018-12-22 12:01:56.896629 UTC] Computing gradient in Euclidean space
[2018-12-22 12:01:56.986616 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:01:58.055975 UTC] Performing line search
[2018-12-22 12:01:58.183775 UTC] Updating baseline
[2018-12-22 12:01:59.364039 UTC] Computing logging information
-------------------------------------
| Iteration            | 1117       |
| ExpectedImprovement  | 0.016887   |
| ActualImprovement    | 0.016385   |
| ImprovementRatio     | 0.97029    |
| MeanKL               | 0.0082029  |
| Entropy              | -1.0344    |
| Perplexity           | 0.35544    |
| AveragePolicyStd     | 0.20703    |
| AveragePolicyStd[0]  | 0.22843    |
| AveragePolicyStd[1]  | 0.21517    |
| AveragePolicyStd[2]  | 0.16328    |
| AveragePolicyStd[3]  | 0.21891    |
| AveragePolicyStd[4]  | 0.15589    |
| AveragePolicyStd[5]  | 0.26051    |
| AverageReturn        | 1711.9     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 313.2      |
| AverageEpisodeLength | 947.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.25     |
| TotalNEpisodes       | 20307      |
| TotalNSamples        | 5.5911e+06 |
| ExplainedVariance    | 0.043452   |
-------------------------------------
[2018-12-22 12:01:59.776875 UTC] Saving snapshot
[2018-12-22 12:01:59.777113 UTC] Starting iteration 1118
[2018-12-22 12:01:59.777230 UTC] Start collecting samples
[2018-12-22 12:02:02.709169 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:02.787330 UTC] Performing policy update
[2018-12-22 12:02:02.787932 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:02.877776 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:03.946724 UTC] Performing line search
[2018-12-22 12:02:04.073686 UTC] Updating baseline
[2018-12-22 12:02:05.361286 UTC] Computing logging information
-------------------------------------
| Iteration            | 1118       |
| ExpectedImprovement  | 0.019768   |
| ActualImprovement    | 0.019182   |
| ImprovementRatio     | 0.97036    |
| MeanKL               | 0.0078848  |
| Entropy              | -1.035     |
| Perplexity           | 0.35521    |
| AveragePolicyStd     | 0.20695    |
| AveragePolicyStd[0]  | 0.22823    |
| AveragePolicyStd[1]  | 0.21528    |
| AveragePolicyStd[2]  | 0.16363    |
| AveragePolicyStd[3]  | 0.21885    |
| AveragePolicyStd[4]  | 0.15605    |
| AveragePolicyStd[5]  | 0.25966    |
| AverageReturn        | 1708.3     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 313.94     |
| AverageEpisodeLength | 945.59     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.69     |
| TotalNEpisodes       | 20311      |
| TotalNSamples        | 5.5949e+06 |
| ExplainedVariance    | 0.062758   |
-------------------------------------
[2018-12-22 12:02:05.773078 UTC] Saving snapshot
[2018-12-22 12:02:05.773333 UTC] Starting iteration 1119
[2018-12-22 12:02:05.773455 UTC] Start collecting samples
[2018-12-22 12:02:08.842387 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:08.920696 UTC] Performing policy update
[2018-12-22 12:02:08.921513 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:09.010684 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:10.081033 UTC] Performing line search
[2018-12-22 12:02:10.209675 UTC] Updating baseline
[2018-12-22 12:02:11.381076 UTC] Computing logging information
-------------------------------------
| Iteration            | 1119       |
| ExpectedImprovement  | 0.018286   |
| ActualImprovement    | 0.017239   |
| ImprovementRatio     | 0.94278    |
| MeanKL               | 0.0078693  |
| Entropy              | -1.0357    |
| Perplexity           | 0.35498    |
| AveragePolicyStd     | 0.20694    |
| AveragePolicyStd[0]  | 0.22858    |
| AveragePolicyStd[1]  | 0.21502    |
| AveragePolicyStd[2]  | 0.16345    |
| AveragePolicyStd[3]  | 0.21846    |
| AveragePolicyStd[4]  | 0.15619    |
| AveragePolicyStd[5]  | 0.25992    |
| AverageReturn        | 1707.4     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 313.72     |
| AverageEpisodeLength | 945.59     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.69     |
| TotalNEpisodes       | 20317      |
| TotalNSamples        | 5.6009e+06 |
| ExplainedVariance    | -0.0016195 |
-------------------------------------
[2018-12-22 12:02:11.792214 UTC] Saving snapshot
[2018-12-22 12:02:11.792475 UTC] Starting iteration 1120
[2018-12-22 12:02:11.792616 UTC] Start collecting samples
[2018-12-22 12:02:14.766953 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:14.845404 UTC] Performing policy update
[2018-12-22 12:02:14.846253 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:14.938121 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:15.998900 UTC] Performing line search
[2018-12-22 12:02:16.125890 UTC] Updating baseline
[2018-12-22 12:02:17.632984 UTC] Computing logging information
-------------------------------------
| Iteration            | 1120       |
| ExpectedImprovement  | 0.018321   |
| ActualImprovement    | 0.016915   |
| ImprovementRatio     | 0.92325    |
| MeanKL               | 0.0077975  |
| Entropy              | -1.0391    |
| Perplexity           | 0.35376    |
| AveragePolicyStd     | 0.20681    |
| AveragePolicyStd[0]  | 0.2284     |
| AveragePolicyStd[1]  | 0.215      |
| AveragePolicyStd[2]  | 0.16346    |
| AveragePolicyStd[3]  | 0.21865    |
| AveragePolicyStd[4]  | 0.15595    |
| AveragePolicyStd[5]  | 0.25942    |
| AverageReturn        | 1714.9     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 308.23     |
| AverageEpisodeLength | 949.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.39     |
| TotalNEpisodes       | 20323      |
| TotalNSamples        | 5.6069e+06 |
| ExplainedVariance    | -0.0073476 |
-------------------------------------
[2018-12-22 12:02:18.048243 UTC] Saving snapshot
[2018-12-22 12:02:18.056507 UTC] Starting iteration 1121
[2018-12-22 12:02:18.056708 UTC] Start collecting samples
[2018-12-22 12:02:20.997224 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:21.074829 UTC] Performing policy update
[2018-12-22 12:02:21.075480 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:21.166860 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:22.230779 UTC] Performing line search
[2018-12-22 12:02:22.358380 UTC] Updating baseline
[2018-12-22 12:02:23.511091 UTC] Computing logging information
-------------------------------------
| Iteration            | 1121       |
| ExpectedImprovement  | 0.018063   |
| ActualImprovement    | 0.01686    |
| ImprovementRatio     | 0.93336    |
| MeanKL               | 0.0074954  |
| Entropy              | -1.0451    |
| Perplexity           | 0.35164    |
| AveragePolicyStd     | 0.20665    |
| AveragePolicyStd[0]  | 0.22835    |
| AveragePolicyStd[1]  | 0.21408    |
| AveragePolicyStd[2]  | 0.16293    |
| AveragePolicyStd[3]  | 0.2184     |
| AveragePolicyStd[4]  | 0.15595    |
| AveragePolicyStd[5]  | 0.26016    |
| AverageReturn        | 1711.4     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 318.7      |
| AverageEpisodeLength | 947.88     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.02     |
| TotalNEpisodes       | 20327      |
| TotalNSamples        | 5.6103e+06 |
| ExplainedVariance    | 0.13101    |
-------------------------------------
[2018-12-22 12:02:23.917081 UTC] Saving snapshot
[2018-12-22 12:02:23.917371 UTC] Starting iteration 1122
[2018-12-22 12:02:23.917520 UTC] Start collecting samples
[2018-12-22 12:02:26.841883 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:26.920597 UTC] Performing policy update
[2018-12-22 12:02:26.921408 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:27.011430 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:28.072871 UTC] Performing line search
[2018-12-22 12:02:28.200094 UTC] Updating baseline
[2018-12-22 12:02:29.455380 UTC] Computing logging information
-------------------------------------
| Iteration            | 1122       |
| ExpectedImprovement  | 0.016615   |
| ActualImprovement    | 0.015756   |
| ImprovementRatio     | 0.94829    |
| MeanKL               | 0.0078558  |
| Entropy              | -1.0384    |
| Perplexity           | 0.35401    |
| AveragePolicyStd     | 0.2069     |
| AveragePolicyStd[0]  | 0.22834    |
| AveragePolicyStd[1]  | 0.21491    |
| AveragePolicyStd[2]  | 0.16315    |
| AveragePolicyStd[3]  | 0.21873    |
| AveragePolicyStd[4]  | 0.15572    |
| AveragePolicyStd[5]  | 0.26055    |
| AverageReturn        | 1711.4     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 318.71     |
| AverageEpisodeLength | 947.88     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.02     |
| TotalNEpisodes       | 20331      |
| TotalNSamples        | 5.6143e+06 |
| ExplainedVariance    | 0.0059565  |
-------------------------------------
[2018-12-22 12:02:29.865005 UTC] Saving snapshot
[2018-12-22 12:02:29.865275 UTC] Starting iteration 1123
[2018-12-22 12:02:29.865401 UTC] Start collecting samples
[2018-12-22 12:02:32.832779 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:32.913515 UTC] Performing policy update
[2018-12-22 12:02:32.914219 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:33.005636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:34.068823 UTC] Performing line search
[2018-12-22 12:02:34.196390 UTC] Updating baseline
[2018-12-22 12:02:35.454969 UTC] Computing logging information
-------------------------------------
| Iteration            | 1123       |
| ExpectedImprovement  | 0.016451   |
| ActualImprovement    | 0.015687   |
| ImprovementRatio     | 0.95355    |
| MeanKL               | 0.0080145  |
| Entropy              | -1.0417    |
| Perplexity           | 0.35285    |
| AveragePolicyStd     | 0.20677    |
| AveragePolicyStd[0]  | 0.22771    |
| AveragePolicyStd[1]  | 0.21487    |
| AveragePolicyStd[2]  | 0.16315    |
| AveragePolicyStd[3]  | 0.21808    |
| AveragePolicyStd[4]  | 0.15599    |
| AveragePolicyStd[5]  | 0.2608     |
| AverageReturn        | 1724.4     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 309.81     |
| AverageEpisodeLength | 954.79     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.8      |
| TotalNEpisodes       | 20339      |
| TotalNSamples        | 5.6223e+06 |
| ExplainedVariance    | -0.0025674 |
-------------------------------------
[2018-12-22 12:02:35.857905 UTC] Saving snapshot
[2018-12-22 12:02:35.858165 UTC] Starting iteration 1124
[2018-12-22 12:02:35.858295 UTC] Start collecting samples
[2018-12-22 12:02:38.787315 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:38.864759 UTC] Performing policy update
[2018-12-22 12:02:38.865401 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:38.955735 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:40.019618 UTC] Performing line search
[2018-12-22 12:02:40.147421 UTC] Updating baseline
[2018-12-22 12:02:41.395050 UTC] Computing logging information
-------------------------------------
| Iteration            | 1124       |
| ExpectedImprovement  | 0.016827   |
| ActualImprovement    | 0.017303   |
| ImprovementRatio     | 1.0283     |
| MeanKL               | 0.0080056  |
| Entropy              | -1.0422    |
| Perplexity           | 0.35268    |
| AveragePolicyStd     | 0.20672    |
| AveragePolicyStd[0]  | 0.22734    |
| AveragePolicyStd[1]  | 0.2145     |
| AveragePolicyStd[2]  | 0.16352    |
| AveragePolicyStd[3]  | 0.21785    |
| AveragePolicyStd[4]  | 0.15618    |
| AveragePolicyStd[5]  | 0.26091    |
| AverageReturn        | 1722.2     |
| MinReturn            | 120.45     |
| MaxReturn            | 1898.9     |
| StdReturn            | 309.17     |
| AverageEpisodeLength | 954.79     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.8      |
| TotalNEpisodes       | 20343      |
| TotalNSamples        | 5.6263e+06 |
| ExplainedVariance    | -0.020264  |
-------------------------------------
[2018-12-22 12:02:41.800451 UTC] Saving snapshot
[2018-12-22 12:02:41.800715 UTC] Starting iteration 1125
[2018-12-22 12:02:41.800848 UTC] Start collecting samples
[2018-12-22 12:02:44.712557 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:44.790643 UTC] Performing policy update
[2018-12-22 12:02:44.791300 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:44.882718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:45.944163 UTC] Performing line search
[2018-12-22 12:02:46.072031 UTC] Updating baseline
[2018-12-22 12:02:47.419907 UTC] Computing logging information
-------------------------------------
| Iteration            | 1125       |
| ExpectedImprovement  | 0.018766   |
| ActualImprovement    | 0.017391   |
| ImprovementRatio     | 0.92675    |
| MeanKL               | 0.0079079  |
| Entropy              | -1.0419    |
| Perplexity           | 0.35278    |
| AveragePolicyStd     | 0.2067     |
| AveragePolicyStd[0]  | 0.22717    |
| AveragePolicyStd[1]  | 0.21525    |
| AveragePolicyStd[2]  | 0.1634     |
| AveragePolicyStd[3]  | 0.21839    |
| AveragePolicyStd[4]  | 0.15616    |
| AveragePolicyStd[5]  | 0.25985    |
| AverageReturn        | 1720       |
| MinReturn            | 120.45     |
| MaxReturn            | 1888.3     |
| StdReturn            | 308.37     |
| AverageEpisodeLength | 954.79     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.8      |
| TotalNEpisodes       | 20346      |
| TotalNSamples        | 5.6293e+06 |
| ExplainedVariance    | -0.010281  |
-------------------------------------
[2018-12-22 12:02:47.832621 UTC] Saving snapshot
[2018-12-22 12:02:47.832931 UTC] Starting iteration 1126
[2018-12-22 12:02:47.833049 UTC] Start collecting samples
[2018-12-22 12:02:50.797377 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:50.876155 UTC] Performing policy update
[2018-12-22 12:02:50.877062 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:50.965698 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:52.023077 UTC] Performing line search
[2018-12-22 12:02:52.150204 UTC] Updating baseline
[2018-12-22 12:02:53.398394 UTC] Computing logging information
-------------------------------------
| Iteration            | 1126       |
| ExpectedImprovement  | 0.018781   |
| ActualImprovement    | 0.017995   |
| ImprovementRatio     | 0.95818    |
| MeanKL               | 0.0077912  |
| Entropy              | -1.0463    |
| Perplexity           | 0.35124    |
| AveragePolicyStd     | 0.20654    |
| AveragePolicyStd[0]  | 0.22732    |
| AveragePolicyStd[1]  | 0.21519    |
| AveragePolicyStd[2]  | 0.16316    |
| AveragePolicyStd[3]  | 0.21826    |
| AveragePolicyStd[4]  | 0.15619    |
| AveragePolicyStd[5]  | 0.25912    |
| AverageReturn        | 1745.7     |
| MinReturn            | 120.45     |
| MaxReturn            | 1888.3     |
| StdReturn            | 266.39     |
| AverageEpisodeLength | 967.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.01     |
| TotalNEpisodes       | 20354      |
| TotalNSamples        | 5.6373e+06 |
| ExplainedVariance    | 0.00097269 |
-------------------------------------
[2018-12-22 12:02:53.808885 UTC] Saving snapshot
[2018-12-22 12:02:53.809125 UTC] Starting iteration 1127
[2018-12-22 12:02:53.809243 UTC] Start collecting samples
[2018-12-22 12:02:56.767707 UTC] Computing input variables for policy optimization
[2018-12-22 12:02:56.845963 UTC] Performing policy update
[2018-12-22 12:02:56.846620 UTC] Computing gradient in Euclidean space
[2018-12-22 12:02:56.938872 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:02:58.003625 UTC] Performing line search
[2018-12-22 12:02:58.130595 UTC] Updating baseline
[2018-12-22 12:02:59.570692 UTC] Computing logging information
-------------------------------------
| Iteration            | 1127       |
| ExpectedImprovement  | 0.019608   |
| ActualImprovement    | 0.018204   |
| ImprovementRatio     | 0.92837    |
| MeanKL               | 0.0075148  |
| Entropy              | -1.0479    |
| Perplexity           | 0.35068    |
| AveragePolicyStd     | 0.20648    |
| AveragePolicyStd[0]  | 0.22689    |
| AveragePolicyStd[1]  | 0.21453    |
| AveragePolicyStd[2]  | 0.1633     |
| AveragePolicyStd[3]  | 0.21855    |
| AveragePolicyStd[4]  | 0.15616    |
| AveragePolicyStd[5]  | 0.25947    |
| AverageReturn        | 1744.3     |
| MinReturn            | 120.45     |
| MaxReturn            | 1888.3     |
| StdReturn            | 266.09     |
| AverageEpisodeLength | 967.45     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.01     |
| TotalNEpisodes       | 20359      |
| TotalNSamples        | 5.6423e+06 |
| ExplainedVariance    | 0.053855   |
-------------------------------------
[2018-12-22 12:02:59.979565 UTC] Saving snapshot
[2018-12-22 12:02:59.979821 UTC] Starting iteration 1128
[2018-12-22 12:02:59.979940 UTC] Start collecting samples
[2018-12-22 12:03:02.926378 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:03.004298 UTC] Performing policy update
[2018-12-22 12:03:03.005166 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:03.094787 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:04.161590 UTC] Performing line search
[2018-12-22 12:03:04.289055 UTC] Updating baseline
[2018-12-22 12:03:05.613030 UTC] Computing logging information
-------------------------------------
| Iteration            | 1128       |
| ExpectedImprovement  | 0.017426   |
| ActualImprovement    | 0.017042   |
| ImprovementRatio     | 0.97794    |
| MeanKL               | 0.0073693  |
| Entropy              | -1.0564    |
| Perplexity           | 0.34772    |
| AveragePolicyStd     | 0.20618    |
| AveragePolicyStd[0]  | 0.22707    |
| AveragePolicyStd[1]  | 0.21421    |
| AveragePolicyStd[2]  | 0.16309    |
| AveragePolicyStd[3]  | 0.21743    |
| AveragePolicyStd[4]  | 0.15612    |
| AveragePolicyStd[5]  | 0.25918    |
| AverageReturn        | 1752.1     |
| MinReturn            | 120.45     |
| MaxReturn            | 1888.3     |
| StdReturn            | 252.28     |
| AverageEpisodeLength | 971.71     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.45     |
| TotalNEpisodes       | 20364      |
| TotalNSamples        | 5.6463e+06 |
| ExplainedVariance    | 0.24282    |
-------------------------------------
[2018-12-22 12:03:06.022248 UTC] Saving snapshot
[2018-12-22 12:03:06.022527 UTC] Starting iteration 1129
[2018-12-22 12:03:06.022652 UTC] Start collecting samples
[2018-12-22 12:03:08.963023 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:09.041330 UTC] Performing policy update
[2018-12-22 12:03:09.042169 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:09.133206 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:10.203645 UTC] Performing line search
[2018-12-22 12:03:10.331774 UTC] Updating baseline
[2018-12-22 12:03:11.675590 UTC] Computing logging information
-------------------------------------
| Iteration            | 1129       |
| ExpectedImprovement  | 0.017533   |
| ActualImprovement    | 0.01655    |
| ImprovementRatio     | 0.94396    |
| MeanKL               | 0.0074706  |
| Entropy              | -1.0586    |
| Perplexity           | 0.34693    |
| AveragePolicyStd     | 0.20608    |
| AveragePolicyStd[0]  | 0.22737    |
| AveragePolicyStd[1]  | 0.21373    |
| AveragePolicyStd[2]  | 0.16344    |
| AveragePolicyStd[3]  | 0.21616    |
| AveragePolicyStd[4]  | 0.15632    |
| AveragePolicyStd[5]  | 0.25947    |
| AverageReturn        | 1748.1     |
| MinReturn            | 120.45     |
| MaxReturn            | 1888.3     |
| StdReturn            | 253.27     |
| AverageEpisodeLength | 970.37     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.85     |
| TotalNEpisodes       | 20369      |
| TotalNSamples        | 5.6512e+06 |
| ExplainedVariance    | 0.10346    |
-------------------------------------
[2018-12-22 12:03:12.092727 UTC] Saving snapshot
[2018-12-22 12:03:12.092973 UTC] Starting iteration 1130
[2018-12-22 12:03:12.093089 UTC] Start collecting samples
[2018-12-22 12:03:15.026970 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:15.107086 UTC] Performing policy update
[2018-12-22 12:03:15.107678 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:15.197938 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:16.252741 UTC] Performing line search
[2018-12-22 12:03:16.379382 UTC] Updating baseline
[2018-12-22 12:03:17.623831 UTC] Computing logging information
-------------------------------------
| Iteration            | 1130       |
| ExpectedImprovement  | 0.018601   |
| ActualImprovement    | 0.018564   |
| ImprovementRatio     | 0.99804    |
| MeanKL               | 0.0077753  |
| Entropy              | -1.0624    |
| Perplexity           | 0.34562    |
| AveragePolicyStd     | 0.20595    |
| AveragePolicyStd[0]  | 0.2275     |
| AveragePolicyStd[1]  | 0.21351    |
| AveragePolicyStd[2]  | 0.16321    |
| AveragePolicyStd[3]  | 0.21579    |
| AveragePolicyStd[4]  | 0.15634    |
| AveragePolicyStd[5]  | 0.25938    |
| AverageReturn        | 1748.1     |
| MinReturn            | 120.45     |
| MaxReturn            | 1886.5     |
| StdReturn            | 253.19     |
| AverageEpisodeLength | 970.37     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.85     |
| TotalNEpisodes       | 20374      |
| TotalNSamples        | 5.6562e+06 |
| ExplainedVariance    | -0.025289  |
-------------------------------------
[2018-12-22 12:03:18.028377 UTC] Saving snapshot
[2018-12-22 12:03:18.036624 UTC] Starting iteration 1131
[2018-12-22 12:03:18.036827 UTC] Start collecting samples
[2018-12-22 12:03:20.957004 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:21.033938 UTC] Performing policy update
[2018-12-22 12:03:21.034790 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:21.125839 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:22.202173 UTC] Performing line search
[2018-12-22 12:03:22.329883 UTC] Updating baseline
[2018-12-22 12:03:23.760784 UTC] Computing logging information
-------------------------------------
| Iteration            | 1131       |
| ExpectedImprovement  | 0.018491   |
| ActualImprovement    | 0.017954   |
| ImprovementRatio     | 0.97097    |
| MeanKL               | 0.0071283  |
| Entropy              | -1.0654    |
| Perplexity           | 0.3446     |
| AveragePolicyStd     | 0.20583    |
| AveragePolicyStd[0]  | 0.22809    |
| AveragePolicyStd[1]  | 0.21251    |
| AveragePolicyStd[2]  | 0.16344    |
| AveragePolicyStd[3]  | 0.21535    |
| AveragePolicyStd[4]  | 0.15645    |
| AveragePolicyStd[5]  | 0.25912    |
| AverageReturn        | 1738.7     |
| MinReturn            | 120.45     |
| MaxReturn            | 1886.5     |
| StdReturn            | 263.04     |
| AverageEpisodeLength | 966.33     |
| MinEpisodeLength     | 103        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.08     |
| TotalNEpisodes       | 20379      |
| TotalNSamples        | 5.6608e+06 |
| ExplainedVariance    | 0.09053    |
-------------------------------------
[2018-12-22 12:03:24.170279 UTC] Saving snapshot
[2018-12-22 12:03:24.170545 UTC] Starting iteration 1132
[2018-12-22 12:03:24.170665 UTC] Start collecting samples
[2018-12-22 12:03:27.115875 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:27.193941 UTC] Performing policy update
[2018-12-22 12:03:27.194520 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:27.284736 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:28.337211 UTC] Performing line search
[2018-12-22 12:03:28.463921 UTC] Updating baseline
[2018-12-22 12:03:29.800484 UTC] Computing logging information
-------------------------------------
| Iteration            | 1132       |
| ExpectedImprovement  | 0.018186   |
| ActualImprovement    | 0.017114   |
| ImprovementRatio     | 0.94105    |
| MeanKL               | 0.0073466  |
| Entropy              | -1.0648    |
| Perplexity           | 0.34478    |
| AveragePolicyStd     | 0.20584    |
| AveragePolicyStd[0]  | 0.22822    |
| AveragePolicyStd[1]  | 0.21248    |
| AveragePolicyStd[2]  | 0.16369    |
| AveragePolicyStd[3]  | 0.21497    |
| AveragePolicyStd[4]  | 0.15642    |
| AveragePolicyStd[5]  | 0.25929    |
| AverageReturn        | 1758.6     |
| MinReturn            | 585.96     |
| MaxReturn            | 1886.5     |
| StdReturn            | 204.4      |
| AverageEpisodeLength | 977.01     |
| MinEpisodeLength     | 384        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 101.22     |
| TotalNEpisodes       | 20385      |
| TotalNSamples        | 5.6668e+06 |
| ExplainedVariance    | 0.0086033  |
-------------------------------------
[2018-12-22 12:03:30.207306 UTC] Saving snapshot
[2018-12-22 12:03:30.207568 UTC] Starting iteration 1133
[2018-12-22 12:03:30.207694 UTC] Start collecting samples
[2018-12-22 12:03:33.100491 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:33.177124 UTC] Performing policy update
[2018-12-22 12:03:33.177832 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:33.266917 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:34.334056 UTC] Performing line search
[2018-12-22 12:03:34.462077 UTC] Updating baseline
[2018-12-22 12:03:35.976552 UTC] Computing logging information
-------------------------------------
| Iteration            | 1133       |
| ExpectedImprovement  | 0.017905   |
| ActualImprovement    | 0.017067   |
| ImprovementRatio     | 0.95318    |
| MeanKL               | 0.0077728  |
| Entropy              | -1.06      |
| Perplexity           | 0.34644    |
| AveragePolicyStd     | 0.20598    |
| AveragePolicyStd[0]  | 0.22847    |
| AveragePolicyStd[1]  | 0.21272    |
| AveragePolicyStd[2]  | 0.16373    |
| AveragePolicyStd[3]  | 0.21473    |
| AveragePolicyStd[4]  | 0.15695    |
| AveragePolicyStd[5]  | 0.25929    |
| AverageReturn        | 1743.6     |
| MinReturn            | 195.37     |
| MaxReturn            | 1886.5     |
| StdReturn            | 256.91     |
| AverageEpisodeLength | 968.31     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.67     |
| TotalNEpisodes       | 20389      |
| TotalNSamples        | 5.6699e+06 |
| ExplainedVariance    | 0.099346   |
-------------------------------------
[2018-12-22 12:03:36.383713 UTC] Saving snapshot
[2018-12-22 12:03:36.383960 UTC] Starting iteration 1134
[2018-12-22 12:03:36.384082 UTC] Start collecting samples
[2018-12-22 12:03:39.323917 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:39.401028 UTC] Performing policy update
[2018-12-22 12:03:39.401616 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:39.491097 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:40.555536 UTC] Performing line search
[2018-12-22 12:03:40.682626 UTC] Updating baseline
[2018-12-22 12:03:41.939831 UTC] Computing logging information
-------------------------------------
| Iteration            | 1134       |
| ExpectedImprovement  | 0.018899   |
| ActualImprovement    | 0.017926   |
| ImprovementRatio     | 0.94856    |
| MeanKL               | 0.0078771  |
| Entropy              | -1.0544    |
| Perplexity           | 0.3484     |
| AveragePolicyStd     | 0.20614    |
| AveragePolicyStd[0]  | 0.22907    |
| AveragePolicyStd[1]  | 0.21318    |
| AveragePolicyStd[2]  | 0.1641     |
| AveragePolicyStd[3]  | 0.21443    |
| AveragePolicyStd[4]  | 0.15719    |
| AveragePolicyStd[5]  | 0.2589     |
| AverageReturn        | 1745.2     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.4     |
| StdReturn            | 257.22     |
| AverageEpisodeLength | 968.31     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.67     |
| TotalNEpisodes       | 20395      |
| TotalNSamples        | 5.6759e+06 |
| ExplainedVariance    | -0.013849  |
-------------------------------------
[2018-12-22 12:03:42.348412 UTC] Saving snapshot
[2018-12-22 12:03:42.348679 UTC] Starting iteration 1135
[2018-12-22 12:03:42.348797 UTC] Start collecting samples
[2018-12-22 12:03:45.283691 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:45.360836 UTC] Performing policy update
[2018-12-22 12:03:45.361465 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:45.450930 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:46.515259 UTC] Performing line search
[2018-12-22 12:03:46.642967 UTC] Updating baseline
[2018-12-22 12:03:48.166682 UTC] Computing logging information
-------------------------------------
| Iteration            | 1135       |
| ExpectedImprovement  | 0.017906   |
| ActualImprovement    | 0.016815   |
| ImprovementRatio     | 0.93908    |
| MeanKL               | 0.0082144  |
| Entropy              | -1.0573    |
| Perplexity           | 0.34739    |
| AveragePolicyStd     | 0.20597    |
| AveragePolicyStd[0]  | 0.22786    |
| AveragePolicyStd[1]  | 0.21357    |
| AveragePolicyStd[2]  | 0.16423    |
| AveragePolicyStd[3]  | 0.21487    |
| AveragePolicyStd[4]  | 0.15752    |
| AveragePolicyStd[5]  | 0.25776    |
| AverageReturn        | 1746.9     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.4     |
| StdReturn            | 257.72     |
| AverageEpisodeLength | 968.31     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.67     |
| TotalNEpisodes       | 20401      |
| TotalNSamples        | 5.6819e+06 |
| ExplainedVariance    | 0.012886   |
-------------------------------------
[2018-12-22 12:03:48.615971 UTC] Saving snapshot
[2018-12-22 12:03:48.616216 UTC] Starting iteration 1136
[2018-12-22 12:03:48.616336 UTC] Start collecting samples
[2018-12-22 12:03:51.767487 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:51.849980 UTC] Performing policy update
[2018-12-22 12:03:51.850604 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:51.949259 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:53.059890 UTC] Performing line search
[2018-12-22 12:03:53.192520 UTC] Updating baseline
[2018-12-22 12:03:54.439206 UTC] Computing logging information
-------------------------------------
| Iteration            | 1136       |
| ExpectedImprovement  | 0.017237   |
| ActualImprovement    | 0.016129   |
| ImprovementRatio     | 0.93573    |
| MeanKL               | 0.0082323  |
| Entropy              | -1.0629    |
| Perplexity           | 0.34544    |
| AveragePolicyStd     | 0.20577    |
| AveragePolicyStd[0]  | 0.22808    |
| AveragePolicyStd[1]  | 0.21333    |
| AveragePolicyStd[2]  | 0.16436    |
| AveragePolicyStd[3]  | 0.214      |
| AveragePolicyStd[4]  | 0.15727    |
| AveragePolicyStd[5]  | 0.25761    |
| AverageReturn        | 1739       |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.4     |
| StdReturn            | 269.74     |
| AverageEpisodeLength | 963.56     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.82     |
| TotalNEpisodes       | 20405      |
| TotalNSamples        | 5.6855e+06 |
| ExplainedVariance    | 0.12105    |
-------------------------------------
[2018-12-22 12:03:54.846096 UTC] Saving snapshot
[2018-12-22 12:03:54.846339 UTC] Starting iteration 1137
[2018-12-22 12:03:54.846460 UTC] Start collecting samples
[2018-12-22 12:03:57.993226 UTC] Computing input variables for policy optimization
[2018-12-22 12:03:58.075162 UTC] Performing policy update
[2018-12-22 12:03:58.075917 UTC] Computing gradient in Euclidean space
[2018-12-22 12:03:58.169144 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:03:59.268588 UTC] Performing line search
[2018-12-22 12:03:59.401386 UTC] Updating baseline
[2018-12-22 12:04:00.785598 UTC] Computing logging information
-------------------------------------
| Iteration            | 1137       |
| ExpectedImprovement  | 0.018114   |
| ActualImprovement    | 0.017125   |
| ImprovementRatio     | 0.9454     |
| MeanKL               | 0.0076627  |
| Entropy              | -1.067     |
| Perplexity           | 0.34405    |
| AveragePolicyStd     | 0.20564    |
| AveragePolicyStd[0]  | 0.22776    |
| AveragePolicyStd[1]  | 0.21291    |
| AveragePolicyStd[2]  | 0.1643     |
| AveragePolicyStd[3]  | 0.21357    |
| AveragePolicyStd[4]  | 0.15726    |
| AveragePolicyStd[5]  | 0.25807    |
| AverageReturn        | 1743.5     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.4     |
| StdReturn            | 268.63     |
| AverageEpisodeLength | 965.42     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.05     |
| TotalNEpisodes       | 20410      |
| TotalNSamples        | 5.6905e+06 |
| ExplainedVariance    | 0.0931     |
-------------------------------------
[2018-12-22 12:04:01.218085 UTC] Saving snapshot
[2018-12-22 12:04:01.218345 UTC] Starting iteration 1138
[2018-12-22 12:04:01.218492 UTC] Start collecting samples
[2018-12-22 12:04:04.416640 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:04.501217 UTC] Performing policy update
[2018-12-22 12:04:04.502107 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:04.597909 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:05.690063 UTC] Performing line search
[2018-12-22 12:04:05.821205 UTC] Updating baseline
[2018-12-22 12:04:07.052084 UTC] Computing logging information
-------------------------------------
| Iteration            | 1138       |
| ExpectedImprovement  | 0.017601   |
| ActualImprovement    | 0.017323   |
| ImprovementRatio     | 0.98417    |
| MeanKL               | 0.0086706  |
| Entropy              | -1.0667    |
| Perplexity           | 0.34416    |
| AveragePolicyStd     | 0.20567    |
| AveragePolicyStd[0]  | 0.22798    |
| AveragePolicyStd[1]  | 0.21308    |
| AveragePolicyStd[2]  | 0.16436    |
| AveragePolicyStd[3]  | 0.21312    |
| AveragePolicyStd[4]  | 0.15711    |
| AveragePolicyStd[5]  | 0.25836    |
| AverageReturn        | 1740.6     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.4     |
| StdReturn            | 272.87     |
| AverageEpisodeLength | 962.43     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.48     |
| TotalNEpisodes       | 20416      |
| TotalNSamples        | 5.6962e+06 |
| ExplainedVariance    | 0.0096136  |
-------------------------------------
[2018-12-22 12:04:07.454983 UTC] Saving snapshot
[2018-12-22 12:04:07.456189 UTC] Starting iteration 1139
[2018-12-22 12:04:07.456469 UTC] Start collecting samples
[2018-12-22 12:04:10.375102 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:10.451904 UTC] Performing policy update
[2018-12-22 12:04:10.452660 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:10.541910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:11.609127 UTC] Performing line search
[2018-12-22 12:04:11.736609 UTC] Updating baseline
[2018-12-22 12:04:13.261989 UTC] Computing logging information
-------------------------------------
| Iteration            | 1139       |
| ExpectedImprovement  | 0.017571   |
| ActualImprovement    | 0.016869   |
| ImprovementRatio     | 0.96005    |
| MeanKL               | 0.0073093  |
| Entropy              | -1.0613    |
| Perplexity           | 0.34601    |
| AveragePolicyStd     | 0.20584    |
| AveragePolicyStd[0]  | 0.22806    |
| AveragePolicyStd[1]  | 0.21336    |
| AveragePolicyStd[2]  | 0.16472    |
| AveragePolicyStd[3]  | 0.21342    |
| AveragePolicyStd[4]  | 0.15716    |
| AveragePolicyStd[5]  | 0.25836    |
| AverageReturn        | 1740.4     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 273.12     |
| AverageEpisodeLength | 962.43     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.48     |
| TotalNEpisodes       | 20420      |
| TotalNSamples        | 5.7002e+06 |
| ExplainedVariance    | 0.12205    |
-------------------------------------
[2018-12-22 12:04:13.670344 UTC] Saving snapshot
[2018-12-22 12:04:13.670610 UTC] Starting iteration 1140
[2018-12-22 12:04:13.670729 UTC] Start collecting samples
[2018-12-22 12:04:16.640358 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:16.719180 UTC] Performing policy update
[2018-12-22 12:04:16.719834 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:16.809944 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:17.866824 UTC] Performing line search
[2018-12-22 12:04:17.997194 UTC] Updating baseline
[2018-12-22 12:04:19.257260 UTC] Computing logging information
-------------------------------------
| Iteration            | 1140       |
| ExpectedImprovement  | 0.017462   |
| ActualImprovement    | 0.016672   |
| ImprovementRatio     | 0.95475    |
| MeanKL               | 0.0084896  |
| Entropy              | -1.0655    |
| Perplexity           | 0.34455    |
| AveragePolicyStd     | 0.20573    |
| AveragePolicyStd[0]  | 0.22715    |
| AveragePolicyStd[1]  | 0.21298    |
| AveragePolicyStd[2]  | 0.16409    |
| AveragePolicyStd[3]  | 0.2135     |
| AveragePolicyStd[4]  | 0.15734    |
| AveragePolicyStd[5]  | 0.25933    |
| AverageReturn        | 1746.4     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 250.88     |
| AverageEpisodeLength | 964.92     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.78     |
| TotalNEpisodes       | 20427      |
| TotalNSamples        | 5.7068e+06 |
| ExplainedVariance    | 0.12407    |
-------------------------------------
[2018-12-22 12:04:19.661295 UTC] Saving snapshot
[2018-12-22 12:04:19.669439 UTC] Starting iteration 1141
[2018-12-22 12:04:19.669652 UTC] Start collecting samples
[2018-12-22 12:04:22.611377 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:22.688750 UTC] Performing policy update
[2018-12-22 12:04:22.689386 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:22.780445 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:23.846302 UTC] Performing line search
[2018-12-22 12:04:23.977216 UTC] Updating baseline
[2018-12-22 12:04:25.226371 UTC] Computing logging information
--------------------------------------
| Iteration            | 1141        |
| ExpectedImprovement  | 0.016286    |
| ActualImprovement    | 0.015499    |
| ImprovementRatio     | 0.95163     |
| MeanKL               | 0.0083552   |
| Entropy              | -1.0647     |
| Perplexity           | 0.34484     |
| AveragePolicyStd     | 0.20579     |
| AveragePolicyStd[0]  | 0.2264      |
| AveragePolicyStd[1]  | 0.21316     |
| AveragePolicyStd[2]  | 0.16419     |
| AveragePolicyStd[3]  | 0.21361     |
| AveragePolicyStd[4]  | 0.15713     |
| AveragePolicyStd[5]  | 0.26025     |
| AverageReturn        | 1747.7      |
| MinReturn            | 195.37      |
| MaxReturn            | 1887.9      |
| StdReturn            | 251.36      |
| AverageEpisodeLength | 964.92      |
| MinEpisodeLength     | 130         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 129.78      |
| TotalNEpisodes       | 20432       |
| TotalNSamples        | 5.7118e+06  |
| ExplainedVariance    | -0.00059036 |
--------------------------------------
[2018-12-22 12:04:25.637136 UTC] Saving snapshot
[2018-12-22 12:04:25.637428 UTC] Starting iteration 1142
[2018-12-22 12:04:25.637565 UTC] Start collecting samples
[2018-12-22 12:04:28.559226 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:28.638377 UTC] Performing policy update
[2018-12-22 12:04:28.638967 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:28.729534 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:29.784081 UTC] Performing line search
[2018-12-22 12:04:29.912737 UTC] Updating baseline
[2018-12-22 12:04:31.218685 UTC] Computing logging information
-------------------------------------
| Iteration            | 1142       |
| ExpectedImprovement  | 0.017251   |
| ActualImprovement    | 0.016573   |
| ImprovementRatio     | 0.96071    |
| MeanKL               | 0.0082801  |
| Entropy              | -1.0634    |
| Perplexity           | 0.34527    |
| AveragePolicyStd     | 0.20584    |
| AveragePolicyStd[0]  | 0.22688    |
| AveragePolicyStd[1]  | 0.21246    |
| AveragePolicyStd[2]  | 0.16393    |
| AveragePolicyStd[3]  | 0.21338    |
| AveragePolicyStd[4]  | 0.15763    |
| AveragePolicyStd[5]  | 0.26075    |
| AverageReturn        | 1748.2     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 251.5      |
| AverageEpisodeLength | 964.92     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.78     |
| TotalNEpisodes       | 20436      |
| TotalNSamples        | 5.7158e+06 |
| ExplainedVariance    | 0.0060544  |
-------------------------------------
[2018-12-22 12:04:31.630815 UTC] Saving snapshot
[2018-12-22 12:04:31.631081 UTC] Starting iteration 1143
[2018-12-22 12:04:31.631203 UTC] Start collecting samples
[2018-12-22 12:04:34.568772 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:34.646784 UTC] Performing policy update
[2018-12-22 12:04:34.647761 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:34.737657 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:35.789563 UTC] Performing line search
[2018-12-22 12:04:35.916738 UTC] Updating baseline
[2018-12-22 12:04:37.325048 UTC] Computing logging information
-------------------------------------
| Iteration            | 1143       |
| ExpectedImprovement  | 0.018621   |
| ActualImprovement    | 0.01782    |
| ImprovementRatio     | 0.95697    |
| MeanKL               | 0.0074051  |
| Entropy              | -1.0607    |
| Perplexity           | 0.34623    |
| AveragePolicyStd     | 0.20593    |
| AveragePolicyStd[0]  | 0.22616    |
| AveragePolicyStd[1]  | 0.21252    |
| AveragePolicyStd[2]  | 0.16417    |
| AveragePolicyStd[3]  | 0.21342    |
| AveragePolicyStd[4]  | 0.15777    |
| AveragePolicyStd[5]  | 0.26156    |
| AverageReturn        | 1740.2     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 264.94     |
| AverageEpisodeLength | 960.19     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.84     |
| TotalNEpisodes       | 20443      |
| TotalNSamples        | 5.7223e+06 |
| ExplainedVariance    | 0.12741    |
-------------------------------------
[2018-12-22 12:04:37.728792 UTC] Saving snapshot
[2018-12-22 12:04:37.730026 UTC] Starting iteration 1144
[2018-12-22 12:04:37.730175 UTC] Start collecting samples
[2018-12-22 12:04:40.671774 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:40.748528 UTC] Performing policy update
[2018-12-22 12:04:40.749178 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:40.836635 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:41.892612 UTC] Performing line search
[2018-12-22 12:04:42.022659 UTC] Updating baseline
[2018-12-22 12:04:43.428611 UTC] Computing logging information
-------------------------------------
| Iteration            | 1144       |
| ExpectedImprovement  | 0.018302   |
| ActualImprovement    | 0.018286   |
| ImprovementRatio     | 0.99914    |
| MeanKL               | 0.0076615  |
| Entropy              | -1.0545    |
| Perplexity           | 0.34838    |
| AveragePolicyStd     | 0.20612    |
| AveragePolicyStd[0]  | 0.22619    |
| AveragePolicyStd[1]  | 0.21309    |
| AveragePolicyStd[2]  | 0.16442    |
| AveragePolicyStd[3]  | 0.21284    |
| AveragePolicyStd[4]  | 0.15834    |
| AveragePolicyStd[5]  | 0.26182    |
| AverageReturn        | 1741.3     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 264.92     |
| AverageEpisodeLength | 960.19     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.84     |
| TotalNEpisodes       | 20448      |
| TotalNSamples        | 5.7273e+06 |
| ExplainedVariance    | 0.0082757  |
-------------------------------------
[2018-12-22 12:04:43.840715 UTC] Saving snapshot
[2018-12-22 12:04:43.840970 UTC] Starting iteration 1145
[2018-12-22 12:04:43.841094 UTC] Start collecting samples
[2018-12-22 12:04:46.744052 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:46.820249 UTC] Performing policy update
[2018-12-22 12:04:46.820829 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:46.912478 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:47.969936 UTC] Performing line search
[2018-12-22 12:04:48.098304 UTC] Updating baseline
[2018-12-22 12:04:49.511078 UTC] Computing logging information
-------------------------------------
| Iteration            | 1145       |
| ExpectedImprovement  | 0.020008   |
| ActualImprovement    | 0.017492   |
| ImprovementRatio     | 0.87429    |
| MeanKL               | 0.0074442  |
| Entropy              | -1.0556    |
| Perplexity           | 0.34797    |
| AveragePolicyStd     | 0.20606    |
| AveragePolicyStd[0]  | 0.22673    |
| AveragePolicyStd[1]  | 0.21207    |
| AveragePolicyStd[2]  | 0.1643     |
| AveragePolicyStd[3]  | 0.21267    |
| AveragePolicyStd[4]  | 0.15879    |
| AveragePolicyStd[5]  | 0.26179    |
| AverageReturn        | 1727.1     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 289.37     |
| AverageEpisodeLength | 953.44     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.66     |
| TotalNEpisodes       | 20451      |
| TotalNSamples        | 5.7296e+06 |
| ExplainedVariance    | 0.27374    |
-------------------------------------
[2018-12-22 12:04:49.920717 UTC] Saving snapshot
[2018-12-22 12:04:49.920968 UTC] Starting iteration 1146
[2018-12-22 12:04:49.921090 UTC] Start collecting samples
[2018-12-22 12:04:52.871179 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:52.948743 UTC] Performing policy update
[2018-12-22 12:04:52.949323 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:53.038343 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:04:54.099356 UTC] Performing line search
[2018-12-22 12:04:54.227074 UTC] Updating baseline
[2018-12-22 12:04:55.707196 UTC] Computing logging information
-------------------------------------
| Iteration            | 1146       |
| ExpectedImprovement  | 0.016039   |
| ActualImprovement    | 0.015242   |
| ImprovementRatio     | 0.95036    |
| MeanKL               | 0.007961   |
| Entropy              | -1.0533    |
| Perplexity           | 0.34879    |
| AveragePolicyStd     | 0.20617    |
| AveragePolicyStd[0]  | 0.22622    |
| AveragePolicyStd[1]  | 0.21231    |
| AveragePolicyStd[2]  | 0.16434    |
| AveragePolicyStd[3]  | 0.21291    |
| AveragePolicyStd[4]  | 0.15865    |
| AveragePolicyStd[5]  | 0.2626     |
| AverageReturn        | 1727.5     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 289.75     |
| AverageEpisodeLength | 952.94     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.59     |
| TotalNEpisodes       | 20458      |
| TotalNSamples        | 5.7366e+06 |
| ExplainedVariance    | 0.0032717  |
-------------------------------------
[2018-12-22 12:04:56.125033 UTC] Saving snapshot
[2018-12-22 12:04:56.125286 UTC] Starting iteration 1147
[2018-12-22 12:04:56.125405 UTC] Start collecting samples
[2018-12-22 12:04:59.066928 UTC] Computing input variables for policy optimization
[2018-12-22 12:04:59.144092 UTC] Performing policy update
[2018-12-22 12:04:59.144918 UTC] Computing gradient in Euclidean space
[2018-12-22 12:04:59.234326 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:00.292157 UTC] Performing line search
[2018-12-22 12:05:00.419019 UTC] Updating baseline
[2018-12-22 12:05:01.888831 UTC] Computing logging information
-------------------------------------
| Iteration            | 1147       |
| ExpectedImprovement  | 0.015893   |
| ActualImprovement    | 0.015134   |
| ImprovementRatio     | 0.95228    |
| MeanKL               | 0.0077922  |
| Entropy              | -1.0537    |
| Perplexity           | 0.34866    |
| AveragePolicyStd     | 0.20617    |
| AveragePolicyStd[0]  | 0.2258     |
| AveragePolicyStd[1]  | 0.21302    |
| AveragePolicyStd[2]  | 0.16412    |
| AveragePolicyStd[3]  | 0.21261    |
| AveragePolicyStd[4]  | 0.15868    |
| AveragePolicyStd[5]  | 0.26275    |
| AverageReturn        | 1734.7     |
| MinReturn            | 195.37     |
| MaxReturn            | 1887.9     |
| StdReturn            | 281.22     |
| AverageEpisodeLength | 956.68     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.02     |
| TotalNEpisodes       | 20463      |
| TotalNSamples        | 5.7416e+06 |
| ExplainedVariance    | 0.0031311  |
-------------------------------------
[2018-12-22 12:05:02.304359 UTC] Saving snapshot
[2018-12-22 12:05:02.304627 UTC] Starting iteration 1148
[2018-12-22 12:05:02.304748 UTC] Start collecting samples
[2018-12-22 12:05:05.230320 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:05.306671 UTC] Performing policy update
[2018-12-22 12:05:05.307434 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:05.398135 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:06.465404 UTC] Performing line search
[2018-12-22 12:05:06.594473 UTC] Updating baseline
[2018-12-22 12:05:07.896466 UTC] Computing logging information
-------------------------------------
| Iteration            | 1148       |
| ExpectedImprovement  | 0.017215   |
| ActualImprovement    | 0.016343   |
| ImprovementRatio     | 0.94935    |
| MeanKL               | 0.0077179  |
| Entropy              | -1.0589    |
| Perplexity           | 0.34685    |
| AveragePolicyStd     | 0.20601    |
| AveragePolicyStd[0]  | 0.22598    |
| AveragePolicyStd[1]  | 0.21288    |
| AveragePolicyStd[2]  | 0.16408    |
| AveragePolicyStd[3]  | 0.21214    |
| AveragePolicyStd[4]  | 0.15824    |
| AveragePolicyStd[5]  | 0.26271    |
| AverageReturn        | 1751.8     |
| MinReturn            | 195.37     |
| MaxReturn            | 1898       |
| StdReturn            | 258.82     |
| AverageEpisodeLength | 963.87     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.33     |
| TotalNEpisodes       | 20467      |
| TotalNSamples        | 5.7456e+06 |
| ExplainedVariance    | 0.0088486  |
-------------------------------------
[2018-12-22 12:05:08.303919 UTC] Saving snapshot
[2018-12-22 12:05:08.304176 UTC] Starting iteration 1149
[2018-12-22 12:05:08.304306 UTC] Start collecting samples
[2018-12-22 12:05:11.256919 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:11.334264 UTC] Performing policy update
[2018-12-22 12:05:11.335058 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:11.425869 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:12.490136 UTC] Performing line search
[2018-12-22 12:05:12.617481 UTC] Updating baseline
[2018-12-22 12:05:13.762246 UTC] Computing logging information
-------------------------------------
| Iteration            | 1149       |
| ExpectedImprovement  | 0.017665   |
| ActualImprovement    | 0.016268   |
| ImprovementRatio     | 0.92087    |
| MeanKL               | 0.0082859  |
| Entropy              | -1.0601    |
| Perplexity           | 0.34641    |
| AveragePolicyStd     | 0.20593    |
| AveragePolicyStd[0]  | 0.22608    |
| AveragePolicyStd[1]  | 0.21313    |
| AveragePolicyStd[2]  | 0.16396    |
| AveragePolicyStd[3]  | 0.21226    |
| AveragePolicyStd[4]  | 0.1584     |
| AveragePolicyStd[5]  | 0.26177    |
| AverageReturn        | 1752.2     |
| MinReturn            | 195.37     |
| MaxReturn            | 1898       |
| StdReturn            | 258.92     |
| AverageEpisodeLength | 963.49     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.28     |
| TotalNEpisodes       | 20473      |
| TotalNSamples        | 5.7516e+06 |
| ExplainedVariance    | 0.088083   |
-------------------------------------
[2018-12-22 12:05:14.176924 UTC] Saving snapshot
[2018-12-22 12:05:14.177172 UTC] Starting iteration 1150
[2018-12-22 12:05:14.177301 UTC] Start collecting samples
[2018-12-22 12:05:17.118205 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:17.196867 UTC] Performing policy update
[2018-12-22 12:05:17.197543 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:17.287568 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:18.349265 UTC] Performing line search
[2018-12-22 12:05:18.478461 UTC] Updating baseline
[2018-12-22 12:05:19.780401 UTC] Computing logging information
-------------------------------------
| Iteration            | 1150       |
| ExpectedImprovement  | 0.01811    |
| ActualImprovement    | 0.017436   |
| ImprovementRatio     | 0.96277    |
| MeanKL               | 0.007646   |
| Entropy              | -1.0669    |
| Perplexity           | 0.34406    |
| AveragePolicyStd     | 0.20571    |
| AveragePolicyStd[0]  | 0.22624    |
| AveragePolicyStd[1]  | 0.21266    |
| AveragePolicyStd[2]  | 0.1635     |
| AveragePolicyStd[3]  | 0.21248    |
| AveragePolicyStd[4]  | 0.15815    |
| AveragePolicyStd[5]  | 0.26127    |
| AverageReturn        | 1761.3     |
| MinReturn            | 195.37     |
| MaxReturn            | 1898       |
| StdReturn            | 248.68     |
| AverageEpisodeLength | 967.53     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.22     |
| TotalNEpisodes       | 20479      |
| TotalNSamples        | 5.7576e+06 |
| ExplainedVariance    | 0.021992   |
-------------------------------------
[2018-12-22 12:05:20.193763 UTC] Saving snapshot
[2018-12-22 12:05:20.201760 UTC] Starting iteration 1151
[2018-12-22 12:05:20.201964 UTC] Start collecting samples
[2018-12-22 12:05:23.109261 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:23.184342 UTC] Performing policy update
[2018-12-22 12:05:23.185238 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:23.275472 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:24.334658 UTC] Performing line search
[2018-12-22 12:05:24.461912 UTC] Updating baseline
[2018-12-22 12:05:25.767241 UTC] Computing logging information
-------------------------------------
| Iteration            | 1151       |
| ExpectedImprovement  | 0.019356   |
| ActualImprovement    | 0.017677   |
| ImprovementRatio     | 0.91326    |
| MeanKL               | 0.0078599  |
| Entropy              | -1.0687    |
| Perplexity           | 0.34345    |
| AveragePolicyStd     | 0.20562    |
| AveragePolicyStd[0]  | 0.22572    |
| AveragePolicyStd[1]  | 0.21243    |
| AveragePolicyStd[2]  | 0.16374    |
| AveragePolicyStd[3]  | 0.21263    |
| AveragePolicyStd[4]  | 0.15823    |
| AveragePolicyStd[5]  | 0.26097    |
| AverageReturn        | 1761.9     |
| MinReturn            | 195.37     |
| MaxReturn            | 1898       |
| StdReturn            | 248.8      |
| AverageEpisodeLength | 967.53     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.22     |
| TotalNEpisodes       | 20482      |
| TotalNSamples        | 5.7606e+06 |
| ExplainedVariance    | 0.035918   |
-------------------------------------
[2018-12-22 12:05:26.180215 UTC] Saving snapshot
[2018-12-22 12:05:26.180494 UTC] Starting iteration 1152
[2018-12-22 12:05:26.180627 UTC] Start collecting samples
[2018-12-22 12:05:29.098765 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:29.174575 UTC] Performing policy update
[2018-12-22 12:05:29.175278 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:29.265687 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:30.330634 UTC] Performing line search
[2018-12-22 12:05:30.457726 UTC] Updating baseline
[2018-12-22 12:05:31.968816 UTC] Computing logging information
-------------------------------------
| Iteration            | 1152       |
| ExpectedImprovement  | 0.016573   |
| ActualImprovement    | 0.015601   |
| ImprovementRatio     | 0.94133    |
| MeanKL               | 0.0079934  |
| Entropy              | -1.0725    |
| Perplexity           | 0.34215    |
| AveragePolicyStd     | 0.20549    |
| AveragePolicyStd[0]  | 0.2254     |
| AveragePolicyStd[1]  | 0.21247    |
| AveragePolicyStd[2]  | 0.16295    |
| AveragePolicyStd[3]  | 0.21247    |
| AveragePolicyStd[4]  | 0.15879    |
| AveragePolicyStd[5]  | 0.26084    |
| AverageReturn        | 1764.9     |
| MinReturn            | 195.37     |
| MaxReturn            | 1898       |
| StdReturn            | 249.62     |
| AverageEpisodeLength | 967.53     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.22     |
| TotalNEpisodes       | 20487      |
| TotalNSamples        | 5.7656e+06 |
| ExplainedVariance    | 0.00392    |
-------------------------------------
[2018-12-22 12:05:32.378564 UTC] Saving snapshot
[2018-12-22 12:05:32.378816 UTC] Starting iteration 1153
[2018-12-22 12:05:32.378938 UTC] Start collecting samples
[2018-12-22 12:05:35.301629 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:35.377473 UTC] Performing policy update
[2018-12-22 12:05:35.378137 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:35.466762 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:36.524988 UTC] Performing line search
[2018-12-22 12:05:36.651752 UTC] Updating baseline
[2018-12-22 12:05:37.969550 UTC] Computing logging information
-------------------------------------
| Iteration            | 1153       |
| ExpectedImprovement  | 0.017215   |
| ActualImprovement    | 0.014971   |
| ImprovementRatio     | 0.86964    |
| MeanKL               | 0.0080717  |
| Entropy              | -1.0772    |
| Perplexity           | 0.34055    |
| AveragePolicyStd     | 0.20525    |
| AveragePolicyStd[0]  | 0.22555    |
| AveragePolicyStd[1]  | 0.21182    |
| AveragePolicyStd[2]  | 0.16344    |
| AveragePolicyStd[3]  | 0.21225    |
| AveragePolicyStd[4]  | 0.15887    |
| AveragePolicyStd[5]  | 0.2596     |
| AverageReturn        | 1781.3     |
| MinReturn            | 555.36     |
| MaxReturn            | 1898       |
| StdReturn            | 193.52     |
| AverageEpisodeLength | 976.23     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.69     |
| TotalNEpisodes       | 20492      |
| TotalNSamples        | 5.7706e+06 |
| ExplainedVariance    | 0.0045165  |
-------------------------------------
[2018-12-22 12:05:38.375001 UTC] Saving snapshot
[2018-12-22 12:05:38.375276 UTC] Starting iteration 1154
[2018-12-22 12:05:38.375396 UTC] Start collecting samples
[2018-12-22 12:05:41.311039 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:41.387303 UTC] Performing policy update
[2018-12-22 12:05:41.388062 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:41.479374 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:42.547849 UTC] Performing line search
[2018-12-22 12:05:42.674681 UTC] Updating baseline
[2018-12-22 12:05:44.341242 UTC] Computing logging information
-------------------------------------
| Iteration            | 1154       |
| ExpectedImprovement  | 0.016726   |
| ActualImprovement    | 0.014909   |
| ImprovementRatio     | 0.89138    |
| MeanKL               | 0.0089383  |
| Entropy              | -1.0832    |
| Perplexity           | 0.33852    |
| AveragePolicyStd     | 0.20505    |
| AveragePolicyStd[0]  | 0.22482    |
| AveragePolicyStd[1]  | 0.21185    |
| AveragePolicyStd[2]  | 0.16345    |
| AveragePolicyStd[3]  | 0.21159    |
| AveragePolicyStd[4]  | 0.1588     |
| AveragePolicyStd[5]  | 0.25977    |
| AverageReturn        | 1779.7     |
| MinReturn            | 555.36     |
| MaxReturn            | 1898       |
| StdReturn            | 193.25     |
| AverageEpisodeLength | 976.23     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.69     |
| TotalNEpisodes       | 20497      |
| TotalNSamples        | 5.7756e+06 |
| ExplainedVariance    | 0.010576   |
-------------------------------------
[2018-12-22 12:05:44.748726 UTC] Saving snapshot
[2018-12-22 12:05:44.748977 UTC] Starting iteration 1155
[2018-12-22 12:05:44.749097 UTC] Start collecting samples
[2018-12-22 12:05:47.682560 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:47.757978 UTC] Performing policy update
[2018-12-22 12:05:47.758634 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:47.847871 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:48.912705 UTC] Performing line search
[2018-12-22 12:05:49.039376 UTC] Updating baseline
[2018-12-22 12:05:50.243020 UTC] Computing logging information
--------------------------------------
| Iteration            | 1155        |
| ExpectedImprovement  | 0.015755    |
| ActualImprovement    | 0.015112    |
| ImprovementRatio     | 0.9592      |
| MeanKL               | 0.0085828   |
| Entropy              | -1.0757     |
| Perplexity           | 0.34105     |
| AveragePolicyStd     | 0.20529     |
| AveragePolicyStd[0]  | 0.22533     |
| AveragePolicyStd[1]  | 0.21221     |
| AveragePolicyStd[2]  | 0.16392     |
| AveragePolicyStd[3]  | 0.21203     |
| AveragePolicyStd[4]  | 0.15867     |
| AveragePolicyStd[5]  | 0.2596      |
| AverageReturn        | 1779.3      |
| MinReturn            | 555.36      |
| MaxReturn            | 1898        |
| StdReturn            | 193.15      |
| AverageEpisodeLength | 976.23      |
| MinEpisodeLength     | 325         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 100.69      |
| TotalNEpisodes       | 20502       |
| TotalNSamples        | 5.7806e+06  |
| ExplainedVariance    | -0.00024753 |
--------------------------------------
[2018-12-22 12:05:50.655219 UTC] Saving snapshot
[2018-12-22 12:05:50.655456 UTC] Starting iteration 1156
[2018-12-22 12:05:50.655591 UTC] Start collecting samples
[2018-12-22 12:05:53.595446 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:53.672447 UTC] Performing policy update
[2018-12-22 12:05:53.673056 UTC] Computing gradient in Euclidean space
[2018-12-22 12:05:53.763835 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:05:54.823415 UTC] Performing line search
[2018-12-22 12:05:54.951288 UTC] Updating baseline
[2018-12-22 12:05:56.531371 UTC] Computing logging information
-------------------------------------
| Iteration            | 1156       |
| ExpectedImprovement  | 0.015732   |
| ActualImprovement    | 0.014912   |
| ImprovementRatio     | 0.94787    |
| MeanKL               | 0.0075632  |
| Entropy              | -1.0706    |
| Perplexity           | 0.34279    |
| AveragePolicyStd     | 0.20549    |
| AveragePolicyStd[0]  | 0.22506    |
| AveragePolicyStd[1]  | 0.21253    |
| AveragePolicyStd[2]  | 0.16393    |
| AveragePolicyStd[3]  | 0.21233    |
| AveragePolicyStd[4]  | 0.15876    |
| AveragePolicyStd[5]  | 0.26031    |
| AverageReturn        | 1789.1     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 174.73     |
| AverageEpisodeLength | 980.98     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 89.921     |
| TotalNEpisodes       | 20508      |
| TotalNSamples        | 5.7866e+06 |
| ExplainedVariance    | 0.00028093 |
-------------------------------------
[2018-12-22 12:05:56.940010 UTC] Saving snapshot
[2018-12-22 12:05:56.940251 UTC] Starting iteration 1157
[2018-12-22 12:05:56.940384 UTC] Start collecting samples
[2018-12-22 12:05:59.864585 UTC] Computing input variables for policy optimization
[2018-12-22 12:05:59.943978 UTC] Performing policy update
[2018-12-22 12:05:59.944620 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:00.034300 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:01.098280 UTC] Performing line search
[2018-12-22 12:06:01.226665 UTC] Updating baseline
[2018-12-22 12:06:02.964158 UTC] Computing logging information
-------------------------------------
| Iteration            | 1157       |
| ExpectedImprovement  | 0.016653   |
| ActualImprovement    | 0.015762   |
| ImprovementRatio     | 0.94646    |
| MeanKL               | 0.0084459  |
| Entropy              | -1.0754    |
| Perplexity           | 0.34116    |
| AveragePolicyStd     | 0.20534    |
| AveragePolicyStd[0]  | 0.22539    |
| AveragePolicyStd[1]  | 0.21207    |
| AveragePolicyStd[2]  | 0.16351    |
| AveragePolicyStd[3]  | 0.21207    |
| AveragePolicyStd[4]  | 0.15878    |
| AveragePolicyStd[5]  | 0.26019    |
| AverageReturn        | 1796       |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 167.5      |
| AverageEpisodeLength | 983.97     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 85.42      |
| TotalNEpisodes       | 20513      |
| TotalNSamples        | 5.7916e+06 |
| ExplainedVariance    | 0.0027959  |
-------------------------------------
[2018-12-22 12:06:03.372967 UTC] Saving snapshot
[2018-12-22 12:06:03.373214 UTC] Starting iteration 1158
[2018-12-22 12:06:03.373333 UTC] Start collecting samples
[2018-12-22 12:06:06.297424 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:06.371840 UTC] Performing policy update
[2018-12-22 12:06:06.372422 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:06.463166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:07.505598 UTC] Performing line search
[2018-12-22 12:06:07.633308 UTC] Updating baseline
[2018-12-22 12:06:09.211047 UTC] Computing logging information
-------------------------------------
| Iteration            | 1158       |
| ExpectedImprovement  | 0.016164   |
| ActualImprovement    | 0.015072   |
| ImprovementRatio     | 0.93247    |
| MeanKL               | 0.0077637  |
| Entropy              | -1.0786    |
| Perplexity           | 0.34005    |
| AveragePolicyStd     | 0.20522    |
| AveragePolicyStd[0]  | 0.22561    |
| AveragePolicyStd[1]  | 0.21183    |
| AveragePolicyStd[2]  | 0.16358    |
| AveragePolicyStd[3]  | 0.21206    |
| AveragePolicyStd[4]  | 0.15854    |
| AveragePolicyStd[5]  | 0.25969    |
| AverageReturn        | 1796.4     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 167.57     |
| AverageEpisodeLength | 983.97     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 85.42      |
| TotalNEpisodes       | 20517      |
| TotalNSamples        | 5.7956e+06 |
| ExplainedVariance    | 0.0035583  |
-------------------------------------
[2018-12-22 12:06:09.614477 UTC] Saving snapshot
[2018-12-22 12:06:09.614742 UTC] Starting iteration 1159
[2018-12-22 12:06:09.614886 UTC] Start collecting samples
[2018-12-22 12:06:12.601575 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:12.677618 UTC] Performing policy update
[2018-12-22 12:06:12.678227 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:12.767233 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:13.825470 UTC] Performing line search
[2018-12-22 12:06:13.955040 UTC] Updating baseline
[2018-12-22 12:06:15.363615 UTC] Computing logging information
-------------------------------------
| Iteration            | 1159       |
| ExpectedImprovement  | 0.016936   |
| ActualImprovement    | 0.016324   |
| ImprovementRatio     | 0.96384    |
| MeanKL               | 0.0081056  |
| Entropy              | -1.0776    |
| Perplexity           | 0.34042    |
| AveragePolicyStd     | 0.20528    |
| AveragePolicyStd[0]  | 0.22529    |
| AveragePolicyStd[1]  | 0.21162    |
| AveragePolicyStd[2]  | 0.16355    |
| AveragePolicyStd[3]  | 0.21258    |
| AveragePolicyStd[4]  | 0.15835    |
| AveragePolicyStd[5]  | 0.26031    |
| AverageReturn        | 1796.7     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 166.47     |
| AverageEpisodeLength | 984        |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 85.378     |
| TotalNEpisodes       | 20525      |
| TotalNSamples        | 5.8034e+06 |
| ExplainedVariance    | 0.047184   |
-------------------------------------
[2018-12-22 12:06:15.777862 UTC] Saving snapshot
[2018-12-22 12:06:15.778112 UTC] Starting iteration 1160
[2018-12-22 12:06:15.778231 UTC] Start collecting samples
[2018-12-22 12:06:18.697247 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:18.771893 UTC] Performing policy update
[2018-12-22 12:06:18.772590 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:18.861683 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:19.931707 UTC] Performing line search
[2018-12-22 12:06:20.060367 UTC] Updating baseline
[2018-12-22 12:06:21.291093 UTC] Computing logging information
-------------------------------------
| Iteration            | 1160       |
| ExpectedImprovement  | 0.016284   |
| ActualImprovement    | 0.015032   |
| ImprovementRatio     | 0.92313    |
| MeanKL               | 0.0072193  |
| Entropy              | -1.0797    |
| Perplexity           | 0.33969    |
| AveragePolicyStd     | 0.20524    |
| AveragePolicyStd[0]  | 0.22552    |
| AveragePolicyStd[1]  | 0.21071    |
| AveragePolicyStd[2]  | 0.16355    |
| AveragePolicyStd[3]  | 0.21225    |
| AveragePolicyStd[4]  | 0.15833    |
| AveragePolicyStd[5]  | 0.26106    |
| AverageReturn        | 1801.1     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 161.26     |
| AverageEpisodeLength | 986.32     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.584     |
| TotalNEpisodes       | 20528      |
| TotalNSamples        | 5.8064e+06 |
| ExplainedVariance    | -0.011827  |
-------------------------------------
[2018-12-22 12:06:21.706070 UTC] Saving snapshot
[2018-12-22 12:06:21.714150 UTC] Starting iteration 1161
[2018-12-22 12:06:21.714379 UTC] Start collecting samples
[2018-12-22 12:06:24.618580 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:24.693046 UTC] Performing policy update
[2018-12-22 12:06:24.693658 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:24.783453 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:25.845136 UTC] Performing line search
[2018-12-22 12:06:25.972629 UTC] Updating baseline
[2018-12-22 12:06:27.550416 UTC] Computing logging information
-------------------------------------
| Iteration            | 1161       |
| ExpectedImprovement  | 0.018449   |
| ActualImprovement    | 0.016912   |
| ImprovementRatio     | 0.91669    |
| MeanKL               | 0.0079533  |
| Entropy              | -1.0853    |
| Perplexity           | 0.33781    |
| AveragePolicyStd     | 0.20506    |
| AveragePolicyStd[0]  | 0.22555    |
| AveragePolicyStd[1]  | 0.21006    |
| AveragePolicyStd[2]  | 0.16305    |
| AveragePolicyStd[3]  | 0.21246    |
| AveragePolicyStd[4]  | 0.15838    |
| AveragePolicyStd[5]  | 0.26084    |
| AverageReturn        | 1800.1     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 161.03     |
| AverageEpisodeLength | 986.32     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.584     |
| TotalNEpisodes       | 20532      |
| TotalNSamples        | 5.8104e+06 |
| ExplainedVariance    | 0.0099004  |
-------------------------------------
[2018-12-22 12:06:27.963015 UTC] Saving snapshot
[2018-12-22 12:06:27.963255 UTC] Starting iteration 1162
[2018-12-22 12:06:27.963389 UTC] Start collecting samples
[2018-12-22 12:06:30.906562 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:30.982960 UTC] Performing policy update
[2018-12-22 12:06:30.983657 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:31.074309 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:32.136833 UTC] Performing line search
[2018-12-22 12:06:32.263285 UTC] Updating baseline
[2018-12-22 12:06:33.663702 UTC] Computing logging information
-------------------------------------
| Iteration            | 1162       |
| ExpectedImprovement  | 0.019194   |
| ActualImprovement    | 0.017863   |
| ImprovementRatio     | 0.93063    |
| MeanKL               | 0.0081979  |
| Entropy              | -1.0974    |
| Perplexity           | 0.33374    |
| AveragePolicyStd     | 0.20464    |
| AveragePolicyStd[0]  | 0.22562    |
| AveragePolicyStd[1]  | 0.20891    |
| AveragePolicyStd[2]  | 0.1629     |
| AveragePolicyStd[3]  | 0.21186    |
| AveragePolicyStd[4]  | 0.15816    |
| AveragePolicyStd[5]  | 0.26038    |
| AverageReturn        | 1799.1     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 162.71     |
| AverageEpisodeLength | 984.85     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 83.629     |
| TotalNEpisodes       | 20538      |
| TotalNSamples        | 5.8163e+06 |
| ExplainedVariance    | 0.11482    |
-------------------------------------
[2018-12-22 12:06:34.081366 UTC] Saving snapshot
[2018-12-22 12:06:34.081633 UTC] Starting iteration 1163
[2018-12-22 12:06:34.081799 UTC] Start collecting samples
[2018-12-22 12:06:37.020593 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:37.095734 UTC] Performing policy update
[2018-12-22 12:06:37.096479 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:37.186203 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:38.236572 UTC] Performing line search
[2018-12-22 12:06:38.362755 UTC] Updating baseline
[2018-12-22 12:06:39.903390 UTC] Computing logging information
-------------------------------------
| Iteration            | 1163       |
| ExpectedImprovement  | 0.015567   |
| ActualImprovement    | 0.014876   |
| ImprovementRatio     | 0.95563    |
| MeanKL               | 0.0086642  |
| Entropy              | -1.0972    |
| Perplexity           | 0.3338     |
| AveragePolicyStd     | 0.20468    |
| AveragePolicyStd[0]  | 0.22594    |
| AveragePolicyStd[1]  | 0.20912    |
| AveragePolicyStd[2]  | 0.16274    |
| AveragePolicyStd[3]  | 0.21174    |
| AveragePolicyStd[4]  | 0.15787    |
| AveragePolicyStd[5]  | 0.26067    |
| AverageReturn        | 1808.4     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 136.45     |
| AverageEpisodeLength | 989.58     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 69.839     |
| TotalNEpisodes       | 20543      |
| TotalNSamples        | 5.8213e+06 |
| ExplainedVariance    | 0.0049119  |
-------------------------------------
[2018-12-22 12:06:40.311101 UTC] Saving snapshot
[2018-12-22 12:06:40.311341 UTC] Starting iteration 1164
[2018-12-22 12:06:40.311459 UTC] Start collecting samples
[2018-12-22 12:06:43.232203 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:43.307447 UTC] Performing policy update
[2018-12-22 12:06:43.308165 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:43.398202 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:44.458366 UTC] Performing line search
[2018-12-22 12:06:44.585260 UTC] Updating baseline
[2018-12-22 12:06:46.293885 UTC] Computing logging information
-------------------------------------
| Iteration            | 1164       |
| ExpectedImprovement  | 0.016483   |
| ActualImprovement    | 0.015974   |
| ImprovementRatio     | 0.96909    |
| MeanKL               | 0.0080853  |
| Entropy              | -1.1093    |
| Perplexity           | 0.32979    |
| AveragePolicyStd     | 0.20427    |
| AveragePolicyStd[0]  | 0.22568    |
| AveragePolicyStd[1]  | 0.20891    |
| AveragePolicyStd[2]  | 0.16238    |
| AveragePolicyStd[3]  | 0.21191    |
| AveragePolicyStd[4]  | 0.15724    |
| AveragePolicyStd[5]  | 0.25949    |
| AverageReturn        | 1810.1     |
| MinReturn            | 555.36     |
| MaxReturn            | 1907.4     |
| StdReturn            | 136.6      |
| AverageEpisodeLength | 989.58     |
| MinEpisodeLength     | 325        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 69.839     |
| TotalNEpisodes       | 20547      |
| TotalNSamples        | 5.8253e+06 |
| ExplainedVariance    | 0.0016111  |
-------------------------------------
[2018-12-22 12:06:46.701232 UTC] Saving snapshot
[2018-12-22 12:06:46.701525 UTC] Starting iteration 1165
[2018-12-22 12:06:46.701648 UTC] Start collecting samples
[2018-12-22 12:06:49.661088 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:49.737889 UTC] Performing policy update
[2018-12-22 12:06:49.738527 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:49.828119 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:50.874857 UTC] Performing line search
[2018-12-22 12:06:51.000609 UTC] Updating baseline
[2018-12-22 12:06:52.554350 UTC] Computing logging information
-------------------------------------
| Iteration            | 1165       |
| ExpectedImprovement  | 0.019395   |
| ActualImprovement    | 0.01861    |
| ImprovementRatio     | 0.95954    |
| MeanKL               | 0.0078874  |
| Entropy              | -1.1089    |
| Perplexity           | 0.32993    |
| AveragePolicyStd     | 0.20424    |
| AveragePolicyStd[0]  | 0.2253     |
| AveragePolicyStd[1]  | 0.20917    |
| AveragePolicyStd[2]  | 0.16276    |
| AveragePolicyStd[3]  | 0.21153    |
| AveragePolicyStd[4]  | 0.15749    |
| AveragePolicyStd[5]  | 0.25917    |
| AverageReturn        | 1826.2     |
| MinReturn            | 1562.5     |
| MaxReturn            | 1907.4     |
| StdReturn            | 51.77      |
| AverageEpisodeLength | 996.75     |
| MinEpisodeLength     | 853        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 19.871     |
| TotalNEpisodes       | 20554      |
| TotalNSamples        | 5.8323e+06 |
| ExplainedVariance    | 0.07919    |
-------------------------------------
[2018-12-22 12:06:52.963494 UTC] Saving snapshot
[2018-12-22 12:06:52.963831 UTC] Starting iteration 1166
[2018-12-22 12:06:52.963950 UTC] Start collecting samples
[2018-12-22 12:06:55.916775 UTC] Computing input variables for policy optimization
[2018-12-22 12:06:55.996692 UTC] Performing policy update
[2018-12-22 12:06:55.997468 UTC] Computing gradient in Euclidean space
[2018-12-22 12:06:56.088360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:06:57.146292 UTC] Performing line search
[2018-12-22 12:06:57.274647 UTC] Updating baseline
[2018-12-22 12:06:58.688634 UTC] Computing logging information
-------------------------------------
| Iteration            | 1166       |
| ExpectedImprovement  | 0.018542   |
| ActualImprovement    | 0.017393   |
| ImprovementRatio     | 0.93801    |
| MeanKL               | 0.0076857  |
| Entropy              | -1.1168    |
| Perplexity           | 0.32734    |
| AveragePolicyStd     | 0.20397    |
| AveragePolicyStd[0]  | 0.22525    |
| AveragePolicyStd[1]  | 0.20896    |
| AveragePolicyStd[2]  | 0.16251    |
| AveragePolicyStd[3]  | 0.21106    |
| AveragePolicyStd[4]  | 0.15725    |
| AveragePolicyStd[5]  | 0.2588     |
| AverageReturn        | 1812       |
| MinReturn            | 420.38     |
| MaxReturn            | 1909.7     |
| StdReturn            | 149.75     |
| AverageEpisodeLength | 988.9      |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 74.93      |
| TotalNEpisodes       | 20560      |
| TotalNSamples        | 5.8375e+06 |
| ExplainedVariance    | 0.21119    |
-------------------------------------
[2018-12-22 12:06:59.097611 UTC] Saving snapshot
[2018-12-22 12:06:59.097910 UTC] Starting iteration 1167
[2018-12-22 12:06:59.098030 UTC] Start collecting samples
[2018-12-22 12:07:02.027354 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:02.102809 UTC] Performing policy update
[2018-12-22 12:07:02.103421 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:02.192773 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:03.240415 UTC] Performing line search
[2018-12-22 12:07:03.367045 UTC] Updating baseline
[2018-12-22 12:07:05.418392 UTC] Computing logging information
-------------------------------------
| Iteration            | 1167       |
| ExpectedImprovement  | 0.01664    |
| ActualImprovement    | 0.015461   |
| ImprovementRatio     | 0.92919    |
| MeanKL               | 0.0075924  |
| Entropy              | -1.1211    |
| Perplexity           | 0.32592    |
| AveragePolicyStd     | 0.20384    |
| AveragePolicyStd[0]  | 0.22538    |
| AveragePolicyStd[1]  | 0.20822    |
| AveragePolicyStd[2]  | 0.1627     |
| AveragePolicyStd[3]  | 0.21026    |
| AveragePolicyStd[4]  | 0.15711    |
| AveragePolicyStd[5]  | 0.25936    |
| AverageReturn        | 1812.8     |
| MinReturn            | 420.38     |
| MaxReturn            | 1909.7     |
| StdReturn            | 149.95     |
| AverageEpisodeLength | 988.9      |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 74.93      |
| TotalNEpisodes       | 20563      |
| TotalNSamples        | 5.8405e+06 |
| ExplainedVariance    | -0.0040392 |
-------------------------------------
[2018-12-22 12:07:05.827626 UTC] Saving snapshot
[2018-12-22 12:07:05.827913 UTC] Starting iteration 1168
[2018-12-22 12:07:05.828032 UTC] Start collecting samples
[2018-12-22 12:07:08.825778 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:08.903948 UTC] Performing policy update
[2018-12-22 12:07:08.904699 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:08.994716 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:10.055933 UTC] Performing line search
[2018-12-22 12:07:10.183666 UTC] Updating baseline
[2018-12-22 12:07:11.693538 UTC] Computing logging information
-------------------------------------
| Iteration            | 1168       |
| ExpectedImprovement  | 0.01658    |
| ActualImprovement    | 0.016047   |
| ImprovementRatio     | 0.96787    |
| MeanKL               | 0.0082138  |
| Entropy              | -1.1226    |
| Perplexity           | 0.32545    |
| AveragePolicyStd     | 0.20375    |
| AveragePolicyStd[0]  | 0.22555    |
| AveragePolicyStd[1]  | 0.20828    |
| AveragePolicyStd[2]  | 0.16281    |
| AveragePolicyStd[3]  | 0.20997    |
| AveragePolicyStd[4]  | 0.15724    |
| AveragePolicyStd[5]  | 0.25867    |
| AverageReturn        | 1806.6     |
| MinReturn            | 420.38     |
| MaxReturn            | 1909.7     |
| StdReturn            | 155.13     |
| AverageEpisodeLength | 985.86     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.235     |
| TotalNEpisodes       | 20571      |
| TotalNSamples        | 5.8481e+06 |
| ExplainedVariance    | 0.14999    |
-------------------------------------
[2018-12-22 12:07:12.115830 UTC] Saving snapshot
[2018-12-22 12:07:12.116091 UTC] Starting iteration 1169
[2018-12-22 12:07:12.116207 UTC] Start collecting samples
[2018-12-22 12:07:15.020221 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:15.094662 UTC] Performing policy update
[2018-12-22 12:07:15.095314 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:15.183845 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:16.242663 UTC] Performing line search
[2018-12-22 12:07:16.369477 UTC] Updating baseline
[2018-12-22 12:07:17.787173 UTC] Computing logging information
-------------------------------------
| Iteration            | 1169       |
| ExpectedImprovement  | 0.016876   |
| ActualImprovement    | 0.015564   |
| ImprovementRatio     | 0.92229    |
| MeanKL               | 0.0089737  |
| Entropy              | -1.119     |
| Perplexity           | 0.3266     |
| AveragePolicyStd     | 0.20389    |
| AveragePolicyStd[0]  | 0.2264     |
| AveragePolicyStd[1]  | 0.2081     |
| AveragePolicyStd[2]  | 0.16268    |
| AveragePolicyStd[3]  | 0.20987    |
| AveragePolicyStd[4]  | 0.1574     |
| AveragePolicyStd[5]  | 0.25891    |
| AverageReturn        | 1807       |
| MinReturn            | 420.38     |
| MaxReturn            | 1909.7     |
| StdReturn            | 155.16     |
| AverageEpisodeLength | 985.86     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.235     |
| TotalNEpisodes       | 20574      |
| TotalNSamples        | 5.8511e+06 |
| ExplainedVariance    | 0.0072554  |
-------------------------------------
[2018-12-22 12:07:18.200865 UTC] Saving snapshot
[2018-12-22 12:07:18.201179 UTC] Starting iteration 1170
[2018-12-22 12:07:18.201298 UTC] Start collecting samples
[2018-12-22 12:07:21.133364 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:21.210547 UTC] Performing policy update
[2018-12-22 12:07:21.211131 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:21.300389 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:22.354361 UTC] Performing line search
[2018-12-22 12:07:22.480905 UTC] Updating baseline
[2018-12-22 12:07:23.802442 UTC] Computing logging information
-------------------------------------
| Iteration            | 1170       |
| ExpectedImprovement  | 0.016713   |
| ActualImprovement    | 0.015388   |
| ImprovementRatio     | 0.92071    |
| MeanKL               | 0.0077684  |
| Entropy              | -1.1256    |
| Perplexity           | 0.32446    |
| AveragePolicyStd     | 0.20362    |
| AveragePolicyStd[0]  | 0.22576    |
| AveragePolicyStd[1]  | 0.20758    |
| AveragePolicyStd[2]  | 0.16309    |
| AveragePolicyStd[3]  | 0.20894    |
| AveragePolicyStd[4]  | 0.1576     |
| AveragePolicyStd[5]  | 0.25876    |
| AverageReturn        | 1807.7     |
| MinReturn            | 420.38     |
| MaxReturn            | 1909.7     |
| StdReturn            | 154.85     |
| AverageEpisodeLength | 985.86     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.235     |
| TotalNEpisodes       | 20578      |
| TotalNSamples        | 5.8551e+06 |
| ExplainedVariance    | -0.0014836 |
-------------------------------------
[2018-12-22 12:07:24.216875 UTC] Saving snapshot
[2018-12-22 12:07:24.224953 UTC] Starting iteration 1171
[2018-12-22 12:07:24.225155 UTC] Start collecting samples
[2018-12-22 12:07:27.232057 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:27.310543 UTC] Performing policy update
[2018-12-22 12:07:27.311122 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:27.400085 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:28.456860 UTC] Performing line search
[2018-12-22 12:07:28.583255 UTC] Updating baseline
[2018-12-22 12:07:29.815175 UTC] Computing logging information
-------------------------------------
| Iteration            | 1171       |
| ExpectedImprovement  | 0.018035   |
| ActualImprovement    | 0.016858   |
| ImprovementRatio     | 0.93471    |
| MeanKL               | 0.0082611  |
| Entropy              | -1.1269    |
| Perplexity           | 0.32405    |
| AveragePolicyStd     | 0.20357    |
| AveragePolicyStd[0]  | 0.22507    |
| AveragePolicyStd[1]  | 0.2082     |
| AveragePolicyStd[2]  | 0.16347    |
| AveragePolicyStd[3]  | 0.20844    |
| AveragePolicyStd[4]  | 0.15735    |
| AveragePolicyStd[5]  | 0.25887    |
| AverageReturn        | 1804.5     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 162.69     |
| AverageEpisodeLength | 983.45     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 81.41      |
| TotalNEpisodes       | 20586      |
| TotalNSamples        | 5.8629e+06 |
| ExplainedVariance    | 0.11845    |
-------------------------------------
[2018-12-22 12:07:30.225070 UTC] Saving snapshot
[2018-12-22 12:07:30.225312 UTC] Starting iteration 1172
[2018-12-22 12:07:30.225432 UTC] Start collecting samples
[2018-12-22 12:07:33.135865 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:33.211097 UTC] Performing policy update
[2018-12-22 12:07:33.211859 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:33.302579 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:34.374648 UTC] Performing line search
[2018-12-22 12:07:34.502715 UTC] Updating baseline
[2018-12-22 12:07:36.018957 UTC] Computing logging information
-------------------------------------
| Iteration            | 1172       |
| ExpectedImprovement  | 0.016539   |
| ActualImprovement    | 0.015455   |
| ImprovementRatio     | 0.93449    |
| MeanKL               | 0.008249   |
| Entropy              | -1.1338    |
| Perplexity           | 0.32182    |
| AveragePolicyStd     | 0.20337    |
| AveragePolicyStd[0]  | 0.22563    |
| AveragePolicyStd[1]  | 0.20721    |
| AveragePolicyStd[2]  | 0.16333    |
| AveragePolicyStd[3]  | 0.2075     |
| AveragePolicyStd[4]  | 0.15712    |
| AveragePolicyStd[5]  | 0.25946    |
| AverageReturn        | 1805.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 162.83     |
| AverageEpisodeLength | 983.45     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 81.41      |
| TotalNEpisodes       | 20589      |
| TotalNSamples        | 5.8659e+06 |
| ExplainedVariance    | 0.0015612  |
-------------------------------------
[2018-12-22 12:07:36.435639 UTC] Saving snapshot
[2018-12-22 12:07:36.435933 UTC] Starting iteration 1173
[2018-12-22 12:07:36.436056 UTC] Start collecting samples
[2018-12-22 12:07:39.372755 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:39.451064 UTC] Performing policy update
[2018-12-22 12:07:39.451895 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:39.542200 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:40.610822 UTC] Performing line search
[2018-12-22 12:07:40.738381 UTC] Updating baseline
[2018-12-22 12:07:42.330791 UTC] Computing logging information
-------------------------------------
| Iteration            | 1173       |
| ExpectedImprovement  | 0.019439   |
| ActualImprovement    | 0.018857   |
| ImprovementRatio     | 0.97009    |
| MeanKL               | 0.0078371  |
| Entropy              | -1.1389    |
| Perplexity           | 0.32018    |
| AveragePolicyStd     | 0.20322    |
| AveragePolicyStd[0]  | 0.22556    |
| AveragePolicyStd[1]  | 0.20701    |
| AveragePolicyStd[2]  | 0.16303    |
| AveragePolicyStd[3]  | 0.2069     |
| AveragePolicyStd[4]  | 0.15707    |
| AveragePolicyStd[5]  | 0.25978    |
| AverageReturn        | 1796.8     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 192.01     |
| AverageEpisodeLength | 977.98     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.998     |
| TotalNEpisodes       | 20595      |
| TotalNSamples        | 5.8714e+06 |
| ExplainedVariance    | 0.14466    |
-------------------------------------
[2018-12-22 12:07:42.742395 UTC] Saving snapshot
[2018-12-22 12:07:42.742666 UTC] Starting iteration 1174
[2018-12-22 12:07:42.742789 UTC] Start collecting samples
[2018-12-22 12:07:45.688160 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:45.764599 UTC] Performing policy update
[2018-12-22 12:07:45.765481 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:45.856155 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:46.929320 UTC] Performing line search
[2018-12-22 12:07:47.057169 UTC] Updating baseline
[2018-12-22 12:07:48.573535 UTC] Computing logging information
-------------------------------------
| Iteration            | 1174       |
| ExpectedImprovement  | 0.016955   |
| ActualImprovement    | 0.015986   |
| ImprovementRatio     | 0.94284    |
| MeanKL               | 0.0085139  |
| Entropy              | -1.1431    |
| Perplexity           | 0.31882    |
| AveragePolicyStd     | 0.2031     |
| AveragePolicyStd[0]  | 0.22544    |
| AveragePolicyStd[1]  | 0.20674    |
| AveragePolicyStd[2]  | 0.1627     |
| AveragePolicyStd[3]  | 0.20669    |
| AveragePolicyStd[4]  | 0.15705    |
| AveragePolicyStd[5]  | 0.25998    |
| AverageReturn        | 1795.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 195.63     |
| AverageEpisodeLength | 975.69     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 99.131     |
| TotalNEpisodes       | 20600      |
| TotalNSamples        | 5.8761e+06 |
| ExplainedVariance    | 0.07683    |
-------------------------------------
[2018-12-22 12:07:48.990752 UTC] Saving snapshot
[2018-12-22 12:07:48.991002 UTC] Starting iteration 1175
[2018-12-22 12:07:48.991140 UTC] Start collecting samples
[2018-12-22 12:07:51.943922 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:52.022058 UTC] Performing policy update
[2018-12-22 12:07:52.022716 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:52.111281 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:53.160612 UTC] Performing line search
[2018-12-22 12:07:53.287044 UTC] Updating baseline
[2018-12-22 12:07:54.616847 UTC] Computing logging information
-------------------------------------
| Iteration            | 1175       |
| ExpectedImprovement  | 0.017178   |
| ActualImprovement    | 0.016309   |
| ImprovementRatio     | 0.94941    |
| MeanKL               | 0.007622   |
| Entropy              | -1.1466    |
| Perplexity           | 0.31771    |
| AveragePolicyStd     | 0.20298    |
| AveragePolicyStd[0]  | 0.22555    |
| AveragePolicyStd[1]  | 0.20663    |
| AveragePolicyStd[2]  | 0.16269    |
| AveragePolicyStd[3]  | 0.20645    |
| AveragePolicyStd[4]  | 0.15689    |
| AveragePolicyStd[5]  | 0.25965    |
| AverageReturn        | 1786.5     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 215.85     |
| AverageEpisodeLength | 970.75     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.56     |
| TotalNEpisodes       | 20606      |
| TotalNSamples        | 5.8816e+06 |
| ExplainedVariance    | 0.16796    |
-------------------------------------
[2018-12-22 12:07:55.029720 UTC] Saving snapshot
[2018-12-22 12:07:55.030016 UTC] Starting iteration 1176
[2018-12-22 12:07:55.030145 UTC] Start collecting samples
[2018-12-22 12:07:57.953700 UTC] Computing input variables for policy optimization
[2018-12-22 12:07:58.030075 UTC] Performing policy update
[2018-12-22 12:07:58.030860 UTC] Computing gradient in Euclidean space
[2018-12-22 12:07:58.120706 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:07:59.183906 UTC] Performing line search
[2018-12-22 12:07:59.312087 UTC] Updating baseline
[2018-12-22 12:08:00.746933 UTC] Computing logging information
-------------------------------------
| Iteration            | 1176       |
| ExpectedImprovement  | 0.019651   |
| ActualImprovement    | 0.017331   |
| ImprovementRatio     | 0.88194    |
| MeanKL               | 0.0081358  |
| Entropy              | -1.1467    |
| Perplexity           | 0.3177     |
| AveragePolicyStd     | 0.20298    |
| AveragePolicyStd[0]  | 0.22537    |
| AveragePolicyStd[1]  | 0.20606    |
| AveragePolicyStd[2]  | 0.1625     |
| AveragePolicyStd[3]  | 0.207      |
| AveragePolicyStd[4]  | 0.15709    |
| AveragePolicyStd[5]  | 0.25985    |
| AverageReturn        | 1786.9     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 216.02     |
| AverageEpisodeLength | 970.75     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.56     |
| TotalNEpisodes       | 20610      |
| TotalNSamples        | 5.8856e+06 |
| ExplainedVariance    | 0.01077    |
-------------------------------------
[2018-12-22 12:08:01.159008 UTC] Saving snapshot
[2018-12-22 12:08:01.159248 UTC] Starting iteration 1177
[2018-12-22 12:08:01.159389 UTC] Start collecting samples
[2018-12-22 12:08:04.094059 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:04.169763 UTC] Performing policy update
[2018-12-22 12:08:04.170563 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:04.260752 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:05.314645 UTC] Performing line search
[2018-12-22 12:08:05.441625 UTC] Updating baseline
[2018-12-22 12:08:06.759061 UTC] Computing logging information
-------------------------------------
| Iteration            | 1177       |
| ExpectedImprovement  | 0.017077   |
| ActualImprovement    | 0.016267   |
| ImprovementRatio     | 0.95257    |
| MeanKL               | 0.0077745  |
| Entropy              | -1.1468    |
| Perplexity           | 0.31765    |
| AveragePolicyStd     | 0.20297    |
| AveragePolicyStd[0]  | 0.22542    |
| AveragePolicyStd[1]  | 0.20601    |
| AveragePolicyStd[2]  | 0.16285    |
| AveragePolicyStd[3]  | 0.20666    |
| AveragePolicyStd[4]  | 0.15691    |
| AveragePolicyStd[5]  | 0.25999    |
| AverageReturn        | 1787       |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 216.02     |
| AverageEpisodeLength | 970.75     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.56     |
| TotalNEpisodes       | 20614      |
| TotalNSamples        | 5.8896e+06 |
| ExplainedVariance    | 0.0027039  |
-------------------------------------
[2018-12-22 12:08:07.171299 UTC] Saving snapshot
[2018-12-22 12:08:07.171549 UTC] Starting iteration 1178
[2018-12-22 12:08:07.171671 UTC] Start collecting samples
[2018-12-22 12:08:10.124140 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:10.202568 UTC] Performing policy update
[2018-12-22 12:08:10.203336 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:10.293674 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:11.349667 UTC] Performing line search
[2018-12-22 12:08:11.477024 UTC] Updating baseline
[2018-12-22 12:08:13.221263 UTC] Computing logging information
-------------------------------------
| Iteration            | 1178       |
| ExpectedImprovement  | 0.014473   |
| ActualImprovement    | 0.013742   |
| ImprovementRatio     | 0.94946    |
| MeanKL               | 0.0081284  |
| Entropy              | -1.1419    |
| Perplexity           | 0.31922    |
| AveragePolicyStd     | 0.20313    |
| AveragePolicyStd[0]  | 0.22544    |
| AveragePolicyStd[1]  | 0.20687    |
| AveragePolicyStd[2]  | 0.1628     |
| AveragePolicyStd[3]  | 0.20667    |
| AveragePolicyStd[4]  | 0.15715    |
| AveragePolicyStd[5]  | 0.25983    |
| AverageReturn        | 1789       |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 216.3      |
| AverageEpisodeLength | 970.75     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.56     |
| TotalNEpisodes       | 20621      |
| TotalNSamples        | 5.8966e+06 |
| ExplainedVariance    | 0.0030809  |
-------------------------------------
[2018-12-22 12:08:13.630760 UTC] Saving snapshot
[2018-12-22 12:08:13.631020 UTC] Starting iteration 1179
[2018-12-22 12:08:13.631137 UTC] Start collecting samples
[2018-12-22 12:08:16.573343 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:16.649206 UTC] Performing policy update
[2018-12-22 12:08:16.649947 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:16.740315 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:17.802163 UTC] Performing line search
[2018-12-22 12:08:17.931441 UTC] Updating baseline
[2018-12-22 12:08:19.162543 UTC] Computing logging information
-------------------------------------
| Iteration            | 1179       |
| ExpectedImprovement  | 0.017317   |
| ActualImprovement    | 0.016449   |
| ImprovementRatio     | 0.94984    |
| MeanKL               | 0.0091551  |
| Entropy              | -1.1378    |
| Perplexity           | 0.32054    |
| AveragePolicyStd     | 0.20328    |
| AveragePolicyStd[0]  | 0.22584    |
| AveragePolicyStd[1]  | 0.2073     |
| AveragePolicyStd[2]  | 0.16274    |
| AveragePolicyStd[3]  | 0.20647    |
| AveragePolicyStd[4]  | 0.15722    |
| AveragePolicyStd[5]  | 0.26013    |
| AverageReturn        | 1793.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 215.67     |
| AverageEpisodeLength | 972.07     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.11     |
| TotalNEpisodes       | 20626      |
| TotalNSamples        | 5.9016e+06 |
| ExplainedVariance    | -0.0040839 |
-------------------------------------
[2018-12-22 12:08:19.575883 UTC] Saving snapshot
[2018-12-22 12:08:19.576123 UTC] Starting iteration 1180
[2018-12-22 12:08:19.576240 UTC] Start collecting samples
[2018-12-22 12:08:22.506535 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:22.583918 UTC] Performing policy update
[2018-12-22 12:08:22.584549 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:22.673180 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:23.731221 UTC] Performing line search
[2018-12-22 12:08:23.862187 UTC] Updating baseline
[2018-12-22 12:08:25.543382 UTC] Computing logging information
-------------------------------------
| Iteration            | 1180       |
| ExpectedImprovement  | 0.020401   |
| ActualImprovement    | 0.018786   |
| ImprovementRatio     | 0.92082    |
| MeanKL               | 0.0074001  |
| Entropy              | -1.1381    |
| Perplexity           | 0.32042    |
| AveragePolicyStd     | 0.20328    |
| AveragePolicyStd[0]  | 0.22558    |
| AveragePolicyStd[1]  | 0.20699    |
| AveragePolicyStd[2]  | 0.16304    |
| AveragePolicyStd[3]  | 0.20644    |
| AveragePolicyStd[4]  | 0.157      |
| AveragePolicyStd[5]  | 0.26064    |
| AverageReturn        | 1790.3     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 218.82     |
| AverageEpisodeLength | 970.14     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 110.29     |
| TotalNEpisodes       | 20631      |
| TotalNSamples        | 5.9064e+06 |
| ExplainedVariance    | 0.099412   |
-------------------------------------
[2018-12-22 12:08:25.961111 UTC] Saving snapshot
[2018-12-22 12:08:25.969143 UTC] Starting iteration 1181
[2018-12-22 12:08:25.969341 UTC] Start collecting samples
[2018-12-22 12:08:28.917881 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:28.997326 UTC] Performing policy update
[2018-12-22 12:08:28.997942 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:29.088700 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:30.155978 UTC] Performing line search
[2018-12-22 12:08:30.284510 UTC] Updating baseline
[2018-12-22 12:08:31.536280 UTC] Computing logging information
-------------------------------------
| Iteration            | 1181       |
| ExpectedImprovement  | 0.016368   |
| ActualImprovement    | 0.0159     |
| ImprovementRatio     | 0.97137    |
| MeanKL               | 0.0079319  |
| Entropy              | -1.1447    |
| Perplexity           | 0.31833    |
| AveragePolicyStd     | 0.20306    |
| AveragePolicyStd[0]  | 0.22541    |
| AveragePolicyStd[1]  | 0.20612    |
| AveragePolicyStd[2]  | 0.16303    |
| AveragePolicyStd[3]  | 0.20628    |
| AveragePolicyStd[4]  | 0.15687    |
| AveragePolicyStd[5]  | 0.26067    |
| AverageReturn        | 1782.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1915.6     |
| StdReturn            | 237.42     |
| AverageEpisodeLength | 966.34     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.35     |
| TotalNEpisodes       | 20638      |
| TotalNSamples        | 5.9129e+06 |
| ExplainedVariance    | 0.090219   |
-------------------------------------
[2018-12-22 12:08:31.952419 UTC] Saving snapshot
[2018-12-22 12:08:31.952687 UTC] Starting iteration 1182
[2018-12-22 12:08:31.952808 UTC] Start collecting samples
[2018-12-22 12:08:34.868851 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:34.946442 UTC] Performing policy update
[2018-12-22 12:08:34.947320 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:35.038412 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:36.111447 UTC] Performing line search
[2018-12-22 12:08:36.240121 UTC] Updating baseline
[2018-12-22 12:08:37.402672 UTC] Computing logging information
-------------------------------------
| Iteration            | 1182       |
| ExpectedImprovement  | 0.01774    |
| ActualImprovement    | 0.017774   |
| ImprovementRatio     | 1.0019     |
| MeanKL               | 0.008016   |
| Entropy              | -1.1397    |
| Perplexity           | 0.31991    |
| AveragePolicyStd     | 0.20324    |
| AveragePolicyStd[0]  | 0.2251     |
| AveragePolicyStd[1]  | 0.20563    |
| AveragePolicyStd[2]  | 0.16307    |
| AveragePolicyStd[3]  | 0.20641    |
| AveragePolicyStd[4]  | 0.15741    |
| AveragePolicyStd[5]  | 0.26183    |
| AverageReturn        | 1784.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1936.4     |
| StdReturn            | 237.99     |
| AverageEpisodeLength | 966.34     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.35     |
| TotalNEpisodes       | 20642      |
| TotalNSamples        | 5.9169e+06 |
| ExplainedVariance    | -0.12044   |
-------------------------------------
[2018-12-22 12:08:37.816878 UTC] Saving snapshot
[2018-12-22 12:08:37.817123 UTC] Starting iteration 1183
[2018-12-22 12:08:37.817248 UTC] Start collecting samples
[2018-12-22 12:08:40.788684 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:40.865863 UTC] Performing policy update
[2018-12-22 12:08:40.866591 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:40.955017 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:42.008414 UTC] Performing line search
[2018-12-22 12:08:42.135096 UTC] Updating baseline
[2018-12-22 12:08:43.375052 UTC] Computing logging information
-------------------------------------
| Iteration            | 1183       |
| ExpectedImprovement  | 0.017797   |
| ActualImprovement    | 0.016931   |
| ImprovementRatio     | 0.95137    |
| MeanKL               | 0.0080843  |
| Entropy              | -1.1373    |
| Perplexity           | 0.32068    |
| AveragePolicyStd     | 0.2033     |
| AveragePolicyStd[0]  | 0.2248     |
| AveragePolicyStd[1]  | 0.20524    |
| AveragePolicyStd[2]  | 0.16341    |
| AveragePolicyStd[3]  | 0.20673    |
| AveragePolicyStd[4]  | 0.15759    |
| AveragePolicyStd[5]  | 0.26204    |
| AverageReturn        | 1776.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1936.4     |
| StdReturn            | 242.79     |
| AverageEpisodeLength | 962.67     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.42     |
| TotalNEpisodes       | 20648      |
| TotalNSamples        | 5.9226e+06 |
| ExplainedVariance    | 0.17841    |
-------------------------------------
[2018-12-22 12:08:43.792023 UTC] Saving snapshot
[2018-12-22 12:08:43.792276 UTC] Starting iteration 1184
[2018-12-22 12:08:43.792396 UTC] Start collecting samples
[2018-12-22 12:08:46.760420 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:46.839629 UTC] Performing policy update
[2018-12-22 12:08:46.840217 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:46.931565 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:47.994047 UTC] Performing line search
[2018-12-22 12:08:48.121660 UTC] Updating baseline
[2018-12-22 12:08:49.540231 UTC] Computing logging information
-------------------------------------
| Iteration            | 1184       |
| ExpectedImprovement  | 0.017388   |
| ActualImprovement    | 0.017469   |
| ImprovementRatio     | 1.0047     |
| MeanKL               | 0.0077845  |
| Entropy              | -1.1289    |
| Perplexity           | 0.3234     |
| AveragePolicyStd     | 0.20361    |
| AveragePolicyStd[0]  | 0.22497    |
| AveragePolicyStd[1]  | 0.2055     |
| AveragePolicyStd[2]  | 0.16332    |
| AveragePolicyStd[3]  | 0.20717    |
| AveragePolicyStd[4]  | 0.15792    |
| AveragePolicyStd[5]  | 0.26275    |
| AverageReturn        | 1776.7     |
| MinReturn            | 420.38     |
| MaxReturn            | 1936.4     |
| StdReturn            | 242.67     |
| AverageEpisodeLength | 962.75     |
| MinEpisodeLength     | 272        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.44     |
| TotalNEpisodes       | 20654      |
| TotalNSamples        | 5.9286e+06 |
| ExplainedVariance    | -0.10154   |
-------------------------------------
[2018-12-22 12:08:49.959163 UTC] Saving snapshot
[2018-12-22 12:08:49.959416 UTC] Starting iteration 1185
[2018-12-22 12:08:49.959551 UTC] Start collecting samples
[2018-12-22 12:08:52.869679 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:52.945101 UTC] Performing policy update
[2018-12-22 12:08:52.945993 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:53.034694 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:08:54.118112 UTC] Performing line search
[2018-12-22 12:08:54.250570 UTC] Updating baseline
[2018-12-22 12:08:56.190020 UTC] Computing logging information
------------------------------------
| Iteration            | 1185      |
| ExpectedImprovement  | 0.018831  |
| ActualImprovement    | 0.017542  |
| ImprovementRatio     | 0.93151   |
| MeanKL               | 0.0076235 |
| Entropy              | -1.129    |
| Perplexity           | 0.32336   |
| AveragePolicyStd     | 0.20354   |
| AveragePolicyStd[0]  | 0.22486   |
| AveragePolicyStd[1]  | 0.20537   |
| AveragePolicyStd[2]  | 0.16357   |
| AveragePolicyStd[3]  | 0.20682   |
| AveragePolicyStd[4]  | 0.15846   |
| AveragePolicyStd[5]  | 0.26218   |
| AverageReturn        | 1766.7    |
| MinReturn            | 420.38    |
| MaxReturn            | 1936.4    |
| StdReturn            | 262.45    |
| AverageEpisodeLength | 957.76    |
| MinEpisodeLength     | 272       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 132.87    |
| TotalNEpisodes       | 20657     |
| TotalNSamples        | 5.931e+06 |
| ExplainedVariance    | 0.14223   |
------------------------------------
[2018-12-22 12:08:56.638629 UTC] Saving snapshot
[2018-12-22 12:08:56.638878 UTC] Starting iteration 1186
[2018-12-22 12:08:56.638998 UTC] Start collecting samples
[2018-12-22 12:08:59.759653 UTC] Computing input variables for policy optimization
[2018-12-22 12:08:59.836594 UTC] Performing policy update
[2018-12-22 12:08:59.837232 UTC] Computing gradient in Euclidean space
[2018-12-22 12:08:59.927709 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:00.981152 UTC] Performing line search
[2018-12-22 12:09:01.109519 UTC] Updating baseline
[2018-12-22 12:09:02.375965 UTC] Computing logging information
------------------------------------
| Iteration            | 1186      |
| ExpectedImprovement  | 0.016242  |
| ActualImprovement    | 0.015495  |
| ImprovementRatio     | 0.954     |
| MeanKL               | 0.0080684 |
| Entropy              | -1.1313   |
| Perplexity           | 0.3226    |
| AveragePolicyStd     | 0.20347   |
| AveragePolicyStd[0]  | 0.22481   |
| AveragePolicyStd[1]  | 0.20507   |
| AveragePolicyStd[2]  | 0.16362   |
| AveragePolicyStd[3]  | 0.20637   |
| AveragePolicyStd[4]  | 0.15849   |
| AveragePolicyStd[5]  | 0.26245   |
| AverageReturn        | 1780.3    |
| MinReturn            | 765.49    |
| MaxReturn            | 1936.4    |
| StdReturn            | 224.78    |
| AverageEpisodeLength | 965.04    |
| MinEpisodeLength     | 444       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 113.65    |
| TotalNEpisodes       | 20663     |
| TotalNSamples        | 5.937e+06 |
| ExplainedVariance    | 0.003239  |
------------------------------------
[2018-12-22 12:09:02.786183 UTC] Saving snapshot
[2018-12-22 12:09:02.786439 UTC] Starting iteration 1187
[2018-12-22 12:09:02.786576 UTC] Start collecting samples
[2018-12-22 12:09:05.716403 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:05.791373 UTC] Performing policy update
[2018-12-22 12:09:05.792057 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:05.883235 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:06.987543 UTC] Performing line search
[2018-12-22 12:09:07.119698 UTC] Updating baseline
[2018-12-22 12:09:08.862827 UTC] Computing logging information
-------------------------------------
| Iteration            | 1187       |
| ExpectedImprovement  | 0.017546   |
| ActualImprovement    | 0.016044   |
| ImprovementRatio     | 0.91436    |
| MeanKL               | 0.0081389  |
| Entropy              | -1.125     |
| Perplexity           | 0.32464    |
| AveragePolicyStd     | 0.20365    |
| AveragePolicyStd[0]  | 0.22469    |
| AveragePolicyStd[1]  | 0.2048     |
| AveragePolicyStd[2]  | 0.1636     |
| AveragePolicyStd[3]  | 0.20667    |
| AveragePolicyStd[4]  | 0.15934    |
| AveragePolicyStd[5]  | 0.26282    |
| AverageReturn        | 1767.3     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 267.11     |
| AverageEpisodeLength | 958.57     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136        |
| TotalNEpisodes       | 20668      |
| TotalNSamples        | 5.9412e+06 |
| ExplainedVariance    | 0.14609    |
-------------------------------------
[2018-12-22 12:09:09.299711 UTC] Saving snapshot
[2018-12-22 12:09:09.300062 UTC] Starting iteration 1188
[2018-12-22 12:09:09.300184 UTC] Start collecting samples
[2018-12-22 12:09:12.465428 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:12.546936 UTC] Performing policy update
[2018-12-22 12:09:12.547847 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:12.642651 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:13.744833 UTC] Performing line search
[2018-12-22 12:09:13.880624 UTC] Updating baseline
[2018-12-22 12:09:15.530195 UTC] Computing logging information
-------------------------------------
| Iteration            | 1188       |
| ExpectedImprovement  | 0.018189   |
| ActualImprovement    | 0.016796   |
| ImprovementRatio     | 0.92344    |
| MeanKL               | 0.0081171  |
| Entropy              | -1.1332    |
| Perplexity           | 0.32199    |
| AveragePolicyStd     | 0.20338    |
| AveragePolicyStd[0]  | 0.22493    |
| AveragePolicyStd[1]  | 0.20387    |
| AveragePolicyStd[2]  | 0.16352    |
| AveragePolicyStd[3]  | 0.20632    |
| AveragePolicyStd[4]  | 0.15916    |
| AveragePolicyStd[5]  | 0.26246    |
| AverageReturn        | 1771.4     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 265.61     |
| AverageEpisodeLength | 960.53     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.17     |
| TotalNEpisodes       | 20673      |
| TotalNSamples        | 5.9462e+06 |
| ExplainedVariance    | 0.0078121  |
-------------------------------------
[2018-12-22 12:09:15.967222 UTC] Saving snapshot
[2018-12-22 12:09:15.967487 UTC] Starting iteration 1189
[2018-12-22 12:09:15.967674 UTC] Start collecting samples
[2018-12-22 12:09:18.951435 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:19.027752 UTC] Performing policy update
[2018-12-22 12:09:19.028436 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:19.117207 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:20.178774 UTC] Performing line search
[2018-12-22 12:09:20.307753 UTC] Updating baseline
[2018-12-22 12:09:21.988920 UTC] Computing logging information
-------------------------------------
| Iteration            | 1189       |
| ExpectedImprovement  | 0.01706    |
| ActualImprovement    | 0.016528   |
| ImprovementRatio     | 0.96881    |
| MeanKL               | 0.0080178  |
| Entropy              | -1.1334    |
| Perplexity           | 0.32193    |
| AveragePolicyStd     | 0.20335    |
| AveragePolicyStd[0]  | 0.22489    |
| AveragePolicyStd[1]  | 0.20347    |
| AveragePolicyStd[2]  | 0.16341    |
| AveragePolicyStd[3]  | 0.20655    |
| AveragePolicyStd[4]  | 0.1595     |
| AveragePolicyStd[5]  | 0.26232    |
| AverageReturn        | 1770.6     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 265.38     |
| AverageEpisodeLength | 960.53     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.17     |
| TotalNEpisodes       | 20678      |
| TotalNSamples        | 5.9512e+06 |
| ExplainedVariance    | -0.015357  |
-------------------------------------
[2018-12-22 12:09:22.401586 UTC] Saving snapshot
[2018-12-22 12:09:22.401861 UTC] Starting iteration 1190
[2018-12-22 12:09:22.401987 UTC] Start collecting samples
[2018-12-22 12:09:25.364455 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:25.440993 UTC] Performing policy update
[2018-12-22 12:09:25.441646 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:25.531052 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:26.599969 UTC] Performing line search
[2018-12-22 12:09:26.729646 UTC] Updating baseline
[2018-12-22 12:09:28.143659 UTC] Computing logging information
-------------------------------------
| Iteration            | 1190       |
| ExpectedImprovement  | 0.016928   |
| ActualImprovement    | 0.016127   |
| ImprovementRatio     | 0.95273    |
| MeanKL               | 0.0085601  |
| Entropy              | -1.125     |
| Perplexity           | 0.32465    |
| AveragePolicyStd     | 0.20365    |
| AveragePolicyStd[0]  | 0.22511    |
| AveragePolicyStd[1]  | 0.20325    |
| AveragePolicyStd[2]  | 0.1634     |
| AveragePolicyStd[3]  | 0.20675    |
| AveragePolicyStd[4]  | 0.16015    |
| AveragePolicyStd[5]  | 0.26323    |
| AverageReturn        | 1770.1     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 265.18     |
| AverageEpisodeLength | 960.53     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.17     |
| TotalNEpisodes       | 20683      |
| TotalNSamples        | 5.9562e+06 |
| ExplainedVariance    | 0.0016821  |
-------------------------------------
[2018-12-22 12:09:28.556433 UTC] Saving snapshot
[2018-12-22 12:09:28.564581 UTC] Starting iteration 1191
[2018-12-22 12:09:28.564792 UTC] Start collecting samples
[2018-12-22 12:09:31.538065 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:31.615282 UTC] Performing policy update
[2018-12-22 12:09:31.615942 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:31.706452 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:32.766237 UTC] Performing line search
[2018-12-22 12:09:32.893396 UTC] Updating baseline
[2018-12-22 12:09:34.956336 UTC] Computing logging information
-------------------------------------
| Iteration            | 1191       |
| ExpectedImprovement  | 0.018081   |
| ActualImprovement    | 0.017246   |
| ImprovementRatio     | 0.9538     |
| MeanKL               | 0.0074415  |
| Entropy              | -1.1273    |
| Perplexity           | 0.3239     |
| AveragePolicyStd     | 0.20359    |
| AveragePolicyStd[0]  | 0.22576    |
| AveragePolicyStd[1]  | 0.2025     |
| AveragePolicyStd[2]  | 0.16311    |
| AveragePolicyStd[3]  | 0.20686    |
| AveragePolicyStd[4]  | 0.16011    |
| AveragePolicyStd[5]  | 0.26323    |
| AverageReturn        | 1774.6     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 261.29     |
| AverageEpisodeLength | 962.94     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.7      |
| TotalNEpisodes       | 20689      |
| TotalNSamples        | 5.9622e+06 |
| ExplainedVariance    | 0.0030498  |
-------------------------------------
[2018-12-22 12:09:35.366731 UTC] Saving snapshot
[2018-12-22 12:09:35.366984 UTC] Starting iteration 1192
[2018-12-22 12:09:35.367102 UTC] Start collecting samples
[2018-12-22 12:09:38.304909 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:38.380667 UTC] Performing policy update
[2018-12-22 12:09:38.381361 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:38.472485 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:39.536997 UTC] Performing line search
[2018-12-22 12:09:39.664839 UTC] Updating baseline
[2018-12-22 12:09:41.728642 UTC] Computing logging information
-------------------------------------
| Iteration            | 1192       |
| ExpectedImprovement  | 0.018453   |
| ActualImprovement    | 0.017479   |
| ImprovementRatio     | 0.9472     |
| MeanKL               | 0.0078925  |
| Entropy              | -1.1304    |
| Perplexity           | 0.32289    |
| AveragePolicyStd     | 0.20346    |
| AveragePolicyStd[0]  | 0.22585    |
| AveragePolicyStd[1]  | 0.20236    |
| AveragePolicyStd[2]  | 0.163      |
| AveragePolicyStd[3]  | 0.20697    |
| AveragePolicyStd[4]  | 0.16021    |
| AveragePolicyStd[5]  | 0.26237    |
| AverageReturn        | 1773.7     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 259.85     |
| AverageEpisodeLength | 963.11     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.06     |
| TotalNEpisodes       | 20694      |
| TotalNSamples        | 5.9667e+06 |
| ExplainedVariance    | 0.085286   |
-------------------------------------
[2018-12-22 12:09:42.148110 UTC] Saving snapshot
[2018-12-22 12:09:42.148382 UTC] Starting iteration 1193
[2018-12-22 12:09:42.148516 UTC] Start collecting samples
[2018-12-22 12:09:45.054402 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:45.129849 UTC] Performing policy update
[2018-12-22 12:09:45.130826 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:45.220567 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:46.276046 UTC] Performing line search
[2018-12-22 12:09:46.403818 UTC] Updating baseline
[2018-12-22 12:09:48.350869 UTC] Computing logging information
-------------------------------------
| Iteration            | 1193       |
| ExpectedImprovement  | 0.017492   |
| ActualImprovement    | 0.016317   |
| ImprovementRatio     | 0.9328     |
| MeanKL               | 0.0078275  |
| Entropy              | -1.1317    |
| Perplexity           | 0.32247    |
| AveragePolicyStd     | 0.20341    |
| AveragePolicyStd[0]  | 0.22582    |
| AveragePolicyStd[1]  | 0.20222    |
| AveragePolicyStd[2]  | 0.16275    |
| AveragePolicyStd[3]  | 0.2068     |
| AveragePolicyStd[4]  | 0.16056    |
| AveragePolicyStd[5]  | 0.26228    |
| AverageReturn        | 1778.2     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 257.91     |
| AverageEpisodeLength | 965.4      |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.7      |
| TotalNEpisodes       | 20697      |
| TotalNSamples        | 5.9697e+06 |
| ExplainedVariance    | 0.01969    |
-------------------------------------
[2018-12-22 12:09:48.764995 UTC] Saving snapshot
[2018-12-22 12:09:48.765260 UTC] Starting iteration 1194
[2018-12-22 12:09:48.765399 UTC] Start collecting samples
[2018-12-22 12:09:51.728456 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:51.807247 UTC] Performing policy update
[2018-12-22 12:09:51.807984 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:51.899137 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:52.961513 UTC] Performing line search
[2018-12-22 12:09:53.089829 UTC] Updating baseline
[2018-12-22 12:09:54.512747 UTC] Computing logging information
-------------------------------------
| Iteration            | 1194       |
| ExpectedImprovement  | 0.017711   |
| ActualImprovement    | 0.016912   |
| ImprovementRatio     | 0.95486    |
| MeanKL               | 0.0075699  |
| Entropy              | -1.1294    |
| Perplexity           | 0.32323    |
| AveragePolicyStd     | 0.20349    |
| AveragePolicyStd[0]  | 0.22572    |
| AveragePolicyStd[1]  | 0.20266    |
| AveragePolicyStd[2]  | 0.16232    |
| AveragePolicyStd[3]  | 0.20687    |
| AveragePolicyStd[4]  | 0.16096    |
| AveragePolicyStd[5]  | 0.2624     |
| AverageReturn        | 1785.8     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 241.23     |
| AverageEpisodeLength | 970.34     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 123.37     |
| TotalNEpisodes       | 20704      |
| TotalNSamples        | 5.9767e+06 |
| ExplainedVariance    | 0.0071525  |
-------------------------------------
[2018-12-22 12:09:54.929792 UTC] Saving snapshot
[2018-12-22 12:09:54.930067 UTC] Starting iteration 1195
[2018-12-22 12:09:54.930184 UTC] Start collecting samples
[2018-12-22 12:09:57.878299 UTC] Computing input variables for policy optimization
[2018-12-22 12:09:57.958698 UTC] Performing policy update
[2018-12-22 12:09:57.959333 UTC] Computing gradient in Euclidean space
[2018-12-22 12:09:58.047898 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:09:59.094148 UTC] Performing line search
[2018-12-22 12:09:59.221023 UTC] Updating baseline
[2018-12-22 12:10:00.720661 UTC] Computing logging information
-------------------------------------
| Iteration            | 1195       |
| ExpectedImprovement  | 0.018351   |
| ActualImprovement    | 0.016993   |
| ImprovementRatio     | 0.926      |
| MeanKL               | 0.0078404  |
| Entropy              | -1.1274    |
| Perplexity           | 0.32389    |
| AveragePolicyStd     | 0.20357    |
| AveragePolicyStd[0]  | 0.22583    |
| AveragePolicyStd[1]  | 0.2025     |
| AveragePolicyStd[2]  | 0.16252    |
| AveragePolicyStd[3]  | 0.2076     |
| AveragePolicyStd[4]  | 0.16055    |
| AveragePolicyStd[5]  | 0.26244    |
| AverageReturn        | 1785.1     |
| MinReturn            | 320.53     |
| MaxReturn            | 1936.4     |
| StdReturn            | 241.01     |
| AverageEpisodeLength | 970.34     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 123.37     |
| TotalNEpisodes       | 20710      |
| TotalNSamples        | 5.9827e+06 |
| ExplainedVariance    | -0.012618  |
-------------------------------------
[2018-12-22 12:10:01.134121 UTC] Saving snapshot
[2018-12-22 12:10:01.134411 UTC] Starting iteration 1196
[2018-12-22 12:10:01.134545 UTC] Start collecting samples
[2018-12-22 12:10:04.096337 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:04.173117 UTC] Performing policy update
[2018-12-22 12:10:04.173811 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:04.263407 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:05.319914 UTC] Performing line search
[2018-12-22 12:10:05.446336 UTC] Updating baseline
[2018-12-22 12:10:07.108324 UTC] Computing logging information
--------------------------------------
| Iteration            | 1196        |
| ExpectedImprovement  | 0.01772     |
| ActualImprovement    | 0.015941    |
| ImprovementRatio     | 0.89958     |
| MeanKL               | 0.0076945   |
| Entropy              | -1.129      |
| Perplexity           | 0.32334     |
| AveragePolicyStd     | 0.20352     |
| AveragePolicyStd[0]  | 0.22584     |
| AveragePolicyStd[1]  | 0.20277     |
| AveragePolicyStd[2]  | 0.16236     |
| AveragePolicyStd[3]  | 0.20783     |
| AveragePolicyStd[4]  | 0.16028     |
| AveragePolicyStd[5]  | 0.26204     |
| AverageReturn        | 1785.4      |
| MinReturn            | 320.53      |
| MaxReturn            | 1936.4      |
| StdReturn            | 241.06      |
| AverageEpisodeLength | 970.34      |
| MinEpisodeLength     | 207         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 123.37      |
| TotalNEpisodes       | 20713       |
| TotalNSamples        | 5.9857e+06  |
| ExplainedVariance    | -0.00069728 |
--------------------------------------
[2018-12-22 12:10:07.517687 UTC] Saving snapshot
[2018-12-22 12:10:07.517963 UTC] Starting iteration 1197
[2018-12-22 12:10:07.518082 UTC] Start collecting samples
[2018-12-22 12:10:10.500982 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:10.578702 UTC] Performing policy update
[2018-12-22 12:10:10.579365 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:10.667610 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:11.724015 UTC] Performing line search
[2018-12-22 12:10:11.851004 UTC] Updating baseline
[2018-12-22 12:10:13.176245 UTC] Computing logging information
-------------------------------------
| Iteration            | 1197       |
| ExpectedImprovement  | 0.018393   |
| ActualImprovement    | 0.017839   |
| ImprovementRatio     | 0.96985    |
| MeanKL               | 0.0078355  |
| Entropy              | -1.1351    |
| Perplexity           | 0.32141    |
| AveragePolicyStd     | 0.20329    |
| AveragePolicyStd[0]  | 0.22532    |
| AveragePolicyStd[1]  | 0.20294    |
| AveragePolicyStd[2]  | 0.16255    |
| AveragePolicyStd[3]  | 0.20738    |
| AveragePolicyStd[4]  | 0.16       |
| AveragePolicyStd[5]  | 0.26158    |
| AverageReturn        | 1763.5     |
| MinReturn            | 290.74     |
| MaxReturn            | 1936.4     |
| StdReturn            | 288.84     |
| AverageEpisodeLength | 959.2      |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.37     |
| TotalNEpisodes       | 20721      |
| TotalNSamples        | 5.9926e+06 |
| ExplainedVariance    | 0.15754    |
-------------------------------------
[2018-12-22 12:10:13.585657 UTC] Saving snapshot
[2018-12-22 12:10:13.585945 UTC] Starting iteration 1198
[2018-12-22 12:10:13.586063 UTC] Start collecting samples
[2018-12-22 12:10:16.515700 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:16.592341 UTC] Performing policy update
[2018-12-22 12:10:16.592989 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:16.684250 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:17.738487 UTC] Performing line search
[2018-12-22 12:10:17.869484 UTC] Updating baseline
[2018-12-22 12:10:19.295290 UTC] Computing logging information
-------------------------------------
| Iteration            | 1198       |
| ExpectedImprovement  | 0.018399   |
| ActualImprovement    | 0.017324   |
| ImprovementRatio     | 0.94156    |
| MeanKL               | 0.0078177  |
| Entropy              | -1.1337    |
| Perplexity           | 0.32186    |
| AveragePolicyStd     | 0.20334    |
| AveragePolicyStd[0]  | 0.22553    |
| AveragePolicyStd[1]  | 0.20326    |
| AveragePolicyStd[2]  | 0.16269    |
| AveragePolicyStd[3]  | 0.20745    |
| AveragePolicyStd[4]  | 0.15976    |
| AveragePolicyStd[5]  | 0.26137    |
| AverageReturn        | 1764       |
| MinReturn            | 290.74     |
| MaxReturn            | 1936.4     |
| StdReturn            | 288.79     |
| AverageEpisodeLength | 959.2      |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.37     |
| TotalNEpisodes       | 20725      |
| TotalNSamples        | 5.9966e+06 |
| ExplainedVariance    | 0.002008   |
-------------------------------------
[2018-12-22 12:10:19.706956 UTC] Saving snapshot
[2018-12-22 12:10:19.707206 UTC] Starting iteration 1199
[2018-12-22 12:10:19.707323 UTC] Start collecting samples
[2018-12-22 12:10:22.648658 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:22.725915 UTC] Performing policy update
[2018-12-22 12:10:22.726545 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:22.815368 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:23.861859 UTC] Performing line search
[2018-12-22 12:10:23.991653 UTC] Updating baseline
[2018-12-22 12:10:25.373081 UTC] Computing logging information
-------------------------------------
| Iteration            | 1199       |
| ExpectedImprovement  | 0.015782   |
| ActualImprovement    | 0.015174   |
| ImprovementRatio     | 0.96145    |
| MeanKL               | 0.0077831  |
| Entropy              | -1.1371    |
| Perplexity           | 0.32076    |
| AveragePolicyStd     | 0.20322    |
| AveragePolicyStd[0]  | 0.22492    |
| AveragePolicyStd[1]  | 0.20362    |
| AveragePolicyStd[2]  | 0.1624     |
| AveragePolicyStd[3]  | 0.20776    |
| AveragePolicyStd[4]  | 0.15962    |
| AveragePolicyStd[5]  | 0.26102    |
| AverageReturn        | 1769.8     |
| MinReturn            | 290.74     |
| MaxReturn            | 1936.4     |
| StdReturn            | 287.47     |
| AverageEpisodeLength | 961.13     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.62     |
| TotalNEpisodes       | 20730      |
| TotalNSamples        | 6.0016e+06 |
| ExplainedVariance    | 0.15784    |
-------------------------------------
[2018-12-22 12:10:25.778464 UTC] Saving snapshot
[2018-12-22 12:10:25.778748 UTC] Starting iteration 1200
[2018-12-22 12:10:25.778882 UTC] Start collecting samples
[2018-12-22 12:10:28.750713 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:28.827970 UTC] Performing policy update
[2018-12-22 12:10:28.828739 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:28.918918 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:29.978927 UTC] Performing line search
[2018-12-22 12:10:30.106166 UTC] Updating baseline
[2018-12-22 12:10:31.353546 UTC] Computing logging information
-------------------------------------
| Iteration            | 1200       |
| ExpectedImprovement  | 0.01775    |
| ActualImprovement    | 0.017389   |
| ImprovementRatio     | 0.97966    |
| MeanKL               | 0.0078125  |
| Entropy              | -1.13      |
| Perplexity           | 0.32304    |
| AveragePolicyStd     | 0.20348    |
| AveragePolicyStd[0]  | 0.22574    |
| AveragePolicyStd[1]  | 0.20364    |
| AveragePolicyStd[2]  | 0.16244    |
| AveragePolicyStd[3]  | 0.20824    |
| AveragePolicyStd[4]  | 0.1597     |
| AveragePolicyStd[5]  | 0.26111    |
| AverageReturn        | 1771.5     |
| MinReturn            | 290.74     |
| MaxReturn            | 1936.4     |
| StdReturn            | 287.92     |
| AverageEpisodeLength | 961.13     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.62     |
| TotalNEpisodes       | 20736      |
| TotalNSamples        | 6.0076e+06 |
| ExplainedVariance    | -0.02039   |
-------------------------------------
[2018-12-22 12:10:31.778662 UTC] Saving snapshot
[2018-12-22 12:10:31.786836 UTC] Starting iteration 1201
[2018-12-22 12:10:31.787043 UTC] Start collecting samples
[2018-12-22 12:10:34.743577 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:34.822804 UTC] Performing policy update
[2018-12-22 12:10:34.823810 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:34.915177 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:35.979781 UTC] Performing line search
[2018-12-22 12:10:36.108040 UTC] Updating baseline
[2018-12-22 12:10:37.539147 UTC] Computing logging information
-------------------------------------
| Iteration            | 1201       |
| ExpectedImprovement  | 0.01832    |
| ActualImprovement    | 0.016673   |
| ImprovementRatio     | 0.91012    |
| MeanKL               | 0.0076164  |
| Entropy              | -1.1284    |
| Perplexity           | 0.32354    |
| AveragePolicyStd     | 0.20348    |
| AveragePolicyStd[0]  | 0.22505    |
| AveragePolicyStd[1]  | 0.20404    |
| AveragePolicyStd[2]  | 0.16289    |
| AveragePolicyStd[3]  | 0.20818    |
| AveragePolicyStd[4]  | 0.15987    |
| AveragePolicyStd[5]  | 0.26086    |
| AverageReturn        | 1757       |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 311.98     |
| AverageEpisodeLength | 954.56     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.91     |
| TotalNEpisodes       | 20742      |
| TotalNSamples        | 6.0124e+06 |
| ExplainedVariance    | 0.22506    |
-------------------------------------
[2018-12-22 12:10:37.953782 UTC] Saving snapshot
[2018-12-22 12:10:37.954065 UTC] Starting iteration 1202
[2018-12-22 12:10:37.954196 UTC] Start collecting samples
[2018-12-22 12:10:40.899033 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:40.975592 UTC] Performing policy update
[2018-12-22 12:10:40.976328 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:41.065090 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:42.127983 UTC] Performing line search
[2018-12-22 12:10:42.254997 UTC] Updating baseline
[2018-12-22 12:10:43.661691 UTC] Computing logging information
-------------------------------------
| Iteration            | 1202       |
| ExpectedImprovement  | 0.0183     |
| ActualImprovement    | 0.017226   |
| ImprovementRatio     | 0.94128    |
| MeanKL               | 0.0081283  |
| Entropy              | -1.132     |
| Perplexity           | 0.32239    |
| AveragePolicyStd     | 0.20332    |
| AveragePolicyStd[0]  | 0.22484    |
| AveragePolicyStd[1]  | 0.20316    |
| AveragePolicyStd[2]  | 0.16277    |
| AveragePolicyStd[3]  | 0.2082     |
| AveragePolicyStd[4]  | 0.16047    |
| AveragePolicyStd[5]  | 0.2605     |
| AverageReturn        | 1764.2     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 308.58     |
| AverageEpisodeLength | 958.23     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.51     |
| TotalNEpisodes       | 20747      |
| TotalNSamples        | 6.0174e+06 |
| ExplainedVariance    | 0.022372   |
-------------------------------------
[2018-12-22 12:10:44.079425 UTC] Saving snapshot
[2018-12-22 12:10:44.079691 UTC] Starting iteration 1203
[2018-12-22 12:10:44.079826 UTC] Start collecting samples
[2018-12-22 12:10:47.001128 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:47.076575 UTC] Performing policy update
[2018-12-22 12:10:47.077152 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:47.166560 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:48.231739 UTC] Performing line search
[2018-12-22 12:10:48.359363 UTC] Updating baseline
[2018-12-22 12:10:49.524952 UTC] Computing logging information
-------------------------------------
| Iteration            | 1203       |
| ExpectedImprovement  | 0.020455   |
| ActualImprovement    | 0.02411    |
| ImprovementRatio     | 1.1787     |
| MeanKL               | 0.0072906  |
| Entropy              | -1.137     |
| Perplexity           | 0.32079    |
| AveragePolicyStd     | 0.20315    |
| AveragePolicyStd[0]  | 0.22442    |
| AveragePolicyStd[1]  | 0.20328    |
| AveragePolicyStd[2]  | 0.16265    |
| AveragePolicyStd[3]  | 0.20821    |
| AveragePolicyStd[4]  | 0.16018    |
| AveragePolicyStd[5]  | 0.26019    |
| AverageReturn        | 1757.3     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 312.4      |
| AverageEpisodeLength | 955.05     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.83     |
| TotalNEpisodes       | 20750      |
| TotalNSamples        | 6.0201e+06 |
| ExplainedVariance    | 0.12914    |
-------------------------------------
[2018-12-22 12:10:49.949924 UTC] Saving snapshot
[2018-12-22 12:10:49.950182 UTC] Starting iteration 1204
[2018-12-22 12:10:49.950340 UTC] Start collecting samples
[2018-12-22 12:10:52.912553 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:52.990294 UTC] Performing policy update
[2018-12-22 12:10:52.991118 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:53.080517 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:10:54.139495 UTC] Performing line search
[2018-12-22 12:10:54.267183 UTC] Updating baseline
[2018-12-22 12:10:56.028112 UTC] Computing logging information
-------------------------------------
| Iteration            | 1204       |
| ExpectedImprovement  | 0.017764   |
| ActualImprovement    | 0.016856   |
| ImprovementRatio     | 0.9489     |
| MeanKL               | 0.0078129  |
| Entropy              | -1.1353    |
| Perplexity           | 0.32134    |
| AveragePolicyStd     | 0.20321    |
| AveragePolicyStd[0]  | 0.22381    |
| AveragePolicyStd[1]  | 0.20309    |
| AveragePolicyStd[2]  | 0.16245    |
| AveragePolicyStd[3]  | 0.20878    |
| AveragePolicyStd[4]  | 0.16061    |
| AveragePolicyStd[5]  | 0.26049    |
| AverageReturn        | 1768.3     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 296.24     |
| AverageEpisodeLength | 960.61     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.4      |
| TotalNEpisodes       | 20757      |
| TotalNSamples        | 6.0271e+06 |
| ExplainedVariance    | 0.0091421  |
-------------------------------------
[2018-12-22 12:10:56.445292 UTC] Saving snapshot
[2018-12-22 12:10:56.445565 UTC] Starting iteration 1205
[2018-12-22 12:10:56.445688 UTC] Start collecting samples
[2018-12-22 12:10:59.396865 UTC] Computing input variables for policy optimization
[2018-12-22 12:10:59.474523 UTC] Performing policy update
[2018-12-22 12:10:59.475094 UTC] Computing gradient in Euclidean space
[2018-12-22 12:10:59.563963 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:00.626703 UTC] Performing line search
[2018-12-22 12:11:00.753129 UTC] Updating baseline
[2018-12-22 12:11:02.016110 UTC] Computing logging information
-------------------------------------
| Iteration            | 1205       |
| ExpectedImprovement  | 0.018042   |
| ActualImprovement    | 0.017382   |
| ImprovementRatio     | 0.96342    |
| MeanKL               | 0.0075854  |
| Entropy              | -1.1426    |
| Perplexity           | 0.31897    |
| AveragePolicyStd     | 0.203      |
| AveragePolicyStd[0]  | 0.22373    |
| AveragePolicyStd[1]  | 0.20233    |
| AveragePolicyStd[2]  | 0.16258    |
| AveragePolicyStd[3]  | 0.2083     |
| AveragePolicyStd[4]  | 0.15994    |
| AveragePolicyStd[5]  | 0.26111    |
| AverageReturn        | 1761.2     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 304.1      |
| AverageEpisodeLength | 956.72     |
| MinEpisodeLength     | 207        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.29     |
| TotalNEpisodes       | 20764      |
| TotalNSamples        | 6.0337e+06 |
| ExplainedVariance    | 0.12299    |
-------------------------------------
[2018-12-22 12:11:02.433723 UTC] Saving snapshot
[2018-12-22 12:11:02.434006 UTC] Starting iteration 1206
[2018-12-22 12:11:02.434121 UTC] Start collecting samples
[2018-12-22 12:11:05.348959 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:05.425978 UTC] Performing policy update
[2018-12-22 12:11:05.426846 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:05.517529 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:06.578974 UTC] Performing line search
[2018-12-22 12:11:06.705467 UTC] Updating baseline
[2018-12-22 12:11:07.938539 UTC] Computing logging information
-------------------------------------
| Iteration            | 1206       |
| ExpectedImprovement  | 0.019409   |
| ActualImprovement    | 0.019015   |
| ImprovementRatio     | 0.97972    |
| MeanKL               | 0.0071625  |
| Entropy              | -1.1506    |
| Perplexity           | 0.31644    |
| AveragePolicyStd     | 0.20273    |
| AveragePolicyStd[0]  | 0.22276    |
| AveragePolicyStd[1]  | 0.20139    |
| AveragePolicyStd[2]  | 0.16267    |
| AveragePolicyStd[3]  | 0.2077     |
| AveragePolicyStd[4]  | 0.16009    |
| AveragePolicyStd[5]  | 0.26175    |
| AverageReturn        | 1751.4     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 303.94     |
| AverageEpisodeLength | 951.84     |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.37     |
| TotalNEpisodes       | 20768      |
| TotalNSamples        | 6.0364e+06 |
| ExplainedVariance    | 0.51618    |
-------------------------------------
[2018-12-22 12:11:08.351473 UTC] Saving snapshot
[2018-12-22 12:11:08.351733 UTC] Starting iteration 1207
[2018-12-22 12:11:08.351856 UTC] Start collecting samples
[2018-12-22 12:11:11.293828 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:11.372639 UTC] Performing policy update
[2018-12-22 12:11:11.373206 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:11.464089 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:12.532432 UTC] Performing line search
[2018-12-22 12:11:12.659714 UTC] Updating baseline
[2018-12-22 12:11:14.089324 UTC] Computing logging information
-------------------------------------
| Iteration            | 1207       |
| ExpectedImprovement  | 0.016625   |
| ActualImprovement    | 0.016248   |
| ImprovementRatio     | 0.97737    |
| MeanKL               | 0.0083511  |
| Entropy              | -1.159     |
| Perplexity           | 0.3138     |
| AveragePolicyStd     | 0.20244    |
| AveragePolicyStd[0]  | 0.22222    |
| AveragePolicyStd[1]  | 0.20141    |
| AveragePolicyStd[2]  | 0.16257    |
| AveragePolicyStd[3]  | 0.20757    |
| AveragePolicyStd[4]  | 0.15968    |
| AveragePolicyStd[5]  | 0.26118    |
| AverageReturn        | 1751.4     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 303.94     |
| AverageEpisodeLength | 951.84     |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.37     |
| TotalNEpisodes       | 20773      |
| TotalNSamples        | 6.0414e+06 |
| ExplainedVariance    | 0.0093508  |
-------------------------------------
[2018-12-22 12:11:14.504395 UTC] Saving snapshot
[2018-12-22 12:11:14.504656 UTC] Starting iteration 1208
[2018-12-22 12:11:14.504772 UTC] Start collecting samples
[2018-12-22 12:11:17.447433 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:17.525321 UTC] Performing policy update
[2018-12-22 12:11:17.526081 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:17.615429 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:18.686466 UTC] Performing line search
[2018-12-22 12:11:18.813868 UTC] Updating baseline
[2018-12-22 12:11:20.577253 UTC] Computing logging information
-------------------------------------
| Iteration            | 1208       |
| ExpectedImprovement  | 0.018248   |
| ActualImprovement    | 0.017253   |
| ImprovementRatio     | 0.9455     |
| MeanKL               | 0.0074249  |
| Entropy              | -1.1676    |
| Perplexity           | 0.31112    |
| AveragePolicyStd     | 0.20215    |
| AveragePolicyStd[0]  | 0.22206    |
| AveragePolicyStd[1]  | 0.20064    |
| AveragePolicyStd[2]  | 0.1625     |
| AveragePolicyStd[3]  | 0.20733    |
| AveragePolicyStd[4]  | 0.1594     |
| AveragePolicyStd[5]  | 0.26098    |
| AverageReturn        | 1753.6     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 304.56     |
| AverageEpisodeLength | 951.84     |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.37     |
| TotalNEpisodes       | 20778      |
| TotalNSamples        | 6.0464e+06 |
| ExplainedVariance    | -0.0089598 |
-------------------------------------
[2018-12-22 12:11:20.991237 UTC] Saving snapshot
[2018-12-22 12:11:20.991528 UTC] Starting iteration 1209
[2018-12-22 12:11:20.991654 UTC] Start collecting samples
[2018-12-22 12:11:23.944744 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:24.023030 UTC] Performing policy update
[2018-12-22 12:11:24.023923 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:24.113624 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:25.173342 UTC] Performing line search
[2018-12-22 12:11:25.300630 UTC] Updating baseline
[2018-12-22 12:11:26.899337 UTC] Computing logging information
-------------------------------------
| Iteration            | 1209       |
| ExpectedImprovement  | 0.018003   |
| ActualImprovement    | 0.016999   |
| ImprovementRatio     | 0.94422    |
| MeanKL               | 0.0076438  |
| Entropy              | -1.1758    |
| Perplexity           | 0.30858    |
| AveragePolicyStd     | 0.20188    |
| AveragePolicyStd[0]  | 0.22148    |
| AveragePolicyStd[1]  | 0.19966    |
| AveragePolicyStd[2]  | 0.16234    |
| AveragePolicyStd[3]  | 0.20767    |
| AveragePolicyStd[4]  | 0.15921    |
| AveragePolicyStd[5]  | 0.26094    |
| AverageReturn        | 1752.8     |
| MinReturn            | 290.74     |
| MaxReturn            | 1909.3     |
| StdReturn            | 304.29     |
| AverageEpisodeLength | 951.84     |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.37     |
| TotalNEpisodes       | 20783      |
| TotalNSamples        | 6.0514e+06 |
| ExplainedVariance    | 0.0054526  |
-------------------------------------
[2018-12-22 12:11:27.310203 UTC] Saving snapshot
[2018-12-22 12:11:27.310461 UTC] Starting iteration 1210
[2018-12-22 12:11:27.310599 UTC] Start collecting samples
[2018-12-22 12:11:30.260980 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:30.338393 UTC] Performing policy update
[2018-12-22 12:11:30.339005 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:30.428258 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:31.494246 UTC] Performing line search
[2018-12-22 12:11:31.621692 UTC] Updating baseline
[2018-12-22 12:11:32.774861 UTC] Computing logging information
-------------------------------------
| Iteration            | 1210       |
| ExpectedImprovement  | 0.019813   |
| ActualImprovement    | 0.018508   |
| ImprovementRatio     | 0.9341     |
| MeanKL               | 0.0076731  |
| Entropy              | -1.1825    |
| Perplexity           | 0.30652    |
| AveragePolicyStd     | 0.20167    |
| AveragePolicyStd[0]  | 0.22131    |
| AveragePolicyStd[1]  | 0.19842    |
| AveragePolicyStd[2]  | 0.16215    |
| AveragePolicyStd[3]  | 0.20777    |
| AveragePolicyStd[4]  | 0.15926    |
| AveragePolicyStd[5]  | 0.26112    |
| AverageReturn        | 1753.4     |
| MinReturn            | 290.74     |
| MaxReturn            | 1907.6     |
| StdReturn            | 304.41     |
| AverageEpisodeLength | 951.84     |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.37     |
| TotalNEpisodes       | 20789      |
| TotalNSamples        | 6.0574e+06 |
| ExplainedVariance    | -0.0072324 |
-------------------------------------
[2018-12-22 12:11:33.190721 UTC] Saving snapshot
[2018-12-22 12:11:33.198877 UTC] Starting iteration 1211
[2018-12-22 12:11:33.199083 UTC] Start collecting samples
[2018-12-22 12:11:36.161496 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:36.238235 UTC] Performing policy update
[2018-12-22 12:11:36.238876 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:36.328079 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:37.376654 UTC] Performing line search
[2018-12-22 12:11:37.502554 UTC] Updating baseline
[2018-12-22 12:11:39.023974 UTC] Computing logging information
-------------------------------------
| Iteration            | 1211       |
| ExpectedImprovement  | 0.018465   |
| ActualImprovement    | 0.017288   |
| ImprovementRatio     | 0.93623    |
| MeanKL               | 0.0073255  |
| Entropy              | -1.1849    |
| Perplexity           | 0.30577    |
| AveragePolicyStd     | 0.20162    |
| AveragePolicyStd[0]  | 0.22137    |
| AveragePolicyStd[1]  | 0.19863    |
| AveragePolicyStd[2]  | 0.16192    |
| AveragePolicyStd[3]  | 0.20713    |
| AveragePolicyStd[4]  | 0.15906    |
| AveragePolicyStd[5]  | 0.26164    |
| AverageReturn        | 1755.9     |
| MinReturn            | 290.74     |
| MaxReturn            | 1911.1     |
| StdReturn            | 300.21     |
| AverageEpisodeLength | 952.6      |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.67     |
| TotalNEpisodes       | 20795      |
| TotalNSamples        | 6.0629e+06 |
| ExplainedVariance    | 0.1664     |
-------------------------------------
[2018-12-22 12:11:39.435581 UTC] Saving snapshot
[2018-12-22 12:11:39.435832 UTC] Starting iteration 1212
[2018-12-22 12:11:39.435955 UTC] Start collecting samples
[2018-12-22 12:11:42.367767 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:42.444213 UTC] Performing policy update
[2018-12-22 12:11:42.444999 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:42.534472 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:43.589784 UTC] Performing line search
[2018-12-22 12:11:43.716857 UTC] Updating baseline
[2018-12-22 12:11:44.964533 UTC] Computing logging information
-------------------------------------
| Iteration            | 1212       |
| ExpectedImprovement  | 0.0179     |
| ActualImprovement    | 0.016914   |
| ImprovementRatio     | 0.94492    |
| MeanKL               | 0.0075015  |
| Entropy              | -1.1877    |
| Perplexity           | 0.30492    |
| AveragePolicyStd     | 0.20156    |
| AveragePolicyStd[0]  | 0.22061    |
| AveragePolicyStd[1]  | 0.1986     |
| AveragePolicyStd[2]  | 0.16163    |
| AveragePolicyStd[3]  | 0.20749    |
| AveragePolicyStd[4]  | 0.15894    |
| AveragePolicyStd[5]  | 0.26207    |
| AverageReturn        | 1757       |
| MinReturn            | 290.74     |
| MaxReturn            | 1911.1     |
| StdReturn            | 300.48     |
| AverageEpisodeLength | 952.6      |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.67     |
| TotalNEpisodes       | 20799      |
| TotalNSamples        | 6.0669e+06 |
| ExplainedVariance    | -0.023502  |
-------------------------------------
[2018-12-22 12:11:45.382517 UTC] Saving snapshot
[2018-12-22 12:11:45.382774 UTC] Starting iteration 1213
[2018-12-22 12:11:45.382895 UTC] Start collecting samples
[2018-12-22 12:11:48.349996 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:48.429514 UTC] Performing policy update
[2018-12-22 12:11:48.430148 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:48.520576 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:49.579760 UTC] Performing line search
[2018-12-22 12:11:49.707467 UTC] Updating baseline
[2018-12-22 12:11:51.216074 UTC] Computing logging information
-------------------------------------
| Iteration            | 1213       |
| ExpectedImprovement  | 0.017928   |
| ActualImprovement    | 0.017297   |
| ImprovementRatio     | 0.96479    |
| MeanKL               | 0.0075625  |
| Entropy              | -1.19      |
| Perplexity           | 0.30421    |
| AveragePolicyStd     | 0.20149    |
| AveragePolicyStd[0]  | 0.22066    |
| AveragePolicyStd[1]  | 0.19825    |
| AveragePolicyStd[2]  | 0.16163    |
| AveragePolicyStd[3]  | 0.20724    |
| AveragePolicyStd[4]  | 0.15887    |
| AveragePolicyStd[5]  | 0.26228    |
| AverageReturn        | 1744.5     |
| MinReturn            | 290.74     |
| MaxReturn            | 1911.1     |
| StdReturn            | 326.57     |
| AverageEpisodeLength | 945.6      |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.9      |
| TotalNEpisodes       | 20805      |
| TotalNSamples        | 6.0722e+06 |
| ExplainedVariance    | 0.097739   |
-------------------------------------
[2018-12-22 12:11:51.628726 UTC] Saving snapshot
[2018-12-22 12:11:51.628964 UTC] Starting iteration 1214
[2018-12-22 12:11:51.629079 UTC] Start collecting samples
[2018-12-22 12:11:54.561433 UTC] Computing input variables for policy optimization
[2018-12-22 12:11:54.637680 UTC] Performing policy update
[2018-12-22 12:11:54.638322 UTC] Computing gradient in Euclidean space
[2018-12-22 12:11:54.727631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:11:55.781207 UTC] Performing line search
[2018-12-22 12:11:55.907221 UTC] Updating baseline
[2018-12-22 12:11:57.142812 UTC] Computing logging information
-------------------------------------
| Iteration            | 1214       |
| ExpectedImprovement  | 0.018634   |
| ActualImprovement    | 0.016796   |
| ImprovementRatio     | 0.90139    |
| MeanKL               | 0.0082854  |
| Entropy              | -1.1862    |
| Perplexity           | 0.30539    |
| AveragePolicyStd     | 0.20161    |
| AveragePolicyStd[0]  | 0.2208     |
| AveragePolicyStd[1]  | 0.19841    |
| AveragePolicyStd[2]  | 0.16191    |
| AveragePolicyStd[3]  | 0.20761    |
| AveragePolicyStd[4]  | 0.15879    |
| AveragePolicyStd[5]  | 0.26215    |
| AverageReturn        | 1738.1     |
| MinReturn            | 290.74     |
| MaxReturn            | 1911.1     |
| StdReturn            | 331.58     |
| AverageEpisodeLength | 942.13     |
| MinEpisodeLength     | 211        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.35     |
| TotalNEpisodes       | 20810      |
| TotalNSamples        | 6.0769e+06 |
| ExplainedVariance    | 0.11625    |
-------------------------------------
[2018-12-22 12:11:57.554561 UTC] Saving snapshot
[2018-12-22 12:11:57.554821 UTC] Starting iteration 1215
[2018-12-22 12:11:57.554942 UTC] Start collecting samples
[2018-12-22 12:12:00.521330 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:00.599075 UTC] Performing policy update
[2018-12-22 12:12:00.600037 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:00.688782 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:01.747686 UTC] Performing line search
[2018-12-22 12:12:01.879314 UTC] Updating baseline
[2018-12-22 12:12:03.125990 UTC] Computing logging information
------------------------------------
| Iteration            | 1215      |
| ExpectedImprovement  | 0.020028  |
| ActualImprovement    | 0.019007  |
| ImprovementRatio     | 0.94906   |
| MeanKL               | 0.0076152 |
| Entropy              | -1.1963   |
| Perplexity           | 0.3023    |
| AveragePolicyStd     | 0.20126   |
| AveragePolicyStd[0]  | 0.22059   |
| AveragePolicyStd[1]  | 0.19834   |
| AveragePolicyStd[2]  | 0.16162   |
| AveragePolicyStd[3]  | 0.20711   |
| AveragePolicyStd[4]  | 0.1585    |
| AveragePolicyStd[5]  | 0.2614    |
| AverageReturn        | 1719.5    |
| MinReturn            | 290.74    |
| MaxReturn            | 1934.2    |
| StdReturn            | 351.62    |
| AverageEpisodeLength | 932.79    |
| MinEpisodeLength     | 211       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 176.64    |
| TotalNEpisodes       | 20817     |
| TotalNSamples        | 6.083e+06 |
| ExplainedVariance    | 0.29234   |
------------------------------------
[2018-12-22 12:12:03.542398 UTC] Saving snapshot
[2018-12-22 12:12:03.542658 UTC] Starting iteration 1216
[2018-12-22 12:12:03.542792 UTC] Start collecting samples
[2018-12-22 12:12:06.521795 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:06.601251 UTC] Performing policy update
[2018-12-22 12:12:06.601902 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:06.692066 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:07.742929 UTC] Performing line search
[2018-12-22 12:12:07.873390 UTC] Updating baseline
[2018-12-22 12:12:09.126816 UTC] Computing logging information
-------------------------------------
| Iteration            | 1216       |
| ExpectedImprovement  | 0.019266   |
| ActualImprovement    | 0.018406   |
| ImprovementRatio     | 0.95537    |
| MeanKL               | 0.007655   |
| Entropy              | -1.1909    |
| Perplexity           | 0.30396    |
| AveragePolicyStd     | 0.20141    |
| AveragePolicyStd[0]  | 0.22046    |
| AveragePolicyStd[1]  | 0.19857    |
| AveragePolicyStd[2]  | 0.16212    |
| AveragePolicyStd[3]  | 0.20742    |
| AveragePolicyStd[4]  | 0.15859    |
| AveragePolicyStd[5]  | 0.26133    |
| AverageReturn        | 1729.2     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 328.04     |
| AverageEpisodeLength | 936.94     |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.71     |
| TotalNEpisodes       | 20822      |
| TotalNSamples        | 6.0873e+06 |
| ExplainedVariance    | 0.2363     |
-------------------------------------
[2018-12-22 12:12:09.541923 UTC] Saving snapshot
[2018-12-22 12:12:09.542189 UTC] Starting iteration 1217
[2018-12-22 12:12:09.542332 UTC] Start collecting samples
[2018-12-22 12:12:12.491868 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:12.568854 UTC] Performing policy update
[2018-12-22 12:12:12.569647 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:12.659571 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:13.714924 UTC] Performing line search
[2018-12-22 12:12:13.841690 UTC] Updating baseline
[2018-12-22 12:12:15.436451 UTC] Computing logging information
-------------------------------------
| Iteration            | 1217       |
| ExpectedImprovement  | 0.018104   |
| ActualImprovement    | 0.017043   |
| ImprovementRatio     | 0.94139    |
| MeanKL               | 0.0084726  |
| Entropy              | -1.1967    |
| Perplexity           | 0.30219    |
| AveragePolicyStd     | 0.20121    |
| AveragePolicyStd[0]  | 0.21969    |
| AveragePolicyStd[1]  | 0.19865    |
| AveragePolicyStd[2]  | 0.16201    |
| AveragePolicyStd[3]  | 0.20682    |
| AveragePolicyStd[4]  | 0.15867    |
| AveragePolicyStd[5]  | 0.26141    |
| AverageReturn        | 1730.5     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 328.54     |
| AverageEpisodeLength | 936.94     |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.71     |
| TotalNEpisodes       | 20827      |
| TotalNSamples        | 6.0923e+06 |
| ExplainedVariance    | -0.0060444 |
-------------------------------------
[2018-12-22 12:12:15.855281 UTC] Saving snapshot
[2018-12-22 12:12:15.855582 UTC] Starting iteration 1218
[2018-12-22 12:12:15.855702 UTC] Start collecting samples
[2018-12-22 12:12:18.802185 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:18.879121 UTC] Performing policy update
[2018-12-22 12:12:18.880094 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:18.969417 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:20.038005 UTC] Performing line search
[2018-12-22 12:12:20.168468 UTC] Updating baseline
[2018-12-22 12:12:21.403783 UTC] Computing logging information
--------------------------------------
| Iteration            | 1218        |
| ExpectedImprovement  | 0.018314    |
| ActualImprovement    | 0.01711     |
| ImprovementRatio     | 0.93426     |
| MeanKL               | 0.0079436   |
| Entropy              | -1.1943     |
| Perplexity           | 0.30292     |
| AveragePolicyStd     | 0.20131     |
| AveragePolicyStd[0]  | 0.21971     |
| AveragePolicyStd[1]  | 0.1982      |
| AveragePolicyStd[2]  | 0.16216     |
| AveragePolicyStd[3]  | 0.20739     |
| AveragePolicyStd[4]  | 0.15858     |
| AveragePolicyStd[5]  | 0.2618      |
| AverageReturn        | 1730.6      |
| MinReturn            | 474.98      |
| MaxReturn            | 1934.2      |
| StdReturn            | 328.71      |
| AverageEpisodeLength | 936.94      |
| MinEpisodeLength     | 300         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 164.71      |
| TotalNEpisodes       | 20832       |
| TotalNSamples        | 6.0973e+06  |
| ExplainedVariance    | -0.00041363 |
--------------------------------------
[2018-12-22 12:12:21.820303 UTC] Saving snapshot
[2018-12-22 12:12:21.820575 UTC] Starting iteration 1219
[2018-12-22 12:12:21.820696 UTC] Start collecting samples
[2018-12-22 12:12:24.748311 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:24.823275 UTC] Performing policy update
[2018-12-22 12:12:24.824167 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:24.913551 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:25.974622 UTC] Performing line search
[2018-12-22 12:12:26.100156 UTC] Updating baseline
[2018-12-22 12:12:27.682969 UTC] Computing logging information
-------------------------------------
| Iteration            | 1219       |
| ExpectedImprovement  | 0.019376   |
| ActualImprovement    | 0.017615   |
| ImprovementRatio     | 0.90911    |
| MeanKL               | 0.0077485  |
| Entropy              | -1.1955    |
| Perplexity           | 0.30255    |
| AveragePolicyStd     | 0.20128    |
| AveragePolicyStd[0]  | 0.21958    |
| AveragePolicyStd[1]  | 0.19846    |
| AveragePolicyStd[2]  | 0.16196    |
| AveragePolicyStd[3]  | 0.2075     |
| AveragePolicyStd[4]  | 0.15843    |
| AveragePolicyStd[5]  | 0.26172    |
| AverageReturn        | 1728.7     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 328.37     |
| AverageEpisodeLength | 936.2      |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.59     |
| TotalNEpisodes       | 20836      |
| TotalNSamples        | 6.1012e+06 |
| ExplainedVariance    | 0.15617    |
-------------------------------------
[2018-12-22 12:12:28.102991 UTC] Saving snapshot
[2018-12-22 12:12:28.103235 UTC] Starting iteration 1220
[2018-12-22 12:12:28.103351 UTC] Start collecting samples
[2018-12-22 12:12:31.091406 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:31.172124 UTC] Performing policy update
[2018-12-22 12:12:31.173041 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:31.264170 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:32.322522 UTC] Performing line search
[2018-12-22 12:12:32.449523 UTC] Updating baseline
[2018-12-22 12:12:34.206091 UTC] Computing logging information
-------------------------------------
| Iteration            | 1220       |
| ExpectedImprovement  | 0.018189   |
| ActualImprovement    | 0.017582   |
| ImprovementRatio     | 0.96664    |
| MeanKL               | 0.0075317  |
| Entropy              | -1.1944    |
| Perplexity           | 0.30287    |
| AveragePolicyStd     | 0.20129    |
| AveragePolicyStd[0]  | 0.21952    |
| AveragePolicyStd[1]  | 0.19838    |
| AveragePolicyStd[2]  | 0.16238    |
| AveragePolicyStd[3]  | 0.20796    |
| AveragePolicyStd[4]  | 0.1582     |
| AveragePolicyStd[5]  | 0.2613     |
| AverageReturn        | 1742.3     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 311.56     |
| AverageEpisodeLength | 942.08     |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.8      |
| TotalNEpisodes       | 20843      |
| TotalNSamples        | 6.1076e+06 |
| ExplainedVariance    | -0.0010822 |
-------------------------------------
[2018-12-22 12:12:34.616094 UTC] Saving snapshot
[2018-12-22 12:12:34.624161 UTC] Starting iteration 1221
[2018-12-22 12:12:34.624364 UTC] Start collecting samples
[2018-12-22 12:12:37.552885 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:37.627942 UTC] Performing policy update
[2018-12-22 12:12:37.628981 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:37.716966 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:38.777943 UTC] Performing line search
[2018-12-22 12:12:38.906123 UTC] Updating baseline
[2018-12-22 12:12:40.580492 UTC] Computing logging information
-------------------------------------
| Iteration            | 1221       |
| ExpectedImprovement  | 0.015717   |
| ActualImprovement    | 0.01489    |
| ImprovementRatio     | 0.94742    |
| MeanKL               | 0.0094073  |
| Entropy              | -1.205     |
| Perplexity           | 0.29968    |
| AveragePolicyStd     | 0.20089    |
| AveragePolicyStd[0]  | 0.21951    |
| AveragePolicyStd[1]  | 0.19775    |
| AveragePolicyStd[2]  | 0.16238    |
| AveragePolicyStd[3]  | 0.20758    |
| AveragePolicyStd[4]  | 0.15806    |
| AveragePolicyStd[5]  | 0.26009    |
| AverageReturn        | 1743       |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 311.75     |
| AverageEpisodeLength | 942.08     |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.8      |
| TotalNEpisodes       | 20847      |
| TotalNSamples        | 6.1116e+06 |
| ExplainedVariance    | 0.0059355  |
-------------------------------------
[2018-12-22 12:12:40.996820 UTC] Saving snapshot
[2018-12-22 12:12:40.997066 UTC] Starting iteration 1222
[2018-12-22 12:12:40.997197 UTC] Start collecting samples
[2018-12-22 12:12:43.924584 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:44.003515 UTC] Performing policy update
[2018-12-22 12:12:44.004139 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:44.094260 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:45.151986 UTC] Performing line search
[2018-12-22 12:12:45.279428 UTC] Updating baseline
[2018-12-22 12:12:46.613703 UTC] Computing logging information
-------------------------------------
| Iteration            | 1222       |
| ExpectedImprovement  | 0.016432   |
| ActualImprovement    | 0.015106   |
| ImprovementRatio     | 0.91927    |
| MeanKL               | 0.0078971  |
| Entropy              | -1.2048    |
| Perplexity           | 0.29976    |
| AveragePolicyStd     | 0.2009     |
| AveragePolicyStd[0]  | 0.21906    |
| AveragePolicyStd[1]  | 0.19796    |
| AveragePolicyStd[2]  | 0.16251    |
| AveragePolicyStd[3]  | 0.20742    |
| AveragePolicyStd[4]  | 0.15806    |
| AveragePolicyStd[5]  | 0.26038    |
| AverageReturn        | 1751.2     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 308.76     |
| AverageEpisodeLength | 945.1      |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.64     |
| TotalNEpisodes       | 20851      |
| TotalNSamples        | 6.1156e+06 |
| ExplainedVariance    | 0.18673    |
-------------------------------------
[2018-12-22 12:12:47.028243 UTC] Saving snapshot
[2018-12-22 12:12:47.028530 UTC] Starting iteration 1223
[2018-12-22 12:12:47.028654 UTC] Start collecting samples
[2018-12-22 12:12:49.995287 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:50.074408 UTC] Performing policy update
[2018-12-22 12:12:50.075155 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:50.165569 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:51.231251 UTC] Performing line search
[2018-12-22 12:12:51.358397 UTC] Updating baseline
[2018-12-22 12:12:52.857408 UTC] Computing logging information
-------------------------------------
| Iteration            | 1223       |
| ExpectedImprovement  | 0.016007   |
| ActualImprovement    | 0.01546    |
| ImprovementRatio     | 0.96583    |
| MeanKL               | 0.008251   |
| Entropy              | -1.2056    |
| Perplexity           | 0.29951    |
| AveragePolicyStd     | 0.20089    |
| AveragePolicyStd[0]  | 0.21896    |
| AveragePolicyStd[1]  | 0.19811    |
| AveragePolicyStd[2]  | 0.16271    |
| AveragePolicyStd[3]  | 0.2076     |
| AveragePolicyStd[4]  | 0.15756    |
| AveragePolicyStd[5]  | 0.26041    |
| AverageReturn        | 1751.5     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 308.86     |
| AverageEpisodeLength | 945.1      |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.64     |
| TotalNEpisodes       | 20858      |
| TotalNSamples        | 6.1226e+06 |
| ExplainedVariance    | -0.0019006 |
-------------------------------------
[2018-12-22 12:12:53.269973 UTC] Saving snapshot
[2018-12-22 12:12:53.270213 UTC] Starting iteration 1224
[2018-12-22 12:12:53.270346 UTC] Start collecting samples
[2018-12-22 12:12:56.187492 UTC] Computing input variables for policy optimization
[2018-12-22 12:12:56.262828 UTC] Performing policy update
[2018-12-22 12:12:56.263604 UTC] Computing gradient in Euclidean space
[2018-12-22 12:12:56.353991 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:12:57.311134 UTC] Performing line search
[2018-12-22 12:12:57.427405 UTC] Updating baseline
[2018-12-22 12:12:59.349931 UTC] Computing logging information
-------------------------------------
| Iteration            | 1224       |
| ExpectedImprovement  | 0.016044   |
| ActualImprovement    | 0.014461   |
| ImprovementRatio     | 0.90134    |
| MeanKL               | 0.0083043  |
| Entropy              | -1.2084    |
| Perplexity           | 0.29867    |
| AveragePolicyStd     | 0.20081    |
| AveragePolicyStd[0]  | 0.21891    |
| AveragePolicyStd[1]  | 0.19887    |
| AveragePolicyStd[2]  | 0.16226    |
| AveragePolicyStd[3]  | 0.20705    |
| AveragePolicyStd[4]  | 0.15746    |
| AveragePolicyStd[5]  | 0.26031    |
| AverageReturn        | 1752.6     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 309.13     |
| AverageEpisodeLength | 945.1      |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.64     |
| TotalNEpisodes       | 20861      |
| TotalNSamples        | 6.1256e+06 |
| ExplainedVariance    | 0.0066806  |
-------------------------------------
[2018-12-22 12:12:59.767978 UTC] Saving snapshot
[2018-12-22 12:12:59.768257 UTC] Starting iteration 1225
[2018-12-22 12:12:59.768387 UTC] Start collecting samples
[2018-12-22 12:13:02.743259 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:02.820967 UTC] Performing policy update
[2018-12-22 12:13:02.822637 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:02.912431 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:03.972688 UTC] Performing line search
[2018-12-22 12:13:04.100577 UTC] Updating baseline
[2018-12-22 12:13:05.520921 UTC] Computing logging information
-------------------------------------
| Iteration            | 1225       |
| ExpectedImprovement  | 0.015895   |
| ActualImprovement    | 0.015046   |
| ImprovementRatio     | 0.94655    |
| MeanKL               | 0.0074358  |
| Entropy              | -1.2117    |
| Perplexity           | 0.29769    |
| AveragePolicyStd     | 0.20068    |
| AveragePolicyStd[0]  | 0.21914    |
| AveragePolicyStd[1]  | 0.19851    |
| AveragePolicyStd[2]  | 0.16227    |
| AveragePolicyStd[3]  | 0.20657    |
| AveragePolicyStd[4]  | 0.15764    |
| AveragePolicyStd[5]  | 0.25993    |
| AverageReturn        | 1786.4     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 264.61     |
| AverageEpisodeLength | 961.8      |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.14     |
| TotalNEpisodes       | 20867      |
| TotalNSamples        | 6.1316e+06 |
| ExplainedVariance    | 0.00044784 |
-------------------------------------
[2018-12-22 12:13:05.941918 UTC] Saving snapshot
[2018-12-22 12:13:05.942169 UTC] Starting iteration 1226
[2018-12-22 12:13:05.942305 UTC] Start collecting samples
[2018-12-22 12:13:08.900151 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:08.977373 UTC] Performing policy update
[2018-12-22 12:13:08.978170 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:09.068548 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:10.136104 UTC] Performing line search
[2018-12-22 12:13:10.265845 UTC] Updating baseline
[2018-12-22 12:13:11.579979 UTC] Computing logging information
--------------------------------------
| Iteration            | 1226        |
| ExpectedImprovement  | 0.015757    |
| ActualImprovement    | 0.014758    |
| ImprovementRatio     | 0.9366      |
| MeanKL               | 0.0080766   |
| Entropy              | -1.2146     |
| Perplexity           | 0.29684     |
| AveragePolicyStd     | 0.20059     |
| AveragePolicyStd[0]  | 0.21887     |
| AveragePolicyStd[1]  | 0.19881     |
| AveragePolicyStd[2]  | 0.16226     |
| AveragePolicyStd[3]  | 0.20614     |
| AveragePolicyStd[4]  | 0.15743     |
| AveragePolicyStd[5]  | 0.26003     |
| AverageReturn        | 1788        |
| MinReturn            | 474.98      |
| MaxReturn            | 1934.2      |
| StdReturn            | 265.01      |
| AverageEpisodeLength | 961.8       |
| MinEpisodeLength     | 300         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 131.14      |
| TotalNEpisodes       | 20873       |
| TotalNSamples        | 6.1376e+06  |
| ExplainedVariance    | -3.0295e-05 |
--------------------------------------
[2018-12-22 12:13:11.999812 UTC] Saving snapshot
[2018-12-22 12:13:12.000062 UTC] Starting iteration 1227
[2018-12-22 12:13:12.000179 UTC] Start collecting samples
[2018-12-22 12:13:14.928165 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:15.003791 UTC] Performing policy update
[2018-12-22 12:13:15.004589 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:15.095686 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:16.163553 UTC] Performing line search
[2018-12-22 12:13:16.291523 UTC] Updating baseline
[2018-12-22 12:13:18.282518 UTC] Computing logging information
-------------------------------------
| Iteration            | 1227       |
| ExpectedImprovement  | 0.016782   |
| ActualImprovement    | 0.01493    |
| ImprovementRatio     | 0.88964    |
| MeanKL               | 0.0084048  |
| Entropy              | -1.2138    |
| Perplexity           | 0.29707    |
| AveragePolicyStd     | 0.20062    |
| AveragePolicyStd[0]  | 0.2188     |
| AveragePolicyStd[1]  | 0.19942    |
| AveragePolicyStd[2]  | 0.16248    |
| AveragePolicyStd[3]  | 0.2059     |
| AveragePolicyStd[4]  | 0.15705    |
| AveragePolicyStd[5]  | 0.2601     |
| AverageReturn        | 1787.9     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 264.96     |
| AverageEpisodeLength | 961.8      |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.14     |
| TotalNEpisodes       | 20877      |
| TotalNSamples        | 6.1416e+06 |
| ExplainedVariance    | 0.0046206  |
-------------------------------------
[2018-12-22 12:13:18.699084 UTC] Saving snapshot
[2018-12-22 12:13:18.699354 UTC] Starting iteration 1228
[2018-12-22 12:13:18.699474 UTC] Start collecting samples
[2018-12-22 12:13:21.638765 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:21.714345 UTC] Performing policy update
[2018-12-22 12:13:21.715162 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:21.805237 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:22.851121 UTC] Performing line search
[2018-12-22 12:13:22.978081 UTC] Updating baseline
[2018-12-22 12:13:24.477478 UTC] Computing logging information
-------------------------------------
| Iteration            | 1228       |
| ExpectedImprovement  | 0.016228   |
| ActualImprovement    | 0.015333   |
| ImprovementRatio     | 0.94488    |
| MeanKL               | 0.0076288  |
| Entropy              | -1.2196    |
| Perplexity           | 0.29536    |
| AveragePolicyStd     | 0.20037    |
| AveragePolicyStd[0]  | 0.21883    |
| AveragePolicyStd[1]  | 0.19928    |
| AveragePolicyStd[2]  | 0.16293    |
| AveragePolicyStd[3]  | 0.20485    |
| AveragePolicyStd[4]  | 0.15715    |
| AveragePolicyStd[5]  | 0.25917    |
| AverageReturn        | 1789.6     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 265.52     |
| AverageEpisodeLength | 961.8      |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.14     |
| TotalNEpisodes       | 20882      |
| TotalNSamples        | 6.1466e+06 |
| ExplainedVariance    | 0.0054804  |
-------------------------------------
[2018-12-22 12:13:24.894114 UTC] Saving snapshot
[2018-12-22 12:13:24.894383 UTC] Starting iteration 1229
[2018-12-22 12:13:24.894514 UTC] Start collecting samples
[2018-12-22 12:13:27.831005 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:27.907162 UTC] Performing policy update
[2018-12-22 12:13:27.908103 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:27.999516 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:29.051381 UTC] Performing line search
[2018-12-22 12:13:29.179037 UTC] Updating baseline
[2018-12-22 12:13:31.534684 UTC] Computing logging information
-------------------------------------
| Iteration            | 1229       |
| ExpectedImprovement  | 0.021088   |
| ActualImprovement    | 0.019452   |
| ImprovementRatio     | 0.92243    |
| MeanKL               | 0.0071545  |
| Entropy              | -1.2333    |
| Perplexity           | 0.29134    |
| AveragePolicyStd     | 0.19992    |
| AveragePolicyStd[0]  | 0.21843    |
| AveragePolicyStd[1]  | 0.19904    |
| AveragePolicyStd[2]  | 0.16286    |
| AveragePolicyStd[3]  | 0.2041     |
| AveragePolicyStd[4]  | 0.15645    |
| AveragePolicyStd[5]  | 0.25862    |
| AverageReturn        | 1788.6     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 265.62     |
| AverageEpisodeLength | 961.01     |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.15     |
| TotalNEpisodes       | 20887      |
| TotalNSamples        | 6.1515e+06 |
| ExplainedVariance    | 0.077624   |
-------------------------------------
[2018-12-22 12:13:31.957311 UTC] Saving snapshot
[2018-12-22 12:13:31.957604 UTC] Starting iteration 1230
[2018-12-22 12:13:31.957754 UTC] Start collecting samples
[2018-12-22 12:13:34.870491 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:34.947524 UTC] Performing policy update
[2018-12-22 12:13:34.948543 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:35.038157 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:36.097581 UTC] Performing line search
[2018-12-22 12:13:36.225940 UTC] Updating baseline
[2018-12-22 12:13:37.474289 UTC] Computing logging information
-------------------------------------
| Iteration            | 1230       |
| ExpectedImprovement  | 0.01738    |
| ActualImprovement    | 0.016545   |
| ImprovementRatio     | 0.95193    |
| MeanKL               | 0.0076762  |
| Entropy              | -1.2418    |
| Perplexity           | 0.28886    |
| AveragePolicyStd     | 0.19962    |
| AveragePolicyStd[0]  | 0.21807    |
| AveragePolicyStd[1]  | 0.19879    |
| AveragePolicyStd[2]  | 0.16301    |
| AveragePolicyStd[3]  | 0.20413    |
| AveragePolicyStd[4]  | 0.15593    |
| AveragePolicyStd[5]  | 0.25777    |
| AverageReturn        | 1799.8     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 253.51     |
| AverageEpisodeLength | 965.38     |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.95     |
| TotalNEpisodes       | 20892      |
| TotalNSamples        | 6.1565e+06 |
| ExplainedVariance    | 0.0050349  |
-------------------------------------
[2018-12-22 12:13:37.883760 UTC] Saving snapshot
[2018-12-22 12:13:37.891927 UTC] Starting iteration 1231
[2018-12-22 12:13:37.892127 UTC] Start collecting samples
[2018-12-22 12:13:40.832016 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:40.910023 UTC] Performing policy update
[2018-12-22 12:13:40.910658 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:41.000261 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:42.052181 UTC] Performing line search
[2018-12-22 12:13:42.178893 UTC] Updating baseline
[2018-12-22 12:13:43.555075 UTC] Computing logging information
-------------------------------------
| Iteration            | 1231       |
| ExpectedImprovement  | 0.017642   |
| ActualImprovement    | 0.016935   |
| ImprovementRatio     | 0.95994    |
| MeanKL               | 0.0081062  |
| Entropy              | -1.2433    |
| Perplexity           | 0.28844    |
| AveragePolicyStd     | 0.19958    |
| AveragePolicyStd[0]  | 0.21805    |
| AveragePolicyStd[1]  | 0.19899    |
| AveragePolicyStd[2]  | 0.16307    |
| AveragePolicyStd[3]  | 0.20379    |
| AveragePolicyStd[4]  | 0.15569    |
| AveragePolicyStd[5]  | 0.25789    |
| AverageReturn        | 1795.4     |
| MinReturn            | 474.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 256.92     |
| AverageEpisodeLength | 963        |
| MinEpisodeLength     | 300        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.84     |
| TotalNEpisodes       | 20898      |
| TotalNSamples        | 6.1622e+06 |
| ExplainedVariance    | 0.12151    |
-------------------------------------
[2018-12-22 12:13:43.975446 UTC] Saving snapshot
[2018-12-22 12:13:43.975716 UTC] Starting iteration 1232
[2018-12-22 12:13:43.975861 UTC] Start collecting samples
[2018-12-22 12:13:46.933538 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:47.012317 UTC] Performing policy update
[2018-12-22 12:13:47.012909 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:47.102007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:48.153061 UTC] Performing line search
[2018-12-22 12:13:48.279882 UTC] Updating baseline
[2018-12-22 12:13:49.749082 UTC] Computing logging information
-------------------------------------
| Iteration            | 1232       |
| ExpectedImprovement  | 0.018618   |
| ActualImprovement    | 0.017318   |
| ImprovementRatio     | 0.93017    |
| MeanKL               | 0.0080225  |
| Entropy              | -1.2441    |
| Perplexity           | 0.28819    |
| AveragePolicyStd     | 0.19959    |
| AveragePolicyStd[0]  | 0.21858    |
| AveragePolicyStd[1]  | 0.19881    |
| AveragePolicyStd[2]  | 0.16262    |
| AveragePolicyStd[3]  | 0.20349    |
| AveragePolicyStd[4]  | 0.15573    |
| AveragePolicyStd[5]  | 0.25829    |
| AverageReturn        | 1783.3     |
| MinReturn            | 461.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 289.66     |
| AverageEpisodeLength | 955.78     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.93     |
| TotalNEpisodes       | 20904      |
| TotalNSamples        | 6.1675e+06 |
| ExplainedVariance    | 0.1145     |
-------------------------------------
[2018-12-22 12:13:50.170275 UTC] Saving snapshot
[2018-12-22 12:13:50.170526 UTC] Starting iteration 1233
[2018-12-22 12:13:50.170648 UTC] Start collecting samples
[2018-12-22 12:13:53.094511 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:53.170916 UTC] Performing policy update
[2018-12-22 12:13:53.171544 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:53.260704 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:13:54.323396 UTC] Performing line search
[2018-12-22 12:13:54.450919 UTC] Updating baseline
[2018-12-22 12:13:55.870542 UTC] Computing logging information
-------------------------------------
| Iteration            | 1233       |
| ExpectedImprovement  | 0.017931   |
| ActualImprovement    | 0.01717    |
| ImprovementRatio     | 0.95758    |
| MeanKL               | 0.0078179  |
| Entropy              | -1.2479    |
| Perplexity           | 0.2871     |
| AveragePolicyStd     | 0.19944    |
| AveragePolicyStd[0]  | 0.21848    |
| AveragePolicyStd[1]  | 0.19912    |
| AveragePolicyStd[2]  | 0.16267    |
| AveragePolicyStd[3]  | 0.20374    |
| AveragePolicyStd[4]  | 0.15532    |
| AveragePolicyStd[5]  | 0.25732    |
| AverageReturn        | 1805.5     |
| MinReturn            | 461.98     |
| MaxReturn            | 1934.2     |
| StdReturn            | 250.8      |
| AverageEpisodeLength | 966.25     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.21     |
| TotalNEpisodes       | 20909      |
| TotalNSamples        | 6.1725e+06 |
| ExplainedVariance    | 0.022657   |
-------------------------------------
[2018-12-22 12:13:56.290630 UTC] Saving snapshot
[2018-12-22 12:13:56.290924 UTC] Starting iteration 1234
[2018-12-22 12:13:56.291045 UTC] Start collecting samples
[2018-12-22 12:13:59.208899 UTC] Computing input variables for policy optimization
[2018-12-22 12:13:59.285321 UTC] Performing policy update
[2018-12-22 12:13:59.286212 UTC] Computing gradient in Euclidean space
[2018-12-22 12:13:59.377489 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:00.494115 UTC] Performing line search
[2018-12-22 12:14:00.627044 UTC] Updating baseline
[2018-12-22 12:14:02.294297 UTC] Computing logging information
-------------------------------------
| Iteration            | 1234       |
| ExpectedImprovement  | 0.016327   |
| ActualImprovement    | 0.016479   |
| ImprovementRatio     | 1.0093     |
| MeanKL               | 0.0077573  |
| Entropy              | -1.2453    |
| Perplexity           | 0.28785    |
| AveragePolicyStd     | 0.19953    |
| AveragePolicyStd[0]  | 0.21861    |
| AveragePolicyStd[1]  | 0.19898    |
| AveragePolicyStd[2]  | 0.16259    |
| AveragePolicyStd[3]  | 0.20369    |
| AveragePolicyStd[4]  | 0.15566    |
| AveragePolicyStd[5]  | 0.25764    |
| AverageReturn        | 1818.4     |
| MinReturn            | 461.98     |
| MaxReturn            | 1939.6     |
| StdReturn            | 236.01     |
| AverageEpisodeLength | 970.91     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 116.4      |
| TotalNEpisodes       | 20913      |
| TotalNSamples        | 6.1765e+06 |
| ExplainedVariance    | -0.04582   |
-------------------------------------
[2018-12-22 12:14:02.744187 UTC] Saving snapshot
[2018-12-22 12:14:02.744448 UTC] Starting iteration 1235
[2018-12-22 12:14:02.744638 UTC] Start collecting samples
[2018-12-22 12:14:05.842048 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:05.919214 UTC] Performing policy update
[2018-12-22 12:14:05.919865 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:06.008996 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:07.073068 UTC] Performing line search
[2018-12-22 12:14:07.201689 UTC] Updating baseline
[2018-12-22 12:14:08.579556 UTC] Computing logging information
-------------------------------------
| Iteration            | 1235       |
| ExpectedImprovement  | 0.01808    |
| ActualImprovement    | 0.01681    |
| ImprovementRatio     | 0.92973    |
| MeanKL               | 0.0075757  |
| Entropy              | -1.2394    |
| Perplexity           | 0.28956    |
| AveragePolicyStd     | 0.19968    |
| AveragePolicyStd[0]  | 0.21861    |
| AveragePolicyStd[1]  | 0.19928    |
| AveragePolicyStd[2]  | 0.16285    |
| AveragePolicyStd[3]  | 0.20415    |
| AveragePolicyStd[4]  | 0.15606    |
| AveragePolicyStd[5]  | 0.25713    |
| AverageReturn        | 1834.9     |
| MinReturn            | 461.98     |
| MaxReturn            | 1953       |
| StdReturn            | 209.87     |
| AverageEpisodeLength | 978.02     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105.5      |
| TotalNEpisodes       | 20918      |
| TotalNSamples        | 6.1815e+06 |
| ExplainedVariance    | 0.0028589  |
-------------------------------------
[2018-12-22 12:14:09.003337 UTC] Saving snapshot
[2018-12-22 12:14:09.003613 UTC] Starting iteration 1236
[2018-12-22 12:14:09.003745 UTC] Start collecting samples
[2018-12-22 12:14:11.963682 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:12.040370 UTC] Performing policy update
[2018-12-22 12:14:12.041224 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:12.130662 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:13.175963 UTC] Performing line search
[2018-12-22 12:14:13.303018 UTC] Updating baseline
[2018-12-22 12:14:14.875227 UTC] Computing logging information
-------------------------------------
| Iteration            | 1236       |
| ExpectedImprovement  | 0.014213   |
| ActualImprovement    | 0.013395   |
| ImprovementRatio     | 0.94249    |
| MeanKL               | 0.0083738  |
| Entropy              | -1.2389    |
| Perplexity           | 0.2897     |
| AveragePolicyStd     | 0.19969    |
| AveragePolicyStd[0]  | 0.21875    |
| AveragePolicyStd[1]  | 0.19897    |
| AveragePolicyStd[2]  | 0.16269    |
| AveragePolicyStd[3]  | 0.20467    |
| AveragePolicyStd[4]  | 0.15616    |
| AveragePolicyStd[5]  | 0.25691    |
| AverageReturn        | 1845.8     |
| MinReturn            | 461.98     |
| MaxReturn            | 1953       |
| StdReturn            | 192.67     |
| AverageEpisodeLength | 982.58     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.081     |
| TotalNEpisodes       | 20924      |
| TotalNSamples        | 6.1875e+06 |
| ExplainedVariance    | 0.0069689  |
-------------------------------------
[2018-12-22 12:14:15.290191 UTC] Saving snapshot
[2018-12-22 12:14:15.290438 UTC] Starting iteration 1237
[2018-12-22 12:14:15.290575 UTC] Start collecting samples
[2018-12-22 12:14:18.335232 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:18.418272 UTC] Performing policy update
[2018-12-22 12:14:18.418962 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:18.511951 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:19.612951 UTC] Performing line search
[2018-12-22 12:14:19.744718 UTC] Updating baseline
[2018-12-22 12:14:21.036901 UTC] Computing logging information
-------------------------------------
| Iteration            | 1237       |
| ExpectedImprovement  | 0.017319   |
| ActualImprovement    | 0.016485   |
| ImprovementRatio     | 0.95182    |
| MeanKL               | 0.0082299  |
| Entropy              | -1.2354    |
| Perplexity           | 0.29072    |
| AveragePolicyStd     | 0.1999     |
| AveragePolicyStd[0]  | 0.21966    |
| AveragePolicyStd[1]  | 0.19915    |
| AveragePolicyStd[2]  | 0.162      |
| AveragePolicyStd[3]  | 0.20408    |
| AveragePolicyStd[4]  | 0.15614    |
| AveragePolicyStd[5]  | 0.2584     |
| AverageReturn        | 1842.5     |
| MinReturn            | 461.98     |
| MaxReturn            | 1953       |
| StdReturn            | 195.31     |
| AverageEpisodeLength | 981.16     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.859     |
| TotalNEpisodes       | 20929      |
| TotalNSamples        | 6.1924e+06 |
| ExplainedVariance    | 0.10186    |
-------------------------------------
[2018-12-22 12:14:21.489642 UTC] Saving snapshot
[2018-12-22 12:14:21.489946 UTC] Starting iteration 1238
[2018-12-22 12:14:21.490078 UTC] Start collecting samples
[2018-12-22 12:14:24.665979 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:24.749189 UTC] Performing policy update
[2018-12-22 12:14:24.750019 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:24.845341 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:25.964490 UTC] Performing line search
[2018-12-22 12:14:26.097688 UTC] Updating baseline
[2018-12-22 12:14:27.633702 UTC] Computing logging information
-------------------------------------
| Iteration            | 1238       |
| ExpectedImprovement  | 0.017336   |
| ActualImprovement    | 0.016625   |
| ImprovementRatio     | 0.95898    |
| MeanKL               | 0.0077746  |
| Entropy              | -1.2428    |
| Perplexity           | 0.28857    |
| AveragePolicyStd     | 0.19969    |
| AveragePolicyStd[0]  | 0.21974    |
| AveragePolicyStd[1]  | 0.19923    |
| AveragePolicyStd[2]  | 0.16136    |
| AveragePolicyStd[3]  | 0.20313    |
| AveragePolicyStd[4]  | 0.15613    |
| AveragePolicyStd[5]  | 0.25852    |
| AverageReturn        | 1843.6     |
| MinReturn            | 461.98     |
| MaxReturn            | 1958.3     |
| StdReturn            | 195.45     |
| AverageEpisodeLength | 980.97     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.986     |
| TotalNEpisodes       | 20934      |
| TotalNSamples        | 6.1973e+06 |
| ExplainedVariance    | 0.14525    |
-------------------------------------
[2018-12-22 12:14:28.057690 UTC] Saving snapshot
[2018-12-22 12:14:28.058034 UTC] Starting iteration 1239
[2018-12-22 12:14:28.058154 UTC] Start collecting samples
[2018-12-22 12:14:31.016697 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:31.094228 UTC] Performing policy update
[2018-12-22 12:14:31.094989 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:31.184455 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:32.249881 UTC] Performing line search
[2018-12-22 12:14:32.376700 UTC] Updating baseline
[2018-12-22 12:14:33.711517 UTC] Computing logging information
-------------------------------------
| Iteration            | 1239       |
| ExpectedImprovement  | 0.017437   |
| ActualImprovement    | 0.017215   |
| ImprovementRatio     | 0.98725    |
| MeanKL               | 0.0083987  |
| Entropy              | -1.2424    |
| Perplexity           | 0.28869    |
| AveragePolicyStd     | 0.19973    |
| AveragePolicyStd[0]  | 0.21981    |
| AveragePolicyStd[1]  | 0.19915    |
| AveragePolicyStd[2]  | 0.16122    |
| AveragePolicyStd[3]  | 0.20314    |
| AveragePolicyStd[4]  | 0.15601    |
| AveragePolicyStd[5]  | 0.25907    |
| AverageReturn        | 1843.6     |
| MinReturn            | 461.98     |
| MaxReturn            | 1958.3     |
| StdReturn            | 193.96     |
| AverageEpisodeLength | 981.12     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.101     |
| TotalNEpisodes       | 20940      |
| TotalNSamples        | 6.2027e+06 |
| ExplainedVariance    | 0.097924   |
-------------------------------------
[2018-12-22 12:14:34.134569 UTC] Saving snapshot
[2018-12-22 12:14:34.134817 UTC] Starting iteration 1240
[2018-12-22 12:14:34.134933 UTC] Start collecting samples
[2018-12-22 12:14:37.081528 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:37.157720 UTC] Performing policy update
[2018-12-22 12:14:37.158660 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:37.248379 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:38.314751 UTC] Performing line search
[2018-12-22 12:14:38.443969 UTC] Updating baseline
[2018-12-22 12:14:39.686111 UTC] Computing logging information
-------------------------------------
| Iteration            | 1240       |
| ExpectedImprovement  | 0.01618    |
| ActualImprovement    | 0.014863   |
| ImprovementRatio     | 0.9186     |
| MeanKL               | 0.0078434  |
| Entropy              | -1.2429    |
| Perplexity           | 0.28853    |
| AveragePolicyStd     | 0.19972    |
| AveragePolicyStd[0]  | 0.21981    |
| AveragePolicyStd[1]  | 0.19935    |
| AveragePolicyStd[2]  | 0.16108    |
| AveragePolicyStd[3]  | 0.20277    |
| AveragePolicyStd[4]  | 0.15613    |
| AveragePolicyStd[5]  | 0.25917    |
| AverageReturn        | 1844.9     |
| MinReturn            | 461.98     |
| MaxReturn            | 1958.3     |
| StdReturn            | 194.21     |
| AverageEpisodeLength | 981.12     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.101     |
| TotalNEpisodes       | 20944      |
| TotalNSamples        | 6.2067e+06 |
| ExplainedVariance    | 0.0047747  |
-------------------------------------
[2018-12-22 12:14:40.108810 UTC] Saving snapshot
[2018-12-22 12:14:40.116897 UTC] Starting iteration 1241
[2018-12-22 12:14:40.117091 UTC] Start collecting samples
[2018-12-22 12:14:43.072931 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:43.150022 UTC] Performing policy update
[2018-12-22 12:14:43.151025 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:43.240002 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:44.304366 UTC] Performing line search
[2018-12-22 12:14:44.433360 UTC] Updating baseline
[2018-12-22 12:14:45.853102 UTC] Computing logging information
-------------------------------------
| Iteration            | 1241       |
| ExpectedImprovement  | 0.01821    |
| ActualImprovement    | 0.016958   |
| ImprovementRatio     | 0.93128    |
| MeanKL               | 0.0077054  |
| Entropy              | -1.2392    |
| Perplexity           | 0.28961    |
| AveragePolicyStd     | 0.19985    |
| AveragePolicyStd[0]  | 0.22004    |
| AveragePolicyStd[1]  | 0.19976    |
| AveragePolicyStd[2]  | 0.16116    |
| AveragePolicyStd[3]  | 0.20256    |
| AveragePolicyStd[4]  | 0.15618    |
| AveragePolicyStd[5]  | 0.25937    |
| AverageReturn        | 1818.2     |
| MinReturn            | 461.98     |
| MaxReturn            | 1958.3     |
| StdReturn            | 272.88     |
| AverageEpisodeLength | 967.02     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.82     |
| TotalNEpisodes       | 20950      |
| TotalNSamples        | 6.2113e+06 |
| ExplainedVariance    | 0.2292     |
-------------------------------------
[2018-12-22 12:14:46.274346 UTC] Saving snapshot
[2018-12-22 12:14:46.274618 UTC] Starting iteration 1242
[2018-12-22 12:14:46.274744 UTC] Start collecting samples
[2018-12-22 12:14:49.250421 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:49.328639 UTC] Performing policy update
[2018-12-22 12:14:49.329278 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:49.419520 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:50.482278 UTC] Performing line search
[2018-12-22 12:14:50.609129 UTC] Updating baseline
[2018-12-22 12:14:51.840807 UTC] Computing logging information
-------------------------------------
| Iteration            | 1242       |
| ExpectedImprovement  | 0.017467   |
| ActualImprovement    | 0.016765   |
| ImprovementRatio     | 0.95979    |
| MeanKL               | 0.008062   |
| Entropy              | -1.2412    |
| Perplexity           | 0.28904    |
| AveragePolicyStd     | 0.19981    |
| AveragePolicyStd[0]  | 0.21992    |
| AveragePolicyStd[1]  | 0.19998    |
| AveragePolicyStd[2]  | 0.16089    |
| AveragePolicyStd[3]  | 0.20243    |
| AveragePolicyStd[4]  | 0.15595    |
| AveragePolicyStd[5]  | 0.25972    |
| AverageReturn        | 1819.8     |
| MinReturn            | 461.98     |
| MaxReturn            | 1958.3     |
| StdReturn            | 273.67     |
| AverageEpisodeLength | 966.42     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.87     |
| TotalNEpisodes       | 20957      |
| TotalNSamples        | 6.2182e+06 |
| ExplainedVariance    | 0.11602    |
-------------------------------------
[2018-12-22 12:14:52.259182 UTC] Saving snapshot
[2018-12-22 12:14:52.259478 UTC] Starting iteration 1243
[2018-12-22 12:14:52.259619 UTC] Start collecting samples
[2018-12-22 12:14:55.179895 UTC] Computing input variables for policy optimization
[2018-12-22 12:14:55.256817 UTC] Performing policy update
[2018-12-22 12:14:55.257596 UTC] Computing gradient in Euclidean space
[2018-12-22 12:14:55.348346 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:14:56.417374 UTC] Performing line search
[2018-12-22 12:14:56.544735 UTC] Updating baseline
[2018-12-22 12:14:58.312959 UTC] Computing logging information
-------------------------------------
| Iteration            | 1243       |
| ExpectedImprovement  | 0.017374   |
| ActualImprovement    | 0.016298   |
| ImprovementRatio     | 0.93804    |
| MeanKL               | 0.0077598  |
| Entropy              | -1.2402    |
| Perplexity           | 0.28933    |
| AveragePolicyStd     | 0.19986    |
| AveragePolicyStd[0]  | 0.21996    |
| AveragePolicyStd[1]  | 0.19989    |
| AveragePolicyStd[2]  | 0.16101    |
| AveragePolicyStd[3]  | 0.20237    |
| AveragePolicyStd[4]  | 0.15587    |
| AveragePolicyStd[5]  | 0.26007    |
| AverageReturn        | 1820.2     |
| MinReturn            | 461.98     |
| MaxReturn            | 1958.3     |
| StdReturn            | 273.77     |
| AverageEpisodeLength | 966.42     |
| MinEpisodeLength     | 278        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.87     |
| TotalNEpisodes       | 20960      |
| TotalNSamples        | 6.2212e+06 |
| ExplainedVariance    | -0.036415  |
-------------------------------------
[2018-12-22 12:14:58.728692 UTC] Saving snapshot
[2018-12-22 12:14:58.728944 UTC] Starting iteration 1244
[2018-12-22 12:14:58.729084 UTC] Start collecting samples
[2018-12-22 12:15:01.721173 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:01.800205 UTC] Performing policy update
[2018-12-22 12:15:01.800821 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:01.891217 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:02.954008 UTC] Performing line search
[2018-12-22 12:15:03.082652 UTC] Updating baseline
[2018-12-22 12:15:04.549365 UTC] Computing logging information
-------------------------------------
| Iteration            | 1244       |
| ExpectedImprovement  | 0.018165   |
| ActualImprovement    | 0.017249   |
| ImprovementRatio     | 0.94961    |
| MeanKL               | 0.0079185  |
| Entropy              | -1.2456    |
| Perplexity           | 0.28776    |
| AveragePolicyStd     | 0.19973    |
| AveragePolicyStd[0]  | 0.2204     |
| AveragePolicyStd[1]  | 0.19941    |
| AveragePolicyStd[2]  | 0.16047    |
| AveragePolicyStd[3]  | 0.20253    |
| AveragePolicyStd[4]  | 0.15543    |
| AveragePolicyStd[5]  | 0.26015    |
| AverageReturn        | 1795.6     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 325.64     |
| AverageEpisodeLength | 953.45     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.9      |
| TotalNEpisodes       | 20966      |
| TotalNSamples        | 6.2259e+06 |
| ExplainedVariance    | 0.2189     |
-------------------------------------
[2018-12-22 12:15:04.970379 UTC] Saving snapshot
[2018-12-22 12:15:04.970659 UTC] Starting iteration 1245
[2018-12-22 12:15:04.970795 UTC] Start collecting samples
[2018-12-22 12:15:07.967173 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:08.046859 UTC] Performing policy update
[2018-12-22 12:15:08.047440 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:08.137680 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:09.204047 UTC] Performing line search
[2018-12-22 12:15:09.333321 UTC] Updating baseline
[2018-12-22 12:15:10.674249 UTC] Computing logging information
-------------------------------------
| Iteration            | 1245       |
| ExpectedImprovement  | 0.018634   |
| ActualImprovement    | 0.017355   |
| ImprovementRatio     | 0.93139    |
| MeanKL               | 0.0074882  |
| Entropy              | -1.2451    |
| Perplexity           | 0.28792    |
| AveragePolicyStd     | 0.19973    |
| AveragePolicyStd[0]  | 0.22057    |
| AveragePolicyStd[1]  | 0.19894    |
| AveragePolicyStd[2]  | 0.16076    |
| AveragePolicyStd[3]  | 0.20296    |
| AveragePolicyStd[4]  | 0.1554     |
| AveragePolicyStd[5]  | 0.25975    |
| AverageReturn        | 1783.7     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 348.09     |
| AverageEpisodeLength | 946.8      |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.12     |
| TotalNEpisodes       | 20974      |
| TotalNSamples        | 6.2332e+06 |
| ExplainedVariance    | 0.1197     |
-------------------------------------
[2018-12-22 12:15:11.098048 UTC] Saving snapshot
[2018-12-22 12:15:11.098327 UTC] Starting iteration 1246
[2018-12-22 12:15:11.098444 UTC] Start collecting samples
[2018-12-22 12:15:14.039157 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:14.114062 UTC] Performing policy update
[2018-12-22 12:15:14.114660 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:14.205091 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:15.270793 UTC] Performing line search
[2018-12-22 12:15:15.400562 UTC] Updating baseline
[2018-12-22 12:15:16.921892 UTC] Computing logging information
-------------------------------------
| Iteration            | 1246       |
| ExpectedImprovement  | 0.018889   |
| ActualImprovement    | 0.020335   |
| ImprovementRatio     | 1.0765     |
| MeanKL               | 0.0075931  |
| Entropy              | -1.245     |
| Perplexity           | 0.28794    |
| AveragePolicyStd     | 0.19974    |
| AveragePolicyStd[0]  | 0.22109    |
| AveragePolicyStd[1]  | 0.19882    |
| AveragePolicyStd[2]  | 0.16055    |
| AveragePolicyStd[3]  | 0.20318    |
| AveragePolicyStd[4]  | 0.15531    |
| AveragePolicyStd[5]  | 0.25952    |
| AverageReturn        | 1785       |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 348.41     |
| AverageEpisodeLength | 946.8      |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.12     |
| TotalNEpisodes       | 20977      |
| TotalNSamples        | 6.2362e+06 |
| ExplainedVariance    | -0.2029    |
-------------------------------------
[2018-12-22 12:15:17.341486 UTC] Saving snapshot
[2018-12-22 12:15:17.341768 UTC] Starting iteration 1247
[2018-12-22 12:15:17.341895 UTC] Start collecting samples
[2018-12-22 12:15:20.310080 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:20.386624 UTC] Performing policy update
[2018-12-22 12:15:20.387219 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:20.477307 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:21.536629 UTC] Performing line search
[2018-12-22 12:15:21.664835 UTC] Updating baseline
[2018-12-22 12:15:23.434291 UTC] Computing logging information
-------------------------------------
| Iteration            | 1247       |
| ExpectedImprovement  | 0.015437   |
| ActualImprovement    | 0.014642   |
| ImprovementRatio     | 0.94855    |
| MeanKL               | 0.0082603  |
| Entropy              | -1.2523    |
| Perplexity           | 0.28586    |
| AveragePolicyStd     | 0.19952    |
| AveragePolicyStd[0]  | 0.22097    |
| AveragePolicyStd[1]  | 0.19909    |
| AveragePolicyStd[2]  | 0.16029    |
| AveragePolicyStd[3]  | 0.2033     |
| AveragePolicyStd[4]  | 0.1546     |
| AveragePolicyStd[5]  | 0.25887    |
| AverageReturn        | 1783.9     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 348.13     |
| AverageEpisodeLength | 946.8      |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.12     |
| TotalNEpisodes       | 20981      |
| TotalNSamples        | 6.2402e+06 |
| ExplainedVariance    | 0.0064735  |
-------------------------------------
[2018-12-22 12:15:23.858699 UTC] Saving snapshot
[2018-12-22 12:15:23.858945 UTC] Starting iteration 1248
[2018-12-22 12:15:23.859061 UTC] Start collecting samples
[2018-12-22 12:15:26.873377 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:26.954109 UTC] Performing policy update
[2018-12-22 12:15:26.954700 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:27.044334 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:28.115255 UTC] Performing line search
[2018-12-22 12:15:28.243486 UTC] Updating baseline
[2018-12-22 12:15:29.564972 UTC] Computing logging information
--------------------------------------
| Iteration            | 1248        |
| ExpectedImprovement  | 0.016728    |
| ActualImprovement    | 0.015565    |
| ImprovementRatio     | 0.93046     |
| MeanKL               | 0.0083717   |
| Entropy              | -1.2583     |
| Perplexity           | 0.28413     |
| AveragePolicyStd     | 0.19934     |
| AveragePolicyStd[0]  | 0.22076     |
| AveragePolicyStd[1]  | 0.19853     |
| AveragePolicyStd[2]  | 0.16016     |
| AveragePolicyStd[3]  | 0.20339     |
| AveragePolicyStd[4]  | 0.15425     |
| AveragePolicyStd[5]  | 0.25896     |
| AverageReturn        | 1784        |
| MinReturn            | 312.75      |
| MaxReturn            | 1958.3      |
| StdReturn            | 348.02      |
| AverageEpisodeLength | 947.59      |
| MinEpisodeLength     | 214         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 173.18      |
| TotalNEpisodes       | 20990       |
| TotalNSamples        | 6.2492e+06  |
| ExplainedVariance    | -0.00047155 |
--------------------------------------
[2018-12-22 12:15:29.993695 UTC] Saving snapshot
[2018-12-22 12:15:29.994212 UTC] Starting iteration 1249
[2018-12-22 12:15:29.994674 UTC] Start collecting samples
[2018-12-22 12:15:32.923988 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:32.999188 UTC] Performing policy update
[2018-12-22 12:15:32.999950 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:33.089676 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:34.150286 UTC] Performing line search
[2018-12-22 12:15:34.277418 UTC] Updating baseline
[2018-12-22 12:15:35.756055 UTC] Computing logging information
-------------------------------------
| Iteration            | 1249       |
| ExpectedImprovement  | 0.017508   |
| ActualImprovement    | 0.016055   |
| ImprovementRatio     | 0.91702    |
| MeanKL               | 0.0073301  |
| Entropy              | -1.2618    |
| Perplexity           | 0.28315    |
| AveragePolicyStd     | 0.19922    |
| AveragePolicyStd[0]  | 0.2207     |
| AveragePolicyStd[1]  | 0.19828    |
| AveragePolicyStd[2]  | 0.16037    |
| AveragePolicyStd[3]  | 0.20334    |
| AveragePolicyStd[4]  | 0.15395    |
| AveragePolicyStd[5]  | 0.2587     |
| AverageReturn        | 1783.9     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 348.02     |
| AverageEpisodeLength | 947.59     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.18     |
| TotalNEpisodes       | 20993      |
| TotalNSamples        | 6.2522e+06 |
| ExplainedVariance    | 0.012003   |
-------------------------------------
[2018-12-22 12:15:36.175033 UTC] Saving snapshot
[2018-12-22 12:15:36.175316 UTC] Starting iteration 1250
[2018-12-22 12:15:36.175436 UTC] Start collecting samples
[2018-12-22 12:15:39.107940 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:39.184562 UTC] Performing policy update
[2018-12-22 12:15:39.185242 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:39.276490 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:40.348937 UTC] Performing line search
[2018-12-22 12:15:40.476596 UTC] Updating baseline
[2018-12-22 12:15:41.983065 UTC] Computing logging information
-------------------------------------
| Iteration            | 1250       |
| ExpectedImprovement  | 0.018775   |
| ActualImprovement    | 0.017507   |
| ImprovementRatio     | 0.93247    |
| MeanKL               | 0.0080208  |
| Entropy              | -1.2649    |
| Perplexity           | 0.28226    |
| AveragePolicyStd     | 0.19907    |
| AveragePolicyStd[0]  | 0.22089    |
| AveragePolicyStd[1]  | 0.19855    |
| AveragePolicyStd[2]  | 0.1602     |
| AveragePolicyStd[3]  | 0.20324    |
| AveragePolicyStd[4]  | 0.15417    |
| AveragePolicyStd[5]  | 0.25734    |
| AverageReturn        | 1782.8     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 348.01     |
| AverageEpisodeLength | 947.59     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.18     |
| TotalNEpisodes       | 20995      |
| TotalNSamples        | 6.2542e+06 |
| ExplainedVariance    | -0.031594  |
-------------------------------------
[2018-12-22 12:15:42.398485 UTC] Saving snapshot
[2018-12-22 12:15:42.406705 UTC] Starting iteration 1251
[2018-12-22 12:15:42.406921 UTC] Start collecting samples
[2018-12-22 12:15:45.424782 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:45.503995 UTC] Performing policy update
[2018-12-22 12:15:45.504610 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:45.593102 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:46.644392 UTC] Performing line search
[2018-12-22 12:15:46.771736 UTC] Updating baseline
[2018-12-22 12:15:48.253021 UTC] Computing logging information
-------------------------------------
| Iteration            | 1251       |
| ExpectedImprovement  | 0.016464   |
| ActualImprovement    | 0.015487   |
| ImprovementRatio     | 0.9406     |
| MeanKL               | 0.0081584  |
| Entropy              | -1.2643    |
| Perplexity           | 0.28245    |
| AveragePolicyStd     | 0.19908    |
| AveragePolicyStd[0]  | 0.22074    |
| AveragePolicyStd[1]  | 0.19828    |
| AveragePolicyStd[2]  | 0.16058    |
| AveragePolicyStd[3]  | 0.20322    |
| AveragePolicyStd[4]  | 0.15409    |
| AveragePolicyStd[5]  | 0.25759    |
| AverageReturn        | 1799.6     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 318.48     |
| AverageEpisodeLength | 957.36     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.29     |
| TotalNEpisodes       | 21005      |
| TotalNSamples        | 6.2642e+06 |
| ExplainedVariance    | -0.003548  |
-------------------------------------
[2018-12-22 12:15:48.666023 UTC] Saving snapshot
[2018-12-22 12:15:48.666276 UTC] Starting iteration 1252
[2018-12-22 12:15:48.666397 UTC] Start collecting samples
[2018-12-22 12:15:51.601804 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:51.677645 UTC] Performing policy update
[2018-12-22 12:15:51.678332 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:51.771249 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:52.826029 UTC] Performing line search
[2018-12-22 12:15:52.953251 UTC] Updating baseline
[2018-12-22 12:15:54.783759 UTC] Computing logging information
-------------------------------------
| Iteration            | 1252       |
| ExpectedImprovement  | 0.017155   |
| ActualImprovement    | 0.016266   |
| ImprovementRatio     | 0.9482     |
| MeanKL               | 0.0087817  |
| Entropy              | -1.2618    |
| Perplexity           | 0.28313    |
| AveragePolicyStd     | 0.19921    |
| AveragePolicyStd[0]  | 0.22148    |
| AveragePolicyStd[1]  | 0.19805    |
| AveragePolicyStd[2]  | 0.16067    |
| AveragePolicyStd[3]  | 0.20333    |
| AveragePolicyStd[4]  | 0.15367    |
| AveragePolicyStd[5]  | 0.25807    |
| AverageReturn        | 1798.1     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 318.16     |
| AverageEpisodeLength | 957.36     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.29     |
| TotalNEpisodes       | 21009      |
| TotalNSamples        | 6.2682e+06 |
| ExplainedVariance    | 0.0042011  |
-------------------------------------
[2018-12-22 12:15:55.201059 UTC] Saving snapshot
[2018-12-22 12:15:55.201330 UTC] Starting iteration 1253
[2018-12-22 12:15:55.201447 UTC] Start collecting samples
[2018-12-22 12:15:58.101845 UTC] Computing input variables for policy optimization
[2018-12-22 12:15:58.176112 UTC] Performing policy update
[2018-12-22 12:15:58.176787 UTC] Computing gradient in Euclidean space
[2018-12-22 12:15:58.265493 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:15:59.314406 UTC] Performing line search
[2018-12-22 12:15:59.441590 UTC] Updating baseline
[2018-12-22 12:16:01.009518 UTC] Computing logging information
-------------------------------------
| Iteration            | 1253       |
| ExpectedImprovement  | 0.016035   |
| ActualImprovement    | 0.014813   |
| ImprovementRatio     | 0.92377    |
| MeanKL               | 0.0081156  |
| Entropy              | -1.2588    |
| Perplexity           | 0.28398    |
| AveragePolicyStd     | 0.19935    |
| AveragePolicyStd[0]  | 0.22192    |
| AveragePolicyStd[1]  | 0.19845    |
| AveragePolicyStd[2]  | 0.16035    |
| AveragePolicyStd[3]  | 0.20393    |
| AveragePolicyStd[4]  | 0.15331    |
| AveragePolicyStd[5]  | 0.25817    |
| AverageReturn        | 1797.1     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 317.91     |
| AverageEpisodeLength | 957.36     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.29     |
| TotalNEpisodes       | 21011      |
| TotalNSamples        | 6.2702e+06 |
| ExplainedVariance    | 0.000117   |
-------------------------------------
[2018-12-22 12:16:01.426036 UTC] Saving snapshot
[2018-12-22 12:16:01.426292 UTC] Starting iteration 1254
[2018-12-22 12:16:01.426415 UTC] Start collecting samples
[2018-12-22 12:16:04.408992 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:04.487371 UTC] Performing policy update
[2018-12-22 12:16:04.487952 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:04.577643 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:05.629374 UTC] Performing line search
[2018-12-22 12:16:05.759836 UTC] Updating baseline
[2018-12-22 12:16:07.867832 UTC] Computing logging information
-------------------------------------
| Iteration            | 1254       |
| ExpectedImprovement  | 0.017117   |
| ActualImprovement    | 0.016141   |
| ImprovementRatio     | 0.94297    |
| MeanKL               | 0.0081291  |
| Entropy              | -1.2597    |
| Perplexity           | 0.28375    |
| AveragePolicyStd     | 0.19929    |
| AveragePolicyStd[0]  | 0.22202    |
| AveragePolicyStd[1]  | 0.19794    |
| AveragePolicyStd[2]  | 0.16127    |
| AveragePolicyStd[3]  | 0.20395    |
| AveragePolicyStd[4]  | 0.15293    |
| AveragePolicyStd[5]  | 0.25765    |
| AverageReturn        | 1776.7     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 344.06     |
| AverageEpisodeLength | 950.11     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.17     |
| TotalNEpisodes       | 21019      |
| TotalNSamples        | 6.2775e+06 |
| ExplainedVariance    | 0.060131   |
-------------------------------------
[2018-12-22 12:16:08.289613 UTC] Saving snapshot
[2018-12-22 12:16:08.289890 UTC] Starting iteration 1255
[2018-12-22 12:16:08.290010 UTC] Start collecting samples
[2018-12-22 12:16:11.234617 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:11.314222 UTC] Performing policy update
[2018-12-22 12:16:11.314920 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:11.403905 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:12.471956 UTC] Performing line search
[2018-12-22 12:16:12.599612 UTC] Updating baseline
[2018-12-22 12:16:14.112192 UTC] Computing logging information
-------------------------------------
| Iteration            | 1255       |
| ExpectedImprovement  | 0.018933   |
| ActualImprovement    | 0.01717    |
| ImprovementRatio     | 0.90691    |
| MeanKL               | 0.0072428  |
| Entropy              | -1.2631    |
| Perplexity           | 0.28278    |
| AveragePolicyStd     | 0.19922    |
| AveragePolicyStd[0]  | 0.22238    |
| AveragePolicyStd[1]  | 0.19759    |
| AveragePolicyStd[2]  | 0.16111    |
| AveragePolicyStd[3]  | 0.20368    |
| AveragePolicyStd[4]  | 0.1526     |
| AveragePolicyStd[5]  | 0.25795    |
| AverageReturn        | 1771.4     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 342.64     |
| AverageEpisodeLength | 950.11     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.17     |
| TotalNEpisodes       | 21025      |
| TotalNSamples        | 6.2835e+06 |
| ExplainedVariance    | -0.011202  |
-------------------------------------
[2018-12-22 12:16:14.527784 UTC] Saving snapshot
[2018-12-22 12:16:14.528031 UTC] Starting iteration 1256
[2018-12-22 12:16:14.528147 UTC] Start collecting samples
[2018-12-22 12:16:17.441816 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:17.515146 UTC] Performing policy update
[2018-12-22 12:16:17.516088 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:17.605148 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:18.676548 UTC] Performing line search
[2018-12-22 12:16:18.805137 UTC] Updating baseline
[2018-12-22 12:16:20.216298 UTC] Computing logging information
-------------------------------------
| Iteration            | 1256       |
| ExpectedImprovement  | 0.020476   |
| ActualImprovement    | 0.017937   |
| ImprovementRatio     | 0.87601    |
| MeanKL               | 0.0075141  |
| Entropy              | -1.2658    |
| Perplexity           | 0.28201    |
| AveragePolicyStd     | 0.19919    |
| AveragePolicyStd[0]  | 0.22188    |
| AveragePolicyStd[1]  | 0.19773    |
| AveragePolicyStd[2]  | 0.16039    |
| AveragePolicyStd[3]  | 0.20426    |
| AveragePolicyStd[4]  | 0.15231    |
| AveragePolicyStd[5]  | 0.25855    |
| AverageReturn        | 1773.4     |
| MinReturn            | 312.75     |
| MaxReturn            | 1958.3     |
| StdReturn            | 341.52     |
| AverageEpisodeLength | 951.53     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.99     |
| TotalNEpisodes       | 21027      |
| TotalNSamples        | 6.2855e+06 |
| ExplainedVariance    | 0.0089281  |
-------------------------------------
[2018-12-22 12:16:20.637623 UTC] Saving snapshot
[2018-12-22 12:16:20.637898 UTC] Starting iteration 1257
[2018-12-22 12:16:20.638013 UTC] Start collecting samples
[2018-12-22 12:16:23.614315 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:23.692311 UTC] Performing policy update
[2018-12-22 12:16:23.693192 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:23.786864 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:24.841936 UTC] Performing line search
[2018-12-22 12:16:24.969351 UTC] Updating baseline
[2018-12-22 12:16:26.538547 UTC] Computing logging information
-------------------------------------
| Iteration            | 1257       |
| ExpectedImprovement  | 0.018308   |
| ActualImprovement    | 0.016881   |
| ImprovementRatio     | 0.92206    |
| MeanKL               | 0.0077382  |
| Entropy              | -1.2743    |
| Perplexity           | 0.27962    |
| AveragePolicyStd     | 0.19883    |
| AveragePolicyStd[0]  | 0.22168    |
| AveragePolicyStd[1]  | 0.19722    |
| AveragePolicyStd[2]  | 0.16079    |
| AveragePolicyStd[3]  | 0.20387    |
| AveragePolicyStd[4]  | 0.15228    |
| AveragePolicyStd[5]  | 0.25716    |
| AverageReturn        | 1770.2     |
| MinReturn            | 312.75     |
| MaxReturn            | 1955.2     |
| StdReturn            | 341.02     |
| AverageEpisodeLength | 951.74     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.95     |
| TotalNEpisodes       | 21034      |
| TotalNSamples        | 6.2924e+06 |
| ExplainedVariance    | 0.067534   |
-------------------------------------
[2018-12-22 12:16:26.954572 UTC] Saving snapshot
[2018-12-22 12:16:26.954818 UTC] Starting iteration 1258
[2018-12-22 12:16:26.954940 UTC] Start collecting samples
[2018-12-22 12:16:29.903055 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:29.986204 UTC] Performing policy update
[2018-12-22 12:16:29.986928 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:30.075614 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:31.129332 UTC] Performing line search
[2018-12-22 12:16:31.257285 UTC] Updating baseline
[2018-12-22 12:16:32.423710 UTC] Computing logging information
-------------------------------------
| Iteration            | 1258       |
| ExpectedImprovement  | 0.016978   |
| ActualImprovement    | 0.01645    |
| ImprovementRatio     | 0.96894    |
| MeanKL               | 0.0078303  |
| Entropy              | -1.2814    |
| Perplexity           | 0.27766    |
| AveragePolicyStd     | 0.1986     |
| AveragePolicyStd[0]  | 0.22096    |
| AveragePolicyStd[1]  | 0.19725    |
| AveragePolicyStd[2]  | 0.16021    |
| AveragePolicyStd[3]  | 0.20415    |
| AveragePolicyStd[4]  | 0.1523     |
| AveragePolicyStd[5]  | 0.25671    |
| AverageReturn        | 1779.4     |
| MinReturn            | 312.75     |
| MaxReturn            | 1955.2     |
| StdReturn            | 323.85     |
| AverageEpisodeLength | 957.55     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.46     |
| TotalNEpisodes       | 21041      |
| TotalNSamples        | 6.2994e+06 |
| ExplainedVariance    | -0.0080941 |
-------------------------------------
[2018-12-22 12:16:32.838661 UTC] Saving snapshot
[2018-12-22 12:16:32.838920 UTC] Starting iteration 1259
[2018-12-22 12:16:32.839037 UTC] Start collecting samples
[2018-12-22 12:16:35.759479 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:35.834849 UTC] Performing policy update
[2018-12-22 12:16:35.835488 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:35.928853 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:36.993018 UTC] Performing line search
[2018-12-22 12:16:37.120427 UTC] Updating baseline
[2018-12-22 12:16:38.359305 UTC] Computing logging information
-------------------------------------
| Iteration            | 1259       |
| ExpectedImprovement  | 0.018582   |
| ActualImprovement    | 0.017576   |
| ImprovementRatio     | 0.94587    |
| MeanKL               | 0.0082702  |
| Entropy              | -1.2862    |
| Perplexity           | 0.27631    |
| AveragePolicyStd     | 0.19842    |
| AveragePolicyStd[0]  | 0.22139    |
| AveragePolicyStd[1]  | 0.1969     |
| AveragePolicyStd[2]  | 0.16007    |
| AveragePolicyStd[3]  | 0.20356    |
| AveragePolicyStd[4]  | 0.15238    |
| AveragePolicyStd[5]  | 0.25623    |
| AverageReturn        | 1753.2     |
| MinReturn            | 312.75     |
| MaxReturn            | 1955.2     |
| StdReturn            | 364.06     |
| AverageEpisodeLength | 944.77     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.85     |
| TotalNEpisodes       | 21044      |
| TotalNSamples        | 6.3012e+06 |
| ExplainedVariance    | 0.43704    |
-------------------------------------
[2018-12-22 12:16:38.777853 UTC] Saving snapshot
[2018-12-22 12:16:38.778170 UTC] Starting iteration 1260
[2018-12-22 12:16:38.778319 UTC] Start collecting samples
[2018-12-22 12:16:41.739718 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:41.822721 UTC] Performing policy update
[2018-12-22 12:16:41.823592 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:41.914448 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:42.972217 UTC] Performing line search
[2018-12-22 12:16:43.099029 UTC] Updating baseline
[2018-12-22 12:16:44.354247 UTC] Computing logging information
-------------------------------------
| Iteration            | 1260       |
| ExpectedImprovement  | 0.017946   |
| ActualImprovement    | 0.01772    |
| ImprovementRatio     | 0.98741    |
| MeanKL               | 0.007983   |
| Entropy              | -1.2897    |
| Perplexity           | 0.27535    |
| AveragePolicyStd     | 0.19829    |
| AveragePolicyStd[0]  | 0.2211     |
| AveragePolicyStd[1]  | 0.19658    |
| AveragePolicyStd[2]  | 0.16027    |
| AveragePolicyStd[3]  | 0.20332    |
| AveragePolicyStd[4]  | 0.1524     |
| AveragePolicyStd[5]  | 0.25604    |
| AverageReturn        | 1778.2     |
| MinReturn            | 312.75     |
| MaxReturn            | 1955.2     |
| StdReturn            | 314.74     |
| AverageEpisodeLength | 958.87     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.8      |
| TotalNEpisodes       | 21049      |
| TotalNSamples        | 6.3062e+06 |
| ExplainedVariance    | -0.081654  |
-------------------------------------
[2018-12-22 12:16:44.766600 UTC] Saving snapshot
[2018-12-22 12:16:44.774942 UTC] Starting iteration 1261
[2018-12-22 12:16:44.775157 UTC] Start collecting samples
[2018-12-22 12:16:47.733933 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:47.812382 UTC] Performing policy update
[2018-12-22 12:16:47.813198 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:47.903583 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:48.965044 UTC] Performing line search
[2018-12-22 12:16:49.092412 UTC] Updating baseline
[2018-12-22 12:16:50.849771 UTC] Computing logging information
-------------------------------------
| Iteration            | 1261       |
| ExpectedImprovement  | 0.01682    |
| ActualImprovement    | 0.015993   |
| ImprovementRatio     | 0.95086    |
| MeanKL               | 0.0074618  |
| Entropy              | -1.2956    |
| Perplexity           | 0.27374    |
| AveragePolicyStd     | 0.19806    |
| AveragePolicyStd[0]  | 0.22076    |
| AveragePolicyStd[1]  | 0.19588    |
| AveragePolicyStd[2]  | 0.15994    |
| AveragePolicyStd[3]  | 0.20345    |
| AveragePolicyStd[4]  | 0.15279    |
| AveragePolicyStd[5]  | 0.25556    |
| AverageReturn        | 1773.1     |
| MinReturn            | 312.75     |
| MaxReturn            | 1926.3     |
| StdReturn            | 313.12     |
| AverageEpisodeLength | 959.63     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.81     |
| TotalNEpisodes       | 21056      |
| TotalNSamples        | 6.3132e+06 |
| ExplainedVariance    | 8.1858e-05 |
-------------------------------------
[2018-12-22 12:16:51.273426 UTC] Saving snapshot
[2018-12-22 12:16:51.273687 UTC] Starting iteration 1262
[2018-12-22 12:16:51.273837 UTC] Start collecting samples
[2018-12-22 12:16:54.200972 UTC] Computing input variables for policy optimization
[2018-12-22 12:16:54.276774 UTC] Performing policy update
[2018-12-22 12:16:54.277583 UTC] Computing gradient in Euclidean space
[2018-12-22 12:16:54.366939 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:16:55.426170 UTC] Performing line search
[2018-12-22 12:16:55.553801 UTC] Updating baseline
[2018-12-22 12:16:57.404283 UTC] Computing logging information
-------------------------------------
| Iteration            | 1262       |
| ExpectedImprovement  | 0.014813   |
| ActualImprovement    | 0.01408    |
| ImprovementRatio     | 0.95049    |
| MeanKL               | 0.0080485  |
| Entropy              | -1.3059    |
| Perplexity           | 0.27092    |
| AveragePolicyStd     | 0.19775    |
| AveragePolicyStd[0]  | 0.22121    |
| AveragePolicyStd[1]  | 0.19512    |
| AveragePolicyStd[2]  | 0.1594     |
| AveragePolicyStd[3]  | 0.20323    |
| AveragePolicyStd[4]  | 0.15248    |
| AveragePolicyStd[5]  | 0.25505    |
| AverageReturn        | 1769.8     |
| MinReturn            | 312.75     |
| MaxReturn            | 1926.3     |
| StdReturn            | 312.41     |
| AverageEpisodeLength | 959.63     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.81     |
| TotalNEpisodes       | 21060      |
| TotalNSamples        | 6.3172e+06 |
| ExplainedVariance    | 0.0057123  |
-------------------------------------
[2018-12-22 12:16:57.823251 UTC] Saving snapshot
[2018-12-22 12:16:57.823528 UTC] Starting iteration 1263
[2018-12-22 12:16:57.823651 UTC] Start collecting samples
[2018-12-22 12:17:00.741603 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:00.817846 UTC] Performing policy update
[2018-12-22 12:17:00.818647 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:00.907362 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:01.973268 UTC] Performing line search
[2018-12-22 12:17:02.101959 UTC] Updating baseline
[2018-12-22 12:17:03.591057 UTC] Computing logging information
-------------------------------------
| Iteration            | 1263       |
| ExpectedImprovement  | 0.016921   |
| ActualImprovement    | 0.015739   |
| ImprovementRatio     | 0.93017    |
| MeanKL               | 0.0077826  |
| Entropy              | -1.3124    |
| Perplexity           | 0.26917    |
| AveragePolicyStd     | 0.19754    |
| AveragePolicyStd[0]  | 0.22091    |
| AveragePolicyStd[1]  | 0.19435    |
| AveragePolicyStd[2]  | 0.15966    |
| AveragePolicyStd[3]  | 0.2029     |
| AveragePolicyStd[4]  | 0.15224    |
| AveragePolicyStd[5]  | 0.25515    |
| AverageReturn        | 1792       |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 259.99     |
| AverageEpisodeLength | 972.6      |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.57     |
| TotalNEpisodes       | 21064      |
| TotalNSamples        | 6.3212e+06 |
| ExplainedVariance    | 0.0017813  |
-------------------------------------
[2018-12-22 12:17:04.012737 UTC] Saving snapshot
[2018-12-22 12:17:04.013001 UTC] Starting iteration 1264
[2018-12-22 12:17:04.013118 UTC] Start collecting samples
[2018-12-22 12:17:06.996068 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:07.074195 UTC] Performing policy update
[2018-12-22 12:17:07.074848 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:07.163202 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:08.214781 UTC] Performing line search
[2018-12-22 12:17:08.342620 UTC] Updating baseline
[2018-12-22 12:17:09.580363 UTC] Computing logging information
-------------------------------------
| Iteration            | 1264       |
| ExpectedImprovement  | 0.01798    |
| ActualImprovement    | 0.017025   |
| ImprovementRatio     | 0.94689    |
| MeanKL               | 0.0082937  |
| Entropy              | -1.3173    |
| Perplexity           | 0.26786    |
| AveragePolicyStd     | 0.19734    |
| AveragePolicyStd[0]  | 0.22026    |
| AveragePolicyStd[1]  | 0.19437    |
| AveragePolicyStd[2]  | 0.15961    |
| AveragePolicyStd[3]  | 0.20267    |
| AveragePolicyStd[4]  | 0.15242    |
| AveragePolicyStd[5]  | 0.2547     |
| AverageReturn        | 1797.9     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 228.07     |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21071      |
| TotalNSamples        | 6.3282e+06 |
| ExplainedVariance    | 0.00028863 |
-------------------------------------
[2018-12-22 12:17:10.004721 UTC] Saving snapshot
[2018-12-22 12:17:10.005015 UTC] Starting iteration 1265
[2018-12-22 12:17:10.005134 UTC] Start collecting samples
[2018-12-22 12:17:12.944360 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:13.019537 UTC] Performing policy update
[2018-12-22 12:17:13.020339 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:13.108539 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:14.171731 UTC] Performing line search
[2018-12-22 12:17:14.298593 UTC] Updating baseline
[2018-12-22 12:17:16.385209 UTC] Computing logging information
-------------------------------------
| Iteration            | 1265       |
| ExpectedImprovement  | 0.017927   |
| ActualImprovement    | 0.016545   |
| ImprovementRatio     | 0.92289    |
| MeanKL               | 0.0075176  |
| Entropy              | -1.3149    |
| Perplexity           | 0.26851    |
| AveragePolicyStd     | 0.19744    |
| AveragePolicyStd[0]  | 0.2199     |
| AveragePolicyStd[1]  | 0.19465    |
| AveragePolicyStd[2]  | 0.15961    |
| AveragePolicyStd[3]  | 0.20287    |
| AveragePolicyStd[4]  | 0.15235    |
| AveragePolicyStd[5]  | 0.25524    |
| AverageReturn        | 1791.9     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 226.98     |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21076      |
| TotalNSamples        | 6.3332e+06 |
| ExplainedVariance    | 0.0027426  |
-------------------------------------
[2018-12-22 12:17:16.808305 UTC] Saving snapshot
[2018-12-22 12:17:16.808595 UTC] Starting iteration 1266
[2018-12-22 12:17:16.808714 UTC] Start collecting samples
[2018-12-22 12:17:19.715763 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:19.793370 UTC] Performing policy update
[2018-12-22 12:17:19.794210 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:19.884355 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:20.934515 UTC] Performing line search
[2018-12-22 12:17:21.060347 UTC] Updating baseline
[2018-12-22 12:17:22.471761 UTC] Computing logging information
-------------------------------------
| Iteration            | 1266       |
| ExpectedImprovement  | 0.017652   |
| ActualImprovement    | 0.015561   |
| ImprovementRatio     | 0.88154    |
| MeanKL               | 0.0075958  |
| Entropy              | -1.3165    |
| Perplexity           | 0.26808    |
| AveragePolicyStd     | 0.19739    |
| AveragePolicyStd[0]  | 0.2198     |
| AveragePolicyStd[1]  | 0.19428    |
| AveragePolicyStd[2]  | 0.15943    |
| AveragePolicyStd[3]  | 0.20254    |
| AveragePolicyStd[4]  | 0.15268    |
| AveragePolicyStd[5]  | 0.2556     |
| AverageReturn        | 1789.7     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 226.7      |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21078      |
| TotalNSamples        | 6.3352e+06 |
| ExplainedVariance    | 0.0020302  |
-------------------------------------
[2018-12-22 12:17:22.889656 UTC] Saving snapshot
[2018-12-22 12:17:22.889927 UTC] Starting iteration 1267
[2018-12-22 12:17:22.890046 UTC] Start collecting samples
[2018-12-22 12:17:25.890677 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:25.972591 UTC] Performing policy update
[2018-12-22 12:17:25.973170 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:26.061376 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:27.111067 UTC] Performing line search
[2018-12-22 12:17:27.238031 UTC] Updating baseline
[2018-12-22 12:17:28.723633 UTC] Computing logging information
-------------------------------------
| Iteration            | 1267       |
| ExpectedImprovement  | 0.016443   |
| ActualImprovement    | 0.015655   |
| ImprovementRatio     | 0.95209    |
| MeanKL               | 0.0084064  |
| Entropy              | -1.3198    |
| Perplexity           | 0.26718    |
| AveragePolicyStd     | 0.19722    |
| AveragePolicyStd[0]  | 0.22019    |
| AveragePolicyStd[1]  | 0.19389    |
| AveragePolicyStd[2]  | 0.15976    |
| AveragePolicyStd[3]  | 0.20187    |
| AveragePolicyStd[4]  | 0.1529     |
| AveragePolicyStd[5]  | 0.25474    |
| AverageReturn        | 1784.3     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 225.9      |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21087      |
| TotalNSamples        | 6.3442e+06 |
| ExplainedVariance    | -0.0019163 |
-------------------------------------
[2018-12-22 12:17:29.141527 UTC] Saving snapshot
[2018-12-22 12:17:29.141804 UTC] Starting iteration 1268
[2018-12-22 12:17:29.141928 UTC] Start collecting samples
[2018-12-22 12:17:32.071177 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:32.146085 UTC] Performing policy update
[2018-12-22 12:17:32.146959 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:32.236519 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:33.292098 UTC] Performing line search
[2018-12-22 12:17:33.418789 UTC] Updating baseline
[2018-12-22 12:17:34.843704 UTC] Computing logging information
-------------------------------------
| Iteration            | 1268       |
| ExpectedImprovement  | 0.016098   |
| ActualImprovement    | 0.015226   |
| ImprovementRatio     | 0.94582    |
| MeanKL               | 0.0088491  |
| Entropy              | -1.3154    |
| Perplexity           | 0.26836    |
| AveragePolicyStd     | 0.19737    |
| AveragePolicyStd[0]  | 0.21984    |
| AveragePolicyStd[1]  | 0.19452    |
| AveragePolicyStd[2]  | 0.15996    |
| AveragePolicyStd[3]  | 0.20204    |
| AveragePolicyStd[4]  | 0.15284    |
| AveragePolicyStd[5]  | 0.255      |
| AverageReturn        | 1780.9     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 225.01     |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21091      |
| TotalNSamples        | 6.3482e+06 |
| ExplainedVariance    | 0.0025041  |
-------------------------------------
[2018-12-22 12:17:35.266277 UTC] Saving snapshot
[2018-12-22 12:17:35.266551 UTC] Starting iteration 1269
[2018-12-22 12:17:35.266676 UTC] Start collecting samples
[2018-12-22 12:17:38.191097 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:38.264840 UTC] Performing policy update
[2018-12-22 12:17:38.265424 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:38.355318 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:39.422711 UTC] Performing line search
[2018-12-22 12:17:39.551420 UTC] Updating baseline
[2018-12-22 12:17:41.139433 UTC] Computing logging information
-------------------------------------
| Iteration            | 1269       |
| ExpectedImprovement  | 0.016878   |
| ActualImprovement    | 0.015849   |
| ImprovementRatio     | 0.93903    |
| MeanKL               | 0.0087777  |
| Entropy              | -1.3096    |
| Perplexity           | 0.26992    |
| AveragePolicyStd     | 0.1976     |
| AveragePolicyStd[0]  | 0.22005    |
| AveragePolicyStd[1]  | 0.19476    |
| AveragePolicyStd[2]  | 0.16003    |
| AveragePolicyStd[3]  | 0.20269    |
| AveragePolicyStd[4]  | 0.15251    |
| AveragePolicyStd[5]  | 0.25554    |
| AverageReturn        | 1778.9     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 224.49     |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21093      |
| TotalNSamples        | 6.3502e+06 |
| ExplainedVariance    | 0.0064366  |
-------------------------------------
[2018-12-22 12:17:41.566197 UTC] Saving snapshot
[2018-12-22 12:17:41.566458 UTC] Starting iteration 1270
[2018-12-22 12:17:41.566595 UTC] Start collecting samples
[2018-12-22 12:17:44.540902 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:44.618827 UTC] Performing policy update
[2018-12-22 12:17:44.619396 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:44.709920 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:45.771141 UTC] Performing line search
[2018-12-22 12:17:45.899643 UTC] Updating baseline
[2018-12-22 12:17:47.388595 UTC] Computing logging information
-------------------------------------
| Iteration            | 1270       |
| ExpectedImprovement  | 0.017677   |
| ActualImprovement    | 0.016962   |
| ImprovementRatio     | 0.95957    |
| MeanKL               | 0.0083135  |
| Entropy              | -1.3062    |
| Perplexity           | 0.27085    |
| AveragePolicyStd     | 0.19771    |
| AveragePolicyStd[0]  | 0.21983    |
| AveragePolicyStd[1]  | 0.19465    |
| AveragePolicyStd[2]  | 0.16046    |
| AveragePolicyStd[3]  | 0.20301    |
| AveragePolicyStd[4]  | 0.15244    |
| AveragePolicyStd[5]  | 0.25584    |
| AverageReturn        | 1773.9     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 223.21     |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21100      |
| TotalNSamples        | 6.3572e+06 |
| ExplainedVariance    | 0.0010711  |
-------------------------------------
[2018-12-22 12:17:47.814353 UTC] Saving snapshot
[2018-12-22 12:17:47.822497 UTC] Starting iteration 1271
[2018-12-22 12:17:47.822710 UTC] Start collecting samples
[2018-12-22 12:17:50.757166 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:50.832635 UTC] Performing policy update
[2018-12-22 12:17:50.833243 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:50.921872 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:51.981797 UTC] Performing line search
[2018-12-22 12:17:52.110494 UTC] Updating baseline
[2018-12-22 12:17:54.282878 UTC] Computing logging information
-------------------------------------
| Iteration            | 1271       |
| ExpectedImprovement  | 0.017523   |
| ActualImprovement    | 0.01662    |
| ImprovementRatio     | 0.94848    |
| MeanKL               | 0.0079027  |
| Entropy              | -1.3035    |
| Perplexity           | 0.27157    |
| AveragePolicyStd     | 0.19777    |
| AveragePolicyStd[0]  | 0.21955    |
| AveragePolicyStd[1]  | 0.19461    |
| AveragePolicyStd[2]  | 0.16119    |
| AveragePolicyStd[3]  | 0.20326    |
| AveragePolicyStd[4]  | 0.15224    |
| AveragePolicyStd[5]  | 0.25576    |
| AverageReturn        | 1771.6     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 222.55     |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21105      |
| TotalNSamples        | 6.3622e+06 |
| ExplainedVariance    | 0.0046478  |
-------------------------------------
[2018-12-22 12:17:54.703544 UTC] Saving snapshot
[2018-12-22 12:17:54.703788 UTC] Starting iteration 1272
[2018-12-22 12:17:54.703912 UTC] Start collecting samples
[2018-12-22 12:17:57.639677 UTC] Computing input variables for policy optimization
[2018-12-22 12:17:57.714208 UTC] Performing policy update
[2018-12-22 12:17:57.714819 UTC] Computing gradient in Euclidean space
[2018-12-22 12:17:57.806382 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:17:58.848956 UTC] Performing line search
[2018-12-22 12:17:58.976056 UTC] Updating baseline
[2018-12-22 12:18:00.623110 UTC] Computing logging information
-------------------------------------
| Iteration            | 1272       |
| ExpectedImprovement  | 0.01794    |
| ActualImprovement    | 0.01676    |
| ImprovementRatio     | 0.93418    |
| MeanKL               | 0.0076183  |
| Entropy              | -1.2974    |
| Perplexity           | 0.27325    |
| AveragePolicyStd     | 0.198      |
| AveragePolicyStd[0]  | 0.21954    |
| AveragePolicyStd[1]  | 0.19475    |
| AveragePolicyStd[2]  | 0.1617     |
| AveragePolicyStd[3]  | 0.20307    |
| AveragePolicyStd[4]  | 0.15209    |
| AveragePolicyStd[5]  | 0.25686    |
| AverageReturn        | 1769.8     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 222.24     |
| AverageEpisodeLength | 979.25     |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.93     |
| TotalNEpisodes       | 21109      |
| TotalNSamples        | 6.3662e+06 |
| ExplainedVariance    | 0.010145   |
-------------------------------------
[2018-12-22 12:18:01.039825 UTC] Saving snapshot
[2018-12-22 12:18:01.040076 UTC] Starting iteration 1273
[2018-12-22 12:18:01.040196 UTC] Start collecting samples
[2018-12-22 12:18:04.014107 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:04.091131 UTC] Performing policy update
[2018-12-22 12:18:04.091994 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:04.182678 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:05.239739 UTC] Performing line search
[2018-12-22 12:18:05.366322 UTC] Updating baseline
[2018-12-22 12:18:06.855537 UTC] Computing logging information
-------------------------------------
| Iteration            | 1273       |
| ExpectedImprovement  | 0.017462   |
| ActualImprovement    | 0.01657    |
| ImprovementRatio     | 0.94895    |
| MeanKL               | 0.0080456  |
| Entropy              | -1.3042    |
| Perplexity           | 0.2714     |
| AveragePolicyStd     | 0.19776    |
| AveragePolicyStd[0]  | 0.21991    |
| AveragePolicyStd[1]  | 0.19426    |
| AveragePolicyStd[2]  | 0.1613     |
| AveragePolicyStd[3]  | 0.20329    |
| AveragePolicyStd[4]  | 0.15207    |
| AveragePolicyStd[5]  | 0.25572    |
| AverageReturn        | 1781.2     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 176.24     |
| AverageEpisodeLength | 986.5      |
| MinEpisodeLength     | 270        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 90.563     |
| TotalNEpisodes       | 21116      |
| TotalNSamples        | 6.3732e+06 |
| ExplainedVariance    | -0.007156  |
-------------------------------------
[2018-12-22 12:18:07.279228 UTC] Saving snapshot
[2018-12-22 12:18:07.279528 UTC] Starting iteration 1274
[2018-12-22 12:18:07.279648 UTC] Start collecting samples
[2018-12-22 12:18:10.252992 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:10.329493 UTC] Performing policy update
[2018-12-22 12:18:10.330211 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:10.418911 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:11.464252 UTC] Performing line search
[2018-12-22 12:18:11.592203 UTC] Updating baseline
[2018-12-22 12:18:13.163109 UTC] Computing logging information
-------------------------------------
| Iteration            | 1274       |
| ExpectedImprovement  | 0.017991   |
| ActualImprovement    | 0.016807   |
| ImprovementRatio     | 0.93418    |
| MeanKL               | 0.0076633  |
| Entropy              | -1.3102    |
| Perplexity           | 0.26977    |
| AveragePolicyStd     | 0.19753    |
| AveragePolicyStd[0]  | 0.2191     |
| AveragePolicyStd[1]  | 0.1939     |
| AveragePolicyStd[2]  | 0.16124    |
| AveragePolicyStd[3]  | 0.20383    |
| AveragePolicyStd[4]  | 0.15206    |
| AveragePolicyStd[5]  | 0.25502    |
| AverageReturn        | 1761.1     |
| MinReturn            | 420.79     |
| MaxReturn            | 1926.3     |
| StdReturn            | 228.01     |
| AverageEpisodeLength | 976.02     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.41     |
| TotalNEpisodes       | 21123      |
| TotalNSamples        | 6.3791e+06 |
| ExplainedVariance    | 0.15559    |
-------------------------------------
[2018-12-22 12:18:13.584833 UTC] Saving snapshot
[2018-12-22 12:18:13.585077 UTC] Starting iteration 1275
[2018-12-22 12:18:13.585200 UTC] Start collecting samples
[2018-12-22 12:18:16.496429 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:16.572517 UTC] Performing policy update
[2018-12-22 12:18:16.573165 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:16.662301 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:17.719995 UTC] Performing line search
[2018-12-22 12:18:17.850378 UTC] Updating baseline
[2018-12-22 12:18:19.783258 UTC] Computing logging information
-------------------------------------
| Iteration            | 1275       |
| ExpectedImprovement  | 0.018691   |
| ActualImprovement    | 0.016228   |
| ImprovementRatio     | 0.86824    |
| MeanKL               | 0.008794   |
| Entropy              | -1.3114    |
| Perplexity           | 0.26944    |
| AveragePolicyStd     | 0.19748    |
| AveragePolicyStd[0]  | 0.21932    |
| AveragePolicyStd[1]  | 0.19412    |
| AveragePolicyStd[2]  | 0.16095    |
| AveragePolicyStd[3]  | 0.20411    |
| AveragePolicyStd[4]  | 0.15203    |
| AveragePolicyStd[5]  | 0.25435    |
| AverageReturn        | 1760.4     |
| MinReturn            | 420.79     |
| MaxReturn            | 1891.1     |
| StdReturn            | 227.57     |
| AverageEpisodeLength | 976.02     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.41     |
| TotalNEpisodes       | 21125      |
| TotalNSamples        | 6.3811e+06 |
| ExplainedVariance    | -0.011036  |
-------------------------------------
[2018-12-22 12:18:20.199559 UTC] Saving snapshot
[2018-12-22 12:18:20.199819 UTC] Starting iteration 1276
[2018-12-22 12:18:20.199938 UTC] Start collecting samples
[2018-12-22 12:18:23.138420 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:23.214029 UTC] Performing policy update
[2018-12-22 12:18:23.214854 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:23.303597 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:24.364928 UTC] Performing line search
[2018-12-22 12:18:24.491204 UTC] Updating baseline
[2018-12-22 12:18:25.714580 UTC] Computing logging information
-------------------------------------
| Iteration            | 1276       |
| ExpectedImprovement  | 0.015269   |
| ActualImprovement    | 0.014652   |
| ImprovementRatio     | 0.95962    |
| MeanKL               | 0.0083395  |
| Entropy              | -1.3114    |
| Perplexity           | 0.26944    |
| AveragePolicyStd     | 0.19751    |
| AveragePolicyStd[0]  | 0.2194     |
| AveragePolicyStd[1]  | 0.19401    |
| AveragePolicyStd[2]  | 0.16077    |
| AveragePolicyStd[3]  | 0.20443    |
| AveragePolicyStd[4]  | 0.15183    |
| AveragePolicyStd[5]  | 0.25459    |
| AverageReturn        | 1758.6     |
| MinReturn            | 420.79     |
| MaxReturn            | 1891.1     |
| StdReturn            | 227.18     |
| AverageEpisodeLength | 976.02     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.41     |
| TotalNEpisodes       | 21130      |
| TotalNSamples        | 6.3861e+06 |
| ExplainedVariance    | 0.0006339  |
-------------------------------------
[2018-12-22 12:18:26.142215 UTC] Saving snapshot
[2018-12-22 12:18:26.142464 UTC] Starting iteration 1277
[2018-12-22 12:18:26.142606 UTC] Start collecting samples
[2018-12-22 12:18:29.153700 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:29.231815 UTC] Performing policy update
[2018-12-22 12:18:29.232489 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:29.322623 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:30.392973 UTC] Performing line search
[2018-12-22 12:18:30.520476 UTC] Updating baseline
[2018-12-22 12:18:32.286132 UTC] Computing logging information
-------------------------------------
| Iteration            | 1277       |
| ExpectedImprovement  | 0.01846    |
| ActualImprovement    | 0.017241   |
| ImprovementRatio     | 0.93395    |
| MeanKL               | 0.0081186  |
| Entropy              | -1.3082    |
| Perplexity           | 0.27031    |
| AveragePolicyStd     | 0.1976     |
| AveragePolicyStd[0]  | 0.22034    |
| AveragePolicyStd[1]  | 0.19466    |
| AveragePolicyStd[2]  | 0.16096    |
| AveragePolicyStd[3]  | 0.20441    |
| AveragePolicyStd[4]  | 0.15153    |
| AveragePolicyStd[5]  | 0.25372    |
| AverageReturn        | 1754.5     |
| MinReturn            | 420.79     |
| MaxReturn            | 1887.6     |
| StdReturn            | 225.58     |
| AverageEpisodeLength | 976.74     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.34     |
| TotalNEpisodes       | 21139      |
| TotalNSamples        | 6.3951e+06 |
| ExplainedVariance    | -0.0010075 |
-------------------------------------
[2018-12-22 12:18:32.713137 UTC] Saving snapshot
[2018-12-22 12:18:32.713424 UTC] Starting iteration 1278
[2018-12-22 12:18:32.713555 UTC] Start collecting samples
[2018-12-22 12:18:35.603986 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:35.676573 UTC] Performing policy update
[2018-12-22 12:18:35.677260 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:35.771166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:36.825527 UTC] Performing line search
[2018-12-22 12:18:36.957124 UTC] Updating baseline
[2018-12-22 12:18:38.766173 UTC] Computing logging information
-------------------------------------
| Iteration            | 1278       |
| ExpectedImprovement  | 0.016031   |
| ActualImprovement    | 0.014596   |
| ImprovementRatio     | 0.91046    |
| MeanKL               | 0.0090709  |
| Entropy              | -1.3142    |
| Perplexity           | 0.26868    |
| AveragePolicyStd     | 0.19742    |
| AveragePolicyStd[0]  | 0.22081    |
| AveragePolicyStd[1]  | 0.1944     |
| AveragePolicyStd[2]  | 0.16071    |
| AveragePolicyStd[3]  | 0.20379    |
| AveragePolicyStd[4]  | 0.15136    |
| AveragePolicyStd[5]  | 0.25342    |
| AverageReturn        | 1753.8     |
| MinReturn            | 420.79     |
| MaxReturn            | 1887.6     |
| StdReturn            | 225.52     |
| AverageEpisodeLength | 976.74     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.34     |
| TotalNEpisodes       | 21141      |
| TotalNSamples        | 6.3971e+06 |
| ExplainedVariance    | -0.0095267 |
-------------------------------------
[2018-12-22 12:18:39.185712 UTC] Saving snapshot
[2018-12-22 12:18:39.186016 UTC] Starting iteration 1279
[2018-12-22 12:18:39.186136 UTC] Start collecting samples
[2018-12-22 12:18:42.127686 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:42.202057 UTC] Performing policy update
[2018-12-22 12:18:42.202668 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:42.290546 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:43.336594 UTC] Performing line search
[2018-12-22 12:18:43.463624 UTC] Updating baseline
[2018-12-22 12:18:45.275025 UTC] Computing logging information
-------------------------------------
| Iteration            | 1279       |
| ExpectedImprovement  | 0.017114   |
| ActualImprovement    | 0.016097   |
| ImprovementRatio     | 0.94053    |
| MeanKL               | 0.0082977  |
| Entropy              | -1.3137    |
| Perplexity           | 0.26883    |
| AveragePolicyStd     | 0.19749    |
| AveragePolicyStd[0]  | 0.22146    |
| AveragePolicyStd[1]  | 0.19393    |
| AveragePolicyStd[2]  | 0.15999    |
| AveragePolicyStd[3]  | 0.20418    |
| AveragePolicyStd[4]  | 0.15145    |
| AveragePolicyStd[5]  | 0.25396    |
| AverageReturn        | 1777.4     |
| MinReturn            | 433.31     |
| MaxReturn            | 1887.6     |
| StdReturn            | 151.36     |
| AverageEpisodeLength | 989.52     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 79.63      |
| TotalNEpisodes       | 21145      |
| TotalNSamples        | 6.4011e+06 |
| ExplainedVariance    | 0.0030017  |
-------------------------------------
[2018-12-22 12:18:45.698550 UTC] Saving snapshot
[2018-12-22 12:18:45.698810 UTC] Starting iteration 1280
[2018-12-22 12:18:45.698933 UTC] Start collecting samples
[2018-12-22 12:18:48.691647 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:48.770966 UTC] Performing policy update
[2018-12-22 12:18:48.771667 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:48.859241 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:49.908144 UTC] Performing line search
[2018-12-22 12:18:50.037392 UTC] Updating baseline
[2018-12-22 12:18:51.500872 UTC] Computing logging information
-------------------------------------
| Iteration            | 1280       |
| ExpectedImprovement  | 0.016973   |
| ActualImprovement    | 0.016241   |
| ImprovementRatio     | 0.95687    |
| MeanKL               | 0.0080807  |
| Entropy              | -1.3096    |
| Perplexity           | 0.26994    |
| AveragePolicyStd     | 0.19761    |
| AveragePolicyStd[0]  | 0.22213    |
| AveragePolicyStd[1]  | 0.19443    |
| AveragePolicyStd[2]  | 0.16027    |
| AveragePolicyStd[3]  | 0.20422    |
| AveragePolicyStd[4]  | 0.15129    |
| AveragePolicyStd[5]  | 0.25334    |
| AverageReturn        | 1777.1     |
| MinReturn            | 433.31     |
| MaxReturn            | 1887.6     |
| StdReturn            | 151.6      |
| AverageEpisodeLength | 989.52     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 79.63      |
| TotalNEpisodes       | 21152      |
| TotalNSamples        | 6.4081e+06 |
| ExplainedVariance    | 0.00015214 |
-------------------------------------
[2018-12-22 12:18:51.924690 UTC] Saving snapshot
[2018-12-22 12:18:51.932694 UTC] Starting iteration 1281
[2018-12-22 12:18:51.932907 UTC] Start collecting samples
[2018-12-22 12:18:54.872628 UTC] Computing input variables for policy optimization
[2018-12-22 12:18:54.948620 UTC] Performing policy update
[2018-12-22 12:18:54.949551 UTC] Computing gradient in Euclidean space
[2018-12-22 12:18:55.040058 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:18:56.096552 UTC] Performing line search
[2018-12-22 12:18:56.224466 UTC] Updating baseline
[2018-12-22 12:18:57.906221 UTC] Computing logging information
-------------------------------------
| Iteration            | 1281       |
| ExpectedImprovement  | 0.01841    |
| ActualImprovement    | 0.017474   |
| ImprovementRatio     | 0.94915    |
| MeanKL               | 0.0080507  |
| Entropy              | -1.3178    |
| Perplexity           | 0.26772    |
| AveragePolicyStd     | 0.19737    |
| AveragePolicyStd[0]  | 0.22192    |
| AveragePolicyStd[1]  | 0.19339    |
| AveragePolicyStd[2]  | 0.16061    |
| AveragePolicyStd[3]  | 0.20435    |
| AveragePolicyStd[4]  | 0.15054    |
| AveragePolicyStd[5]  | 0.25341    |
| AverageReturn        | 1777.6     |
| MinReturn            | 433.31     |
| MaxReturn            | 1887.6     |
| StdReturn            | 151.72     |
| AverageEpisodeLength | 989.52     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 79.63      |
| TotalNEpisodes       | 21157      |
| TotalNSamples        | 6.4131e+06 |
| ExplainedVariance    | 0.065373   |
-------------------------------------
[2018-12-22 12:18:58.324685 UTC] Saving snapshot
[2018-12-22 12:18:58.324924 UTC] Starting iteration 1282
[2018-12-22 12:18:58.325041 UTC] Start collecting samples
[2018-12-22 12:19:01.254219 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:01.330324 UTC] Performing policy update
[2018-12-22 12:19:01.331131 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:01.420664 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:02.379812 UTC] Performing line search
[2018-12-22 12:19:02.497187 UTC] Updating baseline
[2018-12-22 12:19:03.667158 UTC] Computing logging information
-------------------------------------
| Iteration            | 1282       |
| ExpectedImprovement  | 0.018171   |
| ActualImprovement    | 0.017021   |
| ImprovementRatio     | 0.93672    |
| MeanKL               | 0.0084692  |
| Entropy              | -1.318     |
| Perplexity           | 0.26767    |
| AveragePolicyStd     | 0.19737    |
| AveragePolicyStd[0]  | 0.22189    |
| AveragePolicyStd[1]  | 0.19352    |
| AveragePolicyStd[2]  | 0.16022    |
| AveragePolicyStd[3]  | 0.20433    |
| AveragePolicyStd[4]  | 0.15077    |
| AveragePolicyStd[5]  | 0.25348    |
| AverageReturn        | 1766.9     |
| MinReturn            | 433.31     |
| MaxReturn            | 1887.6     |
| StdReturn            | 182.07     |
| AverageEpisodeLength | 983.93     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.526     |
| TotalNEpisodes       | 21160      |
| TotalNSamples        | 6.4156e+06 |
| ExplainedVariance    | 0.0069924  |
-------------------------------------
[2018-12-22 12:19:04.090239 UTC] Saving snapshot
[2018-12-22 12:19:04.090496 UTC] Starting iteration 1283
[2018-12-22 12:19:04.090639 UTC] Start collecting samples
[2018-12-22 12:19:07.221602 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:07.307330 UTC] Performing policy update
[2018-12-22 12:19:07.308952 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:07.405402 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:08.524133 UTC] Performing line search
[2018-12-22 12:19:08.659298 UTC] Updating baseline
[2018-12-22 12:19:10.142477 UTC] Computing logging information
-------------------------------------
| Iteration            | 1283       |
| ExpectedImprovement  | 0.0179     |
| ActualImprovement    | 0.017053   |
| ImprovementRatio     | 0.95267    |
| MeanKL               | 0.0085179  |
| Entropy              | -1.3261    |
| Perplexity           | 0.2655     |
| AveragePolicyStd     | 0.19705    |
| AveragePolicyStd[0]  | 0.2219     |
| AveragePolicyStd[1]  | 0.19352    |
| AveragePolicyStd[2]  | 0.15977    |
| AveragePolicyStd[3]  | 0.2043     |
| AveragePolicyStd[4]  | 0.15092    |
| AveragePolicyStd[5]  | 0.25191    |
| AverageReturn        | 1759.1     |
| MinReturn            | 433.31     |
| MaxReturn            | 1907.3     |
| StdReturn            | 219.1      |
| AverageEpisodeLength | 977.22     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 116.44     |
| TotalNEpisodes       | 21169      |
| TotalNSamples        | 6.4239e+06 |
| ExplainedVariance    | 0.061953   |
-------------------------------------
[2018-12-22 12:19:10.608452 UTC] Saving snapshot
[2018-12-22 12:19:10.608829 UTC] Starting iteration 1284
[2018-12-22 12:19:10.608951 UTC] Start collecting samples
[2018-12-22 12:19:13.568033 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:13.643764 UTC] Performing policy update
[2018-12-22 12:19:13.644689 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:13.734186 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:14.784950 UTC] Performing line search
[2018-12-22 12:19:14.913477 UTC] Updating baseline
[2018-12-22 12:19:16.824180 UTC] Computing logging information
-------------------------------------
| Iteration            | 1284       |
| ExpectedImprovement  | 0.014971   |
| ActualImprovement    | 0.013886   |
| ImprovementRatio     | 0.92753    |
| MeanKL               | 0.0084236  |
| Entropy              | -1.3253    |
| Perplexity           | 0.26572    |
| AveragePolicyStd     | 0.19712    |
| AveragePolicyStd[0]  | 0.22227    |
| AveragePolicyStd[1]  | 0.19266    |
| AveragePolicyStd[2]  | 0.15983    |
| AveragePolicyStd[3]  | 0.20505    |
| AveragePolicyStd[4]  | 0.15064    |
| AveragePolicyStd[5]  | 0.25226    |
| AverageReturn        | 1761.3     |
| MinReturn            | 433.31     |
| MaxReturn            | 1907.3     |
| StdReturn            | 219.3      |
| AverageEpisodeLength | 977.22     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 116.44     |
| TotalNEpisodes       | 21173      |
| TotalNSamples        | 6.4279e+06 |
| ExplainedVariance    | 0.00076928 |
-------------------------------------
[2018-12-22 12:19:17.244528 UTC] Saving snapshot
[2018-12-22 12:19:17.244760 UTC] Starting iteration 1285
[2018-12-22 12:19:17.244880 UTC] Start collecting samples
[2018-12-22 12:19:20.148795 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:20.223529 UTC] Performing policy update
[2018-12-22 12:19:20.224170 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:20.313855 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:21.368111 UTC] Performing line search
[2018-12-22 12:19:21.495158 UTC] Updating baseline
[2018-12-22 12:19:23.036709 UTC] Computing logging information
-------------------------------------
| Iteration            | 1285       |
| ExpectedImprovement  | 0.016865   |
| ActualImprovement    | 0.015247   |
| ImprovementRatio     | 0.90405    |
| MeanKL               | 0.0080184  |
| Entropy              | -1.3233    |
| Perplexity           | 0.26626    |
| AveragePolicyStd     | 0.19723    |
| AveragePolicyStd[0]  | 0.22285    |
| AveragePolicyStd[1]  | 0.19282    |
| AveragePolicyStd[2]  | 0.15986    |
| AveragePolicyStd[3]  | 0.20512    |
| AveragePolicyStd[4]  | 0.15019    |
| AveragePolicyStd[5]  | 0.25255    |
| AverageReturn        | 1762.5     |
| MinReturn            | 433.31     |
| MaxReturn            | 1907.3     |
| StdReturn            | 219.61     |
| AverageEpisodeLength | 977.22     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 116.44     |
| TotalNEpisodes       | 21175      |
| TotalNSamples        | 6.4299e+06 |
| ExplainedVariance    | 0.0054047  |
-------------------------------------
[2018-12-22 12:19:23.458526 UTC] Saving snapshot
[2018-12-22 12:19:23.458783 UTC] Starting iteration 1286
[2018-12-22 12:19:23.458904 UTC] Start collecting samples
[2018-12-22 12:19:26.443686 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:26.523382 UTC] Performing policy update
[2018-12-22 12:19:26.524000 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:26.615672 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:27.677578 UTC] Performing line search
[2018-12-22 12:19:27.811359 UTC] Updating baseline
[2018-12-22 12:19:29.243493 UTC] Computing logging information
--------------------------------------
| Iteration            | 1286        |
| ExpectedImprovement  | 0.018492    |
| ActualImprovement    | 0.017296    |
| ImprovementRatio     | 0.93531     |
| MeanKL               | 0.0082254   |
| Entropy              | -1.3283     |
| Perplexity           | 0.26494     |
| AveragePolicyStd     | 0.19702     |
| AveragePolicyStd[0]  | 0.22313     |
| AveragePolicyStd[1]  | 0.19258     |
| AveragePolicyStd[2]  | 0.15968     |
| AveragePolicyStd[3]  | 0.20522     |
| AveragePolicyStd[4]  | 0.15033     |
| AveragePolicyStd[5]  | 0.2512      |
| AverageReturn        | 1766.6      |
| MinReturn            | 433.31      |
| MaxReturn            | 1920.8      |
| StdReturn            | 220.71      |
| AverageEpisodeLength | 977.22      |
| MinEpisodeLength     | 257         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 116.44      |
| TotalNEpisodes       | 21182       |
| TotalNSamples        | 6.4369e+06  |
| ExplainedVariance    | -0.00032976 |
--------------------------------------
[2018-12-22 12:19:29.685303 UTC] Saving snapshot
[2018-12-22 12:19:29.685606 UTC] Starting iteration 1287
[2018-12-22 12:19:29.685771 UTC] Start collecting samples
[2018-12-22 12:19:32.855287 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:32.935197 UTC] Performing policy update
[2018-12-22 12:19:32.935937 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:33.028479 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:34.128372 UTC] Performing line search
[2018-12-22 12:19:34.261476 UTC] Updating baseline
[2018-12-22 12:19:36.290197 UTC] Computing logging information
-------------------------------------
| Iteration            | 1287       |
| ExpectedImprovement  | 0.016853   |
| ActualImprovement    | 0.016071   |
| ImprovementRatio     | 0.95358    |
| MeanKL               | 0.0084274  |
| Entropy              | -1.333     |
| Perplexity           | 0.26368    |
| AveragePolicyStd     | 0.1969     |
| AveragePolicyStd[0]  | 0.22299    |
| AveragePolicyStd[1]  | 0.19225    |
| AveragePolicyStd[2]  | 0.15916    |
| AveragePolicyStd[3]  | 0.20536    |
| AveragePolicyStd[4]  | 0.15027    |
| AveragePolicyStd[5]  | 0.25135    |
| AverageReturn        | 1769.7     |
| MinReturn            | 433.31     |
| MaxReturn            | 1920.8     |
| StdReturn            | 221.44     |
| AverageEpisodeLength | 977.22     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 116.44     |
| TotalNEpisodes       | 21188      |
| TotalNSamples        | 6.4429e+06 |
| ExplainedVariance    | 0.0017919  |
-------------------------------------
[2018-12-22 12:19:36.733862 UTC] Saving snapshot
[2018-12-22 12:19:36.734116 UTC] Starting iteration 1288
[2018-12-22 12:19:36.734237 UTC] Start collecting samples
[2018-12-22 12:19:39.743688 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:39.820119 UTC] Performing policy update
[2018-12-22 12:19:39.820906 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:39.910199 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:40.961052 UTC] Performing line search
[2018-12-22 12:19:41.090467 UTC] Updating baseline
[2018-12-22 12:19:42.742245 UTC] Computing logging information
-------------------------------------
| Iteration            | 1288       |
| ExpectedImprovement  | 0.020167   |
| ActualImprovement    | 0.018386   |
| ImprovementRatio     | 0.91168    |
| MeanKL               | 0.0074782  |
| Entropy              | -1.332     |
| Perplexity           | 0.26394    |
| AveragePolicyStd     | 0.19689    |
| AveragePolicyStd[0]  | 0.22272    |
| AveragePolicyStd[1]  | 0.1921     |
| AveragePolicyStd[2]  | 0.15942    |
| AveragePolicyStd[3]  | 0.2058     |
| AveragePolicyStd[4]  | 0.15042    |
| AveragePolicyStd[5]  | 0.25091    |
| AverageReturn        | 1759.2     |
| MinReturn            | 433.31     |
| MaxReturn            | 1920.8     |
| StdReturn            | 248.45     |
| AverageEpisodeLength | 970.96     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.97     |
| TotalNEpisodes       | 21192      |
| TotalNSamples        | 6.4463e+06 |
| ExplainedVariance    | 0.14492    |
-------------------------------------
[2018-12-22 12:19:43.163529 UTC] Saving snapshot
[2018-12-22 12:19:43.163789 UTC] Starting iteration 1289
[2018-12-22 12:19:43.163913 UTC] Start collecting samples
[2018-12-22 12:19:46.148106 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:46.225306 UTC] Performing policy update
[2018-12-22 12:19:46.226009 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:46.316553 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:47.368033 UTC] Performing line search
[2018-12-22 12:19:47.495376 UTC] Updating baseline
[2018-12-22 12:19:49.329192 UTC] Computing logging information
-------------------------------------
| Iteration            | 1289       |
| ExpectedImprovement  | 0.015406   |
| ActualImprovement    | 0.014786   |
| ImprovementRatio     | 0.95978    |
| MeanKL               | 0.0075802  |
| Entropy              | -1.3324    |
| Perplexity           | 0.26385    |
| AveragePolicyStd     | 0.1969     |
| AveragePolicyStd[0]  | 0.22221    |
| AveragePolicyStd[1]  | 0.19181    |
| AveragePolicyStd[2]  | 0.1592     |
| AveragePolicyStd[3]  | 0.20578    |
| AveragePolicyStd[4]  | 0.15063    |
| AveragePolicyStd[5]  | 0.25178    |
| AverageReturn        | 1763.4     |
| MinReturn            | 433.31     |
| MaxReturn            | 1920.8     |
| StdReturn            | 249.39     |
| AverageEpisodeLength | 970.96     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.97     |
| TotalNEpisodes       | 21199      |
| TotalNSamples        | 6.4533e+06 |
| ExplainedVariance    | 0.013094   |
-------------------------------------
[2018-12-22 12:19:49.746932 UTC] Saving snapshot
[2018-12-22 12:19:49.747173 UTC] Starting iteration 1290
[2018-12-22 12:19:49.747301 UTC] Start collecting samples
[2018-12-22 12:19:52.695959 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:52.772198 UTC] Performing policy update
[2018-12-22 12:19:52.773066 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:52.862972 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:53.924451 UTC] Performing line search
[2018-12-22 12:19:54.052009 UTC] Updating baseline
[2018-12-22 12:19:55.277164 UTC] Computing logging information
-------------------------------------
| Iteration            | 1290       |
| ExpectedImprovement  | 0.017298   |
| ActualImprovement    | 0.016506   |
| ImprovementRatio     | 0.95426    |
| MeanKL               | 0.0085539  |
| Entropy              | -1.3264    |
| Perplexity           | 0.26544    |
| AveragePolicyStd     | 0.19713    |
| AveragePolicyStd[0]  | 0.22243    |
| AveragePolicyStd[1]  | 0.1923     |
| AveragePolicyStd[2]  | 0.15928    |
| AveragePolicyStd[3]  | 0.20566    |
| AveragePolicyStd[4]  | 0.15057    |
| AveragePolicyStd[5]  | 0.25254    |
| AverageReturn        | 1761.6     |
| MinReturn            | 433.31     |
| MaxReturn            | 1920.8     |
| StdReturn            | 252.83     |
| AverageEpisodeLength | 968.86     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.17     |
| TotalNEpisodes       | 21204      |
| TotalNSamples        | 6.4581e+06 |
| ExplainedVariance    | 0.107      |
-------------------------------------
[2018-12-22 12:19:55.701426 UTC] Saving snapshot
[2018-12-22 12:19:55.709614 UTC] Starting iteration 1291
[2018-12-22 12:19:55.709838 UTC] Start collecting samples
[2018-12-22 12:19:58.677666 UTC] Computing input variables for policy optimization
[2018-12-22 12:19:58.755063 UTC] Performing policy update
[2018-12-22 12:19:58.755704 UTC] Computing gradient in Euclidean space
[2018-12-22 12:19:58.845729 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:19:59.893323 UTC] Performing line search
[2018-12-22 12:20:00.025372 UTC] Updating baseline
[2018-12-22 12:20:01.258682 UTC] Computing logging information
-------------------------------------
| Iteration            | 1291       |
| ExpectedImprovement  | 0.016801   |
| ActualImprovement    | 0.015785   |
| ImprovementRatio     | 0.93954    |
| MeanKL               | 0.0080032  |
| Entropy              | -1.3263    |
| Perplexity           | 0.26545    |
| AveragePolicyStd     | 0.1971     |
| AveragePolicyStd[0]  | 0.22196    |
| AveragePolicyStd[1]  | 0.19242    |
| AveragePolicyStd[2]  | 0.15947    |
| AveragePolicyStd[3]  | 0.20531    |
| AveragePolicyStd[4]  | 0.15081    |
| AveragePolicyStd[5]  | 0.25265    |
| AverageReturn        | 1742.3     |
| MinReturn            | 433.31     |
| MaxReturn            | 1920.8     |
| StdReturn            | 292.45     |
| AverageEpisodeLength | 957.06     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.48     |
| TotalNEpisodes       | 21210      |
| TotalNSamples        | 6.4629e+06 |
| ExplainedVariance    | 0.19991    |
-------------------------------------
[2018-12-22 12:20:01.679168 UTC] Saving snapshot
[2018-12-22 12:20:01.679423 UTC] Starting iteration 1292
[2018-12-22 12:20:01.679560 UTC] Start collecting samples
[2018-12-22 12:20:04.645516 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:04.722058 UTC] Performing policy update
[2018-12-22 12:20:04.722699 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:04.812480 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:05.869601 UTC] Performing line search
[2018-12-22 12:20:05.999383 UTC] Updating baseline
[2018-12-22 12:20:07.336163 UTC] Computing logging information
-------------------------------------
| Iteration            | 1292       |
| ExpectedImprovement  | 0.019041   |
| ActualImprovement    | 0.017857   |
| ImprovementRatio     | 0.93781    |
| MeanKL               | 0.0085046  |
| Entropy              | -1.3312    |
| Perplexity           | 0.26416    |
| AveragePolicyStd     | 0.19693    |
| AveragePolicyStd[0]  | 0.2216     |
| AveragePolicyStd[1]  | 0.1927     |
| AveragePolicyStd[2]  | 0.1594     |
| AveragePolicyStd[3]  | 0.20514    |
| AveragePolicyStd[4]  | 0.15055    |
| AveragePolicyStd[5]  | 0.25222    |
| AverageReturn        | 1732.8     |
| MinReturn            | 433.31     |
| MaxReturn            | 1920.8     |
| StdReturn            | 314.6      |
| AverageEpisodeLength | 950.69     |
| MinEpisodeLength     | 257        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.4      |
| TotalNEpisodes       | 21215      |
| TotalNSamples        | 6.4672e+06 |
| ExplainedVariance    | 0.22051    |
-------------------------------------
[2018-12-22 12:20:07.764951 UTC] Saving snapshot
[2018-12-22 12:20:07.765186 UTC] Starting iteration 1293
[2018-12-22 12:20:07.765314 UTC] Start collecting samples
[2018-12-22 12:20:10.734221 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:10.810802 UTC] Performing policy update
[2018-12-22 12:20:10.811364 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:10.901404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:11.964218 UTC] Performing line search
[2018-12-22 12:20:12.090045 UTC] Updating baseline
[2018-12-22 12:20:13.542753 UTC] Computing logging information
-------------------------------------
| Iteration            | 1293       |
| ExpectedImprovement  | 0.016343   |
| ActualImprovement    | 0.015318   |
| ImprovementRatio     | 0.93728    |
| MeanKL               | 0.0094225  |
| Entropy              | -1.3361    |
| Perplexity           | 0.26287    |
| AveragePolicyStd     | 0.19678    |
| AveragePolicyStd[0]  | 0.22105    |
| AveragePolicyStd[1]  | 0.19276    |
| AveragePolicyStd[2]  | 0.15905    |
| AveragePolicyStd[3]  | 0.20579    |
| AveragePolicyStd[4]  | 0.15027    |
| AveragePolicyStd[5]  | 0.25175    |
| AverageReturn        | 1751.9     |
| MinReturn            | 566.06     |
| MaxReturn            | 1920.8     |
| StdReturn            | 283.9      |
| AverageEpisodeLength | 959.52     |
| MinEpisodeLength     | 329        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.11     |
| TotalNEpisodes       | 21221      |
| TotalNSamples        | 6.4731e+06 |
| ExplainedVariance    | 0.0039188  |
-------------------------------------
[2018-12-22 12:20:13.970394 UTC] Saving snapshot
[2018-12-22 12:20:13.970647 UTC] Starting iteration 1294
[2018-12-22 12:20:13.970781 UTC] Start collecting samples
[2018-12-22 12:20:16.911372 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:16.986085 UTC] Performing policy update
[2018-12-22 12:20:16.986901 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:17.076953 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:18.134249 UTC] Performing line search
[2018-12-22 12:20:18.262244 UTC] Updating baseline
[2018-12-22 12:20:19.673217 UTC] Computing logging information
-------------------------------------
| Iteration            | 1294       |
| ExpectedImprovement  | 0.018289   |
| ActualImprovement    | 0.016921   |
| ImprovementRatio     | 0.92521    |
| MeanKL               | 0.0076566  |
| Entropy              | -1.3352    |
| Perplexity           | 0.26311    |
| AveragePolicyStd     | 0.19678    |
| AveragePolicyStd[0]  | 0.2214     |
| AveragePolicyStd[1]  | 0.19337    |
| AveragePolicyStd[2]  | 0.15922    |
| AveragePolicyStd[3]  | 0.20561    |
| AveragePolicyStd[4]  | 0.15017    |
| AveragePolicyStd[5]  | 0.2509     |
| AverageReturn        | 1753.6     |
| MinReturn            | 566.06     |
| MaxReturn            | 1920.8     |
| StdReturn            | 284.45     |
| AverageEpisodeLength | 959.41     |
| MinEpisodeLength     | 329        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.08     |
| TotalNEpisodes       | 21225      |
| TotalNSamples        | 6.4771e+06 |
| ExplainedVariance    | 0.17473    |
-------------------------------------
[2018-12-22 12:20:20.093686 UTC] Saving snapshot
[2018-12-22 12:20:20.094024 UTC] Starting iteration 1295
[2018-12-22 12:20:20.094143 UTC] Start collecting samples
[2018-12-22 12:20:23.048240 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:23.124214 UTC] Performing policy update
[2018-12-22 12:20:23.124872 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:23.213024 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:24.273548 UTC] Performing line search
[2018-12-22 12:20:24.400099 UTC] Updating baseline
[2018-12-22 12:20:25.727411 UTC] Computing logging information
-------------------------------------
| Iteration            | 1295       |
| ExpectedImprovement  | 0.018168   |
| ActualImprovement    | 0.017283   |
| ImprovementRatio     | 0.95133    |
| MeanKL               | 0.0087283  |
| Entropy              | -1.3416    |
| Perplexity           | 0.26143    |
| AveragePolicyStd     | 0.1966     |
| AveragePolicyStd[0]  | 0.22111    |
| AveragePolicyStd[1]  | 0.19302    |
| AveragePolicyStd[2]  | 0.15914    |
| AveragePolicyStd[3]  | 0.205      |
| AveragePolicyStd[4]  | 0.14989    |
| AveragePolicyStd[5]  | 0.25143    |
| AverageReturn        | 1758.3     |
| MinReturn            | 566.06     |
| MaxReturn            | 1920.8     |
| StdReturn            | 285.48     |
| AverageEpisodeLength | 959.41     |
| MinEpisodeLength     | 329        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.08     |
| TotalNEpisodes       | 21231      |
| TotalNSamples        | 6.4831e+06 |
| ExplainedVariance    | 0.030942   |
-------------------------------------
[2018-12-22 12:20:26.152342 UTC] Saving snapshot
[2018-12-22 12:20:26.152636 UTC] Starting iteration 1296
[2018-12-22 12:20:26.152754 UTC] Start collecting samples
[2018-12-22 12:20:29.094544 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:29.171805 UTC] Performing policy update
[2018-12-22 12:20:29.172444 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:29.262324 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:30.319690 UTC] Performing line search
[2018-12-22 12:20:30.445988 UTC] Updating baseline
[2018-12-22 12:20:32.173272 UTC] Computing logging information
-------------------------------------
| Iteration            | 1296       |
| ExpectedImprovement  | 0.018944   |
| ActualImprovement    | 0.017647   |
| ImprovementRatio     | 0.93154    |
| MeanKL               | 0.0079139  |
| Entropy              | -1.3386    |
| Perplexity           | 0.26221    |
| AveragePolicyStd     | 0.19674    |
| AveragePolicyStd[0]  | 0.22178    |
| AveragePolicyStd[1]  | 0.19265    |
| AveragePolicyStd[2]  | 0.15912    |
| AveragePolicyStd[3]  | 0.20547    |
| AveragePolicyStd[4]  | 0.14964    |
| AveragePolicyStd[5]  | 0.25178    |
| AverageReturn        | 1762.2     |
| MinReturn            | 566.06     |
| MaxReturn            | 1920.8     |
| StdReturn            | 286.13     |
| AverageEpisodeLength | 959.41     |
| MinEpisodeLength     | 329        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.08     |
| TotalNEpisodes       | 21236      |
| TotalNSamples        | 6.4881e+06 |
| ExplainedVariance    | -0.0081073 |
-------------------------------------
[2018-12-22 12:20:32.595545 UTC] Saving snapshot
[2018-12-22 12:20:32.595832 UTC] Starting iteration 1297
[2018-12-22 12:20:32.595953 UTC] Start collecting samples
[2018-12-22 12:20:35.545522 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:35.620076 UTC] Performing policy update
[2018-12-22 12:20:35.620833 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:35.709048 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:36.775445 UTC] Performing line search
[2018-12-22 12:20:36.903205 UTC] Updating baseline
[2018-12-22 12:20:38.389350 UTC] Computing logging information
-------------------------------------
| Iteration            | 1297       |
| ExpectedImprovement  | 0.017085   |
| ActualImprovement    | 0.01626    |
| ImprovementRatio     | 0.95166    |
| MeanKL               | 0.0081775  |
| Entropy              | -1.3342    |
| Perplexity           | 0.26336    |
| AveragePolicyStd     | 0.19691    |
| AveragePolicyStd[0]  | 0.22208    |
| AveragePolicyStd[1]  | 0.19302    |
| AveragePolicyStd[2]  | 0.15919    |
| AveragePolicyStd[3]  | 0.20538    |
| AveragePolicyStd[4]  | 0.1495     |
| AveragePolicyStd[5]  | 0.25229    |
| AverageReturn        | 1765.2     |
| MinReturn            | 566.06     |
| MaxReturn            | 1920.8     |
| StdReturn            | 286.69     |
| AverageEpisodeLength | 959.41     |
| MinEpisodeLength     | 329        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.08     |
| TotalNEpisodes       | 21241      |
| TotalNSamples        | 6.4931e+06 |
| ExplainedVariance    | 0.0028639  |
-------------------------------------
[2018-12-22 12:20:38.813986 UTC] Saving snapshot
[2018-12-22 12:20:38.814231 UTC] Starting iteration 1298
[2018-12-22 12:20:38.814349 UTC] Start collecting samples
[2018-12-22 12:20:41.757047 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:41.832305 UTC] Performing policy update
[2018-12-22 12:20:41.833083 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:41.923912 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:42.984211 UTC] Performing line search
[2018-12-22 12:20:43.110722 UTC] Updating baseline
[2018-12-22 12:20:44.584573 UTC] Computing logging information
-------------------------------------
| Iteration            | 1298       |
| ExpectedImprovement  | 0.018977   |
| ActualImprovement    | 0.017653   |
| ImprovementRatio     | 0.93023    |
| MeanKL               | 0.0078139  |
| Entropy              | -1.335     |
| Perplexity           | 0.26315    |
| AveragePolicyStd     | 0.19684    |
| AveragePolicyStd[0]  | 0.22153    |
| AveragePolicyStd[1]  | 0.19342    |
| AveragePolicyStd[2]  | 0.15926    |
| AveragePolicyStd[3]  | 0.20531    |
| AveragePolicyStd[4]  | 0.14976    |
| AveragePolicyStd[5]  | 0.25173    |
| AverageReturn        | 1752.2     |
| MinReturn            | 364.07     |
| MaxReturn            | 1920.8     |
| StdReturn            | 318.97     |
| AverageEpisodeLength | 951.9      |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.12     |
| TotalNEpisodes       | 21244      |
| TotalNSamples        | 6.4953e+06 |
| ExplainedVariance    | 0.22051    |
-------------------------------------
[2018-12-22 12:20:45.011693 UTC] Saving snapshot
[2018-12-22 12:20:45.012007 UTC] Starting iteration 1299
[2018-12-22 12:20:45.012135 UTC] Start collecting samples
[2018-12-22 12:20:48.017591 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:48.096197 UTC] Performing policy update
[2018-12-22 12:20:48.096788 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:48.186490 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:49.235108 UTC] Performing line search
[2018-12-22 12:20:49.361636 UTC] Updating baseline
[2018-12-22 12:20:51.021641 UTC] Computing logging information
-------------------------------------
| Iteration            | 1299       |
| ExpectedImprovement  | 0.016514   |
| ActualImprovement    | 0.015615   |
| ImprovementRatio     | 0.94558    |
| MeanKL               | 0.0089132  |
| Entropy              | -1.333     |
| Perplexity           | 0.26367    |
| AveragePolicyStd     | 0.19691    |
| AveragePolicyStd[0]  | 0.222      |
| AveragePolicyStd[1]  | 0.19296    |
| AveragePolicyStd[2]  | 0.15935    |
| AveragePolicyStd[3]  | 0.20579    |
| AveragePolicyStd[4]  | 0.14973    |
| AveragePolicyStd[5]  | 0.25162    |
| AverageReturn        | 1756.8     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 320.16     |
| AverageEpisodeLength | 951.9      |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.12     |
| TotalNEpisodes       | 21253      |
| TotalNSamples        | 6.5043e+06 |
| ExplainedVariance    | -0.0052486 |
-------------------------------------
[2018-12-22 12:20:51.439647 UTC] Saving snapshot
[2018-12-22 12:20:51.439906 UTC] Starting iteration 1300
[2018-12-22 12:20:51.440033 UTC] Start collecting samples
[2018-12-22 12:20:54.380962 UTC] Computing input variables for policy optimization
[2018-12-22 12:20:54.455364 UTC] Performing policy update
[2018-12-22 12:20:54.456018 UTC] Computing gradient in Euclidean space
[2018-12-22 12:20:54.547339 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:20:55.601331 UTC] Performing line search
[2018-12-22 12:20:55.727223 UTC] Updating baseline
[2018-12-22 12:20:57.212091 UTC] Computing logging information
-------------------------------------
| Iteration            | 1300       |
| ExpectedImprovement  | 0.01913    |
| ActualImprovement    | 0.018299   |
| ImprovementRatio     | 0.95656    |
| MeanKL               | 0.0079162  |
| Entropy              | -1.3349    |
| Perplexity           | 0.26318    |
| AveragePolicyStd     | 0.19688    |
| AveragePolicyStd[0]  | 0.22232    |
| AveragePolicyStd[1]  | 0.19295    |
| AveragePolicyStd[2]  | 0.15937    |
| AveragePolicyStd[3]  | 0.20593    |
| AveragePolicyStd[4]  | 0.14914    |
| AveragePolicyStd[5]  | 0.25158    |
| AverageReturn        | 1757.1     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 320.31     |
| AverageEpisodeLength | 951.9      |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.12     |
| TotalNEpisodes       | 21257      |
| TotalNSamples        | 6.5083e+06 |
| ExplainedVariance    | 0.016371   |
-------------------------------------
[2018-12-22 12:20:57.633884 UTC] Saving snapshot
[2018-12-22 12:20:57.641941 UTC] Starting iteration 1301
[2018-12-22 12:20:57.642139 UTC] Start collecting samples
[2018-12-22 12:21:00.552266 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:00.625771 UTC] Performing policy update
[2018-12-22 12:21:00.626530 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:00.717910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:01.780444 UTC] Performing line search
[2018-12-22 12:21:01.908386 UTC] Updating baseline
[2018-12-22 12:21:03.056901 UTC] Computing logging information
-------------------------------------
| Iteration            | 1301       |
| ExpectedImprovement  | 0.018582   |
| ActualImprovement    | 0.016911   |
| ImprovementRatio     | 0.91008    |
| MeanKL               | 0.0079205  |
| Entropy              | -1.3358    |
| Perplexity           | 0.26294    |
| AveragePolicyStd     | 0.19686    |
| AveragePolicyStd[0]  | 0.22213    |
| AveragePolicyStd[1]  | 0.19307    |
| AveragePolicyStd[2]  | 0.15907    |
| AveragePolicyStd[3]  | 0.20577    |
| AveragePolicyStd[4]  | 0.14931    |
| AveragePolicyStd[5]  | 0.25178    |
| AverageReturn        | 1768.4     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 304.49     |
| AverageEpisodeLength | 957.49     |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.88     |
| TotalNEpisodes       | 21259      |
| TotalNSamples        | 6.5103e+06 |
| ExplainedVariance    | -0.03069   |
-------------------------------------
[2018-12-22 12:21:03.481923 UTC] Saving snapshot
[2018-12-22 12:21:03.482187 UTC] Starting iteration 1302
[2018-12-22 12:21:03.482317 UTC] Start collecting samples
[2018-12-22 12:21:06.478634 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:06.556561 UTC] Performing policy update
[2018-12-22 12:21:06.557344 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:06.648248 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:07.700102 UTC] Performing line search
[2018-12-22 12:21:07.831000 UTC] Updating baseline
[2018-12-22 12:21:10.091912 UTC] Computing logging information
-------------------------------------
| Iteration            | 1302       |
| ExpectedImprovement  | 0.020167   |
| ActualImprovement    | 0.01925    |
| ImprovementRatio     | 0.95452    |
| MeanKL               | 0.0080497  |
| Entropy              | -1.3397    |
| Perplexity           | 0.26191    |
| AveragePolicyStd     | 0.19677    |
| AveragePolicyStd[0]  | 0.22213    |
| AveragePolicyStd[1]  | 0.19318    |
| AveragePolicyStd[2]  | 0.15849    |
| AveragePolicyStd[3]  | 0.20626    |
| AveragePolicyStd[4]  | 0.1489     |
| AveragePolicyStd[5]  | 0.25169    |
| AverageReturn        | 1780.8     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 279.51     |
| AverageEpisodeLength | 964.2      |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.46     |
| TotalNEpisodes       | 21267      |
| TotalNSamples        | 6.5183e+06 |
| ExplainedVariance    | 0.0001506  |
-------------------------------------
[2018-12-22 12:21:10.511384 UTC] Saving snapshot
[2018-12-22 12:21:10.511657 UTC] Starting iteration 1303
[2018-12-22 12:21:10.511786 UTC] Start collecting samples
[2018-12-22 12:21:13.473133 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:13.548912 UTC] Performing policy update
[2018-12-22 12:21:13.549687 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:13.640427 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:14.706386 UTC] Performing line search
[2018-12-22 12:21:14.834201 UTC] Updating baseline
[2018-12-22 12:21:16.069265 UTC] Computing logging information
-------------------------------------
| Iteration            | 1303       |
| ExpectedImprovement  | 0.018601   |
| ActualImprovement    | 0.016927   |
| ImprovementRatio     | 0.90998    |
| MeanKL               | 0.0073956  |
| Entropy              | -1.3441    |
| Perplexity           | 0.26077    |
| AveragePolicyStd     | 0.19665    |
| AveragePolicyStd[0]  | 0.22226    |
| AveragePolicyStd[1]  | 0.19294    |
| AveragePolicyStd[2]  | 0.15825    |
| AveragePolicyStd[3]  | 0.20645    |
| AveragePolicyStd[4]  | 0.14858    |
| AveragePolicyStd[5]  | 0.25145    |
| AverageReturn        | 1783       |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 280.02     |
| AverageEpisodeLength | 964.2      |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.46     |
| TotalNEpisodes       | 21272      |
| TotalNSamples        | 6.5233e+06 |
| ExplainedVariance    | -0.0099248 |
-------------------------------------
[2018-12-22 12:21:16.491420 UTC] Saving snapshot
[2018-12-22 12:21:16.491689 UTC] Starting iteration 1304
[2018-12-22 12:21:16.491807 UTC] Start collecting samples
[2018-12-22 12:21:19.410550 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:19.484882 UTC] Performing policy update
[2018-12-22 12:21:19.485541 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:19.575854 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:20.634033 UTC] Performing line search
[2018-12-22 12:21:20.761483 UTC] Updating baseline
[2018-12-22 12:21:22.156820 UTC] Computing logging information
-------------------------------------
| Iteration            | 1304       |
| ExpectedImprovement  | 0.016545   |
| ActualImprovement    | 0.015025   |
| ImprovementRatio     | 0.90814    |
| MeanKL               | 0.0082105  |
| Entropy              | -1.3486    |
| Perplexity           | 0.2596     |
| AveragePolicyStd     | 0.19651    |
| AveragePolicyStd[0]  | 0.22237    |
| AveragePolicyStd[1]  | 0.19282    |
| AveragePolicyStd[2]  | 0.15802    |
| AveragePolicyStd[3]  | 0.20655    |
| AveragePolicyStd[4]  | 0.14842    |
| AveragePolicyStd[5]  | 0.25086    |
| AverageReturn        | 1783.7     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 280.17     |
| AverageEpisodeLength | 964.2      |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.46     |
| TotalNEpisodes       | 21274      |
| TotalNSamples        | 6.5253e+06 |
| ExplainedVariance    | 0.0054142  |
-------------------------------------
[2018-12-22 12:21:22.570252 UTC] Saving snapshot
[2018-12-22 12:21:22.570535 UTC] Starting iteration 1305
[2018-12-22 12:21:22.570656 UTC] Start collecting samples
[2018-12-22 12:21:25.569020 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:25.647249 UTC] Performing policy update
[2018-12-22 12:21:25.648084 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:25.738076 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:26.795649 UTC] Performing line search
[2018-12-22 12:21:26.924812 UTC] Updating baseline
[2018-12-22 12:21:28.378884 UTC] Computing logging information
-------------------------------------
| Iteration            | 1305       |
| ExpectedImprovement  | 0.016957   |
| ActualImprovement    | 0.016289   |
| ImprovementRatio     | 0.96058    |
| MeanKL               | 0.0082154  |
| Entropy              | -1.3495    |
| Perplexity           | 0.25938    |
| AveragePolicyStd     | 0.19652    |
| AveragePolicyStd[0]  | 0.22262    |
| AveragePolicyStd[1]  | 0.193      |
| AveragePolicyStd[2]  | 0.15819    |
| AveragePolicyStd[3]  | 0.20606    |
| AveragePolicyStd[4]  | 0.14792    |
| AveragePolicyStd[5]  | 0.25131    |
| AverageReturn        | 1782.6     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 279.81     |
| AverageEpisodeLength | 964.2      |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.46     |
| TotalNEpisodes       | 21283      |
| TotalNSamples        | 6.5343e+06 |
| ExplainedVariance    | -0.0013553 |
-------------------------------------
[2018-12-22 12:21:28.803406 UTC] Saving snapshot
[2018-12-22 12:21:28.803674 UTC] Starting iteration 1306
[2018-12-22 12:21:28.803790 UTC] Start collecting samples
[2018-12-22 12:21:31.753401 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:31.831283 UTC] Performing policy update
[2018-12-22 12:21:31.832007 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:31.924182 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:32.983372 UTC] Performing line search
[2018-12-22 12:21:33.110737 UTC] Updating baseline
[2018-12-22 12:21:34.948645 UTC] Computing logging information
------------------------------------
| Iteration            | 1306      |
| ExpectedImprovement  | 0.017737  |
| ActualImprovement    | 0.016292  |
| ImprovementRatio     | 0.91852   |
| MeanKL               | 0.007784  |
| Entropy              | -1.3434   |
| Perplexity           | 0.26095   |
| AveragePolicyStd     | 0.19671   |
| AveragePolicyStd[0]  | 0.22318   |
| AveragePolicyStd[1]  | 0.19298   |
| AveragePolicyStd[2]  | 0.15804   |
| AveragePolicyStd[3]  | 0.20584   |
| AveragePolicyStd[4]  | 0.14855   |
| AveragePolicyStd[5]  | 0.25166   |
| AverageReturn        | 1780.1    |
| MinReturn            | 364.07    |
| MaxReturn            | 1943.6    |
| StdReturn            | 283.79    |
| AverageEpisodeLength | 961.56    |
| MinEpisodeLength     | 249       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 143.22    |
| TotalNEpisodes       | 21287     |
| TotalNSamples        | 6.538e+06 |
| ExplainedVariance    | 0.12644   |
------------------------------------
[2018-12-22 12:21:35.373856 UTC] Saving snapshot
[2018-12-22 12:21:35.374120 UTC] Starting iteration 1307
[2018-12-22 12:21:35.374249 UTC] Start collecting samples
[2018-12-22 12:21:38.290233 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:38.364588 UTC] Performing policy update
[2018-12-22 12:21:38.365406 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:38.455058 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:39.515492 UTC] Performing line search
[2018-12-22 12:21:39.643071 UTC] Updating baseline
[2018-12-22 12:21:40.887341 UTC] Computing logging information
------------------------------------
| Iteration            | 1307      |
| ExpectedImprovement  | 0.016248  |
| ActualImprovement    | 0.015289  |
| ImprovementRatio     | 0.94095   |
| MeanKL               | 0.0079541 |
| Entropy              | -1.3427   |
| Perplexity           | 0.26114   |
| AveragePolicyStd     | 0.19672   |
| AveragePolicyStd[0]  | 0.22245   |
| AveragePolicyStd[1]  | 0.19314   |
| AveragePolicyStd[2]  | 0.15817   |
| AveragePolicyStd[3]  | 0.20645   |
| AveragePolicyStd[4]  | 0.14845   |
| AveragePolicyStd[5]  | 0.25166   |
| AverageReturn        | 1779.7    |
| MinReturn            | 364.07    |
| MaxReturn            | 1943.6    |
| StdReturn            | 283.63    |
| AverageEpisodeLength | 961.56    |
| MinEpisodeLength     | 249       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 143.22    |
| TotalNEpisodes       | 21290     |
| TotalNSamples        | 6.541e+06 |
| ExplainedVariance    | -0.025145 |
------------------------------------
[2018-12-22 12:21:41.312636 UTC] Saving snapshot
[2018-12-22 12:21:41.312899 UTC] Starting iteration 1308
[2018-12-22 12:21:41.313018 UTC] Start collecting samples
[2018-12-22 12:21:44.309379 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:44.386523 UTC] Performing policy update
[2018-12-22 12:21:44.387103 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:44.475373 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:45.526393 UTC] Performing line search
[2018-12-22 12:21:45.653656 UTC] Updating baseline
[2018-12-22 12:21:46.885400 UTC] Computing logging information
-------------------------------------
| Iteration            | 1308       |
| ExpectedImprovement  | 0.017187   |
| ActualImprovement    | 0.016488   |
| ImprovementRatio     | 0.95933    |
| MeanKL               | 0.0083666  |
| Entropy              | -1.3389    |
| Perplexity           | 0.26214    |
| AveragePolicyStd     | 0.19687    |
| AveragePolicyStd[0]  | 0.22266    |
| AveragePolicyStd[1]  | 0.19339    |
| AveragePolicyStd[2]  | 0.15792    |
| AveragePolicyStd[3]  | 0.20713    |
| AveragePolicyStd[4]  | 0.14845    |
| AveragePolicyStd[5]  | 0.25165    |
| AverageReturn        | 1790.5     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 259.55     |
| AverageEpisodeLength | 967.82     |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.52     |
| TotalNEpisodes       | 21298      |
| TotalNSamples        | 6.549e+06  |
| ExplainedVariance    | -0.0026721 |
-------------------------------------
[2018-12-22 12:21:47.309037 UTC] Saving snapshot
[2018-12-22 12:21:47.309280 UTC] Starting iteration 1309
[2018-12-22 12:21:47.309398 UTC] Start collecting samples
[2018-12-22 12:21:50.255012 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:50.330625 UTC] Performing policy update
[2018-12-22 12:21:50.331191 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:50.420615 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:51.480690 UTC] Performing line search
[2018-12-22 12:21:51.609102 UTC] Updating baseline
[2018-12-22 12:21:52.844953 UTC] Computing logging information
-------------------------------------
| Iteration            | 1309       |
| ExpectedImprovement  | 0.016346   |
| ActualImprovement    | 0.015394   |
| ImprovementRatio     | 0.94172    |
| MeanKL               | 0.0084315  |
| Entropy              | -1.339     |
| Perplexity           | 0.26211    |
| AveragePolicyStd     | 0.19684    |
| AveragePolicyStd[0]  | 0.22293    |
| AveragePolicyStd[1]  | 0.19335    |
| AveragePolicyStd[2]  | 0.15782    |
| AveragePolicyStd[3]  | 0.20709    |
| AveragePolicyStd[4]  | 0.14873    |
| AveragePolicyStd[5]  | 0.2511     |
| AverageReturn        | 1792.5     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 255.97     |
| AverageEpisodeLength | 969.92     |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.33     |
| TotalNEpisodes       | 21303      |
| TotalNSamples        | 6.554e+06  |
| ExplainedVariance    | 0.00099297 |
-------------------------------------
[2018-12-22 12:21:53.272564 UTC] Saving snapshot
[2018-12-22 12:21:53.272818 UTC] Starting iteration 1310
[2018-12-22 12:21:53.272939 UTC] Start collecting samples
[2018-12-22 12:21:56.184607 UTC] Computing input variables for policy optimization
[2018-12-22 12:21:56.260837 UTC] Performing policy update
[2018-12-22 12:21:56.261635 UTC] Computing gradient in Euclidean space
[2018-12-22 12:21:56.350897 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:21:57.409086 UTC] Performing line search
[2018-12-22 12:21:57.537049 UTC] Updating baseline
[2018-12-22 12:21:58.875475 UTC] Computing logging information
------------------------------------
| Iteration            | 1310      |
| ExpectedImprovement  | 0.016753  |
| ActualImprovement    | 0.016684  |
| ImprovementRatio     | 0.99586   |
| MeanKL               | 0.0078231 |
| Entropy              | -1.3391   |
| Perplexity           | 0.26207   |
| AveragePolicyStd     | 0.19683   |
| AveragePolicyStd[0]  | 0.22355   |
| AveragePolicyStd[1]  | 0.19337   |
| AveragePolicyStd[2]  | 0.15751   |
| AveragePolicyStd[3]  | 0.20681   |
| AveragePolicyStd[4]  | 0.14899   |
| AveragePolicyStd[5]  | 0.25073   |
| AverageReturn        | 1803.6    |
| MinReturn            | 364.07    |
| MaxReturn            | 1943.6    |
| StdReturn            | 233.48    |
| AverageEpisodeLength | 975.81    |
| MinEpisodeLength     | 249       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 116.52    |
| TotalNEpisodes       | 21305     |
| TotalNSamples        | 6.556e+06 |
| ExplainedVariance    | 0.056445  |
------------------------------------
[2018-12-22 12:21:59.302630 UTC] Saving snapshot
[2018-12-22 12:21:59.310860 UTC] Starting iteration 1311
[2018-12-22 12:21:59.311063 UTC] Start collecting samples
[2018-12-22 12:22:02.296521 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:02.374121 UTC] Performing policy update
[2018-12-22 12:22:02.374705 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:02.462823 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:03.513133 UTC] Performing line search
[2018-12-22 12:22:03.639102 UTC] Updating baseline
[2018-12-22 12:22:05.371050 UTC] Computing logging information
-------------------------------------
| Iteration            | 1311       |
| ExpectedImprovement  | 0.019045   |
| ActualImprovement    | 0.018378   |
| ImprovementRatio     | 0.96495    |
| MeanKL               | 0.0072372  |
| Entropy              | -1.3321    |
| Perplexity           | 0.26392    |
| AveragePolicyStd     | 0.19708    |
| AveragePolicyStd[0]  | 0.22404    |
| AveragePolicyStd[1]  | 0.19304    |
| AveragePolicyStd[2]  | 0.15757    |
| AveragePolicyStd[3]  | 0.20701    |
| AveragePolicyStd[4]  | 0.14925    |
| AveragePolicyStd[5]  | 0.25159    |
| AverageReturn        | 1823.9     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 164.41     |
| AverageEpisodeLength | 988.09     |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 80.427     |
| TotalNEpisodes       | 21312      |
| TotalNSamples        | 6.563e+06  |
| ExplainedVariance    | -0.0033729 |
-------------------------------------
[2018-12-22 12:22:05.797692 UTC] Saving snapshot
[2018-12-22 12:22:05.797975 UTC] Starting iteration 1312
[2018-12-22 12:22:05.798095 UTC] Start collecting samples
[2018-12-22 12:22:08.764967 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:08.841475 UTC] Performing policy update
[2018-12-22 12:22:08.842149 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:08.931662 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:09.994983 UTC] Performing line search
[2018-12-22 12:22:10.120912 UTC] Updating baseline
[2018-12-22 12:22:11.644857 UTC] Computing logging information
------------------------------------
| Iteration            | 1312      |
| ExpectedImprovement  | 0.018786  |
| ActualImprovement    | 0.017674  |
| ImprovementRatio     | 0.94082   |
| MeanKL               | 0.007968  |
| Entropy              | -1.334    |
| Perplexity           | 0.26342   |
| AveragePolicyStd     | 0.19703   |
| AveragePolicyStd[0]  | 0.22408   |
| AveragePolicyStd[1]  | 0.19326   |
| AveragePolicyStd[2]  | 0.15708   |
| AveragePolicyStd[3]  | 0.20692   |
| AveragePolicyStd[4]  | 0.14941   |
| AveragePolicyStd[5]  | 0.25141   |
| AverageReturn        | 1824.8    |
| MinReturn            | 364.07    |
| MaxReturn            | 1943.6    |
| StdReturn            | 159.92    |
| AverageEpisodeLength | 989.74    |
| MinEpisodeLength     | 249       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 78.949    |
| TotalNEpisodes       | 21318     |
| TotalNSamples        | 6.569e+06 |
| ExplainedVariance    | 0.001536  |
------------------------------------
[2018-12-22 12:22:12.074200 UTC] Saving snapshot
[2018-12-22 12:22:12.074448 UTC] Starting iteration 1313
[2018-12-22 12:22:12.074585 UTC] Start collecting samples
[2018-12-22 12:22:14.999435 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:15.074949 UTC] Performing policy update
[2018-12-22 12:22:15.075722 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:15.164936 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:16.221851 UTC] Performing line search
[2018-12-22 12:22:16.350305 UTC] Updating baseline
[2018-12-22 12:22:17.577689 UTC] Computing logging information
------------------------------------
| Iteration            | 1313      |
| ExpectedImprovement  | 0.015688  |
| ActualImprovement    | 0.014017  |
| ImprovementRatio     | 0.89346   |
| MeanKL               | 0.0091018 |
| Entropy              | -1.3332   |
| Perplexity           | 0.26362   |
| AveragePolicyStd     | 0.19703   |
| AveragePolicyStd[0]  | 0.22398   |
| AveragePolicyStd[1]  | 0.19319   |
| AveragePolicyStd[2]  | 0.1571    |
| AveragePolicyStd[3]  | 0.20702   |
| AveragePolicyStd[4]  | 0.14962   |
| AveragePolicyStd[5]  | 0.2513    |
| AverageReturn        | 1823.9    |
| MinReturn            | 364.07    |
| MaxReturn            | 1943.6    |
| StdReturn            | 159.64    |
| AverageEpisodeLength | 989.74    |
| MinEpisodeLength     | 249       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 78.949    |
| TotalNEpisodes       | 21321     |
| TotalNSamples        | 6.572e+06 |
| ExplainedVariance    | 0.0020479 |
------------------------------------
[2018-12-22 12:22:18.006304 UTC] Saving snapshot
[2018-12-22 12:22:18.006572 UTC] Starting iteration 1314
[2018-12-22 12:22:18.006695 UTC] Start collecting samples
[2018-12-22 12:22:20.953665 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:21.027590 UTC] Performing policy update
[2018-12-22 12:22:21.028270 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:21.117307 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:22.171905 UTC] Performing line search
[2018-12-22 12:22:22.298212 UTC] Updating baseline
[2018-12-22 12:22:23.684053 UTC] Computing logging information
------------------------------------
| Iteration            | 1314      |
| ExpectedImprovement  | 0.017468  |
| ActualImprovement    | 0.01601   |
| ImprovementRatio     | 0.91653   |
| MeanKL               | 0.008284  |
| Entropy              | -1.33     |
| Perplexity           | 0.26447   |
| AveragePolicyStd     | 0.19717   |
| AveragePolicyStd[0]  | 0.22485   |
| AveragePolicyStd[1]  | 0.19292   |
| AveragePolicyStd[2]  | 0.15704   |
| AveragePolicyStd[3]  | 0.20704   |
| AveragePolicyStd[4]  | 0.14965   |
| AveragePolicyStd[5]  | 0.25151   |
| AverageReturn        | 1824.2    |
| MinReturn            | 364.07    |
| MaxReturn            | 1943.6    |
| StdReturn            | 159.58    |
| AverageEpisodeLength | 989.85    |
| MinEpisodeLength     | 249       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 78.955    |
| TotalNEpisodes       | 21324     |
| TotalNSamples        | 6.575e+06 |
| ExplainedVariance    | 0.0017697 |
------------------------------------
[2018-12-22 12:22:24.114258 UTC] Saving snapshot
[2018-12-22 12:22:24.114520 UTC] Starting iteration 1315
[2018-12-22 12:22:24.114644 UTC] Start collecting samples
[2018-12-22 12:22:27.128237 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:27.207298 UTC] Performing policy update
[2018-12-22 12:22:27.207934 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:27.296825 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:28.342098 UTC] Performing line search
[2018-12-22 12:22:28.469592 UTC] Updating baseline
[2018-12-22 12:22:30.636708 UTC] Computing logging information
--------------------------------------
| Iteration            | 1315        |
| ExpectedImprovement  | 0.018258    |
| ActualImprovement    | 0.016735    |
| ImprovementRatio     | 0.9166      |
| MeanKL               | 0.007769    |
| Entropy              | -1.3321     |
| Perplexity           | 0.26392     |
| AveragePolicyStd     | 0.1971      |
| AveragePolicyStd[0]  | 0.22498     |
| AveragePolicyStd[1]  | 0.19288     |
| AveragePolicyStd[2]  | 0.15684     |
| AveragePolicyStd[3]  | 0.20669     |
| AveragePolicyStd[4]  | 0.14978     |
| AveragePolicyStd[5]  | 0.25141     |
| AverageReturn        | 1819.4      |
| MinReturn            | 364.07      |
| MaxReturn            | 1943.6      |
| StdReturn            | 158.98      |
| AverageEpisodeLength | 989.85      |
| MinEpisodeLength     | 249         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 78.955      |
| TotalNEpisodes       | 21334       |
| TotalNSamples        | 6.585e+06   |
| ExplainedVariance    | -0.00036587 |
--------------------------------------
[2018-12-22 12:22:31.059349 UTC] Saving snapshot
[2018-12-22 12:22:31.059632 UTC] Starting iteration 1316
[2018-12-22 12:22:31.059752 UTC] Start collecting samples
[2018-12-22 12:22:33.987761 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:34.062382 UTC] Performing policy update
[2018-12-22 12:22:34.063203 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:34.151069 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:35.205782 UTC] Performing line search
[2018-12-22 12:22:35.333533 UTC] Updating baseline
[2018-12-22 12:22:36.736890 UTC] Computing logging information
-------------------------------------
| Iteration            | 1316       |
| ExpectedImprovement  | 0.017519   |
| ActualImprovement    | 0.016292   |
| ImprovementRatio     | 0.92997    |
| MeanKL               | 0.008649   |
| Entropy              | -1.3381    |
| Perplexity           | 0.26234    |
| AveragePolicyStd     | 0.19689    |
| AveragePolicyStd[0]  | 0.22479    |
| AveragePolicyStd[1]  | 0.19222    |
| AveragePolicyStd[2]  | 0.15694    |
| AveragePolicyStd[3]  | 0.2063     |
| AveragePolicyStd[4]  | 0.14976    |
| AveragePolicyStd[5]  | 0.25131    |
| AverageReturn        | 1817.9     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 158.72     |
| AverageEpisodeLength | 989.85     |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.955     |
| TotalNEpisodes       | 21337      |
| TotalNSamples        | 6.588e+06  |
| ExplainedVariance    | -0.0055756 |
-------------------------------------
[2018-12-22 12:22:37.161927 UTC] Saving snapshot
[2018-12-22 12:22:37.162214 UTC] Starting iteration 1317
[2018-12-22 12:22:37.162365 UTC] Start collecting samples
[2018-12-22 12:22:40.105975 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:40.180419 UTC] Performing policy update
[2018-12-22 12:22:40.181067 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:40.271099 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:41.325804 UTC] Performing line search
[2018-12-22 12:22:41.454381 UTC] Updating baseline
[2018-12-22 12:22:42.867827 UTC] Computing logging information
-------------------------------------
| Iteration            | 1317       |
| ExpectedImprovement  | 0.019733   |
| ActualImprovement    | 0.018199   |
| ImprovementRatio     | 0.92223    |
| MeanKL               | 0.0074544  |
| Entropy              | -1.3389    |
| Perplexity           | 0.26214    |
| AveragePolicyStd     | 0.19687    |
| AveragePolicyStd[0]  | 0.22451    |
| AveragePolicyStd[1]  | 0.19231    |
| AveragePolicyStd[2]  | 0.15719    |
| AveragePolicyStd[3]  | 0.20622    |
| AveragePolicyStd[4]  | 0.14949    |
| AveragePolicyStd[5]  | 0.25148    |
| AverageReturn        | 1801.8     |
| MinReturn            | 364.07     |
| MaxReturn            | 1943.6     |
| StdReturn            | 191.24     |
| AverageEpisodeLength | 982.19     |
| MinEpisodeLength     | 249        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 94.753     |
| TotalNEpisodes       | 21341      |
| TotalNSamples        | 6.5913e+06 |
| ExplainedVariance    | 0.24069    |
-------------------------------------
[2018-12-22 12:22:43.290371 UTC] Saving snapshot
[2018-12-22 12:22:43.290631 UTC] Starting iteration 1318
[2018-12-22 12:22:43.290757 UTC] Start collecting samples
[2018-12-22 12:22:46.298987 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:46.378016 UTC] Performing policy update
[2018-12-22 12:22:46.378608 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:46.466966 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:47.513451 UTC] Performing line search
[2018-12-22 12:22:47.640050 UTC] Updating baseline
[2018-12-22 12:22:49.034416 UTC] Computing logging information
-------------------------------------
| Iteration            | 1318       |
| ExpectedImprovement  | 0.017315   |
| ActualImprovement    | 0.016794   |
| ImprovementRatio     | 0.96987    |
| MeanKL               | 0.0083038  |
| Entropy              | -1.3432    |
| Perplexity           | 0.26101    |
| AveragePolicyStd     | 0.19675    |
| AveragePolicyStd[0]  | 0.22463    |
| AveragePolicyStd[1]  | 0.19212    |
| AveragePolicyStd[2]  | 0.15724    |
| AveragePolicyStd[3]  | 0.20581    |
| AveragePolicyStd[4]  | 0.14912    |
| AveragePolicyStd[5]  | 0.25155    |
| AverageReturn        | 1811.1     |
| MinReturn            | 980.03     |
| MaxReturn            | 1911.8     |
| StdReturn            | 124.38     |
| AverageEpisodeLength | 989.7      |
| MinEpisodeLength     | 582        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 59.575     |
| TotalNEpisodes       | 21349      |
| TotalNSamples        | 6.5993e+06 |
| ExplainedVariance    | -0.0030326 |
-------------------------------------
[2018-12-22 12:22:49.458173 UTC] Saving snapshot
[2018-12-22 12:22:49.458424 UTC] Starting iteration 1319
[2018-12-22 12:22:49.458555 UTC] Start collecting samples
[2018-12-22 12:22:52.392739 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:52.466641 UTC] Performing policy update
[2018-12-22 12:22:52.467292 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:52.556251 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:53.602848 UTC] Performing line search
[2018-12-22 12:22:53.730259 UTC] Updating baseline
[2018-12-22 12:22:55.206961 UTC] Computing logging information
-------------------------------------
| Iteration            | 1319       |
| ExpectedImprovement  | 0.017116   |
| ActualImprovement    | 0.016684   |
| ImprovementRatio     | 0.97476    |
| MeanKL               | 0.0081     |
| Entropy              | -1.3425    |
| Perplexity           | 0.26118    |
| AveragePolicyStd     | 0.1968     |
| AveragePolicyStd[0]  | 0.22529    |
| AveragePolicyStd[1]  | 0.19193    |
| AveragePolicyStd[2]  | 0.15676    |
| AveragePolicyStd[3]  | 0.20561    |
| AveragePolicyStd[4]  | 0.14938    |
| AveragePolicyStd[5]  | 0.25181    |
| AverageReturn        | 1809.5     |
| MinReturn            | 980.03     |
| MaxReturn            | 1911.8     |
| StdReturn            | 124.4      |
| AverageEpisodeLength | 989.7      |
| MinEpisodeLength     | 582        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 59.575     |
| TotalNEpisodes       | 21352      |
| TotalNSamples        | 6.6023e+06 |
| ExplainedVariance    | -0.018288  |
-------------------------------------
[2018-12-22 12:22:55.631653 UTC] Saving snapshot
[2018-12-22 12:22:55.632048 UTC] Starting iteration 1320
[2018-12-22 12:22:55.632170 UTC] Start collecting samples
[2018-12-22 12:22:58.563822 UTC] Computing input variables for policy optimization
[2018-12-22 12:22:58.637207 UTC] Performing policy update
[2018-12-22 12:22:58.637822 UTC] Computing gradient in Euclidean space
[2018-12-22 12:22:58.727947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:22:59.793139 UTC] Performing line search
[2018-12-22 12:22:59.921088 UTC] Updating baseline
[2018-12-22 12:23:01.159708 UTC] Computing logging information
-------------------------------------
| Iteration            | 1320       |
| ExpectedImprovement  | 0.017661   |
| ActualImprovement    | 0.015758   |
| ImprovementRatio     | 0.89225    |
| MeanKL               | 0.0089046  |
| Entropy              | -1.3407    |
| Perplexity           | 0.26167    |
| AveragePolicyStd     | 0.19686    |
| AveragePolicyStd[0]  | 0.22507    |
| AveragePolicyStd[1]  | 0.19206    |
| AveragePolicyStd[2]  | 0.15678    |
| AveragePolicyStd[3]  | 0.20569    |
| AveragePolicyStd[4]  | 0.14939    |
| AveragePolicyStd[5]  | 0.25219    |
| AverageReturn        | 1807.5     |
| MinReturn            | 980.03     |
| MaxReturn            | 1911.8     |
| StdReturn            | 123.96     |
| AverageEpisodeLength | 989.7      |
| MinEpisodeLength     | 582        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 59.575     |
| TotalNEpisodes       | 21355      |
| TotalNSamples        | 6.6053e+06 |
| ExplainedVariance    | 0.0010936  |
-------------------------------------
[2018-12-22 12:23:01.588236 UTC] Saving snapshot
[2018-12-22 12:23:01.596334 UTC] Starting iteration 1321
[2018-12-22 12:23:01.596671 UTC] Start collecting samples
[2018-12-22 12:23:04.617924 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:04.696836 UTC] Performing policy update
[2018-12-22 12:23:04.697520 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:04.787416 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:05.849289 UTC] Performing line search
[2018-12-22 12:23:05.981802 UTC] Updating baseline
[2018-12-22 12:23:07.300152 UTC] Computing logging information
-------------------------------------
| Iteration            | 1321       |
| ExpectedImprovement  | 0.017215   |
| ActualImprovement    | 0.016855   |
| ImprovementRatio     | 0.97906    |
| MeanKL               | 0.0083142  |
| Entropy              | -1.3422    |
| Perplexity           | 0.26127    |
| AveragePolicyStd     | 0.19685    |
| AveragePolicyStd[0]  | 0.2249     |
| AveragePolicyStd[1]  | 0.19165    |
| AveragePolicyStd[2]  | 0.15636    |
| AveragePolicyStd[3]  | 0.20584    |
| AveragePolicyStd[4]  | 0.14953    |
| AveragePolicyStd[5]  | 0.25281    |
| AverageReturn        | 1804.8     |
| MinReturn            | 980.03     |
| MaxReturn            | 1909.8     |
| StdReturn            | 123.17     |
| AverageEpisodeLength | 989.7      |
| MinEpisodeLength     | 582        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 59.575     |
| TotalNEpisodes       | 21365      |
| TotalNSamples        | 6.6153e+06 |
| ExplainedVariance    | -0.0017073 |
-------------------------------------
[2018-12-22 12:23:07.721246 UTC] Saving snapshot
[2018-12-22 12:23:07.721486 UTC] Starting iteration 1322
[2018-12-22 12:23:07.721626 UTC] Start collecting samples
[2018-12-22 12:23:10.649319 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:10.722375 UTC] Performing policy update
[2018-12-22 12:23:10.723018 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:10.812725 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:11.882444 UTC] Performing line search
[2018-12-22 12:23:12.010235 UTC] Updating baseline
[2018-12-22 12:23:13.741497 UTC] Computing logging information
-------------------------------------
| Iteration            | 1322       |
| ExpectedImprovement  | 0.018716   |
| ActualImprovement    | 0.017474   |
| ImprovementRatio     | 0.93364    |
| MeanKL               | 0.0079363  |
| Entropy              | -1.3437    |
| Perplexity           | 0.26087    |
| AveragePolicyStd     | 0.19681    |
| AveragePolicyStd[0]  | 0.22519    |
| AveragePolicyStd[1]  | 0.19129    |
| AveragePolicyStd[2]  | 0.15662    |
| AveragePolicyStd[3]  | 0.20537    |
| AveragePolicyStd[4]  | 0.14934    |
| AveragePolicyStd[5]  | 0.25306    |
| AverageReturn        | 1805.2     |
| MinReturn            | 980.03     |
| MaxReturn            | 1909.8     |
| StdReturn            | 123.23     |
| AverageEpisodeLength | 989.7      |
| MinEpisodeLength     | 582        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 59.575     |
| TotalNEpisodes       | 21367      |
| TotalNSamples        | 6.6173e+06 |
| ExplainedVariance    | 0.0016385  |
-------------------------------------
[2018-12-22 12:23:14.172166 UTC] Saving snapshot
[2018-12-22 12:23:14.172432 UTC] Starting iteration 1323
[2018-12-22 12:23:14.172570 UTC] Start collecting samples
[2018-12-22 12:23:17.111984 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:17.186231 UTC] Performing policy update
[2018-12-22 12:23:17.187111 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:17.277276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:18.331380 UTC] Performing line search
[2018-12-22 12:23:18.459014 UTC] Updating baseline
[2018-12-22 12:23:20.269259 UTC] Computing logging information
--------------------------------------
| Iteration            | 1323        |
| ExpectedImprovement  | 0.019083    |
| ActualImprovement    | 0.017254    |
| ImprovementRatio     | 0.90414     |
| MeanKL               | 0.0074512   |
| Entropy              | -1.3495     |
| Perplexity           | 0.25936     |
| AveragePolicyStd     | 0.19666     |
| AveragePolicyStd[0]  | 0.22474     |
| AveragePolicyStd[1]  | 0.19129     |
| AveragePolicyStd[2]  | 0.15632     |
| AveragePolicyStd[3]  | 0.20479     |
| AveragePolicyStd[4]  | 0.14903     |
| AveragePolicyStd[5]  | 0.25382     |
| AverageReturn        | 1802.5      |
| MinReturn            | 980.03      |
| MaxReturn            | 1909.8      |
| StdReturn            | 122.4       |
| AverageEpisodeLength | 989.7       |
| MinEpisodeLength     | 582         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 59.575      |
| TotalNEpisodes       | 21370       |
| TotalNSamples        | 6.6203e+06  |
| ExplainedVariance    | -0.00056755 |
--------------------------------------
[2018-12-22 12:23:20.683229 UTC] Saving snapshot
[2018-12-22 12:23:20.683478 UTC] Starting iteration 1324
[2018-12-22 12:23:20.683622 UTC] Start collecting samples
[2018-12-22 12:23:23.685578 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:23.764573 UTC] Performing policy update
[2018-12-22 12:23:23.765387 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:23.857486 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:24.918260 UTC] Performing line search
[2018-12-22 12:23:25.044542 UTC] Updating baseline
[2018-12-22 12:23:26.931845 UTC] Computing logging information
-------------------------------------
| Iteration            | 1324       |
| ExpectedImprovement  | 0.019062   |
| ActualImprovement    | 0.017559   |
| ImprovementRatio     | 0.92115    |
| MeanKL               | 0.0079588  |
| Entropy              | -1.352     |
| Perplexity           | 0.25872    |
| AveragePolicyStd     | 0.19659    |
| AveragePolicyStd[0]  | 0.22431    |
| AveragePolicyStd[1]  | 0.19142    |
| AveragePolicyStd[2]  | 0.15633    |
| AveragePolicyStd[3]  | 0.20451    |
| AveragePolicyStd[4]  | 0.14884    |
| AveragePolicyStd[5]  | 0.25416    |
| AverageReturn        | 1798.7     |
| MinReturn            | 980.03     |
| MaxReturn            | 1909.8     |
| StdReturn            | 122.03     |
| AverageEpisodeLength | 989.7      |
| MinEpisodeLength     | 582        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 59.575     |
| TotalNEpisodes       | 21380      |
| TotalNSamples        | 6.6303e+06 |
| ExplainedVariance    | -0.011996  |
-------------------------------------
[2018-12-22 12:23:27.358260 UTC] Saving snapshot
[2018-12-22 12:23:27.358523 UTC] Starting iteration 1325
[2018-12-22 12:23:27.358658 UTC] Start collecting samples
[2018-12-22 12:23:30.299208 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:30.372984 UTC] Performing policy update
[2018-12-22 12:23:30.373936 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:30.463964 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:31.511705 UTC] Performing line search
[2018-12-22 12:23:31.639391 UTC] Updating baseline
[2018-12-22 12:23:33.027817 UTC] Computing logging information
-------------------------------------
| Iteration            | 1325       |
| ExpectedImprovement  | 0.016875   |
| ActualImprovement    | 0.014939   |
| ImprovementRatio     | 0.8853     |
| MeanKL               | 0.0080243  |
| Entropy              | -1.3501    |
| Perplexity           | 0.25922    |
| AveragePolicyStd     | 0.19667    |
| AveragePolicyStd[0]  | 0.22406    |
| AveragePolicyStd[1]  | 0.19129    |
| AveragePolicyStd[2]  | 0.15671    |
| AveragePolicyStd[3]  | 0.20471    |
| AveragePolicyStd[4]  | 0.14857    |
| AveragePolicyStd[5]  | 0.25469    |
| AverageReturn        | 1781.7     |
| MinReturn            | 501.39     |
| MaxReturn            | 1904.9     |
| StdReturn            | 176.77     |
| AverageEpisodeLength | 982.73     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 90.637     |
| TotalNEpisodes       | 21384      |
| TotalNSamples        | 6.6336e+06 |
| ExplainedVariance    | 0.14018    |
-------------------------------------
[2018-12-22 12:23:33.449432 UTC] Saving snapshot
[2018-12-22 12:23:33.449700 UTC] Starting iteration 1326
[2018-12-22 12:23:33.449858 UTC] Start collecting samples
[2018-12-22 12:23:36.362971 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:36.435134 UTC] Performing policy update
[2018-12-22 12:23:36.435887 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:36.524914 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:37.578200 UTC] Performing line search
[2018-12-22 12:23:37.704399 UTC] Updating baseline
[2018-12-22 12:23:39.815669 UTC] Computing logging information
-------------------------------------
| Iteration            | 1326       |
| ExpectedImprovement  | 0.017895   |
| ActualImprovement    | 0.016834   |
| ImprovementRatio     | 0.94073    |
| MeanKL               | 0.0076695  |
| Entropy              | -1.3583    |
| Perplexity           | 0.2571     |
| AveragePolicyStd     | 0.19643    |
| AveragePolicyStd[0]  | 0.22422    |
| AveragePolicyStd[1]  | 0.19078    |
| AveragePolicyStd[2]  | 0.15657    |
| AveragePolicyStd[3]  | 0.20437    |
| AveragePolicyStd[4]  | 0.14817    |
| AveragePolicyStd[5]  | 0.25446    |
| AverageReturn        | 1785.4     |
| MinReturn            | 501.39     |
| MaxReturn            | 1904.9     |
| StdReturn            | 170.57     |
| AverageEpisodeLength | 985.37     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.191     |
| TotalNEpisodes       | 21386      |
| TotalNSamples        | 6.6356e+06 |
| ExplainedVariance    | -0.020683  |
-------------------------------------
[2018-12-22 12:23:40.243589 UTC] Saving snapshot
[2018-12-22 12:23:40.243873 UTC] Starting iteration 1327
[2018-12-22 12:23:40.243995 UTC] Start collecting samples
[2018-12-22 12:23:43.226909 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:43.302539 UTC] Performing policy update
[2018-12-22 12:23:43.303151 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:43.392395 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:44.458683 UTC] Performing line search
[2018-12-22 12:23:44.585860 UTC] Updating baseline
[2018-12-22 12:23:45.530219 UTC] Computing logging information
-------------------------------------
| Iteration            | 1327       |
| ExpectedImprovement  | 0.018451   |
| ActualImprovement    | 0.017736   |
| ImprovementRatio     | 0.96126    |
| MeanKL               | 0.0077436  |
| Entropy              | -1.3585    |
| Perplexity           | 0.25705    |
| AveragePolicyStd     | 0.19644    |
| AveragePolicyStd[0]  | 0.22499    |
| AveragePolicyStd[1]  | 0.19075    |
| AveragePolicyStd[2]  | 0.15633    |
| AveragePolicyStd[3]  | 0.20417    |
| AveragePolicyStd[4]  | 0.14815    |
| AveragePolicyStd[5]  | 0.25425    |
| AverageReturn        | 1781.2     |
| MinReturn            | 501.39     |
| MaxReturn            | 1873.9     |
| StdReturn            | 169.93     |
| AverageEpisodeLength | 985.37     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.191     |
| TotalNEpisodes       | 21394      |
| TotalNSamples        | 6.6436e+06 |
| ExplainedVariance    | 8.1889e-09 |
-------------------------------------
[2018-12-22 12:23:45.957645 UTC] Saving snapshot
[2018-12-22 12:23:45.957976 UTC] Starting iteration 1328
[2018-12-22 12:23:45.958093 UTC] Start collecting samples
[2018-12-22 12:23:48.915069 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:48.989283 UTC] Performing policy update
[2018-12-22 12:23:48.989929 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:49.079597 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:50.131339 UTC] Performing line search
[2018-12-22 12:23:50.258152 UTC] Updating baseline
[2018-12-22 12:23:51.440628 UTC] Computing logging information
--------------------------------------
| Iteration            | 1328        |
| ExpectedImprovement  | 0.017576    |
| ActualImprovement    | 0.016036    |
| ImprovementRatio     | 0.9124      |
| MeanKL               | 0.0073801   |
| Entropy              | -1.358      |
| Perplexity           | 0.25718     |
| AveragePolicyStd     | 0.19644     |
| AveragePolicyStd[0]  | 0.22494     |
| AveragePolicyStd[1]  | 0.19047     |
| AveragePolicyStd[2]  | 0.15669     |
| AveragePolicyStd[3]  | 0.20407     |
| AveragePolicyStd[4]  | 0.1482      |
| AveragePolicyStd[5]  | 0.25424     |
| AverageReturn        | 1781.8      |
| MinReturn            | 501.39      |
| MaxReturn            | 1873.9      |
| StdReturn            | 170.17      |
| AverageEpisodeLength | 985.37      |
| MinEpisodeLength     | 303         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 87.191      |
| TotalNEpisodes       | 21399       |
| TotalNSamples        | 6.6486e+06  |
| ExplainedVariance    | -1.2072e-08 |
--------------------------------------
[2018-12-22 12:23:51.868438 UTC] Saving snapshot
[2018-12-22 12:23:51.868739 UTC] Starting iteration 1329
[2018-12-22 12:23:51.868870 UTC] Start collecting samples
[2018-12-22 12:23:54.781427 UTC] Computing input variables for policy optimization
[2018-12-22 12:23:54.854101 UTC] Performing policy update
[2018-12-22 12:23:54.854747 UTC] Computing gradient in Euclidean space
[2018-12-22 12:23:54.942896 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:23:56.000548 UTC] Performing line search
[2018-12-22 12:23:56.126703 UTC] Updating baseline
[2018-12-22 12:23:57.397787 UTC] Computing logging information
-------------------------------------
| Iteration            | 1329       |
| ExpectedImprovement  | 0.018601   |
| ActualImprovement    | 0.016886   |
| ImprovementRatio     | 0.9078     |
| MeanKL               | 0.008029   |
| Entropy              | -1.3615    |
| Perplexity           | 0.25627    |
| AveragePolicyStd     | 0.19631    |
| AveragePolicyStd[0]  | 0.22505    |
| AveragePolicyStd[1]  | 0.18985    |
| AveragePolicyStd[2]  | 0.15684    |
| AveragePolicyStd[3]  | 0.20355    |
| AveragePolicyStd[4]  | 0.14829    |
| AveragePolicyStd[5]  | 0.25429    |
| AverageReturn        | 1780.4     |
| MinReturn            | 501.39     |
| MaxReturn            | 1873.9     |
| StdReturn            | 170.31     |
| AverageEpisodeLength | 985.37     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.191     |
| TotalNEpisodes       | 21402      |
| TotalNSamples        | 6.6516e+06 |
| ExplainedVariance    | 7.2933e-08 |
-------------------------------------
[2018-12-22 12:23:57.823165 UTC] Saving snapshot
[2018-12-22 12:23:57.823448 UTC] Starting iteration 1330
[2018-12-22 12:23:57.823584 UTC] Start collecting samples
[2018-12-22 12:24:00.781277 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:00.856903 UTC] Performing policy update
[2018-12-22 12:24:00.857513 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:00.947206 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:02.002167 UTC] Performing line search
[2018-12-22 12:24:02.130122 UTC] Updating baseline
[2018-12-22 12:24:07.573407 UTC] Computing logging information
-------------------------------------
| Iteration            | 1330       |
| ExpectedImprovement  | 0.015753   |
| ActualImprovement    | 0.015027   |
| ImprovementRatio     | 0.95395    |
| MeanKL               | 0.0095441  |
| Entropy              | -1.3624    |
| Perplexity           | 0.25604    |
| AveragePolicyStd     | 0.1963     |
| AveragePolicyStd[0]  | 0.22521    |
| AveragePolicyStd[1]  | 0.18969    |
| AveragePolicyStd[2]  | 0.1568     |
| AveragePolicyStd[3]  | 0.20342    |
| AveragePolicyStd[4]  | 0.14824    |
| AveragePolicyStd[5]  | 0.25441    |
| AverageReturn        | 1779.3     |
| MinReturn            | 501.39     |
| MaxReturn            | 1869.2     |
| StdReturn            | 169.98     |
| AverageEpisodeLength | 985.37     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.191     |
| TotalNEpisodes       | 21408      |
| TotalNSamples        | 6.6576e+06 |
| ExplainedVariance    | 5.7643e-08 |
-------------------------------------
[2018-12-22 12:24:08.000159 UTC] Saving snapshot
[2018-12-22 12:24:08.008166 UTC] Starting iteration 1331
[2018-12-22 12:24:08.008360 UTC] Start collecting samples
[2018-12-22 12:24:10.975276 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:11.050579 UTC] Performing policy update
[2018-12-22 12:24:11.051242 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:11.140120 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:12.249357 UTC] Performing line search
[2018-12-22 12:24:12.383185 UTC] Updating baseline
[2018-12-22 12:24:13.738397 UTC] Computing logging information
-------------------------------------
| Iteration            | 1331       |
| ExpectedImprovement  | 0.015604   |
| ActualImprovement    | 0.01515    |
| ImprovementRatio     | 0.97092    |
| MeanKL               | 0.0093227  |
| Entropy              | -1.3561    |
| Perplexity           | 0.25766    |
| AveragePolicyStd     | 0.19654    |
| AveragePolicyStd[0]  | 0.22534    |
| AveragePolicyStd[1]  | 0.19004    |
| AveragePolicyStd[2]  | 0.15675    |
| AveragePolicyStd[3]  | 0.20375    |
| AveragePolicyStd[4]  | 0.14821    |
| AveragePolicyStd[5]  | 0.25514    |
| AverageReturn        | 1778.7     |
| MinReturn            | 501.39     |
| MaxReturn            | 1869.2     |
| StdReturn            | 169.87     |
| AverageEpisodeLength | 985.37     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.191     |
| TotalNEpisodes       | 21414      |
| TotalNSamples        | 6.6636e+06 |
| ExplainedVariance    | 0.00020915 |
-------------------------------------
[2018-12-22 12:24:14.202319 UTC] Saving snapshot
[2018-12-22 12:24:14.202629 UTC] Starting iteration 1332
[2018-12-22 12:24:14.202761 UTC] Start collecting samples
[2018-12-22 12:24:17.346694 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:17.420916 UTC] Performing policy update
[2018-12-22 12:24:17.421672 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:17.511583 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:18.574069 UTC] Performing line search
[2018-12-22 12:24:18.700968 UTC] Updating baseline
[2018-12-22 12:24:20.346746 UTC] Computing logging information
-------------------------------------
| Iteration            | 1332       |
| ExpectedImprovement  | 0.017573   |
| ActualImprovement    | 0.016658   |
| ImprovementRatio     | 0.9479     |
| MeanKL               | 0.0075466  |
| Entropy              | -1.3532    |
| Perplexity           | 0.2584     |
| AveragePolicyStd     | 0.19663    |
| AveragePolicyStd[0]  | 0.22574    |
| AveragePolicyStd[1]  | 0.18977    |
| AveragePolicyStd[2]  | 0.15708    |
| AveragePolicyStd[3]  | 0.2039     |
| AveragePolicyStd[4]  | 0.14813    |
| AveragePolicyStd[5]  | 0.2552     |
| AverageReturn        | 1777       |
| MinReturn            | 501.39     |
| MaxReturn            | 1869.2     |
| StdReturn            | 169.35     |
| AverageEpisodeLength | 985.36     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.19      |
| TotalNEpisodes       | 21418      |
| TotalNSamples        | 6.6676e+06 |
| ExplainedVariance    | 0.15446    |
-------------------------------------
[2018-12-22 12:24:20.770492 UTC] Saving snapshot
[2018-12-22 12:24:20.770761 UTC] Starting iteration 1333
[2018-12-22 12:24:20.770882 UTC] Start collecting samples
[2018-12-22 12:24:23.708718 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:23.784260 UTC] Performing policy update
[2018-12-22 12:24:23.784942 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:23.874794 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:24.925691 UTC] Performing line search
[2018-12-22 12:24:25.053589 UTC] Updating baseline
[2018-12-22 12:24:26.859834 UTC] Computing logging information
--------------------------------------
| Iteration            | 1333        |
| ExpectedImprovement  | 0.016952    |
| ActualImprovement    | 0.015758    |
| ImprovementRatio     | 0.92954     |
| MeanKL               | 0.0083443   |
| Entropy              | -1.3534     |
| Perplexity           | 0.25836     |
| AveragePolicyStd     | 0.19665     |
| AveragePolicyStd[0]  | 0.22574     |
| AveragePolicyStd[1]  | 0.18969     |
| AveragePolicyStd[2]  | 0.15698     |
| AveragePolicyStd[3]  | 0.20417     |
| AveragePolicyStd[4]  | 0.14797     |
| AveragePolicyStd[5]  | 0.25536     |
| AverageReturn        | 1775        |
| MinReturn            | 501.39      |
| MaxReturn            | 1869.2      |
| StdReturn            | 168.85      |
| AverageEpisodeLength | 985.36      |
| MinEpisodeLength     | 303         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 87.19       |
| TotalNEpisodes       | 21422       |
| TotalNSamples        | 6.6716e+06  |
| ExplainedVariance    | -0.00035573 |
--------------------------------------
[2018-12-22 12:24:27.282445 UTC] Saving snapshot
[2018-12-22 12:24:27.282705 UTC] Starting iteration 1334
[2018-12-22 12:24:27.282823 UTC] Start collecting samples
[2018-12-22 12:24:30.276078 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:30.352812 UTC] Performing policy update
[2018-12-22 12:24:30.353794 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:30.446776 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:31.504814 UTC] Performing line search
[2018-12-22 12:24:31.632939 UTC] Updating baseline
[2018-12-22 12:24:32.840159 UTC] Computing logging information
-------------------------------------
| Iteration            | 1334       |
| ExpectedImprovement  | 0.018463   |
| ActualImprovement    | 0.017707   |
| ImprovementRatio     | 0.95905    |
| MeanKL               | 0.0079121  |
| Entropy              | -1.3567    |
| Perplexity           | 0.25751    |
| AveragePolicyStd     | 0.19652    |
| AveragePolicyStd[0]  | 0.22522    |
| AveragePolicyStd[1]  | 0.18987    |
| AveragePolicyStd[2]  | 0.15698    |
| AveragePolicyStd[3]  | 0.20417    |
| AveragePolicyStd[4]  | 0.14789    |
| AveragePolicyStd[5]  | 0.25499    |
| AverageReturn        | 1770.5     |
| MinReturn            | 501.39     |
| MaxReturn            | 1869.2     |
| StdReturn            | 168.5      |
| AverageEpisodeLength | 984.71     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.32      |
| TotalNEpisodes       | 21429      |
| TotalNSamples        | 6.6785e+06 |
| ExplainedVariance    | 0.15684    |
-------------------------------------
[2018-12-22 12:24:33.263822 UTC] Saving snapshot
[2018-12-22 12:24:33.264067 UTC] Starting iteration 1335
[2018-12-22 12:24:33.264183 UTC] Start collecting samples
[2018-12-22 12:24:36.218873 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:36.294140 UTC] Performing policy update
[2018-12-22 12:24:36.294796 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:36.384063 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:37.430851 UTC] Performing line search
[2018-12-22 12:24:37.562855 UTC] Updating baseline
[2018-12-22 12:24:40.515180 UTC] Computing logging information
-------------------------------------
| Iteration            | 1335       |
| ExpectedImprovement  | 0.016983   |
| ActualImprovement    | 0.017542   |
| ImprovementRatio     | 1.0329     |
| MeanKL               | 0.0080067  |
| Entropy              | -1.3605    |
| Perplexity           | 0.25653    |
| AveragePolicyStd     | 0.19643    |
| AveragePolicyStd[0]  | 0.22522    |
| AveragePolicyStd[1]  | 0.19011    |
| AveragePolicyStd[2]  | 0.15727    |
| AveragePolicyStd[3]  | 0.20359    |
| AveragePolicyStd[4]  | 0.14717    |
| AveragePolicyStd[5]  | 0.25521    |
| AverageReturn        | 1771       |
| MinReturn            | 501.39     |
| MaxReturn            | 1869.2     |
| StdReturn            | 168.68     |
| AverageEpisodeLength | 984.71     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.32      |
| TotalNEpisodes       | 21433      |
| TotalNSamples        | 6.6825e+06 |
| ExplainedVariance    | -0.079536  |
-------------------------------------
[2018-12-22 12:24:40.963435 UTC] Saving snapshot
[2018-12-22 12:24:40.963811 UTC] Starting iteration 1336
[2018-12-22 12:24:40.963933 UTC] Start collecting samples
[2018-12-22 12:24:44.111305 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:44.189330 UTC] Performing policy update
[2018-12-22 12:24:44.190419 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:44.283936 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:45.385637 UTC] Performing line search
[2018-12-22 12:24:45.518901 UTC] Updating baseline
[2018-12-22 12:24:47.533545 UTC] Computing logging information
--------------------------------------
| Iteration            | 1336        |
| ExpectedImprovement  | 0.017321    |
| ActualImprovement    | 0.015838    |
| ImprovementRatio     | 0.91439     |
| MeanKL               | 0.0082287   |
| Entropy              | -1.363      |
| Perplexity           | 0.2559      |
| AveragePolicyStd     | 0.19634     |
| AveragePolicyStd[0]  | 0.22502     |
| AveragePolicyStd[1]  | 0.19049     |
| AveragePolicyStd[2]  | 0.15672     |
| AveragePolicyStd[3]  | 0.20353     |
| AveragePolicyStd[4]  | 0.14738     |
| AveragePolicyStd[5]  | 0.25491     |
| AverageReturn        | 1771.7      |
| MinReturn            | 501.39      |
| MaxReturn            | 1869.2      |
| StdReturn            | 168.79      |
| AverageEpisodeLength | 984.71      |
| MinEpisodeLength     | 303         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 87.32       |
| TotalNEpisodes       | 21436       |
| TotalNSamples        | 6.6855e+06  |
| ExplainedVariance    | -5.5696e-06 |
--------------------------------------
[2018-12-22 12:24:47.990843 UTC] Saving snapshot
[2018-12-22 12:24:47.991153 UTC] Starting iteration 1337
[2018-12-22 12:24:47.991272 UTC] Start collecting samples
[2018-12-22 12:24:51.041603 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:51.119258 UTC] Performing policy update
[2018-12-22 12:24:51.119963 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:51.209089 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:52.272874 UTC] Performing line search
[2018-12-22 12:24:52.399462 UTC] Updating baseline
[2018-12-22 12:24:54.167917 UTC] Computing logging information
-------------------------------------
| Iteration            | 1337       |
| ExpectedImprovement  | 0.01766    |
| ActualImprovement    | 0.016786   |
| ImprovementRatio     | 0.95052    |
| MeanKL               | 0.0077268  |
| Entropy              | -1.3746    |
| Perplexity           | 0.25295    |
| AveragePolicyStd     | 0.19595    |
| AveragePolicyStd[0]  | 0.22527    |
| AveragePolicyStd[1]  | 0.18983    |
| AveragePolicyStd[2]  | 0.15664    |
| AveragePolicyStd[3]  | 0.20339    |
| AveragePolicyStd[4]  | 0.14688    |
| AveragePolicyStd[5]  | 0.25371    |
| AverageReturn        | 1788.6     |
| MinReturn            | 501.39     |
| MaxReturn            | 1875.6     |
| StdReturn            | 134.74     |
| AverageEpisodeLength | 992.37     |
| MinEpisodeLength     | 303        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 69.585     |
| TotalNEpisodes       | 21445      |
| TotalNSamples        | 6.6945e+06 |
| ExplainedVariance    | -5.043e-09 |
-------------------------------------
[2018-12-22 12:24:54.593253 UTC] Saving snapshot
[2018-12-22 12:24:54.593538 UTC] Starting iteration 1338
[2018-12-22 12:24:54.593667 UTC] Start collecting samples
[2018-12-22 12:24:57.520786 UTC] Computing input variables for policy optimization
[2018-12-22 12:24:57.594732 UTC] Performing policy update
[2018-12-22 12:24:57.595357 UTC] Computing gradient in Euclidean space
[2018-12-22 12:24:57.684911 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:24:58.742296 UTC] Performing line search
[2018-12-22 12:24:58.869253 UTC] Updating baseline
[2018-12-22 12:25:00.830519 UTC] Computing logging information
-------------------------------------
| Iteration            | 1338       |
| ExpectedImprovement  | 0.0196     |
| ActualImprovement    | 0.018044   |
| ImprovementRatio     | 0.92062    |
| MeanKL               | 0.007153   |
| Entropy              | -1.3738    |
| Perplexity           | 0.25313    |
| AveragePolicyStd     | 0.19598    |
| AveragePolicyStd[0]  | 0.22601    |
| AveragePolicyStd[1]  | 0.18997    |
| AveragePolicyStd[2]  | 0.15648    |
| AveragePolicyStd[3]  | 0.20281    |
| AveragePolicyStd[4]  | 0.14707    |
| AveragePolicyStd[5]  | 0.25355    |
| AverageReturn        | 1775.3     |
| MinReturn            | 320.1      |
| MaxReturn            | 1875.6     |
| StdReturn            | 198.84     |
| AverageEpisodeLength | 984.39     |
| MinEpisodeLength     | 202        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105        |
| TotalNEpisodes       | 21449      |
| TotalNSamples        | 6.6977e+06 |
| ExplainedVariance    | 0.11216    |
-------------------------------------
[2018-12-22 12:25:01.258425 UTC] Saving snapshot
[2018-12-22 12:25:01.258736 UTC] Starting iteration 1339
[2018-12-22 12:25:01.258860 UTC] Start collecting samples
[2018-12-22 12:25:04.178859 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:04.253852 UTC] Performing policy update
[2018-12-22 12:25:04.254622 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:04.345122 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:05.400746 UTC] Performing line search
[2018-12-22 12:25:05.529344 UTC] Updating baseline
[2018-12-22 12:25:06.824280 UTC] Computing logging information
-------------------------------------
| Iteration            | 1339       |
| ExpectedImprovement  | 0.017175   |
| ActualImprovement    | 0.016101   |
| ImprovementRatio     | 0.93746    |
| MeanKL               | 0.0086022  |
| Entropy              | -1.3803    |
| Perplexity           | 0.2515     |
| AveragePolicyStd     | 0.19575    |
| AveragePolicyStd[0]  | 0.22616    |
| AveragePolicyStd[1]  | 0.19028    |
| AveragePolicyStd[2]  | 0.15651    |
| AveragePolicyStd[3]  | 0.20186    |
| AveragePolicyStd[4]  | 0.14683    |
| AveragePolicyStd[5]  | 0.25286    |
| AverageReturn        | 1776       |
| MinReturn            | 320.1      |
| MaxReturn            | 1875.6     |
| StdReturn            | 198.92     |
| AverageEpisodeLength | 984.39     |
| MinEpisodeLength     | 202        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105        |
| TotalNEpisodes       | 21452      |
| TotalNSamples        | 6.7007e+06 |
| ExplainedVariance    | -0.032191  |
-------------------------------------
[2018-12-22 12:25:07.245147 UTC] Saving snapshot
[2018-12-22 12:25:07.245473 UTC] Starting iteration 1340
[2018-12-22 12:25:07.245626 UTC] Start collecting samples
[2018-12-22 12:25:10.274820 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:10.352912 UTC] Performing policy update
[2018-12-22 12:25:10.353496 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:10.443487 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:11.498935 UTC] Performing line search
[2018-12-22 12:25:11.628088 UTC] Updating baseline
[2018-12-22 12:25:12.834990 UTC] Computing logging information
--------------------------------------
| Iteration            | 1340        |
| ExpectedImprovement  | 0.018321    |
| ActualImprovement    | 0.017325    |
| ImprovementRatio     | 0.94567     |
| MeanKL               | 0.0083273   |
| Entropy              | -1.3855     |
| Perplexity           | 0.2502      |
| AveragePolicyStd     | 0.19558     |
| AveragePolicyStd[0]  | 0.22673     |
| AveragePolicyStd[1]  | 0.19065     |
| AveragePolicyStd[2]  | 0.15584     |
| AveragePolicyStd[3]  | 0.20069     |
| AveragePolicyStd[4]  | 0.14719     |
| AveragePolicyStd[5]  | 0.25237     |
| AverageReturn        | 1776.3      |
| MinReturn            | 320.1       |
| MaxReturn            | 1882.6      |
| StdReturn            | 199.28      |
| AverageEpisodeLength | 984.39      |
| MinEpisodeLength     | 202         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 105         |
| TotalNEpisodes       | 21461       |
| TotalNSamples        | 6.7097e+06  |
| ExplainedVariance    | -3.6306e-08 |
--------------------------------------
[2018-12-22 12:25:13.263153 UTC] Saving snapshot
[2018-12-22 12:25:13.271229 UTC] Starting iteration 1341
[2018-12-22 12:25:13.271418 UTC] Start collecting samples
[2018-12-22 12:25:16.214086 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:16.289276 UTC] Performing policy update
[2018-12-22 12:25:16.289967 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:16.379848 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:17.439867 UTC] Performing line search
[2018-12-22 12:25:17.567188 UTC] Updating baseline
[2018-12-22 12:25:18.759800 UTC] Computing logging information
-------------------------------------
| Iteration            | 1341       |
| ExpectedImprovement  | 0.017309   |
| ActualImprovement    | 0.016372   |
| ImprovementRatio     | 0.94587    |
| MeanKL               | 0.0090347  |
| Entropy              | -1.3814    |
| Perplexity           | 0.25124    |
| AveragePolicyStd     | 0.19575    |
| AveragePolicyStd[0]  | 0.22695    |
| AveragePolicyStd[1]  | 0.19047    |
| AveragePolicyStd[2]  | 0.1559     |
| AveragePolicyStd[3]  | 0.20107    |
| AveragePolicyStd[4]  | 0.14709    |
| AveragePolicyStd[5]  | 0.25301    |
| AverageReturn        | 1777.3     |
| MinReturn            | 320.1      |
| MaxReturn            | 1890       |
| StdReturn            | 199.56     |
| AverageEpisodeLength | 984.39     |
| MinEpisodeLength     | 202        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105        |
| TotalNEpisodes       | 21465      |
| TotalNSamples        | 6.7137e+06 |
| ExplainedVariance    | -2.021e-08 |
-------------------------------------
[2018-12-22 12:25:19.187708 UTC] Saving snapshot
[2018-12-22 12:25:19.187957 UTC] Starting iteration 1342
[2018-12-22 12:25:19.188077 UTC] Start collecting samples
[2018-12-22 12:25:22.120640 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:22.194224 UTC] Performing policy update
[2018-12-22 12:25:22.194838 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:22.283923 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:23.330003 UTC] Performing line search
[2018-12-22 12:25:23.456232 UTC] Updating baseline
[2018-12-22 12:25:24.989951 UTC] Computing logging information
--------------------------------------
| Iteration            | 1342        |
| ExpectedImprovement  | 0.016961    |
| ActualImprovement    | 0.015619    |
| ImprovementRatio     | 0.92086     |
| MeanKL               | 0.0083832   |
| Entropy              | -1.3805     |
| Perplexity           | 0.25146     |
| AveragePolicyStd     | 0.1957      |
| AveragePolicyStd[0]  | 0.22655     |
| AveragePolicyStd[1]  | 0.19076     |
| AveragePolicyStd[2]  | 0.15644     |
| AveragePolicyStd[3]  | 0.20128     |
| AveragePolicyStd[4]  | 0.14718     |
| AveragePolicyStd[5]  | 0.252       |
| AverageReturn        | 1777.5      |
| MinReturn            | 320.1       |
| MaxReturn            | 1890        |
| StdReturn            | 199.62      |
| AverageEpisodeLength | 984.39      |
| MinEpisodeLength     | 202         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 105         |
| TotalNEpisodes       | 21467       |
| TotalNSamples        | 6.7157e+06  |
| ExplainedVariance    | -1.9658e-08 |
--------------------------------------
[2018-12-22 12:25:25.415180 UTC] Saving snapshot
[2018-12-22 12:25:25.415449 UTC] Starting iteration 1343
[2018-12-22 12:25:25.415584 UTC] Start collecting samples
[2018-12-22 12:25:28.416816 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:28.494475 UTC] Performing policy update
[2018-12-22 12:25:28.495109 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:28.585638 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:29.646430 UTC] Performing line search
[2018-12-22 12:25:29.778005 UTC] Updating baseline
[2018-12-22 12:25:31.069807 UTC] Computing logging information
-------------------------------------
| Iteration            | 1343       |
| ExpectedImprovement  | 0.016144   |
| ActualImprovement    | 0.015322   |
| ImprovementRatio     | 0.9491     |
| MeanKL               | 0.0085916  |
| Entropy              | -1.3771    |
| Perplexity           | 0.25231    |
| AveragePolicyStd     | 0.19584    |
| AveragePolicyStd[0]  | 0.22667    |
| AveragePolicyStd[1]  | 0.19128    |
| AveragePolicyStd[2]  | 0.1564     |
| AveragePolicyStd[3]  | 0.20157    |
| AveragePolicyStd[4]  | 0.14685    |
| AveragePolicyStd[5]  | 0.25229    |
| AverageReturn        | 1763.6     |
| MinReturn            | 96.209     |
| MaxReturn            | 1907.6     |
| StdReturn            | 261.3      |
| AverageEpisodeLength | 975.16     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.46     |
| TotalNEpisodes       | 21475      |
| TotalNSamples        | 6.7228e+06 |
| ExplainedVariance    | 0.062345   |
-------------------------------------
[2018-12-22 12:25:31.492015 UTC] Saving snapshot
[2018-12-22 12:25:31.492266 UTC] Starting iteration 1344
[2018-12-22 12:25:31.492397 UTC] Start collecting samples
[2018-12-22 12:25:34.465178 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:34.540718 UTC] Performing policy update
[2018-12-22 12:25:34.541462 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:34.629526 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:35.676740 UTC] Performing line search
[2018-12-22 12:25:35.803425 UTC] Updating baseline
[2018-12-22 12:25:38.652530 UTC] Computing logging information
-------------------------------------
| Iteration            | 1344       |
| ExpectedImprovement  | 0.017203   |
| ActualImprovement    | 0.016498   |
| ImprovementRatio     | 0.95906    |
| MeanKL               | 0.0083977  |
| Entropy              | -1.3775    |
| Perplexity           | 0.25221    |
| AveragePolicyStd     | 0.19582    |
| AveragePolicyStd[0]  | 0.22663    |
| AveragePolicyStd[1]  | 0.19091    |
| AveragePolicyStd[2]  | 0.15673    |
| AveragePolicyStd[3]  | 0.201      |
| AveragePolicyStd[4]  | 0.14704    |
| AveragePolicyStd[5]  | 0.25258    |
| AverageReturn        | 1767.4     |
| MinReturn            | 96.209     |
| MaxReturn            | 1918.4     |
| StdReturn            | 262.31     |
| AverageEpisodeLength | 975.16     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.46     |
| TotalNEpisodes       | 21481      |
| TotalNSamples        | 6.7288e+06 |
| ExplainedVariance    | -0.035149  |
-------------------------------------
[2018-12-22 12:25:39.078312 UTC] Saving snapshot
[2018-12-22 12:25:39.078604 UTC] Starting iteration 1345
[2018-12-22 12:25:39.078725 UTC] Start collecting samples
[2018-12-22 12:25:42.028513 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:42.105280 UTC] Performing policy update
[2018-12-22 12:25:42.106002 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:42.197998 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:43.254471 UTC] Performing line search
[2018-12-22 12:25:43.381909 UTC] Updating baseline
[2018-12-22 12:25:44.804001 UTC] Computing logging information
--------------------------------------
| Iteration            | 1345        |
| ExpectedImprovement  | 0.019645    |
| ActualImprovement    | 0.017997    |
| ImprovementRatio     | 0.91612     |
| MeanKL               | 0.0070484   |
| Entropy              | -1.3797     |
| Perplexity           | 0.25165     |
| AveragePolicyStd     | 0.19577     |
| AveragePolicyStd[0]  | 0.2264      |
| AveragePolicyStd[1]  | 0.19069     |
| AveragePolicyStd[2]  | 0.15649     |
| AveragePolicyStd[3]  | 0.20109     |
| AveragePolicyStd[4]  | 0.14695     |
| AveragePolicyStd[5]  | 0.25302     |
| AverageReturn        | 1782        |
| MinReturn            | 96.209      |
| MaxReturn            | 1918.4      |
| StdReturn            | 229.5       |
| AverageEpisodeLength | 982.13      |
| MinEpisodeLength     | 77          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 120.87      |
| TotalNEpisodes       | 21484       |
| TotalNSamples        | 6.7318e+06  |
| ExplainedVariance    | -3.3476e-07 |
--------------------------------------
[2018-12-22 12:25:45.223841 UTC] Saving snapshot
[2018-12-22 12:25:45.224154 UTC] Starting iteration 1346
[2018-12-22 12:25:45.224275 UTC] Start collecting samples
[2018-12-22 12:25:48.199048 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:48.274758 UTC] Performing policy update
[2018-12-22 12:25:48.275382 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:48.364616 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:49.415702 UTC] Performing line search
[2018-12-22 12:25:49.542685 UTC] Updating baseline
[2018-12-22 12:25:50.991014 UTC] Computing logging information
-------------------------------------
| Iteration            | 1346       |
| ExpectedImprovement  | 0.016147   |
| ActualImprovement    | 0.015257   |
| ImprovementRatio     | 0.94483    |
| MeanKL               | 0.0085096  |
| Entropy              | -1.3847    |
| Perplexity           | 0.2504     |
| AveragePolicyStd     | 0.19559    |
| AveragePolicyStd[0]  | 0.22611    |
| AveragePolicyStd[1]  | 0.19017    |
| AveragePolicyStd[2]  | 0.15643    |
| AveragePolicyStd[3]  | 0.20115    |
| AveragePolicyStd[4]  | 0.14701    |
| AveragePolicyStd[5]  | 0.25268    |
| AverageReturn        | 1787.1     |
| MinReturn            | 96.209     |
| MaxReturn            | 1922       |
| StdReturn            | 230.52     |
| AverageEpisodeLength | 982.13     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.87     |
| TotalNEpisodes       | 21490      |
| TotalNSamples        | 6.7378e+06 |
| ExplainedVariance    | 8.9686e-07 |
-------------------------------------
[2018-12-22 12:25:51.416763 UTC] Saving snapshot
[2018-12-22 12:25:51.417010 UTC] Starting iteration 1347
[2018-12-22 12:25:51.417124 UTC] Start collecting samples
[2018-12-22 12:25:54.398677 UTC] Computing input variables for policy optimization
[2018-12-22 12:25:54.475142 UTC] Performing policy update
[2018-12-22 12:25:54.476012 UTC] Computing gradient in Euclidean space
[2018-12-22 12:25:54.565689 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:25:55.629075 UTC] Performing line search
[2018-12-22 12:25:55.760492 UTC] Updating baseline
[2018-12-22 12:25:56.808357 UTC] Computing logging information
-------------------------------------
| Iteration            | 1347       |
| ExpectedImprovement  | 0.016816   |
| ActualImprovement    | 0.01614    |
| ImprovementRatio     | 0.95976    |
| MeanKL               | 0.0090829  |
| Entropy              | -1.3889    |
| Perplexity           | 0.24934    |
| AveragePolicyStd     | 0.19549    |
| AveragePolicyStd[0]  | 0.22621    |
| AveragePolicyStd[1]  | 0.18991    |
| AveragePolicyStd[2]  | 0.1559     |
| AveragePolicyStd[3]  | 0.20128    |
| AveragePolicyStd[4]  | 0.14685    |
| AveragePolicyStd[5]  | 0.25281    |
| AverageReturn        | 1788.6     |
| MinReturn            | 96.209     |
| MaxReturn            | 1922       |
| StdReturn            | 230.57     |
| AverageEpisodeLength | 982.13     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.87     |
| TotalNEpisodes       | 21496      |
| TotalNSamples        | 6.7438e+06 |
| ExplainedVariance    | 6.7011e-09 |
-------------------------------------
[2018-12-22 12:25:57.236851 UTC] Saving snapshot
[2018-12-22 12:25:57.237099 UTC] Starting iteration 1348
[2018-12-22 12:25:57.237231 UTC] Start collecting samples
[2018-12-22 12:26:00.178535 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:00.253724 UTC] Performing policy update
[2018-12-22 12:26:00.254381 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:00.342693 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:01.402012 UTC] Performing line search
[2018-12-22 12:26:01.530125 UTC] Updating baseline
[2018-12-22 12:26:03.612263 UTC] Computing logging information
-------------------------------------
| Iteration            | 1348       |
| ExpectedImprovement  | 0.020556   |
| ActualImprovement    | 0.017862   |
| ImprovementRatio     | 0.86892    |
| MeanKL               | 0.0073584  |
| Entropy              | -1.3903    |
| Perplexity           | 0.24899    |
| AveragePolicyStd     | 0.19544    |
| AveragePolicyStd[0]  | 0.22613    |
| AveragePolicyStd[1]  | 0.18936    |
| AveragePolicyStd[2]  | 0.1559     |
| AveragePolicyStd[3]  | 0.20158    |
| AveragePolicyStd[4]  | 0.14699    |
| AveragePolicyStd[5]  | 0.25266    |
| AverageReturn        | 1791.4     |
| MinReturn            | 96.209     |
| MaxReturn            | 1922       |
| StdReturn            | 230.77     |
| AverageEpisodeLength | 982.13     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.87     |
| TotalNEpisodes       | 21500      |
| TotalNSamples        | 6.7478e+06 |
| ExplainedVariance    | 7.143e-06  |
-------------------------------------
[2018-12-22 12:26:04.042495 UTC] Saving snapshot
[2018-12-22 12:26:04.042749 UTC] Starting iteration 1349
[2018-12-22 12:26:04.042870 UTC] Start collecting samples
[2018-12-22 12:26:06.970338 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:07.045392 UTC] Performing policy update
[2018-12-22 12:26:07.046125 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:07.137499 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:08.198458 UTC] Performing line search
[2018-12-22 12:26:08.325447 UTC] Updating baseline
[2018-12-22 12:26:11.912771 UTC] Computing logging information
-------------------------------------
| Iteration            | 1349       |
| ExpectedImprovement  | 0.018093   |
| ActualImprovement    | 0.016756   |
| ImprovementRatio     | 0.92611    |
| MeanKL               | 0.0075887  |
| Entropy              | -1.3921    |
| Perplexity           | 0.24856    |
| AveragePolicyStd     | 0.19536    |
| AveragePolicyStd[0]  | 0.22591    |
| AveragePolicyStd[1]  | 0.18951    |
| AveragePolicyStd[2]  | 0.1561     |
| AveragePolicyStd[3]  | 0.20128    |
| AveragePolicyStd[4]  | 0.14695    |
| AveragePolicyStd[5]  | 0.25238    |
| AverageReturn        | 1792.9     |
| MinReturn            | 96.209     |
| MaxReturn            | 1922       |
| StdReturn            | 230.92     |
| AverageEpisodeLength | 982.13     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.87     |
| TotalNEpisodes       | 21503      |
| TotalNSamples        | 6.7508e+06 |
| ExplainedVariance    | -0.0086573 |
-------------------------------------
[2018-12-22 12:26:12.342055 UTC] Saving snapshot
[2018-12-22 12:26:12.342323 UTC] Starting iteration 1350
[2018-12-22 12:26:12.342463 UTC] Start collecting samples
[2018-12-22 12:26:15.349310 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:15.428893 UTC] Performing policy update
[2018-12-22 12:26:15.429696 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:15.519473 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:16.587789 UTC] Performing line search
[2018-12-22 12:26:16.717367 UTC] Updating baseline
[2018-12-22 12:26:18.605523 UTC] Computing logging information
-------------------------------------
| Iteration            | 1350       |
| ExpectedImprovement  | 0.016735   |
| ActualImprovement    | 0.016725   |
| ImprovementRatio     | 0.99938    |
| MeanKL               | 0.0082347  |
| Entropy              | -1.3916    |
| Perplexity           | 0.24867    |
| AveragePolicyStd     | 0.19531    |
| AveragePolicyStd[0]  | 0.22515    |
| AveragePolicyStd[1]  | 0.1895     |
| AveragePolicyStd[2]  | 0.15655    |
| AveragePolicyStd[3]  | 0.20128    |
| AveragePolicyStd[4]  | 0.14722    |
| AveragePolicyStd[5]  | 0.25217    |
| AverageReturn        | 1801.3     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 232.87     |
| AverageEpisodeLength | 982.13     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.87     |
| TotalNEpisodes       | 21512      |
| TotalNSamples        | 6.7598e+06 |
| ExplainedVariance    | -0.008592  |
-------------------------------------
[2018-12-22 12:26:19.032134 UTC] Saving snapshot
[2018-12-22 12:26:19.040594 UTC] Starting iteration 1351
[2018-12-22 12:26:19.040795 UTC] Start collecting samples
[2018-12-22 12:26:21.983623 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:22.057897 UTC] Performing policy update
[2018-12-22 12:26:22.058892 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:22.148445 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:23.202429 UTC] Performing line search
[2018-12-22 12:26:23.329384 UTC] Updating baseline
[2018-12-22 12:26:24.448007 UTC] Computing logging information
-------------------------------------
| Iteration            | 1351       |
| ExpectedImprovement  | 0.015398   |
| ActualImprovement    | 0.014372   |
| ImprovementRatio     | 0.93336    |
| MeanKL               | 0.008393   |
| Entropy              | -1.3889    |
| Perplexity           | 0.24935    |
| AveragePolicyStd     | 0.1954     |
| AveragePolicyStd[0]  | 0.22557    |
| AveragePolicyStd[1]  | 0.18926    |
| AveragePolicyStd[2]  | 0.15647    |
| AveragePolicyStd[3]  | 0.20171    |
| AveragePolicyStd[4]  | 0.14737    |
| AveragePolicyStd[5]  | 0.25203    |
| AverageReturn        | 1803.3     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 233.22     |
| AverageEpisodeLength | 982.13     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.87     |
| TotalNEpisodes       | 21515      |
| TotalNSamples        | 6.7628e+06 |
| ExplainedVariance    | 2.7258e-08 |
-------------------------------------
[2018-12-22 12:26:24.874585 UTC] Saving snapshot
[2018-12-22 12:26:24.874953 UTC] Starting iteration 1352
[2018-12-22 12:26:24.875083 UTC] Start collecting samples
[2018-12-22 12:26:27.802967 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:27.876494 UTC] Performing policy update
[2018-12-22 12:26:27.877147 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:27.968531 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:29.027651 UTC] Performing line search
[2018-12-22 12:26:29.154611 UTC] Updating baseline
[2018-12-22 12:26:32.803799 UTC] Computing logging information
-------------------------------------
| Iteration            | 1352       |
| ExpectedImprovement  | 0.017369   |
| ActualImprovement    | 0.015589   |
| ImprovementRatio     | 0.89753    |
| MeanKL               | 0.0078867  |
| Entropy              | -1.3806    |
| Perplexity           | 0.25143    |
| AveragePolicyStd     | 0.19567    |
| AveragePolicyStd[0]  | 0.22586    |
| AveragePolicyStd[1]  | 0.18929    |
| AveragePolicyStd[2]  | 0.15663    |
| AveragePolicyStd[3]  | 0.20156    |
| AveragePolicyStd[4]  | 0.1479     |
| AveragePolicyStd[5]  | 0.25282    |
| AverageReturn        | 1806.9     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 233.79     |
| AverageEpisodeLength | 982.14     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.87     |
| TotalNEpisodes       | 21518      |
| TotalNSamples        | 6.7658e+06 |
| ExplainedVariance    | 4.9403e-10 |
-------------------------------------
[2018-12-22 12:26:33.233323 UTC] Saving snapshot
[2018-12-22 12:26:33.233616 UTC] Starting iteration 1353
[2018-12-22 12:26:33.233763 UTC] Start collecting samples
[2018-12-22 12:26:36.275946 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:36.354905 UTC] Performing policy update
[2018-12-22 12:26:36.355702 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:36.446292 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:37.518366 UTC] Performing line search
[2018-12-22 12:26:37.650433 UTC] Updating baseline
[2018-12-22 12:26:40.140559 UTC] Computing logging information
-------------------------------------
| Iteration            | 1353       |
| ExpectedImprovement  | 0.017859   |
| ActualImprovement    | 0.017099   |
| ImprovementRatio     | 0.95748    |
| MeanKL               | 0.0081152  |
| Entropy              | -1.3839    |
| Perplexity           | 0.25061    |
| AveragePolicyStd     | 0.19548    |
| AveragePolicyStd[0]  | 0.22448    |
| AveragePolicyStd[1]  | 0.18955    |
| AveragePolicyStd[2]  | 0.1566     |
| AveragePolicyStd[3]  | 0.20131    |
| AveragePolicyStd[4]  | 0.14861    |
| AveragePolicyStd[5]  | 0.25233    |
| AverageReturn        | 1806.6     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 255.83     |
| AverageEpisodeLength | 977.36     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.62     |
| TotalNEpisodes       | 21527      |
| TotalNSamples        | 6.7743e+06 |
| ExplainedVariance    | 0.057739   |
-------------------------------------
[2018-12-22 12:26:40.568696 UTC] Saving snapshot
[2018-12-22 12:26:40.568973 UTC] Starting iteration 1354
[2018-12-22 12:26:40.569089 UTC] Start collecting samples
[2018-12-22 12:26:43.521973 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:43.596909 UTC] Performing policy update
[2018-12-22 12:26:43.597563 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:43.685809 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:44.746823 UTC] Performing line search
[2018-12-22 12:26:44.873190 UTC] Updating baseline
[2018-12-22 12:26:46.826609 UTC] Computing logging information
-------------------------------------
| Iteration            | 1354       |
| ExpectedImprovement  | 0.019306   |
| ActualImprovement    | 0.017736   |
| ImprovementRatio     | 0.91867    |
| MeanKL               | 0.0083652  |
| Entropy              | -1.3832    |
| Perplexity           | 0.25078    |
| AveragePolicyStd     | 0.19549    |
| AveragePolicyStd[0]  | 0.22449    |
| AveragePolicyStd[1]  | 0.18972    |
| AveragePolicyStd[2]  | 0.15671    |
| AveragePolicyStd[3]  | 0.20103    |
| AveragePolicyStd[4]  | 0.14871    |
| AveragePolicyStd[5]  | 0.25226    |
| AverageReturn        | 1809.2     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 256.24     |
| AverageEpisodeLength | 977.36     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.62     |
| TotalNEpisodes       | 21531      |
| TotalNSamples        | 6.7783e+06 |
| ExplainedVariance    | -0.0020751 |
-------------------------------------
[2018-12-22 12:26:47.253908 UTC] Saving snapshot
[2018-12-22 12:26:47.254154 UTC] Starting iteration 1355
[2018-12-22 12:26:47.254272 UTC] Start collecting samples
[2018-12-22 12:26:50.182177 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:50.255183 UTC] Performing policy update
[2018-12-22 12:26:50.255959 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:50.344863 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:51.400307 UTC] Performing line search
[2018-12-22 12:26:51.527592 UTC] Updating baseline
[2018-12-22 12:26:52.896673 UTC] Computing logging information
--------------------------------------
| Iteration            | 1355        |
| ExpectedImprovement  | 0.017826    |
| ActualImprovement    | 0.016637    |
| ImprovementRatio     | 0.93326     |
| MeanKL               | 0.0076065   |
| Entropy              | -1.3863     |
| Perplexity           | 0.25        |
| AveragePolicyStd     | 0.19537     |
| AveragePolicyStd[0]  | 0.224       |
| AveragePolicyStd[1]  | 0.1892      |
| AveragePolicyStd[2]  | 0.15659     |
| AveragePolicyStd[3]  | 0.20126     |
| AveragePolicyStd[4]  | 0.149       |
| AveragePolicyStd[5]  | 0.25214     |
| AverageReturn        | 1812        |
| MinReturn            | 96.209      |
| MaxReturn            | 1978.3      |
| StdReturn            | 256.9       |
| AverageEpisodeLength | 977.36      |
| MinEpisodeLength     | 77          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 131.62      |
| TotalNEpisodes       | 21534       |
| TotalNSamples        | 6.7813e+06  |
| ExplainedVariance    | -8.9057e-08 |
--------------------------------------
[2018-12-22 12:26:53.318990 UTC] Saving snapshot
[2018-12-22 12:26:53.319228 UTC] Starting iteration 1356
[2018-12-22 12:26:53.319369 UTC] Start collecting samples
[2018-12-22 12:26:56.344961 UTC] Computing input variables for policy optimization
[2018-12-22 12:26:56.423299 UTC] Performing policy update
[2018-12-22 12:26:56.423920 UTC] Computing gradient in Euclidean space
[2018-12-22 12:26:56.513020 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:26:57.576208 UTC] Performing line search
[2018-12-22 12:26:57.704557 UTC] Updating baseline
[2018-12-22 12:26:59.167042 UTC] Computing logging information
-------------------------------------
| Iteration            | 1356       |
| ExpectedImprovement  | 0.019307   |
| ActualImprovement    | 0.01792    |
| ImprovementRatio     | 0.92815    |
| MeanKL               | 0.0074144  |
| Entropy              | -1.3886    |
| Perplexity           | 0.24943    |
| AveragePolicyStd     | 0.19533    |
| AveragePolicyStd[0]  | 0.22429    |
| AveragePolicyStd[1]  | 0.1891     |
| AveragePolicyStd[2]  | 0.15672    |
| AveragePolicyStd[3]  | 0.2013     |
| AveragePolicyStd[4]  | 0.1483     |
| AveragePolicyStd[5]  | 0.2523     |
| AverageReturn        | 1817.8     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 257.88     |
| AverageEpisodeLength | 977.36     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.62     |
| TotalNEpisodes       | 21543      |
| TotalNSamples        | 6.7903e+06 |
| ExplainedVariance    | 5.9492e-08 |
-------------------------------------
[2018-12-22 12:26:59.594606 UTC] Saving snapshot
[2018-12-22 12:26:59.594857 UTC] Starting iteration 1357
[2018-12-22 12:26:59.594976 UTC] Start collecting samples
[2018-12-22 12:27:02.555850 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:02.629785 UTC] Performing policy update
[2018-12-22 12:27:02.630682 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:02.721278 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:03.783008 UTC] Performing line search
[2018-12-22 12:27:03.911440 UTC] Updating baseline
[2018-12-22 12:27:05.704546 UTC] Computing logging information
--------------------------------------
| Iteration            | 1357        |
| ExpectedImprovement  | 0.016847    |
| ActualImprovement    | 0.015949    |
| ImprovementRatio     | 0.94667     |
| MeanKL               | 0.0076473   |
| Entropy              | -1.3933     |
| Perplexity           | 0.24825     |
| AveragePolicyStd     | 0.19518     |
| AveragePolicyStd[0]  | 0.22445     |
| AveragePolicyStd[1]  | 0.18873     |
| AveragePolicyStd[2]  | 0.15681     |
| AveragePolicyStd[3]  | 0.20106     |
| AveragePolicyStd[4]  | 0.14807     |
| AveragePolicyStd[5]  | 0.25197     |
| AverageReturn        | 1832.9      |
| MinReturn            | 96.209      |
| MaxReturn            | 1978.3      |
| StdReturn            | 209.42      |
| AverageEpisodeLength | 985.34      |
| MinEpisodeLength     | 77          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 106.08      |
| TotalNEpisodes       | 21547       |
| TotalNSamples        | 6.7943e+06  |
| ExplainedVariance    | -4.8926e-10 |
--------------------------------------
[2018-12-22 12:27:06.140839 UTC] Saving snapshot
[2018-12-22 12:27:06.141095 UTC] Starting iteration 1358
[2018-12-22 12:27:06.141214 UTC] Start collecting samples
[2018-12-22 12:27:09.056169 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:09.130285 UTC] Performing policy update
[2018-12-22 12:27:09.130900 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:09.220715 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:10.285461 UTC] Performing line search
[2018-12-22 12:27:10.413709 UTC] Updating baseline
[2018-12-22 12:27:11.978318 UTC] Computing logging information
-------------------------------------
| Iteration            | 1358       |
| ExpectedImprovement  | 0.016945   |
| ActualImprovement    | 0.015504   |
| ImprovementRatio     | 0.915      |
| MeanKL               | 0.0077191  |
| Entropy              | -1.3947    |
| Perplexity           | 0.2479     |
| AveragePolicyStd     | 0.19517    |
| AveragePolicyStd[0]  | 0.22455    |
| AveragePolicyStd[1]  | 0.1887     |
| AveragePolicyStd[2]  | 0.15695    |
| AveragePolicyStd[3]  | 0.20063    |
| AveragePolicyStd[4]  | 0.1477     |
| AveragePolicyStd[5]  | 0.25248    |
| AverageReturn        | 1817.5     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 260.47     |
| AverageEpisodeLength | 977.42     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.27     |
| TotalNEpisodes       | 21549      |
| TotalNSamples        | 6.7955e+06 |
| ExplainedVariance    | 0.29713    |
-------------------------------------
[2018-12-22 12:27:12.405523 UTC] Saving snapshot
[2018-12-22 12:27:12.405801 UTC] Starting iteration 1359
[2018-12-22 12:27:12.405925 UTC] Start collecting samples
[2018-12-22 12:27:15.409228 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:15.487171 UTC] Performing policy update
[2018-12-22 12:27:15.487825 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:15.576604 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:16.644800 UTC] Performing line search
[2018-12-22 12:27:16.774787 UTC] Updating baseline
[2018-12-22 12:27:19.255579 UTC] Computing logging information
-------------------------------------
| Iteration            | 1359       |
| ExpectedImprovement  | 0.01721    |
| ActualImprovement    | 0.016021   |
| ImprovementRatio     | 0.93094    |
| MeanKL               | 0.0087101  |
| Entropy              | -1.4034    |
| Perplexity           | 0.24576    |
| AveragePolicyStd     | 0.19487    |
| AveragePolicyStd[0]  | 0.22444    |
| AveragePolicyStd[1]  | 0.1883     |
| AveragePolicyStd[2]  | 0.15629    |
| AveragePolicyStd[3]  | 0.2002     |
| AveragePolicyStd[4]  | 0.14807    |
| AveragePolicyStd[5]  | 0.25193    |
| AverageReturn        | 1820.9     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 260.71     |
| AverageEpisodeLength | 977.42     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.27     |
| TotalNEpisodes       | 21557      |
| TotalNSamples        | 6.8035e+06 |
| ExplainedVariance    | -0.0031186 |
-------------------------------------
[2018-12-22 12:27:19.689618 UTC] Saving snapshot
[2018-12-22 12:27:19.689891 UTC] Starting iteration 1360
[2018-12-22 12:27:19.690011 UTC] Start collecting samples
[2018-12-22 12:27:22.665187 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:22.741806 UTC] Performing policy update
[2018-12-22 12:27:22.742442 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:22.832386 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:23.896115 UTC] Performing line search
[2018-12-22 12:27:24.022577 UTC] Updating baseline
[2018-12-22 12:27:26.169011 UTC] Computing logging information
--------------------------------------
| Iteration            | 1360        |
| ExpectedImprovement  | 0.017449    |
| ActualImprovement    | 0.016437    |
| ImprovementRatio     | 0.94199     |
| MeanKL               | 0.0078634   |
| Entropy              | -1.4031     |
| Perplexity           | 0.24583     |
| AveragePolicyStd     | 0.19486     |
| AveragePolicyStd[0]  | 0.22397     |
| AveragePolicyStd[1]  | 0.18855     |
| AveragePolicyStd[2]  | 0.15667     |
| AveragePolicyStd[3]  | 0.20041     |
| AveragePolicyStd[4]  | 0.1478      |
| AveragePolicyStd[5]  | 0.25179     |
| AverageReturn        | 1821.3      |
| MinReturn            | 96.209      |
| MaxReturn            | 1978.3      |
| StdReturn            | 260.67      |
| AverageEpisodeLength | 977.42      |
| MinEpisodeLength     | 77          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 131.27      |
| TotalNEpisodes       | 21563       |
| TotalNSamples        | 6.8095e+06  |
| ExplainedVariance    | -1.3772e-05 |
--------------------------------------
[2018-12-22 12:27:26.598997 UTC] Saving snapshot
[2018-12-22 12:27:26.607212 UTC] Starting iteration 1361
[2018-12-22 12:27:26.607423 UTC] Start collecting samples
[2018-12-22 12:27:29.526595 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:29.598680 UTC] Performing policy update
[2018-12-22 12:27:29.599290 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:29.688612 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:30.757793 UTC] Performing line search
[2018-12-22 12:27:30.885852 UTC] Updating baseline
[2018-12-22 12:27:32.961109 UTC] Computing logging information
-------------------------------------
| Iteration            | 1361       |
| ExpectedImprovement  | 0.020396   |
| ActualImprovement    | 0.01743    |
| ImprovementRatio     | 0.85459    |
| MeanKL               | 0.0075779  |
| Entropy              | -1.4036    |
| Perplexity           | 0.24572    |
| AveragePolicyStd     | 0.19489    |
| AveragePolicyStd[0]  | 0.22436    |
| AveragePolicyStd[1]  | 0.18805    |
| AveragePolicyStd[2]  | 0.15665    |
| AveragePolicyStd[3]  | 0.20049    |
| AveragePolicyStd[4]  | 0.14757    |
| AveragePolicyStd[5]  | 0.25221    |
| AverageReturn        | 1822.2     |
| MinReturn            | 96.209     |
| MaxReturn            | 1978.3     |
| StdReturn            | 260.67     |
| AverageEpisodeLength | 977.42     |
| MinEpisodeLength     | 77         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.27     |
| TotalNEpisodes       | 21565      |
| TotalNSamples        | 6.8115e+06 |
| ExplainedVariance    | 0.0032261  |
-------------------------------------
[2018-12-22 12:27:33.391360 UTC] Saving snapshot
[2018-12-22 12:27:33.391618 UTC] Starting iteration 1362
[2018-12-22 12:27:33.391741 UTC] Start collecting samples
[2018-12-22 12:27:36.417563 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:36.495569 UTC] Performing policy update
[2018-12-22 12:27:36.496287 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:36.584925 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:37.633616 UTC] Performing line search
[2018-12-22 12:27:37.760307 UTC] Updating baseline
[2018-12-22 12:27:39.062004 UTC] Computing logging information
-------------------------------------
| Iteration            | 1362       |
| ExpectedImprovement  | 0.016764   |
| ActualImprovement    | 0.016601   |
| ImprovementRatio     | 0.99031    |
| MeanKL               | 0.0084033  |
| Entropy              | -1.406     |
| Perplexity           | 0.24513    |
| AveragePolicyStd     | 0.1948     |
| AveragePolicyStd[0]  | 0.22429    |
| AveragePolicyStd[1]  | 0.18815    |
| AveragePolicyStd[2]  | 0.15688    |
| AveragePolicyStd[3]  | 0.1999     |
| AveragePolicyStd[4]  | 0.14748    |
| AveragePolicyStd[5]  | 0.2521     |
| AverageReturn        | 1833.1     |
| MinReturn            | 276.39     |
| MaxReturn            | 1978.3     |
| StdReturn            | 207.7      |
| AverageEpisodeLength | 982.83     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 101.91     |
| TotalNEpisodes       | 21573      |
| TotalNSamples        | 6.8191e+06 |
| ExplainedVariance    | 0.019159   |
-------------------------------------
[2018-12-22 12:27:39.487650 UTC] Saving snapshot
[2018-12-22 12:27:39.487912 UTC] Starting iteration 1363
[2018-12-22 12:27:39.488031 UTC] Start collecting samples
[2018-12-22 12:27:42.485256 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:42.560996 UTC] Performing policy update
[2018-12-22 12:27:42.561601 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:42.650929 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:43.695646 UTC] Performing line search
[2018-12-22 12:27:43.826537 UTC] Updating baseline
[2018-12-22 12:27:45.478711 UTC] Computing logging information
-------------------------------------
| Iteration            | 1363       |
| ExpectedImprovement  | 0.017718   |
| ActualImprovement    | 0.016598   |
| ImprovementRatio     | 0.93683    |
| MeanKL               | 0.0079913  |
| Entropy              | -1.4068    |
| Perplexity           | 0.24494    |
| AveragePolicyStd     | 0.19476    |
| AveragePolicyStd[0]  | 0.22447    |
| AveragePolicyStd[1]  | 0.18775    |
| AveragePolicyStd[2]  | 0.15709    |
| AveragePolicyStd[3]  | 0.19981    |
| AveragePolicyStd[4]  | 0.14751    |
| AveragePolicyStd[5]  | 0.25194    |
| AverageReturn        | 1826.5     |
| MinReturn            | 276.39     |
| MaxReturn            | 1978.3     |
| StdReturn            | 214.22     |
| AverageEpisodeLength | 980.06     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105.12     |
| TotalNEpisodes       | 21580      |
| TotalNSamples        | 6.8258e+06 |
| ExplainedVariance    | 0.084304   |
-------------------------------------
[2018-12-22 12:27:45.912452 UTC] Saving snapshot
[2018-12-22 12:27:45.912721 UTC] Starting iteration 1364
[2018-12-22 12:27:45.912853 UTC] Start collecting samples
[2018-12-22 12:27:48.841439 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:48.914032 UTC] Performing policy update
[2018-12-22 12:27:48.914702 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:49.003852 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:50.062675 UTC] Performing line search
[2018-12-22 12:27:50.190191 UTC] Updating baseline
[2018-12-22 12:27:52.156126 UTC] Computing logging information
-------------------------------------
| Iteration            | 1364       |
| ExpectedImprovement  | 0.018495   |
| ActualImprovement    | 0.017656   |
| ImprovementRatio     | 0.95463    |
| MeanKL               | 0.0075596  |
| Entropy              | -1.4096    |
| Perplexity           | 0.24425    |
| AveragePolicyStd     | 0.19466    |
| AveragePolicyStd[0]  | 0.22452    |
| AveragePolicyStd[1]  | 0.18746    |
| AveragePolicyStd[2]  | 0.15704    |
| AveragePolicyStd[3]  | 0.19957    |
| AveragePolicyStd[4]  | 0.1476     |
| AveragePolicyStd[5]  | 0.25179    |
| AverageReturn        | 1826.1     |
| MinReturn            | 276.39     |
| MaxReturn            | 1978.3     |
| StdReturn            | 214.15     |
| AverageEpisodeLength | 980.06     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105.12     |
| TotalNEpisodes       | 21581      |
| TotalNSamples        | 6.8268e+06 |
| ExplainedVariance    | -0.027917  |
-------------------------------------
[2018-12-22 12:27:52.586460 UTC] Saving snapshot
[2018-12-22 12:27:52.586745 UTC] Starting iteration 1365
[2018-12-22 12:27:52.586879 UTC] Start collecting samples
[2018-12-22 12:27:55.540170 UTC] Computing input variables for policy optimization
[2018-12-22 12:27:55.614874 UTC] Performing policy update
[2018-12-22 12:27:55.615669 UTC] Computing gradient in Euclidean space
[2018-12-22 12:27:55.704969 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:27:56.772419 UTC] Performing line search
[2018-12-22 12:27:56.900306 UTC] Updating baseline
[2018-12-22 12:27:58.077891 UTC] Computing logging information
-------------------------------------
| Iteration            | 1365       |
| ExpectedImprovement  | 0.017152   |
| ActualImprovement    | 0.016503   |
| ImprovementRatio     | 0.96212    |
| MeanKL               | 0.0073996  |
| Entropy              | -1.4134    |
| Perplexity           | 0.24332    |
| AveragePolicyStd     | 0.19453    |
| AveragePolicyStd[0]  | 0.22449    |
| AveragePolicyStd[1]  | 0.18692    |
| AveragePolicyStd[2]  | 0.15716    |
| AveragePolicyStd[3]  | 0.1999     |
| AveragePolicyStd[4]  | 0.1474     |
| AveragePolicyStd[5]  | 0.25134    |
| AverageReturn        | 1825.5     |
| MinReturn            | 276.39     |
| MaxReturn            | 1978.3     |
| StdReturn            | 214.15     |
| AverageEpisodeLength | 980.06     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105.12     |
| TotalNEpisodes       | 21585      |
| TotalNSamples        | 6.8308e+06 |
| ExplainedVariance    | 1.4783e-08 |
-------------------------------------
[2018-12-22 12:27:58.507693 UTC] Saving snapshot
[2018-12-22 12:27:58.507939 UTC] Starting iteration 1366
[2018-12-22 12:27:58.508057 UTC] Start collecting samples
[2018-12-22 12:28:01.531158 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:01.612154 UTC] Performing policy update
[2018-12-22 12:28:01.612745 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:01.702337 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:02.770784 UTC] Performing line search
[2018-12-22 12:28:02.898176 UTC] Updating baseline
[2018-12-22 12:28:03.839556 UTC] Computing logging information
--------------------------------------
| Iteration            | 1366        |
| ExpectedImprovement  | 0.017964    |
| ActualImprovement    | 0.017258    |
| ImprovementRatio     | 0.9607      |
| MeanKL               | 0.0077667   |
| Entropy              | -1.4159     |
| Perplexity           | 0.2427      |
| AveragePolicyStd     | 0.19448     |
| AveragePolicyStd[0]  | 0.22428     |
| AveragePolicyStd[1]  | 0.1866      |
| AveragePolicyStd[2]  | 0.15705     |
| AveragePolicyStd[3]  | 0.19938     |
| AveragePolicyStd[4]  | 0.1474      |
| AveragePolicyStd[5]  | 0.25218     |
| AverageReturn        | 1824.2      |
| MinReturn            | 276.39      |
| MaxReturn            | 1978.3      |
| StdReturn            | 213.88      |
| AverageEpisodeLength | 980.06      |
| MinEpisodeLength     | 208         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 105.12      |
| TotalNEpisodes       | 21595       |
| TotalNSamples        | 6.8408e+06  |
| ExplainedVariance    | -6.5815e-10 |
--------------------------------------
[2018-12-22 12:28:04.268783 UTC] Saving snapshot
[2018-12-22 12:28:04.269033 UTC] Starting iteration 1367
[2018-12-22 12:28:04.269152 UTC] Start collecting samples
[2018-12-22 12:28:07.186599 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:07.258692 UTC] Performing policy update
[2018-12-22 12:28:07.259464 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:07.348069 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:08.407479 UTC] Performing line search
[2018-12-22 12:28:08.535038 UTC] Updating baseline
[2018-12-22 12:28:10.680270 UTC] Computing logging information
-------------------------------------
| Iteration            | 1367       |
| ExpectedImprovement  | 0.019549   |
| ActualImprovement    | 0.017078   |
| ImprovementRatio     | 0.87359    |
| MeanKL               | 0.0078837  |
| Entropy              | -1.4178    |
| Perplexity           | 0.24226    |
| AveragePolicyStd     | 0.19444    |
| AveragePolicyStd[0]  | 0.22431    |
| AveragePolicyStd[1]  | 0.18699    |
| AveragePolicyStd[2]  | 0.15673    |
| AveragePolicyStd[3]  | 0.19943    |
| AveragePolicyStd[4]  | 0.14721    |
| AveragePolicyStd[5]  | 0.25193    |
| AverageReturn        | 1824.1     |
| MinReturn            | 276.39     |
| MaxReturn            | 1978.3     |
| StdReturn            | 213.88     |
| AverageEpisodeLength | 980.06     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105.12     |
| TotalNEpisodes       | 21596      |
| TotalNSamples        | 6.8418e+06 |
| ExplainedVariance    | 0.00014795 |
-------------------------------------
[2018-12-22 12:28:11.106528 UTC] Saving snapshot
[2018-12-22 12:28:11.106816 UTC] Starting iteration 1368
[2018-12-22 12:28:11.106939 UTC] Start collecting samples
[2018-12-22 12:28:14.046877 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:14.123231 UTC] Performing policy update
[2018-12-22 12:28:14.124019 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:14.214741 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:15.273104 UTC] Performing line search
[2018-12-22 12:28:15.402012 UTC] Updating baseline
[2018-12-22 12:28:17.449058 UTC] Computing logging information
--------------------------------------
| Iteration            | 1368        |
| ExpectedImprovement  | 0.019881    |
| ActualImprovement    | 0.018344    |
| ImprovementRatio     | 0.92267     |
| MeanKL               | 0.0076173   |
| Entropy              | -1.4183     |
| Perplexity           | 0.24214     |
| AveragePolicyStd     | 0.19441     |
| AveragePolicyStd[0]  | 0.22441     |
| AveragePolicyStd[1]  | 0.1865      |
| AveragePolicyStd[2]  | 0.15669     |
| AveragePolicyStd[3]  | 0.19945     |
| AveragePolicyStd[4]  | 0.14752     |
| AveragePolicyStd[5]  | 0.25188     |
| AverageReturn        | 1824.7      |
| MinReturn            | 276.39      |
| MaxReturn            | 1978.3      |
| StdReturn            | 214.12      |
| AverageEpisodeLength | 980.06      |
| MinEpisodeLength     | 208         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 105.12      |
| TotalNEpisodes       | 21600       |
| TotalNSamples        | 6.8458e+06  |
| ExplainedVariance    | -1.2698e-05 |
--------------------------------------
[2018-12-22 12:28:17.879085 UTC] Saving snapshot
[2018-12-22 12:28:17.879357 UTC] Starting iteration 1369
[2018-12-22 12:28:17.879476 UTC] Start collecting samples
[2018-12-22 12:28:20.881238 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:20.958643 UTC] Performing policy update
[2018-12-22 12:28:20.959387 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:21.048916 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:22.123701 UTC] Performing line search
[2018-12-22 12:28:22.250366 UTC] Updating baseline
[2018-12-22 12:28:24.043598 UTC] Computing logging information
--------------------------------------
| Iteration            | 1369        |
| ExpectedImprovement  | 0.017754    |
| ActualImprovement    | 0.017034    |
| ImprovementRatio     | 0.95946     |
| MeanKL               | 0.0077552   |
| Entropy              | -1.4227     |
| Perplexity           | 0.24106     |
| AveragePolicyStd     | 0.1943      |
| AveragePolicyStd[0]  | 0.2243      |
| AveragePolicyStd[1]  | 0.18613     |
| AveragePolicyStd[2]  | 0.15627     |
| AveragePolicyStd[3]  | 0.19953     |
| AveragePolicyStd[4]  | 0.1474      |
| AveragePolicyStd[5]  | 0.25217     |
| AverageReturn        | 1824.6      |
| MinReturn            | 276.39      |
| MaxReturn            | 1978.3      |
| StdReturn            | 214.15      |
| AverageEpisodeLength | 980.06      |
| MinEpisodeLength     | 208         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 105.12      |
| TotalNEpisodes       | 21608       |
| TotalNSamples        | 6.8538e+06  |
| ExplainedVariance    | -4.2737e-08 |
--------------------------------------
[2018-12-22 12:28:24.474933 UTC] Saving snapshot
[2018-12-22 12:28:24.475207 UTC] Starting iteration 1370
[2018-12-22 12:28:24.475338 UTC] Start collecting samples
[2018-12-22 12:28:27.420099 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:27.493982 UTC] Performing policy update
[2018-12-22 12:28:27.494565 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:27.583618 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:28.646480 UTC] Performing line search
[2018-12-22 12:28:28.772754 UTC] Updating baseline
[2018-12-22 12:28:30.349135 UTC] Computing logging information
-------------------------------------
| Iteration            | 1370       |
| ExpectedImprovement  | 0.018804   |
| ActualImprovement    | 0.018041   |
| ImprovementRatio     | 0.95943    |
| MeanKL               | 0.0074106  |
| Entropy              | -1.4258    |
| Perplexity           | 0.24031    |
| AveragePolicyStd     | 0.19416    |
| AveragePolicyStd[0]  | 0.22369    |
| AveragePolicyStd[1]  | 0.18592    |
| AveragePolicyStd[2]  | 0.15665    |
| AveragePolicyStd[3]  | 0.19921    |
| AveragePolicyStd[4]  | 0.14747    |
| AveragePolicyStd[5]  | 0.25204    |
| AverageReturn        | 1823.5     |
| MinReturn            | 276.39     |
| MaxReturn            | 1947.4     |
| StdReturn            | 213.64     |
| AverageEpisodeLength | 980.02     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 105.11     |
| TotalNEpisodes       | 21612      |
| TotalNSamples        | 6.8578e+06 |
| ExplainedVariance    | 0.15856    |
-------------------------------------
[2018-12-22 12:28:30.776185 UTC] Saving snapshot
[2018-12-22 12:28:30.784231 UTC] Starting iteration 1371
[2018-12-22 12:28:30.784460 UTC] Start collecting samples
[2018-12-22 12:28:33.716221 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:33.795205 UTC] Performing policy update
[2018-12-22 12:28:33.795967 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:33.884438 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:34.942334 UTC] Performing line search
[2018-12-22 12:28:35.068501 UTC] Updating baseline
[2018-12-22 12:28:37.471704 UTC] Computing logging information
--------------------------------------
| Iteration            | 1371        |
| ExpectedImprovement  | 0.019458    |
| ActualImprovement    | 0.018443    |
| ImprovementRatio     | 0.9478      |
| MeanKL               | 0.0095443   |
| Entropy              | -1.4264     |
| Perplexity           | 0.24017     |
| AveragePolicyStd     | 0.19416     |
| AveragePolicyStd[0]  | 0.22412     |
| AveragePolicyStd[1]  | 0.18638     |
| AveragePolicyStd[2]  | 0.15598     |
| AveragePolicyStd[3]  | 0.19901     |
| AveragePolicyStd[4]  | 0.14759     |
| AveragePolicyStd[5]  | 0.2519      |
| AverageReturn        | 1825        |
| MinReturn            | 276.39      |
| MaxReturn            | 1947.4      |
| StdReturn            | 214.04      |
| AverageEpisodeLength | 980.02      |
| MinEpisodeLength     | 208         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 105.11      |
| TotalNEpisodes       | 21616       |
| TotalNSamples        | 6.8618e+06  |
| ExplainedVariance    | -0.00010519 |
--------------------------------------
[2018-12-22 12:28:37.911241 UTC] Saving snapshot
[2018-12-22 12:28:37.911483 UTC] Starting iteration 1372
[2018-12-22 12:28:37.911631 UTC] Start collecting samples
[2018-12-22 12:28:40.917403 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:40.994727 UTC] Performing policy update
[2018-12-22 12:28:40.995441 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:41.084826 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:42.158406 UTC] Performing line search
[2018-12-22 12:28:42.286346 UTC] Updating baseline
[2018-12-22 12:28:43.770856 UTC] Computing logging information
-------------------------------------
| Iteration            | 1372       |
| ExpectedImprovement  | 0.017155   |
| ActualImprovement    | 0.016362   |
| ImprovementRatio     | 0.95377    |
| MeanKL               | 0.0080502  |
| Entropy              | -1.4252    |
| Perplexity           | 0.24045    |
| AveragePolicyStd     | 0.19424    |
| AveragePolicyStd[0]  | 0.22432    |
| AveragePolicyStd[1]  | 0.18644    |
| AveragePolicyStd[2]  | 0.15551    |
| AveragePolicyStd[3]  | 0.19918    |
| AveragePolicyStd[4]  | 0.14765    |
| AveragePolicyStd[5]  | 0.25232    |
| AverageReturn        | 1833       |
| MinReturn            | 276.39     |
| MaxReturn            | 1947.4     |
| StdReturn            | 186.4      |
| AverageEpisodeLength | 985.45     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 91.036     |
| TotalNEpisodes       | 21624      |
| TotalNSamples        | 6.8698e+06 |
| ExplainedVariance    | 0.012397   |
-------------------------------------
[2018-12-22 12:28:44.197764 UTC] Saving snapshot
[2018-12-22 12:28:44.198005 UTC] Starting iteration 1373
[2018-12-22 12:28:44.198123 UTC] Start collecting samples
[2018-12-22 12:28:47.121623 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:47.197209 UTC] Performing policy update
[2018-12-22 12:28:47.197818 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:47.286621 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:48.344841 UTC] Performing line search
[2018-12-22 12:28:48.472418 UTC] Updating baseline
[2018-12-22 12:28:49.810250 UTC] Computing logging information
-------------------------------------
| Iteration            | 1373       |
| ExpectedImprovement  | 0.014774   |
| ActualImprovement    | 0.013624   |
| ImprovementRatio     | 0.9222     |
| MeanKL               | 0.008364   |
| Entropy              | -1.4308    |
| Perplexity           | 0.23912    |
| AveragePolicyStd     | 0.19407    |
| AveragePolicyStd[0]  | 0.22449    |
| AveragePolicyStd[1]  | 0.18626    |
| AveragePolicyStd[2]  | 0.15542    |
| AveragePolicyStd[3]  | 0.19905    |
| AveragePolicyStd[4]  | 0.14727    |
| AveragePolicyStd[5]  | 0.25193    |
| AverageReturn        | 1833.1     |
| MinReturn            | 276.39     |
| MaxReturn            | 1947.4     |
| StdReturn            | 186.54     |
| AverageEpisodeLength | 985.45     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 91.036     |
| TotalNEpisodes       | 21628      |
| TotalNSamples        | 6.8738e+06 |
| ExplainedVariance    | -0.0045046 |
-------------------------------------
[2018-12-22 12:28:50.249619 UTC] Saving snapshot
[2018-12-22 12:28:50.249905 UTC] Starting iteration 1374
[2018-12-22 12:28:50.250026 UTC] Start collecting samples
[2018-12-22 12:28:53.158049 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:53.230617 UTC] Performing policy update
[2018-12-22 12:28:53.231178 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:53.320386 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:28:54.384317 UTC] Performing line search
[2018-12-22 12:28:54.510686 UTC] Updating baseline
[2018-12-22 12:28:55.986095 UTC] Computing logging information
-------------------------------------
| Iteration            | 1374       |
| ExpectedImprovement  | 0.018378   |
| ActualImprovement    | 0.016276   |
| ImprovementRatio     | 0.88566    |
| MeanKL               | 0.0078416  |
| Entropy              | -1.4277    |
| Perplexity           | 0.23986    |
| AveragePolicyStd     | 0.19417    |
| AveragePolicyStd[0]  | 0.22496    |
| AveragePolicyStd[1]  | 0.18644    |
| AveragePolicyStd[2]  | 0.15553    |
| AveragePolicyStd[3]  | 0.19931    |
| AveragePolicyStd[4]  | 0.1472     |
| AveragePolicyStd[5]  | 0.25157    |
| AverageReturn        | 1833.2     |
| MinReturn            | 276.39     |
| MaxReturn            | 1947.4     |
| StdReturn            | 186.61     |
| AverageEpisodeLength | 985.45     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 91.036     |
| TotalNEpisodes       | 21630      |
| TotalNSamples        | 6.8758e+06 |
| ExplainedVariance    | -0.0076051 |
-------------------------------------
[2018-12-22 12:28:56.414179 UTC] Saving snapshot
[2018-12-22 12:28:56.414423 UTC] Starting iteration 1375
[2018-12-22 12:28:56.414564 UTC] Start collecting samples
[2018-12-22 12:28:59.416973 UTC] Computing input variables for policy optimization
[2018-12-22 12:28:59.495479 UTC] Performing policy update
[2018-12-22 12:28:59.496060 UTC] Computing gradient in Euclidean space
[2018-12-22 12:28:59.587705 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:00.659013 UTC] Performing line search
[2018-12-22 12:29:00.786082 UTC] Updating baseline
[2018-12-22 12:29:02.294912 UTC] Computing logging information
-------------------------------------
| Iteration            | 1375       |
| ExpectedImprovement  | 0.017245   |
| ActualImprovement    | 0.016186   |
| ImprovementRatio     | 0.93859    |
| MeanKL               | 0.0080323  |
| Entropy              | -1.4354    |
| Perplexity           | 0.23802    |
| AveragePolicyStd     | 0.19392    |
| AveragePolicyStd[0]  | 0.22478    |
| AveragePolicyStd[1]  | 0.18554    |
| AveragePolicyStd[2]  | 0.15562    |
| AveragePolicyStd[3]  | 0.19949    |
| AveragePolicyStd[4]  | 0.14691    |
| AveragePolicyStd[5]  | 0.25117    |
| AverageReturn        | 1832.4     |
| MinReturn            | 276.39     |
| MaxReturn            | 1941.1     |
| StdReturn            | 186.43     |
| AverageEpisodeLength | 985.45     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 91.036     |
| TotalNEpisodes       | 21639      |
| TotalNSamples        | 6.8848e+06 |
| ExplainedVariance    | 0.00056407 |
-------------------------------------
[2018-12-22 12:29:02.726544 UTC] Saving snapshot
[2018-12-22 12:29:02.726797 UTC] Starting iteration 1376
[2018-12-22 12:29:02.726915 UTC] Start collecting samples
[2018-12-22 12:29:05.692204 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:05.769560 UTC] Performing policy update
[2018-12-22 12:29:05.770138 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:05.859973 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:06.928836 UTC] Performing line search
[2018-12-22 12:29:07.058761 UTC] Updating baseline
[2018-12-22 12:29:09.297598 UTC] Computing logging information
-------------------------------------
| Iteration            | 1376       |
| ExpectedImprovement  | 0.016664   |
| ActualImprovement    | 0.016484   |
| ImprovementRatio     | 0.98923    |
| MeanKL               | 0.007605   |
| Entropy              | -1.442     |
| Perplexity           | 0.23645    |
| AveragePolicyStd     | 0.19373    |
| AveragePolicyStd[0]  | 0.22429    |
| AveragePolicyStd[1]  | 0.18556    |
| AveragePolicyStd[2]  | 0.15539    |
| AveragePolicyStd[3]  | 0.19951    |
| AveragePolicyStd[4]  | 0.14638    |
| AveragePolicyStd[5]  | 0.25127    |
| AverageReturn        | 1834.9     |
| MinReturn            | 276.39     |
| MaxReturn            | 1975.2     |
| StdReturn            | 187.51     |
| AverageEpisodeLength | 985.45     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 91.036     |
| TotalNEpisodes       | 21644      |
| TotalNSamples        | 6.8898e+06 |
| ExplainedVariance    | -0.029829  |
-------------------------------------
[2018-12-22 12:29:09.728397 UTC] Saving snapshot
[2018-12-22 12:29:09.728663 UTC] Starting iteration 1377
[2018-12-22 12:29:09.728785 UTC] Start collecting samples
[2018-12-22 12:29:12.655632 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:12.728581 UTC] Performing policy update
[2018-12-22 12:29:12.729183 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:12.818799 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:13.871167 UTC] Performing line search
[2018-12-22 12:29:14.000336 UTC] Updating baseline
[2018-12-22 12:29:15.565525 UTC] Computing logging information
-------------------------------------
| Iteration            | 1377       |
| ExpectedImprovement  | 0.017211   |
| ActualImprovement    | 0.015591   |
| ImprovementRatio     | 0.90584    |
| MeanKL               | 0.0076022  |
| Entropy              | -1.4446    |
| Perplexity           | 0.23583    |
| AveragePolicyStd     | 0.19366    |
| AveragePolicyStd[0]  | 0.2243     |
| AveragePolicyStd[1]  | 0.18563    |
| AveragePolicyStd[2]  | 0.15549    |
| AveragePolicyStd[3]  | 0.19923    |
| AveragePolicyStd[4]  | 0.1461     |
| AveragePolicyStd[5]  | 0.25118    |
| AverageReturn        | 1835.4     |
| MinReturn            | 276.39     |
| MaxReturn            | 1975.2     |
| StdReturn            | 187.55     |
| AverageEpisodeLength | 985.45     |
| MinEpisodeLength     | 208        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 91.036     |
| TotalNEpisodes       | 21645      |
| TotalNSamples        | 6.8908e+06 |
| ExplainedVariance    | -0.0072406 |
-------------------------------------
[2018-12-22 12:29:15.993892 UTC] Saving snapshot
[2018-12-22 12:29:15.994139 UTC] Starting iteration 1378
[2018-12-22 12:29:15.994256 UTC] Start collecting samples
[2018-12-22 12:29:19.133582 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:19.216377 UTC] Performing policy update
[2018-12-22 12:29:19.217073 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:19.310429 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:20.427473 UTC] Performing line search
[2018-12-22 12:29:20.561981 UTC] Updating baseline
[2018-12-22 12:29:21.837616 UTC] Computing logging information
-------------------------------------
| Iteration            | 1378       |
| ExpectedImprovement  | 0.015898   |
| ActualImprovement    | 0.015338   |
| ImprovementRatio     | 0.96479    |
| MeanKL               | 0.0077174  |
| Entropy              | -1.4495    |
| Perplexity           | 0.23468    |
| AveragePolicyStd     | 0.19353    |
| AveragePolicyStd[0]  | 0.22464    |
| AveragePolicyStd[1]  | 0.18522    |
| AveragePolicyStd[2]  | 0.15497    |
| AveragePolicyStd[3]  | 0.19901    |
| AveragePolicyStd[4]  | 0.14612    |
| AveragePolicyStd[5]  | 0.25122    |
| AverageReturn        | 1856.1     |
| MinReturn            | 1114.2     |
| MaxReturn            | 1975.2     |
| StdReturn            | 103.87     |
| AverageEpisodeLength | 993.37     |
| MinEpisodeLength     | 618        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 46.72      |
| TotalNEpisodes       | 21653      |
| TotalNSamples        | 6.8988e+06 |
| ExplainedVariance    | 0.011458   |
-------------------------------------
[2018-12-22 12:29:22.304380 UTC] Saving snapshot
[2018-12-22 12:29:22.304705 UTC] Starting iteration 1379
[2018-12-22 12:29:22.304828 UTC] Start collecting samples
[2018-12-22 12:29:25.356053 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:25.433982 UTC] Performing policy update
[2018-12-22 12:29:25.434732 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:25.525007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:26.594860 UTC] Performing line search
[2018-12-22 12:29:26.724046 UTC] Updating baseline
[2018-12-22 12:29:28.302579 UTC] Computing logging information
-------------------------------------
| Iteration            | 1379       |
| ExpectedImprovement  | 0.020149   |
| ActualImprovement    | 0.018969   |
| ImprovementRatio     | 0.94145    |
| MeanKL               | 0.0089391  |
| Entropy              | -1.4569    |
| Perplexity           | 0.23295    |
| AveragePolicyStd     | 0.1933     |
| AveragePolicyStd[0]  | 0.22438    |
| AveragePolicyStd[1]  | 0.18498    |
| AveragePolicyStd[2]  | 0.15471    |
| AveragePolicyStd[3]  | 0.19902    |
| AveragePolicyStd[4]  | 0.14588    |
| AveragePolicyStd[5]  | 0.2508     |
| AverageReturn        | 1848.3     |
| MinReturn            | 542.48     |
| MaxReturn            | 1975.4     |
| StdReturn            | 168.47     |
| AverageEpisodeLength | 986.45     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.654     |
| TotalNEpisodes       | 21661      |
| TotalNSamples        | 6.9061e+06 |
| ExplainedVariance    | 0.079598   |
-------------------------------------
[2018-12-22 12:29:28.735788 UTC] Saving snapshot
[2018-12-22 12:29:28.736038 UTC] Starting iteration 1380
[2018-12-22 12:29:28.736157 UTC] Start collecting samples
[2018-12-22 12:29:31.646792 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:31.720819 UTC] Performing policy update
[2018-12-22 12:29:31.721453 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:31.814846 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:32.883706 UTC] Performing line search
[2018-12-22 12:29:33.011763 UTC] Updating baseline
[2018-12-22 12:29:34.169845 UTC] Computing logging information
-------------------------------------
| Iteration            | 1380       |
| ExpectedImprovement  | 0.016674   |
| ActualImprovement    | 0.015691   |
| ImprovementRatio     | 0.94102    |
| MeanKL               | 0.008039   |
| Entropy              | -1.4576    |
| Perplexity           | 0.23279    |
| AveragePolicyStd     | 0.19326    |
| AveragePolicyStd[0]  | 0.22462    |
| AveragePolicyStd[1]  | 0.18492    |
| AveragePolicyStd[2]  | 0.15469    |
| AveragePolicyStd[3]  | 0.19888    |
| AveragePolicyStd[4]  | 0.146      |
| AveragePolicyStd[5]  | 0.25042    |
| AverageReturn        | 1849.7     |
| MinReturn            | 542.48     |
| MaxReturn            | 1975.4     |
| StdReturn            | 168.4      |
| AverageEpisodeLength | 986.45     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.654     |
| TotalNEpisodes       | 21662      |
| TotalNSamples        | 6.9071e+06 |
| ExplainedVariance    | 0.016482   |
-------------------------------------
[2018-12-22 12:29:34.600342 UTC] Saving snapshot
[2018-12-22 12:29:34.608478 UTC] Starting iteration 1381
[2018-12-22 12:29:34.608690 UTC] Start collecting samples
[2018-12-22 12:29:37.548419 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:37.622847 UTC] Performing policy update
[2018-12-22 12:29:37.623610 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:37.713211 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:38.784865 UTC] Performing line search
[2018-12-22 12:29:38.912752 UTC] Updating baseline
[2018-12-22 12:29:40.411658 UTC] Computing logging information
-------------------------------------
| Iteration            | 1381       |
| ExpectedImprovement  | 0.018632   |
| ActualImprovement    | 0.017606   |
| ImprovementRatio     | 0.94494    |
| MeanKL               | 0.007771   |
| Entropy              | -1.456     |
| Perplexity           | 0.23317    |
| AveragePolicyStd     | 0.19334    |
| AveragePolicyStd[0]  | 0.225      |
| AveragePolicyStd[1]  | 0.18501    |
| AveragePolicyStd[2]  | 0.15479    |
| AveragePolicyStd[3]  | 0.19848    |
| AveragePolicyStd[4]  | 0.14581    |
| AveragePolicyStd[5]  | 0.25097    |
| AverageReturn        | 1851       |
| MinReturn            | 542.48     |
| MaxReturn            | 1975.4     |
| StdReturn            | 164.48     |
| AverageEpisodeLength | 985.31     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.786     |
| TotalNEpisodes       | 21666      |
| TotalNSamples        | 6.9106e+06 |
| ExplainedVariance    | 0.23503    |
-------------------------------------
[2018-12-22 12:29:40.839053 UTC] Saving snapshot
[2018-12-22 12:29:40.839307 UTC] Starting iteration 1382
[2018-12-22 12:29:40.839428 UTC] Start collecting samples
[2018-12-22 12:29:43.865045 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:43.947816 UTC] Performing policy update
[2018-12-22 12:29:43.948525 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:44.038246 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:45.089083 UTC] Performing line search
[2018-12-22 12:29:45.216345 UTC] Updating baseline
[2018-12-22 12:29:46.362006 UTC] Computing logging information
-------------------------------------
| Iteration            | 1382       |
| ExpectedImprovement  | 0.019121   |
| ActualImprovement    | 0.018751   |
| ImprovementRatio     | 0.98065    |
| MeanKL               | 0.0080965  |
| Entropy              | -1.4619    |
| Perplexity           | 0.23179    |
| AveragePolicyStd     | 0.19314    |
| AveragePolicyStd[0]  | 0.22432    |
| AveragePolicyStd[1]  | 0.18436    |
| AveragePolicyStd[2]  | 0.15475    |
| AveragePolicyStd[3]  | 0.19868    |
| AveragePolicyStd[4]  | 0.14584    |
| AveragePolicyStd[5]  | 0.25087    |
| AverageReturn        | 1859       |
| MinReturn            | 542.48     |
| MaxReturn            | 1977.3     |
| StdReturn            | 166.36     |
| AverageEpisodeLength | 985.17     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.773     |
| TotalNEpisodes       | 21676      |
| TotalNSamples        | 6.9206e+06 |
| ExplainedVariance    | 0.079998   |
-------------------------------------
[2018-12-22 12:29:46.792406 UTC] Saving snapshot
[2018-12-22 12:29:46.792669 UTC] Starting iteration 1383
[2018-12-22 12:29:46.792798 UTC] Start collecting samples
[2018-12-22 12:29:49.720653 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:49.799584 UTC] Performing policy update
[2018-12-22 12:29:49.800169 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:49.896682 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:51.005920 UTC] Performing line search
[2018-12-22 12:29:51.138694 UTC] Updating baseline
[2018-12-22 12:29:52.515318 UTC] Computing logging information
-------------------------------------
| Iteration            | 1383       |
| ExpectedImprovement  | 0.017649   |
| ActualImprovement    | 0.016835   |
| ImprovementRatio     | 0.95384    |
| MeanKL               | 0.0073054  |
| Entropy              | -1.4671    |
| Perplexity           | 0.2306     |
| AveragePolicyStd     | 0.19298    |
| AveragePolicyStd[0]  | 0.22427    |
| AveragePolicyStd[1]  | 0.18411    |
| AveragePolicyStd[2]  | 0.15455    |
| AveragePolicyStd[3]  | 0.19876    |
| AveragePolicyStd[4]  | 0.14565    |
| AveragePolicyStd[5]  | 0.25054    |
| AverageReturn        | 1860.4     |
| MinReturn            | 542.48     |
| MaxReturn            | 1984.4     |
| StdReturn            | 166.83     |
| AverageEpisodeLength | 985.17     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.773     |
| TotalNEpisodes       | 21677      |
| TotalNSamples        | 6.9216e+06 |
| ExplainedVariance    | -0.14483   |
-------------------------------------
[2018-12-22 12:29:52.961985 UTC] Saving snapshot
[2018-12-22 12:29:52.962222 UTC] Starting iteration 1384
[2018-12-22 12:29:52.962357 UTC] Start collecting samples
[2018-12-22 12:29:56.118409 UTC] Computing input variables for policy optimization
[2018-12-22 12:29:56.198666 UTC] Performing policy update
[2018-12-22 12:29:56.199316 UTC] Computing gradient in Euclidean space
[2018-12-22 12:29:56.294211 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:29:57.396469 UTC] Performing line search
[2018-12-22 12:29:57.528455 UTC] Updating baseline
[2018-12-22 12:29:58.828334 UTC] Computing logging information
-------------------------------------
| Iteration            | 1384       |
| ExpectedImprovement  | 0.017047   |
| ActualImprovement    | 0.015358   |
| ImprovementRatio     | 0.90089    |
| MeanKL               | 0.0077244  |
| Entropy              | -1.4616    |
| Perplexity           | 0.23186    |
| AveragePolicyStd     | 0.19315    |
| AveragePolicyStd[0]  | 0.22439    |
| AveragePolicyStd[1]  | 0.18447    |
| AveragePolicyStd[2]  | 0.15456    |
| AveragePolicyStd[3]  | 0.19874    |
| AveragePolicyStd[4]  | 0.14594    |
| AveragePolicyStd[5]  | 0.25077    |
| AverageReturn        | 1871.8     |
| MinReturn            | 542.48     |
| MaxReturn            | 1984.4     |
| StdReturn            | 157.2      |
| AverageEpisodeLength | 987.94     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.477     |
| TotalNEpisodes       | 21682      |
| TotalNSamples        | 6.9266e+06 |
| ExplainedVariance    | 0.00011282 |
-------------------------------------
[2018-12-22 12:29:59.289466 UTC] Saving snapshot
[2018-12-22 12:29:59.289890 UTC] Starting iteration 1385
[2018-12-22 12:29:59.290051 UTC] Start collecting samples
[2018-12-22 12:30:02.325872 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:02.403141 UTC] Performing policy update
[2018-12-22 12:30:02.403899 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:02.493872 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:03.551705 UTC] Performing line search
[2018-12-22 12:30:03.678753 UTC] Updating baseline
[2018-12-22 12:30:04.821470 UTC] Computing logging information
-------------------------------------
| Iteration            | 1385       |
| ExpectedImprovement  | 0.017691   |
| ActualImprovement    | 0.01707    |
| ImprovementRatio     | 0.96489    |
| MeanKL               | 0.0080697  |
| Entropy              | -1.4622    |
| Perplexity           | 0.23173    |
| AveragePolicyStd     | 0.19312    |
| AveragePolicyStd[0]  | 0.22451    |
| AveragePolicyStd[1]  | 0.18419    |
| AveragePolicyStd[2]  | 0.15462    |
| AveragePolicyStd[3]  | 0.19837    |
| AveragePolicyStd[4]  | 0.14617    |
| AveragePolicyStd[5]  | 0.25088    |
| AverageReturn        | 1881.6     |
| MinReturn            | 542.48     |
| MaxReturn            | 1984.4     |
| StdReturn            | 157.98     |
| AverageEpisodeLength | 987.94     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.477     |
| TotalNEpisodes       | 21690      |
| TotalNSamples        | 6.9346e+06 |
| ExplainedVariance    | 0.025505   |
-------------------------------------
[2018-12-22 12:30:05.252688 UTC] Saving snapshot
[2018-12-22 12:30:05.252955 UTC] Starting iteration 1386
[2018-12-22 12:30:05.253073 UTC] Start collecting samples
[2018-12-22 12:30:08.158228 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:08.233580 UTC] Performing policy update
[2018-12-22 12:30:08.234268 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:08.324949 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:09.474494 UTC] Performing line search
[2018-12-22 12:30:09.603117 UTC] Updating baseline
[2018-12-22 12:30:10.754557 UTC] Computing logging information
-------------------------------------
| Iteration            | 1386       |
| ExpectedImprovement  | 0.017066   |
| ActualImprovement    | 0.016511   |
| ImprovementRatio     | 0.9675     |
| MeanKL               | 0.0080677  |
| Entropy              | -1.4603    |
| Perplexity           | 0.23218    |
| AveragePolicyStd     | 0.19317    |
| AveragePolicyStd[0]  | 0.2247     |
| AveragePolicyStd[1]  | 0.1843     |
| AveragePolicyStd[2]  | 0.155      |
| AveragePolicyStd[3]  | 0.19825    |
| AveragePolicyStd[4]  | 0.1461     |
| AveragePolicyStd[5]  | 0.25065    |
| AverageReturn        | 1884.2     |
| MinReturn            | 542.48     |
| MaxReturn            | 1984.4     |
| StdReturn            | 158.26     |
| AverageEpisodeLength | 987.94     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.477     |
| TotalNEpisodes       | 21693      |
| TotalNSamples        | 6.9376e+06 |
| ExplainedVariance    | -0.027159  |
-------------------------------------
[2018-12-22 12:30:11.182975 UTC] Saving snapshot
[2018-12-22 12:30:11.183219 UTC] Starting iteration 1387
[2018-12-22 12:30:11.183353 UTC] Start collecting samples
[2018-12-22 12:30:14.122915 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:14.197118 UTC] Performing policy update
[2018-12-22 12:30:14.197802 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:14.287196 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:15.342555 UTC] Performing line search
[2018-12-22 12:30:15.469524 UTC] Updating baseline
[2018-12-22 12:30:17.541124 UTC] Computing logging information
--------------------------------------
| Iteration            | 1387        |
| ExpectedImprovement  | 0.017945    |
| ActualImprovement    | 0.016623    |
| ImprovementRatio     | 0.92636     |
| MeanKL               | 0.0080662   |
| Entropy              | -1.4599     |
| Perplexity           | 0.23226     |
| AveragePolicyStd     | 0.1932      |
| AveragePolicyStd[0]  | 0.22482     |
| AveragePolicyStd[1]  | 0.18417     |
| AveragePolicyStd[2]  | 0.15483     |
| AveragePolicyStd[3]  | 0.19865     |
| AveragePolicyStd[4]  | 0.14603     |
| AveragePolicyStd[5]  | 0.25068     |
| AverageReturn        | 1888.3      |
| MinReturn            | 542.48      |
| MaxReturn            | 2000.3      |
| StdReturn            | 158.67      |
| AverageEpisodeLength | 987.94      |
| MinEpisodeLength     | 308         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 78.477      |
| TotalNEpisodes       | 21697       |
| TotalNSamples        | 6.9416e+06  |
| ExplainedVariance    | -0.00020919 |
--------------------------------------
[2018-12-22 12:30:17.976834 UTC] Saving snapshot
[2018-12-22 12:30:17.977073 UTC] Starting iteration 1388
[2018-12-22 12:30:17.977190 UTC] Start collecting samples
[2018-12-22 12:30:20.981302 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:21.057159 UTC] Performing policy update
[2018-12-22 12:30:21.057817 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:21.145555 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:22.207119 UTC] Performing line search
[2018-12-22 12:30:22.333830 UTC] Updating baseline
[2018-12-22 12:30:23.803167 UTC] Computing logging information
-------------------------------------
| Iteration            | 1388       |
| ExpectedImprovement  | 0.016451   |
| ActualImprovement    | 0.015524   |
| ImprovementRatio     | 0.94368    |
| MeanKL               | 0.0085057  |
| Entropy              | -1.4613    |
| Perplexity           | 0.23193    |
| AveragePolicyStd     | 0.19321    |
| AveragePolicyStd[0]  | 0.22491    |
| AveragePolicyStd[1]  | 0.18363    |
| AveragePolicyStd[2]  | 0.15527    |
| AveragePolicyStd[3]  | 0.19839    |
| AveragePolicyStd[4]  | 0.14542    |
| AveragePolicyStd[5]  | 0.25163    |
| AverageReturn        | 1896.6     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 159.93     |
| AverageEpisodeLength | 987.94     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.477     |
| TotalNEpisodes       | 21706      |
| TotalNSamples        | 6.9506e+06 |
| ExplainedVariance    | -0.0066321 |
-------------------------------------
[2018-12-22 12:30:24.232962 UTC] Saving snapshot
[2018-12-22 12:30:24.233265 UTC] Starting iteration 1389
[2018-12-22 12:30:24.233387 UTC] Start collecting samples
[2018-12-22 12:30:27.152460 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:27.227217 UTC] Performing policy update
[2018-12-22 12:30:27.228108 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:27.317487 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:28.373647 UTC] Performing line search
[2018-12-22 12:30:28.500418 UTC] Updating baseline
[2018-12-22 12:30:30.582087 UTC] Computing logging information
-------------------------------------
| Iteration            | 1389       |
| ExpectedImprovement  | 0.019263   |
| ActualImprovement    | 0.017797   |
| ImprovementRatio     | 0.92389    |
| MeanKL               | 0.0073765  |
| Entropy              | -1.4606    |
| Perplexity           | 0.23209    |
| AveragePolicyStd     | 0.19327    |
| AveragePolicyStd[0]  | 0.22544    |
| AveragePolicyStd[1]  | 0.18315    |
| AveragePolicyStd[2]  | 0.15498    |
| AveragePolicyStd[3]  | 0.19847    |
| AveragePolicyStd[4]  | 0.14555    |
| AveragePolicyStd[5]  | 0.25199    |
| AverageReturn        | 1898.2     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 160.29     |
| AverageEpisodeLength | 987.49     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.535     |
| TotalNEpisodes       | 21709      |
| TotalNSamples        | 6.9536e+06 |
| ExplainedVariance    | 0.15391    |
-------------------------------------
[2018-12-22 12:30:31.008415 UTC] Saving snapshot
[2018-12-22 12:30:31.008654 UTC] Starting iteration 1390
[2018-12-22 12:30:31.008797 UTC] Start collecting samples
[2018-12-22 12:30:33.909492 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:33.988314 UTC] Performing policy update
[2018-12-22 12:30:33.988924 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:34.078458 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:35.141707 UTC] Performing line search
[2018-12-22 12:30:35.269396 UTC] Updating baseline
[2018-12-22 12:30:36.426986 UTC] Computing logging information
-------------------------------------
| Iteration            | 1390       |
| ExpectedImprovement  | 0.017666   |
| ActualImprovement    | 0.015894   |
| ImprovementRatio     | 0.89969    |
| MeanKL               | 0.0085693  |
| Entropy              | -1.4562    |
| Perplexity           | 0.23312    |
| AveragePolicyStd     | 0.19338    |
| AveragePolicyStd[0]  | 0.22515    |
| AveragePolicyStd[1]  | 0.18324    |
| AveragePolicyStd[2]  | 0.15526    |
| AveragePolicyStd[3]  | 0.1986     |
| AveragePolicyStd[4]  | 0.14585    |
| AveragePolicyStd[5]  | 0.25219    |
| AverageReturn        | 1899.7     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 160.29     |
| AverageEpisodeLength | 987.49     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.535     |
| TotalNEpisodes       | 21711      |
| TotalNSamples        | 6.9556e+06 |
| ExplainedVariance    | -0.014505  |
-------------------------------------
[2018-12-22 12:30:36.860863 UTC] Saving snapshot
[2018-12-22 12:30:36.869334 UTC] Starting iteration 1391
[2018-12-22 12:30:36.869552 UTC] Start collecting samples
[2018-12-22 12:30:39.902346 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:39.984668 UTC] Performing policy update
[2018-12-22 12:30:39.985241 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:40.075060 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:41.133541 UTC] Performing line search
[2018-12-22 12:30:41.263100 UTC] Updating baseline
[2018-12-22 12:30:42.517262 UTC] Computing logging information
-------------------------------------
| Iteration            | 1391       |
| ExpectedImprovement  | 0.01971    |
| ActualImprovement    | 0.018585   |
| ImprovementRatio     | 0.94289    |
| MeanKL               | 0.0076083  |
| Entropy              | -1.4608    |
| Perplexity           | 0.23205    |
| AveragePolicyStd     | 0.19324    |
| AveragePolicyStd[0]  | 0.22453    |
| AveragePolicyStd[1]  | 0.18363    |
| AveragePolicyStd[2]  | 0.15534    |
| AveragePolicyStd[3]  | 0.19855    |
| AveragePolicyStd[4]  | 0.14525    |
| AveragePolicyStd[5]  | 0.25216    |
| AverageReturn        | 1904.7     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 160.64     |
| AverageEpisodeLength | 987.53     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.541     |
| TotalNEpisodes       | 21721      |
| TotalNSamples        | 6.9656e+06 |
| ExplainedVariance    | 0.0094143  |
-------------------------------------
[2018-12-22 12:30:42.947646 UTC] Saving snapshot
[2018-12-22 12:30:42.947893 UTC] Starting iteration 1392
[2018-12-22 12:30:42.948009 UTC] Start collecting samples
[2018-12-22 12:30:45.891080 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:45.970276 UTC] Performing policy update
[2018-12-22 12:30:45.971053 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:46.060238 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:47.124498 UTC] Performing line search
[2018-12-22 12:30:47.251687 UTC] Updating baseline
[2018-12-22 12:30:48.476939 UTC] Computing logging information
-------------------------------------
| Iteration            | 1392       |
| ExpectedImprovement  | 0.020859   |
| ActualImprovement    | 0.019135   |
| ImprovementRatio     | 0.91735    |
| MeanKL               | 0.0078799  |
| Entropy              | -1.4667    |
| Perplexity           | 0.23069    |
| AveragePolicyStd     | 0.19305    |
| AveragePolicyStd[0]  | 0.22458    |
| AveragePolicyStd[1]  | 0.18345    |
| AveragePolicyStd[2]  | 0.15515    |
| AveragePolicyStd[3]  | 0.19862    |
| AveragePolicyStd[4]  | 0.14507    |
| AveragePolicyStd[5]  | 0.2514     |
| AverageReturn        | 1909       |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 160.1      |
| AverageEpisodeLength | 987.53     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.541     |
| TotalNEpisodes       | 21725      |
| TotalNSamples        | 6.9696e+06 |
| ExplainedVariance    | -0.0022124 |
-------------------------------------
[2018-12-22 12:30:48.907658 UTC] Saving snapshot
[2018-12-22 12:30:48.907911 UTC] Starting iteration 1393
[2018-12-22 12:30:48.908031 UTC] Start collecting samples
[2018-12-22 12:30:51.847916 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:51.922685 UTC] Performing policy update
[2018-12-22 12:30:51.923322 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:52.014335 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:53.077538 UTC] Performing line search
[2018-12-22 12:30:53.208394 UTC] Updating baseline
[2018-12-22 12:30:54.543081 UTC] Computing logging information
-------------------------------------
| Iteration            | 1393       |
| ExpectedImprovement  | 0.022794   |
| ActualImprovement    | 0.020527   |
| ImprovementRatio     | 0.90053    |
| MeanKL               | 0.007438   |
| Entropy              | -1.479     |
| Perplexity           | 0.22787    |
| AveragePolicyStd     | 0.19261    |
| AveragePolicyStd[0]  | 0.22404    |
| AveragePolicyStd[1]  | 0.18368    |
| AveragePolicyStd[2]  | 0.15503    |
| AveragePolicyStd[3]  | 0.19848    |
| AveragePolicyStd[4]  | 0.14452    |
| AveragePolicyStd[5]  | 0.24992    |
| AverageReturn        | 1903.9     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 168.51     |
| AverageEpisodeLength | 984.95     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 82.239     |
| TotalNEpisodes       | 21727      |
| TotalNSamples        | 6.9713e+06 |
| ExplainedVariance    | 0.36156    |
-------------------------------------
[2018-12-22 12:30:54.975194 UTC] Saving snapshot
[2018-12-22 12:30:54.975481 UTC] Starting iteration 1394
[2018-12-22 12:30:54.975643 UTC] Start collecting samples
[2018-12-22 12:30:57.974907 UTC] Computing input variables for policy optimization
[2018-12-22 12:30:58.053154 UTC] Performing policy update
[2018-12-22 12:30:58.053795 UTC] Computing gradient in Euclidean space
[2018-12-22 12:30:58.142853 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:30:59.195076 UTC] Performing line search
[2018-12-22 12:30:59.323432 UTC] Updating baseline
[2018-12-22 12:31:00.644298 UTC] Computing logging information
-------------------------------------
| Iteration            | 1394       |
| ExpectedImprovement  | 0.020667   |
| ActualImprovement    | 0.019131   |
| ImprovementRatio     | 0.9257     |
| MeanKL               | 0.0080249  |
| Entropy              | -1.4809    |
| Perplexity           | 0.22744    |
| AveragePolicyStd     | 0.19254    |
| AveragePolicyStd[0]  | 0.22442    |
| AveragePolicyStd[1]  | 0.18338    |
| AveragePolicyStd[2]  | 0.15509    |
| AveragePolicyStd[3]  | 0.19811    |
| AveragePolicyStd[4]  | 0.14465    |
| AveragePolicyStd[5]  | 0.2496     |
| AverageReturn        | 1895.8     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 205.62     |
| AverageEpisodeLength | 978.93     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.85     |
| TotalNEpisodes       | 21735      |
| TotalNSamples        | 6.9787e+06 |
| ExplainedVariance    | 0.051032   |
-------------------------------------
[2018-12-22 12:31:01.069864 UTC] Saving snapshot
[2018-12-22 12:31:01.070140 UTC] Starting iteration 1395
[2018-12-22 12:31:01.070272 UTC] Start collecting samples
[2018-12-22 12:31:04.068090 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:04.145574 UTC] Performing policy update
[2018-12-22 12:31:04.146233 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:04.234623 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:05.292712 UTC] Performing line search
[2018-12-22 12:31:05.419765 UTC] Updating baseline
[2018-12-22 12:31:07.502330 UTC] Computing logging information
-------------------------------------
| Iteration            | 1395       |
| ExpectedImprovement  | 0.016666   |
| ActualImprovement    | 0.01595    |
| ImprovementRatio     | 0.95709    |
| MeanKL               | 0.007805   |
| Entropy              | -1.4837    |
| Perplexity           | 0.2268     |
| AveragePolicyStd     | 0.19245    |
| AveragePolicyStd[0]  | 0.22428    |
| AveragePolicyStd[1]  | 0.18323    |
| AveragePolicyStd[2]  | 0.15512    |
| AveragePolicyStd[3]  | 0.19785    |
| AveragePolicyStd[4]  | 0.14461    |
| AveragePolicyStd[5]  | 0.24961    |
| AverageReturn        | 1892.2     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 210.49     |
| AverageEpisodeLength | 976.76     |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.69     |
| TotalNEpisodes       | 21742      |
| TotalNSamples        | 6.9855e+06 |
| ExplainedVariance    | 0.13758    |
-------------------------------------
[2018-12-22 12:31:07.935975 UTC] Saving snapshot
[2018-12-22 12:31:07.936219 UTC] Starting iteration 1396
[2018-12-22 12:31:07.936336 UTC] Start collecting samples
[2018-12-22 12:31:10.843232 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:10.917986 UTC] Performing policy update
[2018-12-22 12:31:10.918739 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:11.008295 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:12.067598 UTC] Performing line search
[2018-12-22 12:31:12.194566 UTC] Updating baseline
[2018-12-22 12:31:13.433167 UTC] Computing logging information
------------------------------------
| Iteration            | 1396      |
| ExpectedImprovement  | 0.021295  |
| ActualImprovement    | 0.021035  |
| ImprovementRatio     | 0.9878    |
| MeanKL               | 0.007657  |
| Entropy              | -1.4859   |
| Perplexity           | 0.22631   |
| AveragePolicyStd     | 0.19238   |
| AveragePolicyStd[0]  | 0.22412   |
| AveragePolicyStd[1]  | 0.18337   |
| AveragePolicyStd[2]  | 0.15496   |
| AveragePolicyStd[3]  | 0.19788   |
| AveragePolicyStd[4]  | 0.14453   |
| AveragePolicyStd[5]  | 0.2494    |
| AverageReturn        | 1883.4    |
| MinReturn            | 542.48    |
| MaxReturn            | 2004.6    |
| StdReturn            | 228.22    |
| AverageEpisodeLength | 972.1     |
| MinEpisodeLength     | 308       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 111.7     |
| TotalNEpisodes       | 21744     |
| TotalNSamples        | 6.987e+06 |
| ExplainedVariance    | 0.16325   |
------------------------------------
[2018-12-22 12:31:13.857005 UTC] Saving snapshot
[2018-12-22 12:31:13.857260 UTC] Starting iteration 1397
[2018-12-22 12:31:13.857384 UTC] Start collecting samples
[2018-12-22 12:31:16.810471 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:16.887451 UTC] Performing policy update
[2018-12-22 12:31:16.888102 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:16.977429 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:18.040232 UTC] Performing line search
[2018-12-22 12:31:18.167443 UTC] Updating baseline
[2018-12-22 12:31:20.231807 UTC] Computing logging information
-------------------------------------
| Iteration            | 1397       |
| ExpectedImprovement  | 0.01749    |
| ActualImprovement    | 0.01594    |
| ImprovementRatio     | 0.91137    |
| MeanKL               | 0.0081232  |
| Entropy              | -1.4926    |
| Perplexity           | 0.22479    |
| AveragePolicyStd     | 0.19219    |
| AveragePolicyStd[0]  | 0.22422    |
| AveragePolicyStd[1]  | 0.18315    |
| AveragePolicyStd[2]  | 0.15483    |
| AveragePolicyStd[3]  | 0.19749    |
| AveragePolicyStd[4]  | 0.14409    |
| AveragePolicyStd[5]  | 0.24938    |
| AverageReturn        | 1887.2     |
| MinReturn            | 542.48     |
| MaxReturn            | 2004.6     |
| StdReturn            | 229.05     |
| AverageEpisodeLength | 972.1      |
| MinEpisodeLength     | 308        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.7      |
| TotalNEpisodes       | 21749      |
| TotalNSamples        | 6.992e+06  |
| ExplainedVariance    | -0.0063937 |
-------------------------------------
[2018-12-22 12:31:20.660294 UTC] Saving snapshot
[2018-12-22 12:31:20.660550 UTC] Starting iteration 1398
[2018-12-22 12:31:20.660669 UTC] Start collecting samples
[2018-12-22 12:31:23.644701 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:23.722034 UTC] Performing policy update
[2018-12-22 12:31:23.722979 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:23.813530 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:24.879013 UTC] Performing line search
[2018-12-22 12:31:25.006279 UTC] Updating baseline
[2018-12-22 12:31:26.147933 UTC] Computing logging information
-------------------------------------
| Iteration            | 1398       |
| ExpectedImprovement  | 0.017369   |
| ActualImprovement    | 0.016242   |
| ImprovementRatio     | 0.93511    |
| MeanKL               | 0.008568   |
| Entropy              | -1.4968    |
| Perplexity           | 0.22385    |
| AveragePolicyStd     | 0.19206    |
| AveragePolicyStd[0]  | 0.22426    |
| AveragePolicyStd[1]  | 0.18292    |
| AveragePolicyStd[2]  | 0.15425    |
| AveragePolicyStd[3]  | 0.19782    |
| AveragePolicyStd[4]  | 0.1442     |
| AveragePolicyStd[5]  | 0.24892    |
| AverageReturn        | 1903.4     |
| MinReturn            | 724.77     |
| MaxReturn            | 2004.6     |
| StdReturn            | 185.53     |
| AverageEpisodeLength | 979.02     |
| MinEpisodeLength     | 398        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 89.596     |
| TotalNEpisodes       | 21758      |
| TotalNSamples        | 7.001e+06  |
| ExplainedVariance    | -0.0044435 |
-------------------------------------
[2018-12-22 12:31:26.578066 UTC] Saving snapshot
[2018-12-22 12:31:26.578348 UTC] Starting iteration 1399
[2018-12-22 12:31:26.578471 UTC] Start collecting samples
[2018-12-22 12:31:29.489526 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:29.562944 UTC] Performing policy update
[2018-12-22 12:31:29.563896 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:29.653630 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:30.734476 UTC] Performing line search
[2018-12-22 12:31:30.862756 UTC] Updating baseline
[2018-12-22 12:31:33.019583 UTC] Computing logging information
------------------------------------
| Iteration            | 1399      |
| ExpectedImprovement  | 0.017184  |
| ActualImprovement    | 0.015117  |
| ImprovementRatio     | 0.87973   |
| MeanKL               | 0.0087808 |
| Entropy              | -1.4945   |
| Perplexity           | 0.22435   |
| AveragePolicyStd     | 0.19218   |
| AveragePolicyStd[0]  | 0.22465   |
| AveragePolicyStd[1]  | 0.18291   |
| AveragePolicyStd[2]  | 0.15376   |
| AveragePolicyStd[3]  | 0.19786   |
| AveragePolicyStd[4]  | 0.14432   |
| AveragePolicyStd[5]  | 0.2496    |
| AverageReturn        | 1902.4    |
| MinReturn            | 724.77    |
| MaxReturn            | 2004.6    |
| StdReturn            | 185.42    |
| AverageEpisodeLength | 979.02    |
| MinEpisodeLength     | 398       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 89.596    |
| TotalNEpisodes       | 21759     |
| TotalNSamples        | 7.002e+06 |
| ExplainedVariance    | 0.0016067 |
------------------------------------
[2018-12-22 12:31:33.450566 UTC] Saving snapshot
[2018-12-22 12:31:33.450814 UTC] Starting iteration 1400
[2018-12-22 12:31:33.450935 UTC] Start collecting samples
[2018-12-22 12:31:36.396764 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:36.472198 UTC] Performing policy update
[2018-12-22 12:31:36.473110 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:36.563804 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:37.624026 UTC] Performing line search
[2018-12-22 12:31:37.753159 UTC] Updating baseline
[2018-12-22 12:31:39.142499 UTC] Computing logging information
-------------------------------------
| Iteration            | 1400       |
| ExpectedImprovement  | 0.016126   |
| ActualImprovement    | 0.015049   |
| ImprovementRatio     | 0.93326    |
| MeanKL               | 0.0085001  |
| Entropy              | -1.4942    |
| Perplexity           | 0.22442    |
| AveragePolicyStd     | 0.19223    |
| AveragePolicyStd[0]  | 0.22502    |
| AveragePolicyStd[1]  | 0.18264    |
| AveragePolicyStd[2]  | 0.15362    |
| AveragePolicyStd[3]  | 0.1982     |
| AveragePolicyStd[4]  | 0.14415    |
| AveragePolicyStd[5]  | 0.24972    |
| AverageReturn        | 1903.4     |
| MinReturn            | 724.77     |
| MaxReturn            | 2004.6     |
| StdReturn            | 185.8      |
| AverageEpisodeLength | 979.02     |
| MinEpisodeLength     | 398        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 89.596     |
| TotalNEpisodes       | 21763      |
| TotalNSamples        | 7.006e+06  |
| ExplainedVariance    | -0.0032264 |
-------------------------------------
[2018-12-22 12:31:39.568993 UTC] Saving snapshot
[2018-12-22 12:31:39.577070 UTC] Starting iteration 1401
[2018-12-22 12:31:39.577282 UTC] Start collecting samples
[2018-12-22 12:31:42.604039 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:42.682151 UTC] Performing policy update
[2018-12-22 12:31:42.682807 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:42.771939 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:43.831142 UTC] Performing line search
[2018-12-22 12:31:43.962219 UTC] Updating baseline
[2018-12-22 12:31:45.277474 UTC] Computing logging information
-------------------------------------
| Iteration            | 1401       |
| ExpectedImprovement  | 0.016727   |
| ActualImprovement    | 0.015978   |
| ImprovementRatio     | 0.95527    |
| MeanKL               | 0.0081223  |
| Entropy              | -1.4904    |
| Perplexity           | 0.22529    |
| AveragePolicyStd     | 0.19231    |
| AveragePolicyStd[0]  | 0.22569    |
| AveragePolicyStd[1]  | 0.18244    |
| AveragePolicyStd[2]  | 0.15395    |
| AveragePolicyStd[3]  | 0.19823    |
| AveragePolicyStd[4]  | 0.14447    |
| AveragePolicyStd[5]  | 0.24909    |
| AverageReturn        | 1904.8     |
| MinReturn            | 724.77     |
| MaxReturn            | 2004.6     |
| StdReturn            | 183.12     |
| AverageEpisodeLength | 979.81     |
| MinEpisodeLength     | 398        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 86.886     |
| TotalNEpisodes       | 21773      |
| TotalNSamples        | 7.0156e+06 |
| ExplainedVariance    | 0.070978   |
-------------------------------------
[2018-12-22 12:31:45.710725 UTC] Saving snapshot
[2018-12-22 12:31:45.710975 UTC] Starting iteration 1402
[2018-12-22 12:31:45.711093 UTC] Start collecting samples
[2018-12-22 12:31:48.601889 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:48.675587 UTC] Performing policy update
[2018-12-22 12:31:48.676379 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:48.765403 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:49.818814 UTC] Performing line search
[2018-12-22 12:31:49.949862 UTC] Updating baseline
[2018-12-22 12:31:51.260881 UTC] Computing logging information
-------------------------------------
| Iteration            | 1402       |
| ExpectedImprovement  | 0.017621   |
| ActualImprovement    | 0.016491   |
| ImprovementRatio     | 0.93587    |
| MeanKL               | 0.0079865  |
| Entropy              | -1.4906    |
| Perplexity           | 0.22523    |
| AveragePolicyStd     | 0.19233    |
| AveragePolicyStd[0]  | 0.22542    |
| AveragePolicyStd[1]  | 0.18247    |
| AveragePolicyStd[2]  | 0.15403    |
| AveragePolicyStd[3]  | 0.19794    |
| AveragePolicyStd[4]  | 0.1443     |
| AveragePolicyStd[5]  | 0.2498     |
| AverageReturn        | 1890       |
| MinReturn            | 488.25     |
| MaxReturn            | 2004.6     |
| StdReturn            | 230.97     |
| AverageEpisodeLength | 972.65     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.06     |
| TotalNEpisodes       | 21775      |
| TotalNSamples        | 7.0169e+06 |
| ExplainedVariance    | 0.33671    |
-------------------------------------
[2018-12-22 12:31:51.692493 UTC] Saving snapshot
[2018-12-22 12:31:51.692790 UTC] Starting iteration 1403
[2018-12-22 12:31:51.692909 UTC] Start collecting samples
[2018-12-22 12:31:54.631448 UTC] Computing input variables for policy optimization
[2018-12-22 12:31:54.706937 UTC] Performing policy update
[2018-12-22 12:31:54.707565 UTC] Computing gradient in Euclidean space
[2018-12-22 12:31:54.798302 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:31:55.862029 UTC] Performing line search
[2018-12-22 12:31:55.993054 UTC] Updating baseline
[2018-12-22 12:31:57.310570 UTC] Computing logging information
-------------------------------------
| Iteration            | 1403       |
| ExpectedImprovement  | 0.017475   |
| ActualImprovement    | 0.017045   |
| ImprovementRatio     | 0.97539    |
| MeanKL               | 0.0075452  |
| Entropy              | -1.4895    |
| Perplexity           | 0.2255     |
| AveragePolicyStd     | 0.19237    |
| AveragePolicyStd[0]  | 0.22531    |
| AveragePolicyStd[1]  | 0.18283    |
| AveragePolicyStd[2]  | 0.15387    |
| AveragePolicyStd[3]  | 0.19776    |
| AveragePolicyStd[4]  | 0.14444    |
| AveragePolicyStd[5]  | 0.24999    |
| AverageReturn        | 1889.7     |
| MinReturn            | 488.25     |
| MaxReturn            | 2004.6     |
| StdReturn            | 230.99     |
| AverageEpisodeLength | 972.65     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.06     |
| TotalNEpisodes       | 21779      |
| TotalNSamples        | 7.0209e+06 |
| ExplainedVariance    | 0.065963   |
-------------------------------------
[2018-12-22 12:31:57.741251 UTC] Saving snapshot
[2018-12-22 12:31:57.741548 UTC] Starting iteration 1404
[2018-12-22 12:31:57.741670 UTC] Start collecting samples
[2018-12-22 12:32:00.785345 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:00.865680 UTC] Performing policy update
[2018-12-22 12:32:00.866367 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:00.957189 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:02.027654 UTC] Performing line search
[2018-12-22 12:32:02.156956 UTC] Updating baseline
[2018-12-22 12:32:03.736206 UTC] Computing logging information
-------------------------------------
| Iteration            | 1404       |
| ExpectedImprovement  | 0.01905    |
| ActualImprovement    | 0.017673   |
| ImprovementRatio     | 0.92772    |
| MeanKL               | 0.0075534  |
| Entropy              | -1.493     |
| Perplexity           | 0.2247     |
| AveragePolicyStd     | 0.19223    |
| AveragePolicyStd[0]  | 0.22577    |
| AveragePolicyStd[1]  | 0.18275    |
| AveragePolicyStd[2]  | 0.15377    |
| AveragePolicyStd[3]  | 0.19749    |
| AveragePolicyStd[4]  | 0.14449    |
| AveragePolicyStd[5]  | 0.24913    |
| AverageReturn        | 1878.6     |
| MinReturn            | 488.25     |
| MaxReturn            | 2004.6     |
| StdReturn            | 245.72     |
| AverageEpisodeLength | 967.04     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.23     |
| TotalNEpisodes       | 21790      |
| TotalNSamples        | 7.0313e+06 |
| ExplainedVariance    | 0.19301    |
-------------------------------------
[2018-12-22 12:32:04.162703 UTC] Saving snapshot
[2018-12-22 12:32:04.162962 UTC] Starting iteration 1405
[2018-12-22 12:32:04.163080 UTC] Start collecting samples
[2018-12-22 12:32:07.071334 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:07.145321 UTC] Performing policy update
[2018-12-22 12:32:07.146119 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:07.235934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:08.296780 UTC] Performing line search
[2018-12-22 12:32:08.425312 UTC] Updating baseline
[2018-12-22 12:32:09.916848 UTC] Computing logging information
-------------------------------------
| Iteration            | 1405       |
| ExpectedImprovement  | 0.020302   |
| ActualImprovement    | 0.01964    |
| ImprovementRatio     | 0.96738    |
| MeanKL               | 0.0072807  |
| Entropy              | -1.5005    |
| Perplexity           | 0.22303    |
| AveragePolicyStd     | 0.19202    |
| AveragePolicyStd[0]  | 0.22612    |
| AveragePolicyStd[1]  | 0.18231    |
| AveragePolicyStd[2]  | 0.15372    |
| AveragePolicyStd[3]  | 0.197      |
| AveragePolicyStd[4]  | 0.14406    |
| AveragePolicyStd[5]  | 0.24892    |
| AverageReturn        | 1878.2     |
| MinReturn            | 488.25     |
| MaxReturn            | 2004.6     |
| StdReturn            | 245.63     |
| AverageEpisodeLength | 967.04     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.23     |
| TotalNEpisodes       | 21791      |
| TotalNSamples        | 7.0323e+06 |
| ExplainedVariance    | -0.29279   |
-------------------------------------
[2018-12-22 12:32:10.352202 UTC] Saving snapshot
[2018-12-22 12:32:10.352477 UTC] Starting iteration 1406
[2018-12-22 12:32:10.352617 UTC] Start collecting samples
[2018-12-22 12:32:13.299355 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:13.375077 UTC] Performing policy update
[2018-12-22 12:32:13.375681 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:13.466636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:14.528763 UTC] Performing line search
[2018-12-22 12:32:14.657087 UTC] Updating baseline
[2018-12-22 12:32:16.949366 UTC] Computing logging information
--------------------------------------
| Iteration            | 1406        |
| ExpectedImprovement  | 0.017616    |
| ActualImprovement    | 0.016579    |
| ImprovementRatio     | 0.94112     |
| MeanKL               | 0.007763    |
| Entropy              | -1.4985     |
| Perplexity           | 0.22347     |
| AveragePolicyStd     | 0.19207     |
| AveragePolicyStd[0]  | 0.22636     |
| AveragePolicyStd[1]  | 0.18227     |
| AveragePolicyStd[2]  | 0.15415     |
| AveragePolicyStd[3]  | 0.19694     |
| AveragePolicyStd[4]  | 0.14399     |
| AveragePolicyStd[5]  | 0.2487      |
| AverageReturn        | 1878.3      |
| MinReturn            | 488.25      |
| MaxReturn            | 2005.8      |
| StdReturn            | 245.71      |
| AverageEpisodeLength | 967.04      |
| MinEpisodeLength     | 284         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 118.23      |
| TotalNEpisodes       | 21795       |
| TotalNSamples        | 7.0363e+06  |
| ExplainedVariance    | -6.2345e-06 |
--------------------------------------
[2018-12-22 12:32:17.379645 UTC] Saving snapshot
[2018-12-22 12:32:17.379908 UTC] Starting iteration 1407
[2018-12-22 12:32:17.380024 UTC] Start collecting samples
[2018-12-22 12:32:20.423525 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:20.504638 UTC] Performing policy update
[2018-12-22 12:32:20.505333 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:20.594881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:21.654064 UTC] Performing line search
[2018-12-22 12:32:21.784743 UTC] Updating baseline
[2018-12-22 12:32:23.460083 UTC] Computing logging information
-------------------------------------
| Iteration            | 1407       |
| ExpectedImprovement  | 0.017624   |
| ActualImprovement    | 0.016956   |
| ImprovementRatio     | 0.96214    |
| MeanKL               | 0.0082723  |
| Entropy              | -1.5059    |
| Perplexity           | 0.22182    |
| AveragePolicyStd     | 0.19188    |
| AveragePolicyStd[0]  | 0.22607    |
| AveragePolicyStd[1]  | 0.18217    |
| AveragePolicyStd[2]  | 0.15365    |
| AveragePolicyStd[3]  | 0.19707    |
| AveragePolicyStd[4]  | 0.14351    |
| AveragePolicyStd[5]  | 0.24879    |
| AverageReturn        | 1879.7     |
| MinReturn            | 488.25     |
| MaxReturn            | 2006.1     |
| StdReturn            | 246.17     |
| AverageEpisodeLength | 967.04     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.23     |
| TotalNEpisodes       | 21805      |
| TotalNSamples        | 7.0463e+06 |
| ExplainedVariance    | 0.00020645 |
-------------------------------------
[2018-12-22 12:32:23.894247 UTC] Saving snapshot
[2018-12-22 12:32:23.894524 UTC] Starting iteration 1408
[2018-12-22 12:32:23.894649 UTC] Start collecting samples
[2018-12-22 12:32:26.818978 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:26.894395 UTC] Performing policy update
[2018-12-22 12:32:26.895103 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:26.985463 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:28.059229 UTC] Performing line search
[2018-12-22 12:32:28.186684 UTC] Updating baseline
[2018-12-22 12:32:31.378025 UTC] Computing logging information
-------------------------------------
| Iteration            | 1408       |
| ExpectedImprovement  | 0.018363   |
| ActualImprovement    | 0.016932   |
| ImprovementRatio     | 0.92211    |
| MeanKL               | 0.0080028  |
| Entropy              | -1.5117    |
| Perplexity           | 0.22054    |
| AveragePolicyStd     | 0.19172    |
| AveragePolicyStd[0]  | 0.22611    |
| AveragePolicyStd[1]  | 0.18201    |
| AveragePolicyStd[2]  | 0.15349    |
| AveragePolicyStd[3]  | 0.19682    |
| AveragePolicyStd[4]  | 0.14314    |
| AveragePolicyStd[5]  | 0.24873    |
| AverageReturn        | 1882       |
| MinReturn            | 488.25     |
| MaxReturn            | 2036.1     |
| StdReturn            | 246.65     |
| AverageEpisodeLength | 967.49     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.27     |
| TotalNEpisodes       | 21807      |
| TotalNSamples        | 7.0483e+06 |
| ExplainedVariance    | -0.012263  |
-------------------------------------
[2018-12-22 12:32:31.816545 UTC] Saving snapshot
[2018-12-22 12:32:31.816798 UTC] Starting iteration 1409
[2018-12-22 12:32:31.816916 UTC] Start collecting samples
[2018-12-22 12:32:34.727465 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:34.802344 UTC] Performing policy update
[2018-12-22 12:32:34.803158 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:34.893701 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:35.956902 UTC] Performing line search
[2018-12-22 12:32:36.084874 UTC] Updating baseline
[2018-12-22 12:32:37.316657 UTC] Computing logging information
-------------------------------------
| Iteration            | 1409       |
| ExpectedImprovement  | 0.018995   |
| ActualImprovement    | 0.017456   |
| ImprovementRatio     | 0.91898    |
| MeanKL               | 0.0077847  |
| Entropy              | -1.5128    |
| Perplexity           | 0.2203     |
| AveragePolicyStd     | 0.19164    |
| AveragePolicyStd[0]  | 0.22603    |
| AveragePolicyStd[1]  | 0.18203    |
| AveragePolicyStd[2]  | 0.15362    |
| AveragePolicyStd[3]  | 0.19687    |
| AveragePolicyStd[4]  | 0.14326    |
| AveragePolicyStd[5]  | 0.24805    |
| AverageReturn        | 1881.9     |
| MinReturn            | 488.25     |
| MaxReturn            | 2036.1     |
| StdReturn            | 246.56     |
| AverageEpisodeLength | 967.49     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.27     |
| TotalNEpisodes       | 21809      |
| TotalNSamples        | 7.0503e+06 |
| ExplainedVariance    | -0.019357  |
-------------------------------------
[2018-12-22 12:32:37.742523 UTC] Saving snapshot
[2018-12-22 12:32:37.742786 UTC] Starting iteration 1410
[2018-12-22 12:32:37.742911 UTC] Start collecting samples
[2018-12-22 12:32:40.784783 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:40.865146 UTC] Performing policy update
[2018-12-22 12:32:40.865837 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:40.955032 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:42.016748 UTC] Performing line search
[2018-12-22 12:32:42.145368 UTC] Updating baseline
[2018-12-22 12:32:44.164921 UTC] Computing logging information
-------------------------------------
| Iteration            | 1410       |
| ExpectedImprovement  | 0.018272   |
| ActualImprovement    | 0.017678   |
| ImprovementRatio     | 0.96748    |
| MeanKL               | 0.0085673  |
| Entropy              | -1.5198    |
| Perplexity           | 0.21876    |
| AveragePolicyStd     | 0.19143    |
| AveragePolicyStd[0]  | 0.22551    |
| AveragePolicyStd[1]  | 0.18142    |
| AveragePolicyStd[2]  | 0.1534     |
| AveragePolicyStd[3]  | 0.19701    |
| AveragePolicyStd[4]  | 0.14315    |
| AveragePolicyStd[5]  | 0.24807    |
| AverageReturn        | 1883.7     |
| MinReturn            | 488.25     |
| MaxReturn            | 2036.1     |
| StdReturn            | 247.24     |
| AverageEpisodeLength | 967.11     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.22     |
| TotalNEpisodes       | 21820      |
| TotalNSamples        | 7.0613e+06 |
| ExplainedVariance    | 0.04773    |
-------------------------------------
[2018-12-22 12:32:44.591372 UTC] Saving snapshot
[2018-12-22 12:32:44.599412 UTC] Starting iteration 1411
[2018-12-22 12:32:44.599617 UTC] Start collecting samples
[2018-12-22 12:32:47.536336 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:47.611997 UTC] Performing policy update
[2018-12-22 12:32:47.612739 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:47.703106 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:48.764576 UTC] Performing line search
[2018-12-22 12:32:48.892307 UTC] Updating baseline
[2018-12-22 12:32:50.633803 UTC] Computing logging information
-------------------------------------
| Iteration            | 1411       |
| ExpectedImprovement  | 0.016279   |
| ActualImprovement    | 0.015382   |
| ImprovementRatio     | 0.94489    |
| MeanKL               | 0.0080745  |
| Entropy              | -1.5168    |
| Perplexity           | 0.21941    |
| AveragePolicyStd     | 0.19154    |
| AveragePolicyStd[0]  | 0.22599    |
| AveragePolicyStd[1]  | 0.18139    |
| AveragePolicyStd[2]  | 0.15337    |
| AveragePolicyStd[3]  | 0.1969     |
| AveragePolicyStd[4]  | 0.14322    |
| AveragePolicyStd[5]  | 0.2484     |
| AverageReturn        | 1884       |
| MinReturn            | 488.25     |
| MaxReturn            | 2036.1     |
| StdReturn            | 247.36     |
| AverageEpisodeLength | 967.11     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.22     |
| TotalNEpisodes       | 21823      |
| TotalNSamples        | 7.0643e+06 |
| ExplainedVariance    | -0.033827  |
-------------------------------------
[2018-12-22 12:32:51.065940 UTC] Saving snapshot
[2018-12-22 12:32:51.066185 UTC] Starting iteration 1412
[2018-12-22 12:32:51.066301 UTC] Start collecting samples
[2018-12-22 12:32:53.979397 UTC] Computing input variables for policy optimization
[2018-12-22 12:32:54.052763 UTC] Performing policy update
[2018-12-22 12:32:54.053334 UTC] Computing gradient in Euclidean space
[2018-12-22 12:32:54.142452 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:32:55.192066 UTC] Performing line search
[2018-12-22 12:32:55.319270 UTC] Updating baseline
[2018-12-22 12:32:57.628939 UTC] Computing logging information
-------------------------------------
| Iteration            | 1412       |
| ExpectedImprovement  | 0.018196   |
| ActualImprovement    | 0.015383   |
| ImprovementRatio     | 0.84543    |
| MeanKL               | 0.0082585  |
| Entropy              | -1.5158    |
| Perplexity           | 0.21963    |
| AveragePolicyStd     | 0.1916     |
| AveragePolicyStd[0]  | 0.22672    |
| AveragePolicyStd[1]  | 0.18148    |
| AveragePolicyStd[2]  | 0.15348    |
| AveragePolicyStd[3]  | 0.19657    |
| AveragePolicyStd[4]  | 0.14299    |
| AveragePolicyStd[5]  | 0.24836    |
| AverageReturn        | 1884.1     |
| MinReturn            | 488.25     |
| MaxReturn            | 2036.1     |
| StdReturn            | 247.4      |
| AverageEpisodeLength | 967.11     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.22     |
| TotalNEpisodes       | 21825      |
| TotalNSamples        | 7.0663e+06 |
| ExplainedVariance    | 3.6423e-06 |
-------------------------------------
[2018-12-22 12:32:58.059976 UTC] Saving snapshot
[2018-12-22 12:32:58.060218 UTC] Starting iteration 1413
[2018-12-22 12:32:58.060335 UTC] Start collecting samples
[2018-12-22 12:33:01.064675 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:01.143049 UTC] Performing policy update
[2018-12-22 12:33:01.143650 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:01.233934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:02.307267 UTC] Performing line search
[2018-12-22 12:33:02.435738 UTC] Updating baseline
[2018-12-22 12:33:04.436919 UTC] Computing logging information
-------------------------------------
| Iteration            | 1413       |
| ExpectedImprovement  | 0.019308   |
| ActualImprovement    | 0.016779   |
| ImprovementRatio     | 0.86901    |
| MeanKL               | 0.0074437  |
| Entropy              | -1.514     |
| Perplexity           | 0.22003    |
| AveragePolicyStd     | 0.19165    |
| AveragePolicyStd[0]  | 0.22678    |
| AveragePolicyStd[1]  | 0.18209    |
| AveragePolicyStd[2]  | 0.1533     |
| AveragePolicyStd[3]  | 0.19612    |
| AveragePolicyStd[4]  | 0.14321    |
| AveragePolicyStd[5]  | 0.24838    |
| AverageReturn        | 1902.9     |
| MinReturn            | 488.25     |
| MaxReturn            | 2036.1     |
| StdReturn            | 212.25     |
| AverageEpisodeLength | 975.71     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.89     |
| TotalNEpisodes       | 21834      |
| TotalNSamples        | 7.0753e+06 |
| ExplainedVariance    | 0.029282   |
-------------------------------------
[2018-12-22 12:33:04.869340 UTC] Saving snapshot
[2018-12-22 12:33:04.869607 UTC] Starting iteration 1414
[2018-12-22 12:33:04.869727 UTC] Start collecting samples
[2018-12-22 12:33:07.831935 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:07.909938 UTC] Performing policy update
[2018-12-22 12:33:07.910591 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:08.003106 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:09.070934 UTC] Performing line search
[2018-12-22 12:33:09.200711 UTC] Updating baseline
[2018-12-22 12:33:10.810736 UTC] Computing logging information
-------------------------------------
| Iteration            | 1414       |
| ExpectedImprovement  | 0.01894    |
| ActualImprovement    | 0.017655   |
| ImprovementRatio     | 0.93214    |
| MeanKL               | 0.0073944  |
| Entropy              | -1.518     |
| Perplexity           | 0.21914    |
| AveragePolicyStd     | 0.19155    |
| AveragePolicyStd[0]  | 0.22616    |
| AveragePolicyStd[1]  | 0.18183    |
| AveragePolicyStd[2]  | 0.1529     |
| AveragePolicyStd[3]  | 0.19627    |
| AveragePolicyStd[4]  | 0.14318    |
| AveragePolicyStd[5]  | 0.24896    |
| AverageReturn        | 1897.3     |
| MinReturn            | 488.25     |
| MaxReturn            | 2038.7     |
| StdReturn            | 246.9      |
| AverageEpisodeLength | 971.07     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.74     |
| TotalNEpisodes       | 21840      |
| TotalNSamples        | 7.0806e+06 |
| ExplainedVariance    | 0.06861    |
-------------------------------------
[2018-12-22 12:33:11.247719 UTC] Saving snapshot
[2018-12-22 12:33:11.247975 UTC] Starting iteration 1415
[2018-12-22 12:33:11.248094 UTC] Start collecting samples
[2018-12-22 12:33:14.169817 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:14.244340 UTC] Performing policy update
[2018-12-22 12:33:14.245093 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:14.335933 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:15.395071 UTC] Performing line search
[2018-12-22 12:33:15.523805 UTC] Updating baseline
[2018-12-22 12:33:16.950873 UTC] Computing logging information
-------------------------------------
| Iteration            | 1415       |
| ExpectedImprovement  | 0.019762   |
| ActualImprovement    | 0.018303   |
| ImprovementRatio     | 0.92617    |
| MeanKL               | 0.0074613  |
| Entropy              | -1.5229    |
| Perplexity           | 0.21807    |
| AveragePolicyStd     | 0.19139    |
| AveragePolicyStd[0]  | 0.22605    |
| AveragePolicyStd[1]  | 0.18146    |
| AveragePolicyStd[2]  | 0.15256    |
| AveragePolicyStd[3]  | 0.19594    |
| AveragePolicyStd[4]  | 0.14338    |
| AveragePolicyStd[5]  | 0.24898    |
| AverageReturn        | 1892.6     |
| MinReturn            | 488.25     |
| MaxReturn            | 2038.7     |
| StdReturn            | 252.38     |
| AverageEpisodeLength | 968.35     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.13     |
| TotalNEpisodes       | 21842      |
| TotalNSamples        | 7.0823e+06 |
| ExplainedVariance    | 0.22245    |
-------------------------------------
[2018-12-22 12:33:17.379952 UTC] Saving snapshot
[2018-12-22 12:33:17.380198 UTC] Starting iteration 1416
[2018-12-22 12:33:17.380331 UTC] Start collecting samples
[2018-12-22 12:33:20.329314 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:20.404460 UTC] Performing policy update
[2018-12-22 12:33:20.405187 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:20.497627 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:21.566711 UTC] Performing line search
[2018-12-22 12:33:21.694601 UTC] Updating baseline
[2018-12-22 12:33:22.943580 UTC] Computing logging information
-------------------------------------
| Iteration            | 1416       |
| ExpectedImprovement  | 0.019376   |
| ActualImprovement    | 0.017669   |
| ImprovementRatio     | 0.91188    |
| MeanKL               | 0.0080462  |
| Entropy              | -1.5266    |
| Perplexity           | 0.21728    |
| AveragePolicyStd     | 0.1913     |
| AveragePolicyStd[0]  | 0.22631    |
| AveragePolicyStd[1]  | 0.18133    |
| AveragePolicyStd[2]  | 0.15244    |
| AveragePolicyStd[3]  | 0.1962     |
| AveragePolicyStd[4]  | 0.14295    |
| AveragePolicyStd[5]  | 0.24859    |
| AverageReturn        | 1901.8     |
| MinReturn            | 488.25     |
| MaxReturn            | 2038.7     |
| StdReturn            | 236.14     |
| AverageEpisodeLength | 973.01     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.03     |
| TotalNEpisodes       | 21846      |
| TotalNSamples        | 7.0863e+06 |
| ExplainedVariance    | 0.038624   |
-------------------------------------
[2018-12-22 12:33:23.377677 UTC] Saving snapshot
[2018-12-22 12:33:23.377956 UTC] Starting iteration 1417
[2018-12-22 12:33:23.378088 UTC] Start collecting samples
[2018-12-22 12:33:26.391251 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:26.471178 UTC] Performing policy update
[2018-12-22 12:33:26.471893 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:26.562694 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:27.620381 UTC] Performing line search
[2018-12-22 12:33:27.748163 UTC] Updating baseline
[2018-12-22 12:33:28.971499 UTC] Computing logging information
-------------------------------------
| Iteration            | 1417       |
| ExpectedImprovement  | 0.018088   |
| ActualImprovement    | 0.017075   |
| ImprovementRatio     | 0.94396    |
| MeanKL               | 0.0078065  |
| Entropy              | -1.526     |
| Perplexity           | 0.2174     |
| AveragePolicyStd     | 0.19132    |
| AveragePolicyStd[0]  | 0.22635    |
| AveragePolicyStd[1]  | 0.18121    |
| AveragePolicyStd[2]  | 0.15254    |
| AveragePolicyStd[3]  | 0.19613    |
| AveragePolicyStd[4]  | 0.14297    |
| AveragePolicyStd[5]  | 0.24872    |
| AverageReturn        | 1904.1     |
| MinReturn            | 488.25     |
| MaxReturn            | 2038.7     |
| StdReturn            | 236.86     |
| AverageEpisodeLength | 973.01     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.03     |
| TotalNEpisodes       | 21855      |
| TotalNSamples        | 7.0953e+06 |
| ExplainedVariance    | -0.039305  |
-------------------------------------
[2018-12-22 12:33:29.399531 UTC] Saving snapshot
[2018-12-22 12:33:29.399787 UTC] Starting iteration 1418
[2018-12-22 12:33:29.399907 UTC] Start collecting samples
[2018-12-22 12:33:32.329254 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:32.404983 UTC] Performing policy update
[2018-12-22 12:33:32.405719 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:32.495142 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:33.559957 UTC] Performing line search
[2018-12-22 12:33:33.689359 UTC] Updating baseline
[2018-12-22 12:33:34.946004 UTC] Computing logging information
-------------------------------------
| Iteration            | 1418       |
| ExpectedImprovement  | 0.020246   |
| ActualImprovement    | 0.018422   |
| ImprovementRatio     | 0.90992    |
| MeanKL               | 0.0070091  |
| Entropy              | -1.5299    |
| Perplexity           | 0.21655    |
| AveragePolicyStd     | 0.19125    |
| AveragePolicyStd[0]  | 0.22651    |
| AveragePolicyStd[1]  | 0.18124    |
| AveragePolicyStd[2]  | 0.15187    |
| AveragePolicyStd[3]  | 0.19634    |
| AveragePolicyStd[4]  | 0.14271    |
| AveragePolicyStd[5]  | 0.24881    |
| AverageReturn        | 1896.7     |
| MinReturn            | 488.25     |
| MaxReturn            | 2038.7     |
| StdReturn            | 251.86     |
| AverageEpisodeLength | 968.51     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.57     |
| TotalNEpisodes       | 21858      |
| TotalNSamples        | 7.0979e+06 |
| ExplainedVariance    | 0.20279    |
-------------------------------------
[2018-12-22 12:33:35.379792 UTC] Saving snapshot
[2018-12-22 12:33:35.380035 UTC] Starting iteration 1419
[2018-12-22 12:33:35.380150 UTC] Start collecting samples
[2018-12-22 12:33:38.284865 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:38.359913 UTC] Performing policy update
[2018-12-22 12:33:38.360564 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:38.449534 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:39.512865 UTC] Performing line search
[2018-12-22 12:33:39.642408 UTC] Updating baseline
[2018-12-22 12:33:40.966028 UTC] Computing logging information
-------------------------------------
| Iteration            | 1419       |
| ExpectedImprovement  | 0.017915   |
| ActualImprovement    | 0.01784    |
| ImprovementRatio     | 0.99582    |
| MeanKL               | 0.0078463  |
| Entropy              | -1.5327    |
| Perplexity           | 0.21596    |
| AveragePolicyStd     | 0.19119    |
| AveragePolicyStd[0]  | 0.22645    |
| AveragePolicyStd[1]  | 0.18103    |
| AveragePolicyStd[2]  | 0.15151    |
| AveragePolicyStd[3]  | 0.19638    |
| AveragePolicyStd[4]  | 0.14269    |
| AveragePolicyStd[5]  | 0.24906    |
| AverageReturn        | 1897.2     |
| MinReturn            | 488.25     |
| MaxReturn            | 2038.7     |
| StdReturn            | 251.85     |
| AverageEpisodeLength | 968.51     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.57     |
| TotalNEpisodes       | 21860      |
| TotalNSamples        | 7.0999e+06 |
| ExplainedVariance    | -0.1095    |
-------------------------------------
[2018-12-22 12:33:41.397878 UTC] Saving snapshot
[2018-12-22 12:33:41.398131 UTC] Starting iteration 1420
[2018-12-22 12:33:41.398247 UTC] Start collecting samples
[2018-12-22 12:33:44.427392 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:44.508579 UTC] Performing policy update
[2018-12-22 12:33:44.509277 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:44.598210 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:45.650192 UTC] Performing line search
[2018-12-22 12:33:45.779700 UTC] Updating baseline
[2018-12-22 12:33:47.081628 UTC] Computing logging information
-------------------------------------
| Iteration            | 1420       |
| ExpectedImprovement  | 0.01975    |
| ActualImprovement    | 0.019241   |
| ImprovementRatio     | 0.97424    |
| MeanKL               | 0.0076678  |
| Entropy              | -1.5329    |
| Perplexity           | 0.2159     |
| AveragePolicyStd     | 0.19119    |
| AveragePolicyStd[0]  | 0.22611    |
| AveragePolicyStd[1]  | 0.18128    |
| AveragePolicyStd[2]  | 0.15123    |
| AveragePolicyStd[3]  | 0.1966     |
| AveragePolicyStd[4]  | 0.14266    |
| AveragePolicyStd[5]  | 0.24924    |
| AverageReturn        | 1910.1     |
| MinReturn            | 488.25     |
| MaxReturn            | 2041.4     |
| StdReturn            | 244.96     |
| AverageEpisodeLength | 972.82     |
| MinEpisodeLength     | 284        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 117.41     |
| TotalNEpisodes       | 21871      |
| TotalNSamples        | 7.1109e+06 |
| ExplainedVariance    | 0.018402   |
-------------------------------------
[2018-12-22 12:33:47.507276 UTC] Saving snapshot
[2018-12-22 12:33:47.515433 UTC] Starting iteration 1421
[2018-12-22 12:33:47.515643 UTC] Start collecting samples
[2018-12-22 12:33:50.471812 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:50.549855 UTC] Performing policy update
[2018-12-22 12:33:50.550522 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:50.639149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:51.692238 UTC] Performing line search
[2018-12-22 12:33:51.819574 UTC] Updating baseline
[2018-12-22 12:33:53.064496 UTC] Computing logging information
-------------------------------------
| Iteration            | 1421       |
| ExpectedImprovement  | 0.018973   |
| ActualImprovement    | 0.018475   |
| ImprovementRatio     | 0.97376    |
| MeanKL               | 0.0087799  |
| Entropy              | -1.5384    |
| Perplexity           | 0.21472    |
| AveragePolicyStd     | 0.19104    |
| AveragePolicyStd[0]  | 0.22614    |
| AveragePolicyStd[1]  | 0.18125    |
| AveragePolicyStd[2]  | 0.15072    |
| AveragePolicyStd[3]  | 0.1964     |
| AveragePolicyStd[4]  | 0.14256    |
| AveragePolicyStd[5]  | 0.24917    |
| AverageReturn        | 1902.9     |
| MinReturn            | 571.77     |
| MaxReturn            | 2041.4     |
| StdReturn            | 234.6      |
| AverageEpisodeLength | 969.06     |
| MinEpisodeLength     | 319        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.06     |
| TotalNEpisodes       | 21876      |
| TotalNSamples        | 7.1148e+06 |
| ExplainedVariance    | 0.44147    |
-------------------------------------
[2018-12-22 12:33:53.497029 UTC] Saving snapshot
[2018-12-22 12:33:53.497271 UTC] Starting iteration 1422
[2018-12-22 12:33:53.497390 UTC] Start collecting samples
[2018-12-22 12:33:56.403724 UTC] Computing input variables for policy optimization
[2018-12-22 12:33:56.480152 UTC] Performing policy update
[2018-12-22 12:33:56.481019 UTC] Computing gradient in Euclidean space
[2018-12-22 12:33:56.572359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:33:57.636649 UTC] Performing line search
[2018-12-22 12:33:57.767901 UTC] Updating baseline
[2018-12-22 12:33:59.086183 UTC] Computing logging information
-------------------------------------
| Iteration            | 1422       |
| ExpectedImprovement  | 0.015981   |
| ActualImprovement    | 0.014562   |
| ImprovementRatio     | 0.91118    |
| MeanKL               | 0.0084483  |
| Entropy              | -1.5395    |
| Perplexity           | 0.21449    |
| AveragePolicyStd     | 0.19101    |
| AveragePolicyStd[0]  | 0.22589    |
| AveragePolicyStd[1]  | 0.18157    |
| AveragePolicyStd[2]  | 0.15079    |
| AveragePolicyStd[3]  | 0.19626    |
| AveragePolicyStd[4]  | 0.14228    |
| AveragePolicyStd[5]  | 0.24929    |
| AverageReturn        | 1903.4     |
| MinReturn            | 571.77     |
| MaxReturn            | 2041.4     |
| StdReturn            | 234.72     |
| AverageEpisodeLength | 969.06     |
| MinEpisodeLength     | 319        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.06     |
| TotalNEpisodes       | 21877      |
| TotalNSamples        | 7.1158e+06 |
| ExplainedVariance    | -0.017316  |
-------------------------------------
[2018-12-22 12:33:59.527257 UTC] Saving snapshot
[2018-12-22 12:33:59.527539 UTC] Starting iteration 1423
[2018-12-22 12:33:59.527677 UTC] Start collecting samples
[2018-12-22 12:34:02.593819 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:02.674448 UTC] Performing policy update
[2018-12-22 12:34:02.675148 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:02.765073 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:03.829225 UTC] Performing line search
[2018-12-22 12:34:03.962370 UTC] Updating baseline
[2018-12-22 12:34:05.387596 UTC] Computing logging information
-------------------------------------
| Iteration            | 1423       |
| ExpectedImprovement  | 0.017891   |
| ActualImprovement    | 0.017106   |
| ImprovementRatio     | 0.9561     |
| MeanKL               | 0.0084457  |
| Entropy              | -1.5447    |
| Perplexity           | 0.21337    |
| AveragePolicyStd     | 0.19082    |
| AveragePolicyStd[0]  | 0.22472    |
| AveragePolicyStd[1]  | 0.18179    |
| AveragePolicyStd[2]  | 0.15093    |
| AveragePolicyStd[3]  | 0.19568    |
| AveragePolicyStd[4]  | 0.14221    |
| AveragePolicyStd[5]  | 0.24961    |
| AverageReturn        | 1905.9     |
| MinReturn            | 571.77     |
| MaxReturn            | 2041.4     |
| StdReturn            | 248.38     |
| AverageEpisodeLength | 968.66     |
| MinEpisodeLength     | 319        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.02     |
| TotalNEpisodes       | 21888      |
| TotalNSamples        | 7.1262e+06 |
| ExplainedVariance    | 0.049935   |
-------------------------------------
[2018-12-22 12:34:05.827545 UTC] Saving snapshot
[2018-12-22 12:34:05.827802 UTC] Starting iteration 1424
[2018-12-22 12:34:05.827923 UTC] Start collecting samples
[2018-12-22 12:34:08.768091 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:08.845778 UTC] Performing policy update
[2018-12-22 12:34:08.846557 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:08.937262 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:09.994228 UTC] Performing line search
[2018-12-22 12:34:10.122742 UTC] Updating baseline
[2018-12-22 12:34:11.344777 UTC] Computing logging information
-------------------------------------
| Iteration            | 1424       |
| ExpectedImprovement  | 0.018395   |
| ActualImprovement    | 0.01724    |
| ImprovementRatio     | 0.9372     |
| MeanKL               | 0.0079335  |
| Entropy              | -1.5516    |
| Perplexity           | 0.21192    |
| AveragePolicyStd     | 0.19056    |
| AveragePolicyStd[0]  | 0.2244     |
| AveragePolicyStd[1]  | 0.1816     |
| AveragePolicyStd[2]  | 0.15083    |
| AveragePolicyStd[3]  | 0.19596    |
| AveragePolicyStd[4]  | 0.14214    |
| AveragePolicyStd[5]  | 0.24845    |
| AverageReturn        | 1891.3     |
| MinReturn            | 471.65     |
| MaxReturn            | 2041.4     |
| StdReturn            | 286.36     |
| AverageEpisodeLength | 961.3      |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.08     |
| TotalNEpisodes       | 21892      |
| TotalNSamples        | 7.1294e+06 |
| ExplainedVariance    | 0.13972    |
-------------------------------------
[2018-12-22 12:34:11.782963 UTC] Saving snapshot
[2018-12-22 12:34:11.784828 UTC] Starting iteration 1425
[2018-12-22 12:34:11.784950 UTC] Start collecting samples
[2018-12-22 12:34:14.692225 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:14.768019 UTC] Performing policy update
[2018-12-22 12:34:14.768669 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:14.858640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:15.914300 UTC] Performing line search
[2018-12-22 12:34:16.044985 UTC] Updating baseline
[2018-12-22 12:34:17.424516 UTC] Computing logging information
-------------------------------------
| Iteration            | 1425       |
| ExpectedImprovement  | 0.01703    |
| ActualImprovement    | 0.016083   |
| ImprovementRatio     | 0.94442    |
| MeanKL               | 0.0077214  |
| Entropy              | -1.5524    |
| Perplexity           | 0.21175    |
| AveragePolicyStd     | 0.19053    |
| AveragePolicyStd[0]  | 0.2247     |
| AveragePolicyStd[1]  | 0.1815     |
| AveragePolicyStd[2]  | 0.15087    |
| AveragePolicyStd[3]  | 0.19596    |
| AveragePolicyStd[4]  | 0.14209    |
| AveragePolicyStd[5]  | 0.24809    |
| AverageReturn        | 1892.7     |
| MinReturn            | 471.65     |
| MaxReturn            | 2043.9     |
| StdReturn            | 286.78     |
| AverageEpisodeLength | 961.3      |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.08     |
| TotalNEpisodes       | 21894      |
| TotalNSamples        | 7.1314e+06 |
| ExplainedVariance    | 0.18632    |
-------------------------------------
[2018-12-22 12:34:17.855904 UTC] Saving snapshot
[2018-12-22 12:34:17.856156 UTC] Starting iteration 1426
[2018-12-22 12:34:17.856273 UTC] Start collecting samples
[2018-12-22 12:34:20.863873 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:20.941803 UTC] Performing policy update
[2018-12-22 12:34:20.942535 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:21.032320 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:22.101194 UTC] Performing line search
[2018-12-22 12:34:22.228054 UTC] Updating baseline
[2018-12-22 12:34:23.566250 UTC] Computing logging information
-------------------------------------
| Iteration            | 1426       |
| ExpectedImprovement  | 0.017266   |
| ActualImprovement    | 0.016158   |
| ImprovementRatio     | 0.93582    |
| MeanKL               | 0.0074823  |
| Entropy              | -1.5538    |
| Perplexity           | 0.21143    |
| AveragePolicyStd     | 0.19055    |
| AveragePolicyStd[0]  | 0.22491    |
| AveragePolicyStd[1]  | 0.18143    |
| AveragePolicyStd[2]  | 0.15014    |
| AveragePolicyStd[3]  | 0.19598    |
| AveragePolicyStd[4]  | 0.14211    |
| AveragePolicyStd[5]  | 0.24872    |
| AverageReturn        | 1886.8     |
| MinReturn            | 471.65     |
| MaxReturn            | 2064.4     |
| StdReturn            | 294.26     |
| AverageEpisodeLength | 957.66     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.76     |
| TotalNEpisodes       | 21902      |
| TotalNSamples        | 7.1391e+06 |
| ExplainedVariance    | 0.091763   |
-------------------------------------
[2018-12-22 12:34:24.047874 UTC] Saving snapshot
[2018-12-22 12:34:24.048119 UTC] Starting iteration 1427
[2018-12-22 12:34:24.048235 UTC] Start collecting samples
[2018-12-22 12:34:27.260262 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:27.343669 UTC] Performing policy update
[2018-12-22 12:34:27.344286 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:27.437821 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:28.546765 UTC] Performing line search
[2018-12-22 12:34:28.680821 UTC] Updating baseline
[2018-12-22 12:34:30.178044 UTC] Computing logging information
-------------------------------------
| Iteration            | 1427       |
| ExpectedImprovement  | 0.019091   |
| ActualImprovement    | 0.01809    |
| ImprovementRatio     | 0.94753    |
| MeanKL               | 0.0078463  |
| Entropy              | -1.5537    |
| Perplexity           | 0.21147    |
| AveragePolicyStd     | 0.19054    |
| AveragePolicyStd[0]  | 0.22535    |
| AveragePolicyStd[1]  | 0.18144    |
| AveragePolicyStd[2]  | 0.14995    |
| AveragePolicyStd[3]  | 0.196      |
| AveragePolicyStd[4]  | 0.14229    |
| AveragePolicyStd[5]  | 0.24823    |
| AverageReturn        | 1887.7     |
| MinReturn            | 471.65     |
| MaxReturn            | 2064.4     |
| StdReturn            | 294.44     |
| AverageEpisodeLength | 957.66     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.76     |
| TotalNEpisodes       | 21908      |
| TotalNSamples        | 7.1451e+06 |
| ExplainedVariance    | -0.0039252 |
-------------------------------------
[2018-12-22 12:34:30.611238 UTC] Saving snapshot
[2018-12-22 12:34:30.611475 UTC] Starting iteration 1428
[2018-12-22 12:34:30.611611 UTC] Start collecting samples
[2018-12-22 12:34:33.520180 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:33.596786 UTC] Performing policy update
[2018-12-22 12:34:33.597496 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:33.686878 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:34.751137 UTC] Performing line search
[2018-12-22 12:34:34.881807 UTC] Updating baseline
[2018-12-22 12:34:36.191169 UTC] Computing logging information
-------------------------------------
| Iteration            | 1428       |
| ExpectedImprovement  | 0.018807   |
| ActualImprovement    | 0.017418   |
| ImprovementRatio     | 0.92613    |
| MeanKL               | 0.0078661  |
| Entropy              | -1.5542    |
| Perplexity           | 0.21137    |
| AveragePolicyStd     | 0.19051    |
| AveragePolicyStd[0]  | 0.22543    |
| AveragePolicyStd[1]  | 0.18149    |
| AveragePolicyStd[2]  | 0.14979    |
| AveragePolicyStd[3]  | 0.1964     |
| AveragePolicyStd[4]  | 0.14232    |
| AveragePolicyStd[5]  | 0.24765    |
| AverageReturn        | 1888.2     |
| MinReturn            | 471.65     |
| MaxReturn            | 2064.4     |
| StdReturn            | 294.6      |
| AverageEpisodeLength | 957.66     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.76     |
| TotalNEpisodes       | 21910      |
| TotalNSamples        | 7.1471e+06 |
| ExplainedVariance    | -0.037415  |
-------------------------------------
[2018-12-22 12:34:36.621776 UTC] Saving snapshot
[2018-12-22 12:34:36.622038 UTC] Starting iteration 1429
[2018-12-22 12:34:36.622157 UTC] Start collecting samples
[2018-12-22 12:34:39.602846 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:39.679143 UTC] Performing policy update
[2018-12-22 12:34:39.679835 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:39.773030 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:40.829297 UTC] Performing line search
[2018-12-22 12:34:40.956833 UTC] Updating baseline
[2018-12-22 12:34:43.077046 UTC] Computing logging information
-------------------------------------
| Iteration            | 1429       |
| ExpectedImprovement  | 0.016656   |
| ActualImprovement    | 0.015863   |
| ImprovementRatio     | 0.95239    |
| MeanKL               | 0.0082231  |
| Entropy              | -1.5549    |
| Perplexity           | 0.2112     |
| AveragePolicyStd     | 0.19052    |
| AveragePolicyStd[0]  | 0.22553    |
| AveragePolicyStd[1]  | 0.18125    |
| AveragePolicyStd[2]  | 0.1498     |
| AveragePolicyStd[3]  | 0.19618    |
| AveragePolicyStd[4]  | 0.14215    |
| AveragePolicyStd[5]  | 0.24822    |
| AverageReturn        | 1892.2     |
| MinReturn            | 471.65     |
| MaxReturn            | 2064.4     |
| StdReturn            | 295.57     |
| AverageEpisodeLength | 958.04     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.82     |
| TotalNEpisodes       | 21917      |
| TotalNSamples        | 7.1541e+06 |
| ExplainedVariance    | -0.0079488 |
-------------------------------------
[2018-12-22 12:34:43.510089 UTC] Saving snapshot
[2018-12-22 12:34:43.510453 UTC] Starting iteration 1430
[2018-12-22 12:34:43.510617 UTC] Start collecting samples
[2018-12-22 12:34:46.520370 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:46.599548 UTC] Performing policy update
[2018-12-22 12:34:46.600350 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:46.690803 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:47.744064 UTC] Performing line search
[2018-12-22 12:34:47.875423 UTC] Updating baseline
[2018-12-22 12:34:50.554936 UTC] Computing logging information
-------------------------------------
| Iteration            | 1430       |
| ExpectedImprovement  | 0.019057   |
| ActualImprovement    | 0.017721   |
| ImprovementRatio     | 0.92987    |
| MeanKL               | 0.0076925  |
| Entropy              | -1.5558    |
| Perplexity           | 0.21102    |
| AveragePolicyStd     | 0.19051    |
| AveragePolicyStd[0]  | 0.2258     |
| AveragePolicyStd[1]  | 0.1816     |
| AveragePolicyStd[2]  | 0.14956    |
| AveragePolicyStd[3]  | 0.19583    |
| AveragePolicyStd[4]  | 0.14214    |
| AveragePolicyStd[5]  | 0.24811    |
| AverageReturn        | 1876.1     |
| MinReturn            | 152.63     |
| MaxReturn            | 2064.4     |
| StdReturn            | 343        |
| AverageEpisodeLength | 949.14     |
| MinEpisodeLength     | 110        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.95     |
| TotalNEpisodes       | 21925      |
| TotalNSamples        | 7.1612e+06 |
| ExplainedVariance    | 0.057847   |
-------------------------------------
[2018-12-22 12:34:50.989942 UTC] Saving snapshot
[2018-12-22 12:34:50.998462 UTC] Starting iteration 1431
[2018-12-22 12:34:50.998677 UTC] Start collecting samples
[2018-12-22 12:34:54.042755 UTC] Computing input variables for policy optimization
[2018-12-22 12:34:54.122256 UTC] Performing policy update
[2018-12-22 12:34:54.123156 UTC] Computing gradient in Euclidean space
[2018-12-22 12:34:54.214563 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:34:55.295064 UTC] Performing line search
[2018-12-22 12:34:55.425279 UTC] Updating baseline
[2018-12-22 12:34:56.772738 UTC] Computing logging information
-------------------------------------
| Iteration            | 1431       |
| ExpectedImprovement  | 0.018642   |
| ActualImprovement    | 0.017979   |
| ImprovementRatio     | 0.96443    |
| MeanKL               | 0.0080971  |
| Entropy              | -1.5563    |
| Perplexity           | 0.21091    |
| AveragePolicyStd     | 0.19052    |
| AveragePolicyStd[0]  | 0.22558    |
| AveragePolicyStd[1]  | 0.18151    |
| AveragePolicyStd[2]  | 0.14944    |
| AveragePolicyStd[3]  | 0.19571    |
| AveragePolicyStd[4]  | 0.14211    |
| AveragePolicyStd[5]  | 0.24875    |
| AverageReturn        | 1848.4     |
| MinReturn            | 152.63     |
| MaxReturn            | 2064.4     |
| StdReturn            | 378.27     |
| AverageEpisodeLength | 934.52     |
| MinEpisodeLength     | 110        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.54     |
| TotalNEpisodes       | 21930      |
| TotalNSamples        | 7.1647e+06 |
| ExplainedVariance    | 0.34102    |
-------------------------------------
[2018-12-22 12:34:57.218968 UTC] Saving snapshot
[2018-12-22 12:34:57.219228 UTC] Starting iteration 1432
[2018-12-22 12:34:57.219360 UTC] Start collecting samples
[2018-12-22 12:35:00.270947 UTC] Computing input variables for policy optimization
[2018-12-22 12:35:00.355216 UTC] Performing policy update
[2018-12-22 12:35:00.356000 UTC] Computing gradient in Euclidean space
[2018-12-22 12:35:00.451891 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:35:01.606223 UTC] Performing line search
[2018-12-22 12:35:01.746349 UTC] Updating baseline
[2018-12-22 12:35:03.190762 UTC] Computing logging information
-------------------------------------
| Iteration            | 1432       |
| ExpectedImprovement  | 0.017745   |
| ActualImprovement    | 0.016965   |
| ImprovementRatio     | 0.95606    |
| MeanKL               | 0.0076951  |
| Entropy              | -1.5569    |
| Perplexity           | 0.21079    |
| AveragePolicyStd     | 0.19044    |
| AveragePolicyStd[0]  | 0.22517    |
| AveragePolicyStd[1]  | 0.18154    |
| AveragePolicyStd[2]  | 0.14998    |
| AveragePolicyStd[3]  | 0.19545    |
| AveragePolicyStd[4]  | 0.14217    |
| AveragePolicyStd[5]  | 0.24837    |
| AverageReturn        | 1841.8     |
| MinReturn            | 152.63     |
| MaxReturn            | 2064.4     |
| StdReturn            | 382.13     |
| AverageEpisodeLength | 931.13     |
| MinEpisodeLength     | 110        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.44     |
| TotalNEpisodes       | 21935      |
| TotalNSamples        | 7.1694e+06 |
| ExplainedVariance    | 0.12978    |
-------------------------------------
[2018-12-22 12:35:03.663591 UTC] Saving snapshot
[2018-12-22 12:35:03.663875 UTC] Starting iteration 1433
[2018-12-22 12:35:03.664008 UTC] Start collecting samples
[2018-12-22 12:35:06.962653 UTC] Computing input variables for policy optimization
[2018-12-22 12:35:07.047300 UTC] Performing policy update
[2018-12-22 12:35:07.048024 UTC] Computing gradient in Euclidean space
[2018-12-22 12:35:07.144431 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:35:08.277094 UTC] Performing line search
[2018-12-22 12:35:08.413872 UTC] Updating baseline
[2018-12-22 12:35:10.122766 UTC] Computing logging information
-------------------------------------
| Iteration            | 1433       |
| ExpectedImprovement  | 0.016087   |
| ActualImprovement    | 0.015109   |
| ImprovementRatio     | 0.93919    |
| MeanKL               | 0.0087969  |
| Entropy              | -1.5563    |
| Perplexity           | 0.21092    |
| AveragePolicyStd     | 0.19048    |
| AveragePolicyStd[0]  | 0.22543    |
| AveragePolicyStd[1]  | 0.18145    |
| AveragePolicyStd[2]  | 0.15006    |
| AveragePolicyStd[3]  | 0.19562    |
| AveragePolicyStd[4]  | 0.14198    |
| AveragePolicyStd[5]  | 0.24833    |
| AverageReturn        | 1849.2     |
| MinReturn            | 152.63     |
| MaxReturn            | 2064.4     |
| StdReturn            | 363.85     |
| AverageEpisodeLength | 934.99     |
| MinEpisodeLength     | 110        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.35     |
| TotalNEpisodes       | 21940      |
| TotalNSamples        | 7.1741e+06 |
| ExplainedVariance    | 0.14502    |
-------------------------------------
[2018-12-22 12:35:10.598569 UTC] Saving snapshot
[2018-12-22 12:35:10.598831 UTC] Starting iteration 1434
[2018-12-22 12:35:10.598950 UTC] Start collecting samples
[2018-12-22 12:35:13.601416 UTC] Computing input variables for policy optimization
[2018-12-22 12:35:13.680557 UTC] Performing policy update
[2018-12-22 12:35:13.681394 UTC] Computing gradient in Euclidean space
[2018-12-22 12:35:13.772276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:35:14.846336 UTC] Performing line search
[2018-12-22 12:35:14.974713 UTC] Updating baseline
[2018-12-22 12:35:16.334156 UTC] Computing logging information
------------------------------------
| Iteration            | 1434      |
| ExpectedImprovement  | 0.018007  |
| ActualImprovement    | 0.017049  |
| ImprovementRatio     | 0.94681   |
| MeanKL               | 0.0075285 |
| Entropy              | -1.5508   |
| Perplexity           | 0.21208   |
| AveragePolicyStd     | 0.19067   |
| AveragePolicyStd[0]  | 0.22571   |
| AveragePolicyStd[1]  | 0.18176   |
| AveragePolicyStd[2]  | 0.15013   |
| AveragePolicyStd[3]  | 0.19616   |
| AveragePolicyStd[4]  | 0.14182   |
| AveragePolicyStd[5]  | 0.24842   |
| AverageReturn        | 1854.2    |
| MinReturn            | 152.63    |
| MaxReturn            | 2064.4    |
| StdReturn            | 360.89    |
| AverageEpisodeLength | 937.21    |
| MinEpisodeLength     | 110       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 173.11    |
| TotalNEpisodes       | 21943     |
| TotalNSamples        | 7.177e+06 |
| ExplainedVariance    | 0.41909   |
------------------------------------
[2018-12-22 12:35:16.771879 UTC] Saving snapshot
[2018-12-22 12:35:16.772130 UTC] Starting iteration 1435
[2018-12-22 12:35:16.772246 UTC] Start collecting samples
[2018-12-22 12:35:19.818124 UTC] Computing input variables for policy optimization
[2018-12-22 12:35:19.897181 UTC] Performing policy update
[2018-12-22 12:35:19.897818 UTC] Computing gradient in Euclidean space
[2018-12-22 12:35:19.993499 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-12-22 12:35:21.077450 UTC] Performing line search
[2018-12-22 12:35:21.206890 UTC] Updating baseline
